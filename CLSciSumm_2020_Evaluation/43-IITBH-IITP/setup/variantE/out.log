parsing: input/ref/Task1/W99-0613_swastika.csv
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
<S sid="159" ssid="26">To prevent this we &amp;quot;smooth&amp;quot; the confidence by adding a small value, e, to both W+ and W_, giving at = Plugging the value of at from Equ.</S>
original cit marker offset is 0
new cit marker offset is 0



['159']
159
['159']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S>
original cit marker offset is 0
new cit marker offset is 0



['137']
137
['137']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['method_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
 <S sid="250" ssid="1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S>
original cit marker offset is 0
new cit marker offset is 0



['250']
250
['250']
parsed_discourse_facet ['result_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
    <S sid="36" ssid="30">Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.</S>
original cit marker offset is 0
new cit marker offset is 0



['36']
36
['36']
parsed_discourse_facet ['result_citation']
<S sid="29" ssid="23">Unfortunately, Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective function.</S>
original cit marker offset is 0
new cit marker offset is 0



['29']
29
['29']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
    <S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['method_citation']
    <S sid="95" ssid="28">(Specifically, the limit n starts at 5 and increases by 5 at each iteration.)</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['method_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'228'", "'0'", "'75'", "'77'"]
'142'
'228'
'0'
'75'
'77'
['142', '228', '0', '75', '77']
parsed_discourse_facet ['results_citation']
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="102" ssid = "35">(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'77'", "'102'", "'52'", "'75'"]
'142'
'77'
'102'
'52'
'75'
['142', '77', '102', '52', '75']
parsed_discourse_facet ['method_citation']
<S sid ="225" ssid = "4">We again assume a training set of n examples {x1 . xri} where the first m examples have labels {y1 ... yin}  and the last (n — m) examples are unlabeled.</S><S sid ="203" ssid = "70">Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).</S><S sid ="140" ssid = "7">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S><S sid ="79" ssid = "12">2 We now introduce a new algorithm for learning from unlabeled examples  which we will call DLCoTrain (DL stands for decision list  the term Cotrain is taken from (Blum and Mitchell 98)).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'225'", "'203'", "'140'", "'79'", "'31'"]
'225'
'203'
'140'
'79'
'31'
['225', '203', '140', '79', '31']
parsed_discourse_facet ['method_citation']
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="102" ssid = "35">(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'102'", "'77'", "'52'", "'228'"]
'142'
'102'
'77'
'52'
'228'
['142', '102', '77', '52', '228']
parsed_discourse_facet ['method_citation']
<S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="207" ssid = "74">Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S><S sid ="166" ssid = "33">The first m pairs have labels yi  whereas for i = m + 1    n the pairs are unlabeled.</S><S sid ="84" ssid = "17">For each label (Per s on  organization and Location)  take the n contextual rules with the highest value of Count' (x) whose unsmoothed3 strength is above some threshold pmin.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'", "'207'", "'248'", "'166'", "'84'"]
'236'
'207'
'248'
'166'
'84'
['236', '207', '248', '166', '84']
parsed_discourse_facet ['method_citation']
<S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'228'", "'142'", "'75'", "'77'", "'0'"]
'228'
'142'
'75'
'77'
'0'
['228', '142', '75', '77', '0']
parsed_discourse_facet ['method_citation']
<S sid ="207" ssid = "74">Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S><S sid ="86" ssid = "19">Thus at each iteration the method induces at most n x k rules  where k is the number of possible labels (k = 3 in the experiments in this paper). step 3.</S><S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="10" ssid = "4">The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'248'", "'86'", "'204'", "'10'"]
'207'
'248'
'86'
'204'
'10'
['207', '248', '86', '204', '10']
parsed_discourse_facet ['method_citation']
<S sid ="202" ssid = "69">The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels  and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.</S><S sid ="124" ssid = "57">In particular  it may not be possible to learn functions fi (x f2(x2 t) for i = m + 1...n: either because there is some noise in the data  or because it is just not realistic to expect to learn perfect classifiers given the features used for representation.</S><S sid ="121" ssid = "54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).</S><S sid ="50" ssid = "4">It is a sequence of proper nouns within an NP; its last word Cooper is the head of the NP; and the NP has an appositive modifier (a vice president at S.&P.) whose head is a singular noun (president).</S><S sid ="25" ssid = "19">The unlabeled data gives many such &quot;hints&quot; that two features should predict the same label  and these hints turn out to be surprisingly useful when building a classifier.</S>
original cit marker offset is 0
new cit marker offset is 0



["'202'", "'124'", "'121'", "'50'", "'25'"]
'202'
'124'
'121'
'50'
'25'
['202', '124', '121', '50', '25']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="12" ssid = "6">The approach uses both spelling and contextual rules.</S><S sid ="150" ssid = "17">Pseudo-code describing the generalized boosting algorithm of Schapire and Singer is given in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'52'", "'75'", "'12'", "'150'"]
'157'
'52'
'75'
'12'
'150'
['157', '52', '75', '12', '150']
parsed_discourse_facet ['method_citation']
<S sid ="63" ssid = "17">In this case nonalpha is the string formed by removing all upper/lower case letters from the spelling (e.g.  for Thomas E. Petry nonalpha= .</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="47" ssid = "1">971 746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'52'", "'157'", "'77'", "'47'"]
'63'
'52'
'157'
'77'
'47'
['63', '52', '157', '77', '47']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="239" ssid = "6">Of these cases  38 were temporal expressions (either a day of the week or month of the year).</S><S sid ="93" ssid = "26">To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.</S><S sid ="249" ssid = "16">The test accuracy more or less asymptotes.</S><S sid ="197" ssid = "64">Note that in our formalism a weakhypothesis can abstain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'239'", "'93'", "'249'", "'197'"]
'204'
'239'
'93'
'249'
'197'
['204', '239', '93', '249', '197']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S><S sid ="93" ssid = "26">To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'75'", "'228'", "'0'", "'93'"]
'204'
'75'
'228'
'0'
'93'
['204', '75', '228', '0', '93']
parsed_discourse_facet ['method_citation']
<S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="193" ssid = "60">In Input: {(x1 i  Initialize: Vi  j : e(xi) = 0.</S><S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'", "'157'", "'77'", "'193'", "'142'"]
'52'
'157'
'77'
'193'
'142'
['52', '157', '77', '193', '142']
parsed_discourse_facet ['results_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/E03-1005_swastika.csv
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
    <S sid="41" ssid="38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



['41']
41
['41']
parsed_discourse_facet ['aim_citation']
<S sid="80" ssid="32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S>
original cit marker offset is 0
new cit marker offset is 0



['80']
80
['80']
parsed_discourse_facet ['result_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



['143']
143
['143']
parsed_discourse_facet ['method_citation']
<S sid="146" ssid="11">This paper also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic depen dencies (as e.g. in Collins 1999 and Charniak 2000).</S>
original cit marker offset is 0
new cit marker offset is 0



['146']
146
['146']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
<S sid="100" ssid="3">In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['result_citation']
    <S sid="30" ssid="27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S>
original cit marker offset is 0
new cit marker offset is 0



['30']
30
['30']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
    <S sid="30" ssid="27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S>
original cit marker offset is 0
new cit marker offset is 0



['30']
30
['30']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="71" ssid = "23">For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'64'", "'99'", "'71'", "'39'"]
'7'
'64'
'99'
'71'
'39'
['7', '64', '99', '71', '39']
parsed_discourse_facet ['results_citation']
<S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="27" ssid = "24">Goodman (1996  1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set  thus converting the exponential number of subtrees to a compact grammar.</S><S sid ="88" ssid = "40">In this paper  we will test a simple extension of Goodman's compact PCFG-reduction of DOP which has the same property as the normalization proposed in Bod (2001) in that it assigns roughly equal weight to each node in the training data.</S><S sid ="69" ssid = "21">Thus  rather than using the large  explicit DOP1 model  one can also use this small PCFG that generates isomorphic derivations  with identical probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'130'", "'27'", "'88'", "'69'"]
'140'
'130'
'27'
'88'
'69'
['140', '130', '27', '88', '69']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "2">The probability of a parse tree is computed from the occurrencefrequencies of the subtrees in the treebank.</S><S sid ="76" ssid = "28">Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.</S><S sid ="46" ssid = "43">Most previous notions of best parse tree in DOP1 were based on a probabilistic metric  with Bod (2000b) as a notable exception  who used a simplicity metric based on the shortest derivation.</S><S sid ="91" ssid = "43">It easy to see that this is equivalent to reducing the probability of a tree by a factor of four for each pair of nonterminals it contains  resulting in the PCFG reduction in figure 4.</S><S sid ="52" ssid = "4">The probability of a parse tree T is the sum of the probabilities of its distinct derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'76'", "'46'", "'91'", "'52'"]
'50'
'76'
'46'
'91'
'52'
['50', '76', '46', '91', '52']
parsed_discourse_facet ['method_citation']
<S sid ="74" ssid = "26">Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.</S><S sid ="124" ssid = "5">We employed the same unknown (category) word model as in Bod (2001)  based on statistics on word-endings  hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 87).</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="96" ssid = "48">However  as mentioned above  efficient parsing does not necessarily mean efficient disambiguation: the exact computation of the most probable parse remains exponential.</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'124'", "'40'", "'13'", "'96'"]
'74'
'124'
'40'
'13'
'96'
['74', '124', '40', '13', '96']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">An Efficient Implementation of a New DOP Model</S><S sid ="120" ssid = "1">For our experiments we used the standard division of the WSJ (Marcus et al. 1993)  with sections 2 through 21 for training (approx.</S><S sid ="34" ssid = "31">But even with cross-validation  ML-DOP is outperformed by the much simpler DOP1 model on both the ATIS and OVIS treebanks (Bod 2000b).</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'120'", "'34'", "'15'", "'40'"]
'0'
'120'
'34'
'15'
'40'
['0', '120', '34', '15', '40']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="3" ssid = "3">Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models  and an average processing time of 3.6 seconds per WSJ sentence.</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'3'", "'37'", "'39'", "'85'"]
'7'
'3'
'37'
'39'
'85'
['7', '3', '37', '39', '85']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'64'", "'136'", "'99'", "'140'"]
'7'
'64'
'136'
'99'
'140'
['7', '64', '136', '99', '140']
parsed_discourse_facet ['method_citation']
<S sid ="145" ssid = "10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S><S sid ="46" ssid = "43">Most previous notions of best parse tree in DOP1 were based on a probabilistic metric  with Bod (2000b) as a notable exception  who used a simplicity metric based on the shortest derivation.</S><S sid ="124" ssid = "5">We employed the same unknown (category) word model as in Bod (2001)  based on statistics on word-endings  hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 87).</S><S sid ="105" ssid = "8">The derivation with the smallest sum  or highest rank  is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP  its results are still rather impressive for such a simple model.</S><S sid ="41" ssid = "38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'", "'46'", "'124'", "'105'", "'41'"]
'145'
'46'
'124'
'105'
'41'
['145', '46', '124', '105', '41']
parsed_discourse_facet ['method_citation']
<S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="42" ssid = "39">We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'64'", "'99'", "'42'", "'136'"]
'140'
'64'
'99'
'42'
'136'
['140', '64', '99', '42', '136']
parsed_discourse_facet ['method_citation']
<S sid ="142" ssid = "7">Compared to the reranking technique in Collins (2000)  who obtained an LP of 89.9% and an LR of 89.6%  our results show a 9% relative error rate reduction.</S><S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="141" ssid = "6">This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S><S sid ="59" ssid = "11">A new nonterminal is created for each node in the training data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'7'", "'141'", "'136'", "'59'"]
'142'
'7'
'141'
'136'
'59'
['142', '7', '141', '136', '59']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="116" ssid = "19">Then the shortest derivation is equal to the most probable derivation and can be computed by standard Viterbi optimization  which can be seen as follows: if each subtree has a probability p then the probability of a derivation involving n subtrees is equal to pn  and since 0<p<1  the derivation with the fewest subtrees has the greatest probability.</S><S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'116'", "'85'", "'37'", "'39'"]
'7'
'116'
'85'
'37'
'39'
['7', '116', '85', '37', '39']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="103" ssid = "6">That is  all subtrees of each root label are assigned a rank according to their frequency in the treebank: the most frequent subtree (or subtrees) of each root label gets rank 1  the second most frequent subtree gets rank 2  etc.</S><S sid ="134" ssid = "15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'37'", "'99'", "'103'", "'134'"]
'7'
'37'
'99'
'103'
'134'
['7', '37', '99', '103', '134']
parsed_discourse_facet ['method_citation']
<S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S><S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="88" ssid = "40">In this paper  we will test a simple extension of Goodman's compact PCFG-reduction of DOP which has the same property as the normalization proposed in Bod (2001) in that it assigns roughly equal weight to each node in the training data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'85'", "'7'", "'39'", "'88'"]
'37'
'85'
'7'
'39'
'88'
['37', '85', '7', '39', '88']
parsed_discourse_facet ['results_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'37'", "'140'", "'39'", "'64'"]
'7'
'37'
'140'
'39'
'64'
['7', '37', '140', '39', '64']
parsed_discourse_facet ['method_citation']
<S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S><S sid ="42" ssid = "39">We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="141" ssid = "6">This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'99'", "'136'", "'42'", "'140'", "'141'"]
'99'
'136'
'42'
'140'
'141'
['99', '136', '42', '140', '141']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



E03-1005
W04-0305
0
result_citation
['method_citation']
IGNORE THIS: Key error 5
parsing: input/ref/Task1/A97-1014_swastika.csv
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



['151']
151
['151']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
<S sid="47" ssid="37">Argument structure can be represented in terms of unordered trees (with crossing branches).</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



['167']
167
['167']
parsed_discourse_facet ['method_citation']
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



['160']
160
['160']
parsed_discourse_facet ['method_citation']
<S sid="127" ssid="8">As the need for certain functionalities becomes obvious with growing annotation experience, we have decided to implement the tool in two stages.</S>
original cit marker offset is 0
new cit marker offset is 0



['127']
127
['127']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



['39']
39
['39']
parsed_discourse_facet ['method_citation']
<S sid="72" ssid="17">In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.</S>
original cit marker offset is 0
new cit marker offset is 0



['72']
72
['72']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/A97-1014.csv
<S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="81" ssid = "26">During the second annotation stage  the annotation is enriched with information about thematic roles  quantifier scope and anaphoric reference.</S><S sid ="79" ssid = "24">PM stands for morphological particle  a label for German infinitival Z7t and superlative am.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'81'", "'79'", "'137'", "'162'"]
'4'
'81'
'79'
'137'
'162'
['4', '81', '79', '137', '162']
parsed_discourse_facet ['results_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="79" ssid = "24">PM stands for morphological particle  a label for German infinitival Z7t and superlative am.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="140" ssid = "21">For the implementation  we used Tcl/Tk Version 4.1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'4'", "'79'", "'58'", "'140'"]
'80'
'4'
'79'
'58'
'140'
['80', '4', '79', '58', '140']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'4'", "'137'", "'56'", "'149'"]
'80'
'4'
'137'
'56'
'149'
['80', '4', '137', '56', '149']
parsed_discourse_facet ['method_citation']
<S sid ="129" ssid = "10">In the second phase  secondary links and additional structural functions are supported.</S><S sid ="140" ssid = "21">For the implementation  we used Tcl/Tk Version 4.1.</S><S sid ="141" ssid = "22">The corpus is stored in a SQL database.</S><S sid ="116" ssid = "29">This extra marking makes it easy to distinguish between 'normal' and coordinated categories.</S><S sid ="105" ssid = "18">In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'129'", "'140'", "'141'", "'116'", "'105'"]
'129'
'140'
'141'
'116'
'105'
['129', '140', '141', '116', '105']
parsed_discourse_facet ['method_citation']
<S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="128" ssid = "9">In the first phase  the main functionality for building and displaying unordered trees is supplied.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="105" ssid = "18">In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'137'", "'128'", "'56'", "'105'"]
'74'
'137'
'128'
'56'
'105'
['74', '137', '128', '56', '105']
parsed_discourse_facet ['method_citation']
<S sid ="55" ssid = "45">A uniform representation of local and non-local dependencies makes the structure more transparent'.</S><S sid ="53" ssid = "43">A tree meeting these requirements is given below: Adv V NP NP V CPL NP V damn wird ihn Anna erkennen  dais er 'vein!</S><S sid ="120" ssid = "1">The development of linguistically interpreted corpora presents a laborious and time-consuming task.</S><S sid ="65" ssid = "10">The tree resembles traditional constituent structures.</S><S sid ="14" ssid = "4">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'53'", "'120'", "'65'", "'14'"]
'55'
'53'
'120'
'65'
'14'
['55', '53', '120', '65', '14']
parsed_discourse_facet ['method_citation']
<S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'58'", "'80'", "'149'", "'56'"]
'74'
'58'
'80'
'149'
'56'
['74', '58', '80', '149', '56']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'74'", "'56'", "'58'", "'149'"]
'80'
'74'
'56'
'58'
'149'
['80', '74', '56', '58', '149']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'56'", "'58'", "'137'", "'149'"]
'80'
'56'
'58'
'137'
'149'
['80', '56', '58', '137', '149']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.</S><S sid ="88" ssid = "1">As theory-independence is one of our objectives  the annotation scheme incorporates a number of widely accepted linguistic analyses  especially in the area of verbal  adverbial and adjectival syntax.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="164" ssid = "6">As modern linguistics is also becoming more aware of the importance of larger sets of naturally occurring data.  interpreted corpora. are a. valuable resource for theoretical and descriptive linguistic research.</S><S sid ="168" ssid = "10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'88'", "'5'", "'164'", "'168'"]
'3'
'88'
'5'
'164'
'168'
['3', '88', '5', '164', '168']
parsed_discourse_facet ['method_citation']
<S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'137'", "'74'", "'56'", "'149'"]
'58'
'137'
'74'
'56'
'149'
['58', '137', '74', '56', '149']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="101" ssid = "14">All components of this kernel are assigned the label NK and treated as sibling nodes.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'56'", "'101'", "'4'", "'149'"]
'80'
'56'
'101'
'4'
'149'
['80', '56', '101', '4', '149']
parsed_discourse_facet ['method_citation']
<S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'74'", "'58'", "'149'", "'80'"]
'56'
'74'
'58'
'149'
'80'
['56', '74', '58', '149', '80']
parsed_discourse_facet ['results_citation']
<S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="105" ssid = "18">In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'149'", "'56'", "'105'", "'137'", "'74'"]
'149'
'56'
'105'
'137'
'74'
['149', '56', '105', '137', '74']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/A00-2030_sweta.csv
<S sid="18" ssid="1">Almost all approaches to information extraction &#8212; even at the sentence level &#8212; are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="100" ssid="5">Given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a While our focus throughout the project was on TE and TR, we became curious about how well the model did at part-of-speech tagging, syntactic parsing, and at name finding.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="15">Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
 <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="1">Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.<
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="49" ssid = "9">By necessity  we adopted the strategy of hand marking only the semantics.</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="70" ssid = "11">Post-modifier constituents for the PER/NP.</S><S sid ="73" ssid = "14">We now briefly summarize the probability structure of the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'", "'49'", "'68'", "'70'", "'73'"]
'28'
'49'
'68'
'70'
'73'
['28', '49', '68', '70', '73']
parsed_discourse_facet ['results_citation']
<S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="76" ssid = "17">Finally  word features  fm  for modifiers are predicted based on the modifier  cm  the partof-speech tag of the modifier word   t„„ the part-of-speech tag of the head word th  the head word itself  wh  and whether or not the modifier head word  w„„ is known or unknown.</S><S sid ="39" ssid = "7">For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.</S><S sid ="30" ssid = "13">Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'101'", "'76'", "'39'", "'30'"]
'61'
'101'
'76'
'39'
'30'
['61', '101', '76', '39', '30']
parsed_discourse_facet ['method_citation']
<S sid ="33" ssid = "1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S sid ="41" ssid = "1">To train our integrated model  we required a large corpus of augmented parse trees.</S><S sid ="64" ssid = "5">Word features are introduced primarily to help with unknown words  as in (Weischedel et al. 1993).</S><S sid ="2" ssid = "2">In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'41'", "'64'", "'2'", "'23'"]
'33'
'41'
'64'
'2'
'23'
['33', '41', '64', '2', '23']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'101'", "'81'", "'74'", "'0'"]
'61'
'101'
'81'
'74'
'0'
['61', '101', '81', '74', '0']
parsed_discourse_facet ['method_citation']
<S sid ="60" ssid = "1">In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).</S><S sid ="72" ssid = "13">This generation process is continued until the entire tree has been produced.</S><S sid ="77" ssid = "18">The probability of a complete tree is the product of the probabilities of generating each element in the tree.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'72'", "'77'", "'88'", "'28'"]
'60'
'72'
'77'
'88'
'28'
['60', '72', '77', '88', '28']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="54" ssid = "3">These steps are given below:</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'46'", "'57'", "'54'", "'106'"]
'0'
'46'
'57'
'54'
'106'
['0', '46', '57', '54', '106']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="94" ssid = "13">Given a new sentence  the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S><S sid ="30" ssid = "13">Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.</S><S sid ="60" ssid = "1">In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).</S><S sid ="39" ssid = "7">For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'94'", "'30'", "'60'", "'39'"]
'61'
'94'
'30'
'60'
'39'
['61', '94', '30', '60', '39']
parsed_discourse_facet ['method_citation']
<S sid ="110" ssid = "2">Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.</S><S sid ="41" ssid = "1">To train our integrated model  we required a large corpus of augmented parse trees.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'", "'41'", "'57'", "'51'", "'46'"]
'110'
'41'
'57'
'51'
'46'
['110', '41', '57', '51', '46']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'81'", "'0'", "'61'", "'74'"]
'51'
'81'
'0'
'61'
'74'
['51', '81', '0', '61', '74']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "15">Because generative statistical models had already proven successful for each of the first three stages  we were optimistic that some of their properties — especially their ability to learn from large amounts of data  and their robustness when presented with unexpected inputs — would also benefit semantic analysis.</S><S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="19" ssid = "2">Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'61'", "'19'", "'101'", "'51'"]
'32'
'61'
'19'
'101'
'51'
['32', '61', '19', '101', '51']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="54" ssid = "3">These steps are given below:</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'54'", "'81'", "'57'", "'46'"]
'0'
'54'
'81'
'57'
'46'
['0', '54', '81', '57', '46']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "13">Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'46'", "'105'", "'106'", "'11'"]
'30'
'46'
'105'
'106'
'11'
['30', '46', '105', '106', '11']
parsed_discourse_facet ['method_citation']
<S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="54" ssid = "3">These steps are given below:</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'", "'0'", "'46'", "'57'", "'54'"]
'105'
'0'
'46'
'57'
'54'
['105', '0', '46', '57', '54']
parsed_discourse_facet ['results_citation']
<S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'0'", "'57'", "'51'", "'105'"]
'46'
'0'
'57'
'51'
'105'
['46', '0', '57', '51', '105']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'57'", "'46'", "'51'", "'81'"]
'0'
'57'
'46'
'51'
'81'
['0', '57', '46', '51', '81']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="32" ssid = "15">Because generative statistical models had already proven successful for each of the first three stages  we were optimistic that some of their properties — especially their ability to learn from large amounts of data  and their robustness when presented with unexpected inputs — would also benefit semantic analysis.</S><S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S><S sid ="26" ssid = "9">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993)  and more recently  had begun using a generative statistical model for name finding (Bikel et al.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'32'", "'61'", "'11'", "'26'"]
'51'
'32'
'61'
'11'
'26'
['51', '32', '61', '11', '26']
parsed_discourse_facet ['method_citation']
<S sid ="110" ssid = "2">Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'", "'106'", "'46'", "'57'", "'105'"]
'110'
'106'
'46'
'57'
'105'
['110', '106', '46', '57', '105']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "3">These steps are given below:</S><S sid ="55" ssid = "1">syntactic modifier of the other  the inserted node serves to indicate the relation as well as the argument.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="7" ssid = "5">The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'55'", "'0'", "'62'", "'7'"]
'54'
'55'
'0'
'62'
'7'
['54', '55', '0', '62', '7']
parsed_discourse_facet ['method_citation']
<S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="110" ssid = "2">Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'110'", "'46'", "'105'", "'57'"]
'106'
'110'
'46'
'105'
'57'
['106', '110', '46', '105', '57']
parsed_discourse_facet ['aim_citation', 'results_citation']
IGNORE THIS: key error 1



A00-2030
H05-1094
0
method_citation
['method_citation']
parsing: input/ref/Task1/P08-1028_aakansha.csv
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="185" ssid="19">The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.</S>
    <S sid="186" ssid="20">The combined model is best overall with &#961; = 0.19.</S>
    <S sid="187" ssid="21">However, the difference between the two models is not statistically significant.</S>
original cit marker offset is 0
new cit marker offset is 0



["'185'", "'186'"]
'185'
'186'
['185', '186']
parsed_discourse_facet ['result_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="21">The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'"]
'48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="53" ssid="1">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.</S><S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'53'", "'57'"]
'53'
'57'
['53', '57']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="16">Although the composition model in (5) is commonly used in the literature, from a linguistic perspective, the model in (6) is more appealing.</S>
    <S sid="69" ssid="17">Simply adding the vectors u and v lumps their contents together rather than allowing the content of one vector to pick out the relevant content of the other.</S>
    <S sid="70" ssid="18">Instead, it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v, and vice versa.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'69'", "'70'"]
'68'
'69'
'70'
['68', '69', '70']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
    <S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'", "'25'"]
'24'
'25'
['24', '25']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="12">Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'"]
'64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="176" ssid="10">The multiplicative and combined models yield means closer to the human ratings.</S>
    <S sid="177" ssid="11">The difference between High and Low similarity values estimated by these models are statistically significant (p &lt; 0.01 using the Wilcoxon rank sum test).</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'", "'177'"]
'176'
'177'
['176', '177']
parsed_discourse_facet ['method_citation']
<S sid="191" ssid="3">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'191'"]
'191'
['191']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1028.csv
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="140" ssid = "53">Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid ="40" ssid = "13">Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.</S><S sid ="155" ssid = "68">Secondly  we optimized the weightings in the combined model (11) with a similar grid search over its three parameters.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'140'", "'51'", "'40'", "'155'"]
'194'
'140'
'51'
'40'
'155'
['194', '140', '51', '40', '155']
parsed_discourse_facet ['results_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="58" ssid = "6">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="10" ssid = "6">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'58'", "'59'", "'10'", "'51'"]
'194'
'58'
'59'
'10'
'51'
['194', '58', '59', '10', '51']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="38" ssid = "11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S><S sid ="141" ssid = "54">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S><S sid ="36" ssid = "9">To overcome this problem  other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'194'", "'38'", "'141'", "'36'"]
'108'
'194'
'38'
'141'
'36'
['108', '194', '38', '141', '36']
parsed_discourse_facet ['method_citation']
<S sid ="160" ssid = "73">Specifically  m was set to 20 and m to 1.</S><S sid ="156" ssid = "69">This yielded a weighted sum consisting of 95% verb  0% noun and 5% of their multiplicative combination.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="190" ssid = "2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid ="117" ssid = "30">This yielded a reduced set of experimental items (120 in total) consisting of 15 reference verbs  each with 4 nouns  and 2 landmarks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'", "'156'", "'34'", "'190'", "'117'"]
'160'
'156'
'34'
'190'
'117'
['160', '156', '34', '190', '117']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">Vector-based Models of Semantic Composition</S><S sid ="36" ssid = "9">To overcome this problem  other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid ="142" ssid = "55">We experimented with a variety of dimensions (ranging from 50 to 500 000)  vector component definitions (e.g.  pointwise mutual information or log likelihood ratio) and similarity measures (e.g.  cosine or confusion probability).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'36'", "'51'", "'142'", "'194'"]
'0'
'36'
'51'
'142'
'194'
['0', '36', '51', '142', '194']
parsed_discourse_facet ['method_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid ="58" ssid = "6">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S><S sid ="201" ssid = "13">We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al.  1988) and inductive inference (Heit and Rubinstein  1994).</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'59'", "'51'", "'58'", "'201'"]
'194'
'59'
'51'
'58'
'201'
['194', '59', '51', '58', '201']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "3">For the hierarchical structure of natural language this binding problem becomes particularly acute.</S><S sid ="96" ssid = "9">Any adequate model of composition must be able to represent argument-verb meaning.</S><S sid ="1" ssid = "1">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid ="24" ssid = "20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S><S sid ="44" ssid = "17">For example  assuming that individual words are represented by vectors  we can compute the meaning of a sentence by taking their mean (Foltz et al.  1998; Landauer and Dumais  1997).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'96'", "'1'", "'24'", "'44'"]
'30'
'96'
'1'
'24'
'44'
['30', '96', '1', '24', '44']
parsed_discourse_facet ['method_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="37" ssid = "10">Holographic reduced representations (Plate  1991) are one implementation of this idea where the tensor product is projected back onto the space of its components.</S><S sid ="141" ssid = "54">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'3'", "'101'", "'37'", "'141'"]
'194'
'3'
'101'
'37'
'141'
['194', '3', '101', '37', '141']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid ="140" ssid = "53">Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.</S><S sid ="144" ssid = "57">We obtained best results with a model using a context window of five words on either side of the target word  the cosine measure  and 2 000 vector components.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="37" ssid = "10">Holographic reduced representations (Plate  1991) are one implementation of this idea where the tensor product is projected back onto the space of its components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'140'", "'144'", "'194'", "'37'"]
'39'
'140'
'144'
'194'
'37'
['39', '140', '144', '194', '37']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="168" ssid = "2">These included three additive models  i.e.  simple addition (equation (5)  Add)  weighted addition (equation (7)  WeightAdd)  and Kintsch’s (2001) model (equation (10)  Kintsch)  a multiplicative model (equation (6)  Multiply)  and also a model which combines multiplication with addition (equation (11)  Combined).</S><S sid ="35" ssid = "8">The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely  the tensor product has dimensionality m x n).</S><S sid ="144" ssid = "57">We obtained best results with a model using a context window of five words on either side of the target word  the cosine measure  and 2 000 vector components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'194'", "'168'", "'35'", "'144'"]
'108'
'194'
'168'
'35'
'144'
['108', '194', '168', '35', '144']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid ="117" ssid = "30">This yielded a reduced set of experimental items (120 in total) consisting of 15 reference verbs  each with 4 nouns  and 2 landmarks.</S><S sid ="35" ssid = "8">The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely  the tensor product has dimensionality m x n).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'117'", "'35'", "'194'", "'108'"]
'39'
'117'
'35'
'194'
'108'
['39', '117', '35', '194', '108']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="153" ssid = "66">Specifically  we considered eleven models  varying in their weightings  in steps of 10%  from 100% noun through 50% of both verb and noun to 100% verb.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'101'", "'3'", "'153'", "'194'"]
'108'
'101'
'3'
'153'
'194'
['108', '101', '3', '153', '194']
parsed_discourse_facet ['method_citation']
<S sid ="148" ssid = "61">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S><S sid ="177" ssid = "11">The difference between High and Low similarity values estimated by these models are statistically significant (p < 0.01 using the Wilcoxon rank sum test).</S><S sid ="4" ssid = "4">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S><S sid ="127" ssid = "40">Analysis of Similarity Ratings The reliability of the collected judgments is important for our evaluation experiments; we therefore performed several tests to validate the quality of the ratings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'148'", "'164'", "'177'", "'4'", "'127'"]
'148'
'164'
'177'
'4'
'127'
['148', '164', '177', '4', '127']
parsed_discourse_facet ['results_citation']
<S sid ="115" ssid = "28">Each cell recorded the number of times our subjects selected the landmark as compatible with the noun or not.</S><S sid ="131" ssid = "44">A Wilcoxon rank sum test confirmed that the difference is statistically significant (p < 0.01).</S><S sid ="147" ssid = "60">This configuration gave high correlations with the WordSim353 similarity judgments using the cosine measure.</S><S sid ="173" ssid = "7">Model similarities have been estimated using cosine which ranges from 0 to 1  whereas our subjects rated the sentences on a scale from 1 to 7.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'131'", "'147'", "'173'", "'164'"]
'115'
'131'
'147'
'173'
'164'
['115', '131', '147', '173', '164']
parsed_discourse_facet ['method_citation']
<S sid ="140" ssid = "53">Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.</S><S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="2" ssid = "2">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S><S sid ="139" ssid = "52">The semantic space we used in our experiments was built on a lemmatised version of the BNC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'3'", "'101'", "'2'", "'139'"]
'140'
'3'
'101'
'2'
'139'
['140', '3', '101', '2', '139']
parsed_discourse_facet ['method_citation']
<S sid ="115" ssid = "28">Each cell recorded the number of times our subjects selected the landmark as compatible with the noun or not.</S><S sid ="147" ssid = "60">This configuration gave high correlations with the WordSim353 similarity judgments using the cosine measure.</S><S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid ="173" ssid = "7">Model similarities have been estimated using cosine which ranges from 0 to 1  whereas our subjects rated the sentences on a scale from 1 to 7.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'147'", "'39'", "'173'", "'164'"]
'115'
'147'
'39'
'173'
'164'
['115', '147', '39', '173', '164']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S><S sid ="153" ssid = "66">Specifically  we considered eleven models  varying in their weightings  in steps of 10%  from 100% noun through 50% of both verb and noun to 100% verb.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="183" ssid = "17">The weighted additive model (p = 0.09) is not significantly different from the baseline either or Kintsch (2001) (p = 0.09).</S><S sid ="40" ssid = "13">Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'153'", "'194'", "'183'", "'40'"]
'38'
'153'
'194'
'183'
'40'
['38', '153', '194', '183', '40']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



P08-1028
D08-1094
0
method_citation
['results_citation']



P08-1028
D11-1094
0
method_citation
['method_citation']
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P05-1013_swastika.csv
<S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



['49']
49
['49']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="6">In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
<S sid="38" ssid="9">Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="6">In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
Error in Reference Offset
S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
Error in Reference Offset
    <S sid="86" ssid="13">As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
 <S sid="51" ssid="22">In the second scheme, Head+Path, we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p&#8595;.</S>
original cit marker offset is 0
new cit marker offset is 0



['51']
51
['51']
parsed_discourse_facet ['method_citation']
<S sid="99" ssid="10">This may seem surprising, given the experiments reported in section 4, but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift, where the encoding of path information is often redundant.</S>
original cit marker offset is 0
new cit marker offset is 0



['99']
99
['99']
parsed_discourse_facet ['result_citation']
<S sid="7" ssid="3">From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



['2']
2
['2']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/P05-1013.csv
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'48'", "'21'", "'36'"]
'50'
'75'
'48'
'21'
'36'
['50', '75', '48', '21', '36']
parsed_discourse_facet ['results_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="62" ssid = "1">In the experiments below  we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs 3 previously tested on Swedish (Nivre et al.  2004) and English (Nivre and Scholz  2004).</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="91" ssid = "2">Table 5 shows the overall parsing accuracy attained with the three different encoding schemes  compared to the baseline (no special arc labels) and to training directly on non-projective dependency graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'75'", "'62'", "'21'", "'91'"]
'48'
'75'
'62'
'21'
'91'
['48', '75', '62', '21', '91']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="77" ssid = "4">The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'77'", "'48'", "'21'"]
'50'
'75'
'77'
'48'
'21'
['50', '75', '77', '48', '21']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'71'", "'21'", "'45'"]
'50'
'75'
'71'
'21'
'45'
['50', '75', '71', '21', '45']
parsed_discourse_facet ['method_citation']
<S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'50'", "'48'", "'45'", "'21'"]
'75'
'50'
'48'
'45'
'21'
['75', '50', '48', '45', '21']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p↓.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'51'", "'36'", "'21'", "'48'"]
'50'
'51'
'36'
'21'
'48'
['50', '51', '36', '21', '48']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'48'", "'21'", "'36'", "'75'"]
'50'
'48'
'21'
'36'
'75'
['50', '48', '21', '36', '75']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p↓.</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'21'", "'51'", "'48'", "'75'"]
'50'
'21'
'51'
'48'
'75'
['50', '21', '51', '48', '75']
parsed_discourse_facet ['method_citation']
<S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p↓.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="23" ssid = "19">By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'", "'51'", "'21'", "'48'", "'23'"]
'36'
'51'
'21'
'48'
'23'
['36', '51', '21', '48', '23']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'48'", "'21'", "'45'"]
'50'
'75'
'48'
'21'
'45'
['50', '75', '48', '21', '45']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="83" ssid = "10">It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="6" ssid = "2">However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.</S><S sid ="35" ssid = "6">The dependency graph in Figure 1 satisfies all the defining conditions above  but it fails to satisfy the condition ofprojectivity (Kahane et al.  1998): The arc connecting the head jedna (one) to the dependent Z (out-of) spans the token je (is)  which is not dominated by jedna.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'83'", "'7'", "'6'", "'35'"]
'2'
'83'
'7'
'6'
'35'
['2', '83', '7', '6', '35']
parsed_discourse_facet ['method_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'75'", "'45'", "'21'", "'20'"]
'48'
'75'
'45'
'21'
'20'
['48', '75', '45', '21', '20']
parsed_discourse_facet ['method_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S><S sid ="73" ssid = "12">More details on the memory-based prediction can be found in Nivre et al. (2004) and Nivre and Scholz (2004).</S><S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'45'", "'73'", "'20'", "'106'"]
'48'
'45'
'73'
'20'
'106'
['48', '45', '73', '20', '106']
parsed_discourse_facet ['results_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="52" ssid = "23">Thus  the arc from je to jedna will be labeled 5b↓ (to indicate that there is a syntactic head below it).</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'48'", "'75'", "'36'", "'52'"]
'50'
'48'
'75'
'36'
'52'
['50', '48', '75', '36', '52']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="84" ssid = "11">In the second part of the experiment  we applied the inverse transformation based on breadth-first search under the three different encoding schemes.</S><S sid ="79" ssid = "6">In the first part of the experiment  dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'48'", "'21'", "'84'", "'79'"]
'50'
'48'
'21'
'84'
'79'
['50', '48', '21', '84', '79']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "8">For each token  three types of features may be taken into account: the word form; the part-of-speech assigned by an automatic tagger; and labels on previously assigned dependency arcs involving the token – the arc from its head and the arcs to its leftmost and rightmost dependent  respectively.</S><S sid ="23" ssid = "19">By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S><S sid ="64" ssid = "3">At each point during the derivation  the parser has a choice between pushing the next input token onto the stack – with or without adding an arc from the token on top of the stack to the token pushed – and popping a token from the stack – with or without adding an arc from the next input token to the token popped.</S><S sid ="59" ssid = "30">The details of the transformation procedure are slightly different depending on the encoding schemes: d↑h let the linear head be the syntactic head). target arc must have the form wl −→ wm; if no target arc is found  Head is used as backoff. must have the form wl −→ wm and no outgoing arcs of the form wm p'↓ −→ wo; no backoff.</S><S sid ="46" ssid = "17">In principle  it would be possible to encode the exact position of the syntactic head in the label of the arc from the linear head  but this would give a potentially infinite set of arc labels and would make the training of the parser very hard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'23'", "'64'", "'59'", "'46'"]
'69'
'23'
'64'
'59'
'46'
['69', '23', '64', '59', '46']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'71'", "'21'", "'45'"]
'50'
'75'
'71'
'21'
'45'
['50', '75', '71', '21', '45']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "18">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 15–25%  the total proportion of non-projective arcs is normally only 1–2%.</S><S sid ="101" ssid = "12">However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.</S><S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S><S sid ="87" ssid = "14">However  it can be noted that the results for the least informative encoding  Path  are almost comparable  while the third encoding  Head  gives substantially worse results for both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'14'", "'101'", "'89'", "'87'"]
'107'
'14'
'101'
'89'
'87'
['107', '14', '101', '89', '87']
parsed_discourse_facet ['method_citation']
<S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 15–25%  the total proportion of non-projective arcs is normally only 1–2%.</S><S sid ="88" ssid = "15">We also see that the increase in the size of the label sets for Head and Head+Path is far below the theoretical upper bounds given in Table 1.</S><S sid ="28" ssid = "24">First  in section 4  we evaluate the graph transformation techniques in themselves  with data from the Prague Dependency Treebank and the Danish Dependency Treebank.</S><S sid ="103" ssid = "14">On the other hand  given that all schemes have similar parsing accuracy overall  this means that the Path scheme is the least likely to introduce errors on projective arcs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'14'", "'88'", "'28'", "'103'"]
'89'
'14'
'88'
'28'
'103'
['89', '14', '88', '28', '103']
parsed_discourse_facet ['aim_citation', 'results_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'50'", "'75'", "'21'", "'36'"]
'48'
'50'
'75'
'21'
'36'
['48', '50', '75', '21', '36']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/A00-2018_vardha.csv
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'91'"]
'91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="8">In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'"]
'39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="162" ssid="53">Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'"]
'162'
['162']
parsed_discourse_facet ['method_citation']
 <S sid="87" ssid="56">In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17].</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'"]
'87'
['87']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'91'"]
'91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="176" ssid="3">That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'"]
'176'
['176']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['method_citation']
 <S sid="101" ssid="12">In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning).</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'"]
'101'
['101']
parsed_discourse_facet ['method_citation']
<S sid="155" ssid="46">For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
  <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="162" ssid="53">Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'"]
'162'
['162']
parsed_discourse_facet ['method_citation']
 <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
 <S sid="79" ssid="48">We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
    <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
    <S sid="175" ssid="2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["'175'"]
'175'
['175']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2018.csv
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'23'", "'62'", "'33'", "'27'"]
'30'
'23'
'62'
'33'
'27'
['30', '23', '62', '33', '27']
parsed_discourse_facet ['results_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'27'", "'62'", "'33'", "'23'"]
'30'
'27'
'62'
'33'
'23'
['30', '27', '62', '33', '23']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'62'", "'33'", "'89'", "'23'"]
'30'
'62'
'33'
'89'
'23'
['30', '62', '33', '89', '23']
parsed_discourse_facet ['method_citation']
<S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="20" ssid = "9">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="29" ssid = "18">So  for example  in a second-order Markov PCFG  L2 would be conditioned on L1 and M. In our complete model  of course  the probability of each label in the expansions is also conditioned on other material as specified in Equation 1  e.g.  p(e t  h  H).</S><S sid ="91" ssid = "2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'", "'20'", "'174'", "'29'", "'91'"]
'27'
'20'
'174'
'29'
'91'
['27', '20', '174', '29', '91']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'23'", "'62'", "'27'", "'33'"]
'30'
'23'
'62'
'27'
'33'
['30', '23', '62', '27', '33']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="63" ssid = "32">As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'1'", "'5'", "'27'", "'63'"]
'101'
'1'
'5'
'27'
'63'
['101', '1', '5', '27', '63']
parsed_discourse_facet ['method_citation']
<S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'", "'30'", "'62'", "'89'", "'23'"]
'27'
'30'
'62'
'89'
'23'
['27', '30', '62', '89', '23']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'62'", "'23'", "'89'", "'33'"]
'30'
'62'
'23'
'89'
'33'
['30', '62', '23', '89', '33']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "7">To compute a probability in a log-linear model one first defines a set of &quot;features&quot;  functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.</S><S sid ="91" ssid = "2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="20" ssid = "9">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid ="26" ssid = "15">Thus an expansion e(c) looks like: The expansion is generated by guessing first M  then in order L1 through L „.+1 (= A)  and similarly for RI through In a pure Markov PCFG we are given the left-hand side label 1 and then probabilistically generate the right-hand side conditioning on no information other than 1 and (possibly) previously generated pieces of the right-hand side itself.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'91'", "'174'", "'20'", "'26'"]
'38'
'91'
'174'
'20'
'26'
['38', '91', '174', '20', '26']
parsed_discourse_facet ['method_citation']
<S sid ="29" ssid = "18">So  for example  in a second-order Markov PCFG  L2 would be conditioned on L1 and M. In our complete model  of course  the probability of each label in the expansions is also conditioned on other material as specified in Equation 1  e.g.  p(e t  h  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="91" ssid = "2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'", "'27'", "'174'", "'91'", "'33'"]
'29'
'27'
'174'
'91'
'33'
['29', '27', '174', '91', '33']
parsed_discourse_facet ['method_citation']
<S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="101" ssid = "12">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S><S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'", "'27'", "'89'", "'101'", "'30'"]
'62'
'27'
'89'
'101'
'30'
['62', '27', '89', '101', '30']
parsed_discourse_facet ['method_citation']
<S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="115" ssid = "6">As noted in [5]  that system is based upon a &quot;tree-bank grammar&quot; - a grammar read directly off the training corpus.</S><S sid ="141" ssid = "32">(For example  part-ofspeech tagging using the most probable preterminal for each word is 90% accurate [8].)</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="184" ssid = "11">The first is the slight  but important  improvement achieved by using this model over conventional deleted interpolation  as indicated in Figure 2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'115'", "'141'", "'62'", "'184'"]
'23'
'115'
'141'
'62'
'184'
['23', '115', '141', '62', '184']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "9">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid ="76" ssid = "45">This requires finding the appropriate Ais for Equation 3  which is accomplished using an algorithm such as iterative scaling [II] in which values for the Ai are initially &quot;guessed&quot; and then modified until they converge on stable values.</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="110" ssid = "1">In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach  the aspect of the parser that is most novel.</S><S sid ="101" ssid = "12">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'76'", "'27'", "'110'", "'101'"]
'20'
'76'
'27'
'110'
'101'
['20', '76', '27', '110', '101']
parsed_discourse_facet ['results_citation']
<S sid ="74" ssid = "43">Suppose we were  in fact  going to compute a true maximum entropy model based upon the features used in Equation 7  Ii (t 1)  f2(t 1 1p)  f3(t 1 lp) .</S><S sid ="126" ssid = "17">This is indicated in Figure 2  where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%  0.4% lower than the results on the official test corpus.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'126'", "'125'", "'174'", "'27'"]
'74'
'126'
'125'
'174'
'27'
['74', '126', '125', '174', '27']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="42" ssid = "11">This feature is obviously composed of two sub-features  one recognizing t  the other 1.</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'89'", "'23'", "'42'", "'62'"]
'30'
'89'
'23'
'42'
'62'
['30', '89', '23', '42', '62']
parsed_discourse_facet ['method_citation']
<S sid ="176" ssid = "3">That the previous three best parsers on this test [5 9 17] all perform within a percentage point of each other  despite quite different basic mechanisms  led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training  and to conjecture that perhaps we were at it.</S><S sid ="58" ssid = "27">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid ="180" ssid = "7">From our perspective  perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.</S><S sid ="44" ssid = "13">Now consider computing a conditional probability p(a H) with a set of features h that connect a to the history H. In a log-linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability  the higher the absolute value of the associated A.</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'", "'58'", "'180'", "'44'", "'13'"]
'176'
'58'
'180'
'44'
'13'
['176', '58', '180', '44', '13']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P11-1060_swastika.csv
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['aim_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['aim_citation']
    <S sid="148" ssid="33">This bootstrapping behavior occurs naturally: The &#8220;easy&#8221; examples are processed first, where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
    <S sid="13" ssid="9">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['result_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="47" ssid="23">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
S sid="44" ssid="20">The recurrence is as follows: At each node, we compute the set of tuples v consistent with the predicate at that node (v &#8712; w(p)), and S(x)}, where a set of pairs S is treated as a set-valued function S(x) = {y : (x, y) &#8712; S} with domain S1 = {x : (x, y) &#8712; S}.</S>
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
Error in Reference Offset
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



['106']
106
['106']
parsed_discourse_facet ['aim_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
    <S sid="172" ssid="57">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms, we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus, and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S>
original cit marker offset is 0
new cit marker offset is 0



['172']
172
['172']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



['171']
171
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="158" ssid = "43">5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation  DCS  which offers a new perspective lexical semantics.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="23" ssid = "19">We trained our model using an EM-like algorithm (Section 3) on two benchmarks  GEO and JOBS (Section 4).</S><S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'158'", "'21'", "'23'", "'132'"]
'26'
'158'
'21'
'23'
'132'
['26', '158', '21', '23', '132']
parsed_discourse_facet ['results_citation']
<S sid ="39" ssid = "15">Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.</S><S sid ="56" ssid = "32">The basic version of DCS described thus far handles a core subset of language.</S><S sid ="4" ssid = "4">On two stansemantic parsing benchmarks our system obtains the highest published accuracies  despite requiring no annotated logical forms.</S><S sid ="136" ssid = "21">In fact  DCS performs comparably to even the version of SEMRESP trained using logical forms.</S><S sid ="83" ssid = "59">It suffices to define Xi(d) for a single column i.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'56'", "'4'", "'136'", "'83'"]
'39'
'56'
'4'
'136'
'83'
['39', '56', '4', '136', '83']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S><S sid ="148" ssid = "33">This bootstrapping behavior occurs naturally: The “easy” examples are processed first  where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'100'", "'115'", "'88'", "'148'"]
'26'
'100'
'115'
'88'
'148'
['26', '100', '115', '88', '148']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="138" ssid = "23">Next  we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="53" ssid = "29">Having instantiated s as a value  everything above this node is an ordinary CSP: s constrains the count node  which in turns constrains the root node to |s|.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'138'", "'26'", "'53'"]
'100'
'21'
'138'
'26'
'53'
['100', '21', '138', '26', '53']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S><S sid ="94" ssid = "70">We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'94'", "'134'", "'8'", "'58'"]
'18'
'94'
'134'
'8'
'58'
['18', '94', '134', '8', '58']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'21'", "'100'", "'115'", "'42'"]
'26'
'21'
'100'
'115'
'42'
['26', '21', '100', '115', '42']
parsed_discourse_facet ['method_citation']
<S sid ="87" ssid = "63">For example  in Figure 4(a)  before execution  the denotation of the DCS tree is hh{[(CA  OR)  (OR)] ... }; ø; (E  Qhstatei�w  ø)ii; after applying X1  we have hh{[(OR)]  ... }; øii.</S><S sid ="113" ssid = "89">Formally  let ˜O(θ  θ') be the objective function O(θ) with ZL(x) ˜ZL θI(x).</S><S sid ="20" ssid = "16">At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="74" ssid = "50">Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'", "'113'", "'20'", "'154'", "'74'"]
'87'
'113'
'20'
'154'
'74'
['87', '113', '20', '154', '74']
parsed_discourse_facet ['method_citation']
<S sid ="117" ssid = "2">In each dataset  each sentence x is annotated with a Prolog logical form  which we use only to evaluate and get an answer y.</S><S sid ="1" ssid = "1">Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'117'", "'1'", "'15'", "'88'", "'62'"]
'117'
'1'
'15'
'88'
'62'
['117', '1', '15', '88', '62']
parsed_discourse_facet ['method_citation']
<S sid ="83" ssid = "59">It suffices to define Xi(d) for a single column i.</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="101" ssid = "77">Formally  pθ(z  |x) ∝ eφ(x z)Tθ  where θ and φ(x  z) are parameter and feature vectors  respectively.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'169'", "'8'", "'101'", "'134'"]
'83'
'169'
'8'
'101'
'134'
['83', '169', '8', '101', '134']
parsed_discourse_facet ['method_citation']
<S sid ="1" ssid = "1">Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid ="99" ssid = "75">To further reduce the search space  F imposes a few additional constraints  e.g.  limiting the number of marked nodes to 2 and only allowing trace predicates between arity 1 predicates.</S><S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S><S sid ="129" ssid = "14">We also define an augmented lexicon L+ which includes a prototype word x for each predicate appearing in (iii) above (e.g.  (large  size))  which cancels the predicates triggered by x’s POS tag.</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'99'", "'132'", "'129'", "'26'"]
'1'
'99'
'132'
'129'
'26'
['1', '99', '132', '129', '26']
parsed_discourse_facet ['method_citation']
<S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="54" ssid = "30">A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S><S sid ="166" ssid = "51">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'54'", "'42'", "'166'", "'132'"]
'21'
'54'
'42'
'166'
'132'
['21', '54', '42', '166', '132']
parsed_discourse_facet ['method_citation']
<S sid ="59" ssid = "35">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S><S sid ="54" ssid = "30">A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="45" ssid = "21">The logical forms in DCS are called DCS trees  where nodes are labeled with predicates  and edges are labeled with relations.</S><S sid ="43" ssid = "19">Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter  2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'54'", "'154'", "'45'", "'43'"]
'59'
'54'
'154'
'45'
'43'
['59', '54', '154', '45', '43']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "30">A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="74" ssid = "50">Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S><S sid ="107" ssid = "83">Our model is arc-factored  so we can sum over all DCS trees in ZL(x) using dynamic programming.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'42'", "'21'", "'74'", "'107'"]
'54'
'42'
'21'
'74'
'107'
['54', '42', '21', '74', '107']
parsed_discourse_facet ['results_citation']
<S sid ="59" ssid = "35">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S><S sid ="148" ssid = "33">This bootstrapping behavior occurs naturally: The “easy” examples are processed first  where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S><S sid ="47" ssid = "23">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.</S><S sid ="103" ssid = "79">To define the features  we technically need to augment each tree z ∈ ZL(x) with alignment information—namely  for each predicate in z  the span in x (if any) that triggered it.</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'148'", "'47'", "'103'", "'62'"]
'59'
'148'
'47'
'103'
'62'
['59', '148', '47', '103', '62']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="23" ssid = "19">We trained our model using an EM-like algorithm (Section 3) on two benchmarks  GEO and JOBS (Section 4).</S><S sid ="86" ssid = "62">Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  −(i  ø)]{α1 = ø}.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'23'", "'86'", "'115'", "'100'"]
'26'
'23'
'86'
'115'
'100'
['26', '23', '86', '115', '100']
parsed_discourse_facet ['method_citation']
<S sid ="113" ssid = "89">Formally  let ˜O(θ  θ') be the objective function O(θ) with ZL(x) ˜ZL θI(x).</S><S sid ="78" ssid = "54">The stores are also concatenated (α + α').</S><S sid ="74" ssid = "50">Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="111" ssid = "87">Let ˜ZL θ(x) be this approximation of ZL(x).</S>
original cit marker offset is 0
new cit marker offset is 0



["'113'", "'78'", "'74'", "'154'", "'111'"]
'113'
'78'
'74'
'154'
'111'
['113', '78', '74', '154', '111']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="63" ssid = "39">Think of d as consisting of n columns  one for each active node according to a pre-order traversal of z.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="86" ssid = "62">Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  −(i  ø)]{α1 = ø}.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'100'", "'63'", "'115'", "'86'"]
'26'
'100'
'63'
'115'
'86'
['26', '100', '63', '115', '86']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'26'", "'115'", "'42'"]
'100'
'21'
'26'
'115'
'42'
['100', '21', '26', '115', '42']
parsed_discourse_facet ['method_citation']
<S sid ="151" ssid = "36">Inspecting the final parameters calculus formulae.</S><S sid ="56" ssid = "32">The basic version of DCS described thus far handles a core subset of language.</S><S sid ="16" ssid = "12">Which one should we use?</S><S sid ="118" ssid = "3">This evaluation is done with respect to a world w. Recall that a world w maps each predicate p ∈ P to a set of tuples w(p).</S><S sid ="33" ssid = "9">As another example  w(average) = {(S  ¯x) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'56'", "'16'", "'118'", "'33'"]
'151'
'56'
'16'
'118'
'33'
['151', '56', '16', '118', '33']
parsed_discourse_facet ['aim_citation', 'results_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



P11-1060
D11-1022
0
aim_citation
['method_citation']
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P08-1028_sweta.csv
<S sid="65" ssid="13">So, if we assume that only the ith components of u and v contribute to the ith component of p, that these components are not dependent on i, and that the function is symmetric with regard to the interchange of u and v, we obtain a simpler instantiation of an additive model: Analogously, under the same assumptions, we obtain the following simpler multiplicative model: only the ith components of u and v contribute to the ith component of p. Another class of models can be derived by relaxing this constraint.</S>
original cit marker offset is 0
new cit marker offset is 0



["65'"]
65'
['65']
parsed_discourse_facet ['method_citation']
 <S sid="75" ssid="23">An extreme form of this differential in the contribution of constituents is where one of the vectors, say u, contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic, however it can serve as a simple baseline against which to compare more sophisticated models.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["42'"]
42'
['42']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="17">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="195" ssid="7">The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately.</S>
original cit marker offset is 0
new cit marker offset is 0



["195'"]
195'
['195']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["57'"]
57'
['57']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["190'"]
190'
['190']
parsed_discourse_facet ['method_citation']
 <S sid="29" ssid="2">While neural networks can readily represent single distinct objects, in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>
original cit marker offset is 0
new cit marker offset is 0



["29'"]
29'
['29']
parsed_discourse_facet ['method_citation']
    <S sid="38" ssid="11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="12">Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="163" ssid="76">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["163'"]
163'
['163']
parsed_discourse_facet ['method_citation']
<S sid="185" ssid="19">The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.</S>
original cit marker offset is 0
new cit marker offset is 0



["185'"]
185'
['185']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["190'"]
190'
['190']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1028.csv
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="140" ssid = "53">Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid ="40" ssid = "13">Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.</S><S sid ="155" ssid = "68">Secondly  we optimized the weightings in the combined model (11) with a similar grid search over its three parameters.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'140'", "'51'", "'40'", "'155'"]
'194'
'140'
'51'
'40'
'155'
['194', '140', '51', '40', '155']
parsed_discourse_facet ['results_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="58" ssid = "6">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="10" ssid = "6">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'58'", "'59'", "'10'", "'51'"]
'194'
'58'
'59'
'10'
'51'
['194', '58', '59', '10', '51']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="38" ssid = "11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S><S sid ="141" ssid = "54">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S><S sid ="36" ssid = "9">To overcome this problem  other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'194'", "'38'", "'141'", "'36'"]
'108'
'194'
'38'
'141'
'36'
['108', '194', '38', '141', '36']
parsed_discourse_facet ['method_citation']
<S sid ="160" ssid = "73">Specifically  m was set to 20 and m to 1.</S><S sid ="156" ssid = "69">This yielded a weighted sum consisting of 95% verb  0% noun and 5% of their multiplicative combination.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="190" ssid = "2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid ="117" ssid = "30">This yielded a reduced set of experimental items (120 in total) consisting of 15 reference verbs  each with 4 nouns  and 2 landmarks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'", "'156'", "'34'", "'190'", "'117'"]
'160'
'156'
'34'
'190'
'117'
['160', '156', '34', '190', '117']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">Vector-based Models of Semantic Composition</S><S sid ="36" ssid = "9">To overcome this problem  other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid ="142" ssid = "55">We experimented with a variety of dimensions (ranging from 50 to 500 000)  vector component definitions (e.g.  pointwise mutual information or log likelihood ratio) and similarity measures (e.g.  cosine or confusion probability).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'36'", "'51'", "'142'", "'194'"]
'0'
'36'
'51'
'142'
'194'
['0', '36', '51', '142', '194']
parsed_discourse_facet ['method_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid ="58" ssid = "6">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S><S sid ="201" ssid = "13">We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al.  1988) and inductive inference (Heit and Rubinstein  1994).</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'59'", "'51'", "'58'", "'201'"]
'194'
'59'
'51'
'58'
'201'
['194', '59', '51', '58', '201']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "3">For the hierarchical structure of natural language this binding problem becomes particularly acute.</S><S sid ="96" ssid = "9">Any adequate model of composition must be able to represent argument-verb meaning.</S><S sid ="1" ssid = "1">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid ="24" ssid = "20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S><S sid ="44" ssid = "17">For example  assuming that individual words are represented by vectors  we can compute the meaning of a sentence by taking their mean (Foltz et al.  1998; Landauer and Dumais  1997).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'96'", "'1'", "'24'", "'44'"]
'30'
'96'
'1'
'24'
'44'
['30', '96', '1', '24', '44']
parsed_discourse_facet ['method_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="37" ssid = "10">Holographic reduced representations (Plate  1991) are one implementation of this idea where the tensor product is projected back onto the space of its components.</S><S sid ="141" ssid = "54">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'3'", "'101'", "'37'", "'141'"]
'194'
'3'
'101'
'37'
'141'
['194', '3', '101', '37', '141']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid ="140" ssid = "53">Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.</S><S sid ="144" ssid = "57">We obtained best results with a model using a context window of five words on either side of the target word  the cosine measure  and 2 000 vector components.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="37" ssid = "10">Holographic reduced representations (Plate  1991) are one implementation of this idea where the tensor product is projected back onto the space of its components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'140'", "'144'", "'194'", "'37'"]
'39'
'140'
'144'
'194'
'37'
['39', '140', '144', '194', '37']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="168" ssid = "2">These included three additive models  i.e.  simple addition (equation (5)  Add)  weighted addition (equation (7)  WeightAdd)  and Kintsch’s (2001) model (equation (10)  Kintsch)  a multiplicative model (equation (6)  Multiply)  and also a model which combines multiplication with addition (equation (11)  Combined).</S><S sid ="35" ssid = "8">The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely  the tensor product has dimensionality m x n).</S><S sid ="144" ssid = "57">We obtained best results with a model using a context window of five words on either side of the target word  the cosine measure  and 2 000 vector components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'194'", "'168'", "'35'", "'144'"]
'108'
'194'
'168'
'35'
'144'
['108', '194', '168', '35', '144']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid ="117" ssid = "30">This yielded a reduced set of experimental items (120 in total) consisting of 15 reference verbs  each with 4 nouns  and 2 landmarks.</S><S sid ="35" ssid = "8">The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely  the tensor product has dimensionality m x n).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'117'", "'35'", "'194'", "'108'"]
'39'
'117'
'35'
'194'
'108'
['39', '117', '35', '194', '108']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="153" ssid = "66">Specifically  we considered eleven models  varying in their weightings  in steps of 10%  from 100% noun through 50% of both verb and noun to 100% verb.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'101'", "'3'", "'153'", "'194'"]
'108'
'101'
'3'
'153'
'194'
['108', '101', '3', '153', '194']
parsed_discourse_facet ['method_citation']
<S sid ="148" ssid = "61">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S><S sid ="177" ssid = "11">The difference between High and Low similarity values estimated by these models are statistically significant (p < 0.01 using the Wilcoxon rank sum test).</S><S sid ="4" ssid = "4">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S><S sid ="127" ssid = "40">Analysis of Similarity Ratings The reliability of the collected judgments is important for our evaluation experiments; we therefore performed several tests to validate the quality of the ratings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'148'", "'164'", "'177'", "'4'", "'127'"]
'148'
'164'
'177'
'4'
'127'
['148', '164', '177', '4', '127']
parsed_discourse_facet ['results_citation']
<S sid ="115" ssid = "28">Each cell recorded the number of times our subjects selected the landmark as compatible with the noun or not.</S><S sid ="131" ssid = "44">A Wilcoxon rank sum test confirmed that the difference is statistically significant (p < 0.01).</S><S sid ="147" ssid = "60">This configuration gave high correlations with the WordSim353 similarity judgments using the cosine measure.</S><S sid ="173" ssid = "7">Model similarities have been estimated using cosine which ranges from 0 to 1  whereas our subjects rated the sentences on a scale from 1 to 7.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'131'", "'147'", "'173'", "'164'"]
'115'
'131'
'147'
'173'
'164'
['115', '131', '147', '173', '164']
parsed_discourse_facet ['method_citation']
<S sid ="140" ssid = "53">Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.</S><S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="2" ssid = "2">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S><S sid ="139" ssid = "52">The semantic space we used in our experiments was built on a lemmatised version of the BNC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'3'", "'101'", "'2'", "'139'"]
'140'
'3'
'101'
'2'
'139'
['140', '3', '101', '2', '139']
parsed_discourse_facet ['method_citation']
<S sid ="115" ssid = "28">Each cell recorded the number of times our subjects selected the landmark as compatible with the noun or not.</S><S sid ="147" ssid = "60">This configuration gave high correlations with the WordSim353 similarity judgments using the cosine measure.</S><S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid ="173" ssid = "7">Model similarities have been estimated using cosine which ranges from 0 to 1  whereas our subjects rated the sentences on a scale from 1 to 7.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'147'", "'39'", "'173'", "'164'"]
'115'
'147'
'39'
'173'
'164'
['115', '147', '39', '173', '164']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S><S sid ="153" ssid = "66">Specifically  we considered eleven models  varying in their weightings  in steps of 10%  from 100% noun through 50% of both verb and noun to 100% verb.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="183" ssid = "17">The weighted additive model (p = 0.09) is not significantly different from the baseline either or Kintsch (2001) (p = 0.09).</S><S sid ="40" ssid = "13">Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'153'", "'194'", "'183'", "'40'"]
'38'
'153'
'194'
'183'
'40'
['38', '153', '194', '183', '40']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/D09-1092_vardha.csv
<S sid="17" ssid="13">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
 <S sid="20" ssid="16">We also explore how the characteristics of different languages affect topic model performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
    <S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
   <S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'"]
'55'
['55']
parsed_discourse_facet ['method_citation']
 sid="9" ssid="5">In this paper, we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
Error in Reference Offset
<S sid="118" ssid="67">We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
 <S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'"]
'55'
['55']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="51">In contrast, PLTM assigns a significant number of tokens to almost all 800 topics, in very similar proportions in both languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'"]
'102'
['102']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="4">This is unlike LDA, in which each document is assumed to have its own document-specific distribution over topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'"]
'38'
['38']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="105">We use both Jensen-Shannon divergence and cosine distance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'"]
'156'
['156']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="2">Each tuple is a set of documents that are loosely equivalent to each other, but written in different languages, e.g., corresponding Wikipedia articles in French, English and German.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
  <S sid="170" ssid="4">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
    <S sid="29" ssid="5">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
  <S sid="170" ssid="4">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="168" ssid="2">However, the growth of the web, and in particular Wikipedia, has made comparable text corpora &#8211; documents that are topically similar but are not direct translations of one another &#8211; considerably more abundant than true parallel corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'168'"]
'168'
['168']
parsed_discourse_facet ['method_citation']
    <S sid="29" ssid="5">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
 <S sid="110" ssid="59">Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'"]
'110'
['110']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'4'", "'134'", "'54'", "'35'"]
'131'
'4'
'134'
'54'
'35'
['131', '4', '134', '54', '35']
parsed_discourse_facet ['results_citation']
<S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="6" ssid = "2">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'148'", "'146'", "'4'", "'131'", "'6'"]
'148'
'146'
'4'
'131'
'6'
['148', '146', '4', '131', '6']
parsed_discourse_facet ['method_citation']
<S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of “objective.” The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S>
original cit marker offset is 0
new cit marker offset is 0



["'146'", "'131'", "'148'", "'25'", "'69'"]
'146'
'131'
'148'
'25'
'69'
['146', '131', '148', '25', '69']
parsed_discourse_facet ['method_citation']
<S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="195" ssid = "4">Additionally  PLTM can support the creation of bilingual lexica for low resource language pairs  providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'146'", "'195'", "'4'", "'19'"]
'25'
'146'
'195'
'4'
'19'
['25', '146', '195', '4', '19']
parsed_discourse_facet ['method_citation']
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="47" ssid = "13">We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="86" ssid = "35">Given a set of training document tuples  PLTM can be used to obtain posterior estimates of Φ'  ...   ΦL and αm.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'47'", "'148'", "'86'", "'35'"]
'131'
'47'
'148'
'86'
'35'
['131', '47', '148', '86', '35']
parsed_discourse_facet ['method_citation']
<S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S><S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="182" ssid = "16">As with EuroParl  we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.</S><S sid ="37" ssid = "3">PLTM assumes that the documents in a tuple share the same tuple-specific distribution over topics.</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'161'", "'59'", "'182'", "'37'", "'91'"]
'161'
'59'
'182'
'37'
'91'
['161', '59', '182', '37', '91']
parsed_discourse_facet ['method_citation']
<S sid ="158" ssid = "107">Results averaged over all query/target language pairs are shown in figure 7 for Jensen-Shannon divergence.</S><S sid ="187" ssid = "21">To demonstrate the wide variation in topics  we calculated the proportion of tokens in each language assigned to each topic.</S><S sid ="95" ssid = "44">Additionally  the predictive ability of PLTM is consistently slightly worse than that of (monolingual) LDA.</S><S sid ="72" ssid = "21">To answer this question  we compute the posterior probability of each topic in each tuple under the trained model.</S><S sid ="154" ssid = "103">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al.  2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'", "'187'", "'95'", "'72'", "'154'"]
'158'
'187'
'95'
'72'
'154'
['158', '187', '95', '72', '154']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'131'", "'35'", "'54'", "'91'"]
'134'
'131'
'35'
'54'
'91'
['134', '131', '35', '54', '91']
parsed_discourse_facet ['method_citation']
<S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="86" ssid = "35">Given a set of training document tuples  PLTM can be used to obtain posterior estimates of Φ'  ...   ΦL and αm.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S><S sid ="46" ssid = "12">  ΦL and αm from P(Φ1  ...   ΦL  αm  |W'  β) or by evaluating a point estimate.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'", "'86'", "'131'", "'91'", "'46'"]
'35'
'86'
'131'
'91'
'46'
['35', '86', '131', '91', '46']
parsed_discourse_facet ['method_citation']
<S sid ="67" ssid = "16">(Interestingly  all languages except Greek and Finnish use closely related words for “youth” or “young” in a separate topic.)</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S><S sid ="36" ssid = "2">Each tuple is a set of documents that are loosely equivalent to each other  but written in different languages  e.g.  corresponding Wikipedia articles in French  English and German.</S><S sid ="65" ssid = "14">This topic provides an illustration of the variation in technical terminology captured by PLTM  including the wide array of acronyms used by different languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'54'", "'38'", "'36'", "'65'"]
'67'
'54'
'38'
'36'
'65'
['67', '54', '38', '36', '65']
parsed_discourse_facet ['method_citation']
<S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="143" ssid = "92">We also do not count morphological variants: the model finds EN “rules” and DE “vorschriften ” but the lexicon contains only “rule” and “vorschrift.” Results remain strong as we increase K. With K = 3  T = 800  1349 of the 7200 candidate pairs for Spanish appeared in the lexicon. topic in different languages translations of each other?</S><S sid ="39" ssid = "5">Additionally  PLTM assumes that each “topic” consists of a set of discrete distributions over words—one for each language l = 1  ...   L. In other words  rather than using a single set of topics Φ = {φ1  ...   φT}  as in LDA  there are L sets of language-specific topics  Φ1  ...   ΦL  each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter βl.</S><S sid ="78" ssid = "27">Although the model does not distinguish between topic assignment variables within a given document tuple (so it is technically incorrect to speak of different posterior distributions over topics for different documents in a given tuple)  we can nevertheless divide topic assignment variables between languages and use them to estimate a Dirichlet-multinomial posterior distribution for each language in each tuple.</S><S sid ="40" ssid = "6">Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter α and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl ∼ P(wl  |zl Φl) = 11n φlwl |zl .</S>
original cit marker offset is 0
new cit marker offset is 0



["'148'", "'143'", "'39'", "'78'", "'40'"]
'148'
'143'
'39'
'78'
'40'
['148', '143', '39', '78', '40']
parsed_discourse_facet ['method_citation']
<S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="68" ssid = "17">The third topic demonstrates differences in inflectional variation.</S><S sid ="193" ssid = "2">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="129" ssid = "78">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'", "'68'", "'193'", "'25'", "'129'"]
'52'
'68'
'193'
'25'
'129'
['52', '68', '193', '25', '129']
parsed_discourse_facet ['method_citation']
<S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="92" ssid = "41">We perform five estimation runs for each document and then calculate standard errors using a bootstrap method.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'43'", "'52'", "'25'", "'92'"]
'147'
'43'
'52'
'25'
'92'
['147', '43', '52', '25', '92']
parsed_discourse_facet ['results_citation']
<S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="47" ssid = "13">We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'43'", "'148'", "'47'", "'131'"]
'25'
'43'
'148'
'47'
'131'
['25', '43', '148', '47', '131']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="55" ssid = "4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish  German  Greek  English  Spanish  Finnish  French  Italian  Dutch  Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'35'", "'134'", "'131'", "'55'"]
'4'
'35'
'134'
'131'
'55'
['4', '35', '134', '131', '55']
parsed_discourse_facet ['method_citation']
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'134'", "'4'", "'35'", "'25'"]
'131'
'134'
'4'
'35'
'25'
['131', '134', '4', '35', '25']
parsed_discourse_facet ['method_citation']
<S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S><S sid ="164" ssid = "113">Results vary by language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'161'", "'134'", "'91'", "'164'"]
'59'
'161'
'134'
'91'
'164'
['59', '161', '134', '91', '164']
parsed_discourse_facet ['method_citation']
<S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="195" ssid = "4">Additionally  PLTM can support the creation of bilingual lexica for low resource language pairs  providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S><S sid ="24" ssid = "20">By linking topics across languages  polylingual topic models can increase cross-cultural understanding by providing readers with the ability to characterize the contents of collections in unfamiliar languages and identify trends in topic prevalence.</S><S sid ="83" ssid = "32">The vast majority of divergences are relatively low (1.0 indicates no overlap in topics between languages in a given document tuple) indicating that  for each tuple  the model is not simply assigning all tokens in a particular language to a single topic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'146'", "'195'", "'24'", "'83'"]
'25'
'146'
'195'
'24'
'83'
['25', '146', '195', '24', '83']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid ="47" ssid = "13">We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="120" ssid = "69">From the results in figure 4  we know that leaving all document tuples intact should result in a mean JS divergence of less than 0.1.</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'47'", "'25'", "'120'", "'148'"]
'43'
'47'
'25'
'120'
'148'
['43', '47', '25', '120', '148']
parsed_discourse_facet ['aim_citation', 'results_citation']
parsing: input/ref/Task1/P08-1102_sweta.csv
<S sid="21" ssid="17">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="24">A subsequence of boundary-POS labelling result indicates a word with POS t only if the boundary tag sequence composed of its boundary part conforms to s or bm*e style, and all POS tags in its POS part equal to t. For example, a tag sequence b NN m NN e NN represents a threecharacter word with POS tag NN.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="43" ssid="15">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["43'"]
43'
['43']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["92'"]
92'
['92']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="5">To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&amp;T).</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="5">In following subsections, we describe the feature templates and the perceptron training algorithm.</S>
    <S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["33'", "'34'"]
33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="4">By maintaining a stack of size N at each position i of the sequence, we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S>
original cit marker offset is 0
new cit marker offset is 0



["79'"]
79'
['79']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="7">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["96'"]
96'
['96']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="16" ssid="12">Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="7">To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="22" ssid = "18">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here  Ci (i = L.n) denotes Chinese character  ti (i = L.m) denotes POS tag  and Cl:r (l < r) denotes character sequence ranges from Cl to Cr.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="104" ssid = "15">According to the usual practice in syntactic analysis  we choose chapters 1 − 260 (18074 sentences) as training set  chapter 271 − 300 (348 sentences) as test set and chapter 301 − 325 (350 sentences) as development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'", "'22'", "'16'", "'94'", "'104'"]
'87'
'22'
'16'
'94'
'104'
['87', '22', '16', '94', '104']
parsed_discourse_facet ['results_citation']
<S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'47'", "'48'", "'32'", "'133'"]
'69'
'47'
'48'
'32'
'133'
['69', '47', '48', '32', '133']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="99" ssid = "10">Then we trained LEX on each of the four corpora for 7 iterations.</S><S sid ="30" ssid = "2">It has comparable performance to CRFs  while with much faster training.</S><S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="122" ssid = "33">Another important feature is the labelling model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'99'", "'30'", "'101'", "'122'"]
'68'
'99'
'30'
'101'
'122'
['68', '99', '30', '101', '122']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="30" ssid = "2">It has comparable performance to CRFs  while with much faster training.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'87'", "'31'", "'94'", "'30'"]
'101'
'87'
'31'
'94'
'30'
['101', '87', '31', '94', '30']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="0" ssid = "0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'0'", "'87'", "'69'", "'48'"]
'47'
'0'
'87'
'69'
'48'
['47', '0', '87', '69', '48']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'48'", "'49'", "'133'", "'32'"]
'47'
'48'
'49'
'133'
'32'
['47', '48', '49', '133', '32']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'68'", "'94'", "'87'", "'7'"]
'16'
'68'
'94'
'87'
'7'
['16', '68', '94', '87', '7']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "19">We find that Joint S&T can also improve the segmentation accuracy.</S><S sid ="138" ssid = "1">This work was done while L. H. was visiting CAS/ICT.</S><S sid ="61" ssid = "12">In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.</S><S sid ="90" ssid = "1">We reported results from two set of experiments.</S><S sid ="107" ssid = "18">The evaluation results are shown in Table 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'138'", "'61'", "'90'", "'107'"]
'108'
'138'
'61'
'90'
'107'
['108', '138', '61', '90', '107']
parsed_discourse_facet ['method_citation']
<S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S><S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'133'", "'32'", "'69'", "'49'"]
'48'
'133'
'32'
'69'
'49'
['48', '133', '32', '69', '49']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'7'", "'49'", "'69'", "'16'"]
'47'
'7'
'49'
'69'
'16'
['47', '7', '49', '69', '16']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="54" ssid = "5">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S><S sid ="64" ssid = "15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid ="135" ssid = "6">In addition  all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus  whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'54'", "'64'", "'58'", "'135'"]
'2'
'54'
'64'
'58'
'135'
['2', '54', '64', '58', '135']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "11">Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS  CityU and MSR).</S><S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S><S sid ="121" ssid = "32">Among other features  the 4-gram POS LM plays the most important role  removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'47'", "'134'", "'32'", "'121'"]
'100'
'47'
'134'
'32'
'121'
['100', '47', '134', '32', '121']
parsed_discourse_facet ['method_citation']
<S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="52" ssid = "3">However  such features are generated dynamically during the decoding procedure so that the feature space enlarges much more rapidly.</S><S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="131" ssid = "2">Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.</S><S sid ="135" ssid = "6">In addition  all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus  whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'52'", "'2'", "'131'", "'135'"]
'133'
'52'
'2'
'131'
'135'
['133', '52', '2', '131', '135']
parsed_discourse_facet ['results_citation']
<S sid ="79" ssid = "4">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid ="73" ssid = "24">For instance  if the word w appears N times in training corpus and is labelled as POS t for n times  the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.</S><S sid ="98" ssid = "9">We found that LEX outperforms NON-LEX with a margin of about 0.002 at each iteration  and its learning curve reaches a tableland at iteration 7.</S><S sid ="37" ssid = "9">C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).</S><S sid ="70" ssid = "21">Given a training corpus with POS tags  we can train a word-POS co-occurrence model to approximate the probability that the word sequence of the labelled result co-exists with its corresponding POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'", "'73'", "'98'", "'37'", "'70'"]
'79'
'73'
'98'
'37'
'70'
['79', '73', '98', '37', '70']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="103" ssid = "14">We turned to experiments on CTB 5.0 to test the performance of the cascaded model.</S><S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'87'", "'103'", "'68'", "'101'"]
'16'
'87'
'103'
'68'
'101'
['16', '87', '103', '68', '101']
parsed_discourse_facet ['method_citation']
<S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="22" ssid = "18">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here  Ci (i = L.n) denotes Chinese character  ti (i = L.m) denotes POS tag  and Cl:r (l < r) denotes character sequence ranges from Cl to Cr.</S><S sid ="91" ssid = "2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2  including the Academia Sinica Corpus (AS)  the Hong Kong City University Corpus (CityU)  the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'", "'22'", "'91'", "'45'", "'16'"]
'31'
'22'
'91'
'45'
'16'
['31', '22', '91', '45', '16']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/A97-1014_vardha.csv
<S sid="72" ssid="17">In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
 <S sid="144" ssid="25">We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="4">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
  <S sid="36" ssid="26">As for free word order languages, the following features may cause problems: sition between the two poles.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
 <S sid="24" ssid="14">The typical treebank architecture is as follows: Structures: A context-free backbone is augmented with trace-filler representations of non-local dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
 <S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'"]
'151'
['151']
parsed_discourse_facet ['method_citation']
 <S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
 <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["'167'"]
'167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'"]
'39'
['39']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'"]
'151'
['151']
parsed_discourse_facet ['method_citation']
<S sid="71" ssid="16">However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'71'"]
'71'
['71']
parsed_discourse_facet ['method_citation']
 <S sid="144" ssid="25">We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A97-1014.csv
<S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="81" ssid = "26">During the second annotation stage  the annotation is enriched with information about thematic roles  quantifier scope and anaphoric reference.</S><S sid ="79" ssid = "24">PM stands for morphological particle  a label for German infinitival Z7t and superlative am.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'81'", "'79'", "'137'", "'162'"]
'4'
'81'
'79'
'137'
'162'
['4', '81', '79', '137', '162']
parsed_discourse_facet ['results_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="79" ssid = "24">PM stands for morphological particle  a label for German infinitival Z7t and superlative am.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="140" ssid = "21">For the implementation  we used Tcl/Tk Version 4.1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'4'", "'79'", "'58'", "'140'"]
'80'
'4'
'79'
'58'
'140'
['80', '4', '79', '58', '140']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'4'", "'137'", "'56'", "'149'"]
'80'
'4'
'137'
'56'
'149'
['80', '4', '137', '56', '149']
parsed_discourse_facet ['method_citation']
<S sid ="129" ssid = "10">In the second phase  secondary links and additional structural functions are supported.</S><S sid ="140" ssid = "21">For the implementation  we used Tcl/Tk Version 4.1.</S><S sid ="141" ssid = "22">The corpus is stored in a SQL database.</S><S sid ="116" ssid = "29">This extra marking makes it easy to distinguish between 'normal' and coordinated categories.</S><S sid ="105" ssid = "18">In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'129'", "'140'", "'141'", "'116'", "'105'"]
'129'
'140'
'141'
'116'
'105'
['129', '140', '141', '116', '105']
parsed_discourse_facet ['method_citation']
<S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="128" ssid = "9">In the first phase  the main functionality for building and displaying unordered trees is supplied.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="105" ssid = "18">In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'137'", "'128'", "'56'", "'105'"]
'74'
'137'
'128'
'56'
'105'
['74', '137', '128', '56', '105']
parsed_discourse_facet ['method_citation']
<S sid ="55" ssid = "45">A uniform representation of local and non-local dependencies makes the structure more transparent'.</S><S sid ="53" ssid = "43">A tree meeting these requirements is given below: Adv V NP NP V CPL NP V damn wird ihn Anna erkennen  dais er 'vein!</S><S sid ="120" ssid = "1">The development of linguistically interpreted corpora presents a laborious and time-consuming task.</S><S sid ="65" ssid = "10">The tree resembles traditional constituent structures.</S><S sid ="14" ssid = "4">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'53'", "'120'", "'65'", "'14'"]
'55'
'53'
'120'
'65'
'14'
['55', '53', '120', '65', '14']
parsed_discourse_facet ['method_citation']
<S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'58'", "'80'", "'149'", "'56'"]
'74'
'58'
'80'
'149'
'56'
['74', '58', '80', '149', '56']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'74'", "'56'", "'58'", "'149'"]
'80'
'74'
'56'
'58'
'149'
['80', '74', '56', '58', '149']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'56'", "'58'", "'137'", "'149'"]
'80'
'56'
'58'
'137'
'149'
['80', '56', '58', '137', '149']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.</S><S sid ="88" ssid = "1">As theory-independence is one of our objectives  the annotation scheme incorporates a number of widely accepted linguistic analyses  especially in the area of verbal  adverbial and adjectival syntax.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="164" ssid = "6">As modern linguistics is also becoming more aware of the importance of larger sets of naturally occurring data.  interpreted corpora. are a. valuable resource for theoretical and descriptive linguistic research.</S><S sid ="168" ssid = "10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'88'", "'5'", "'164'", "'168'"]
'3'
'88'
'5'
'164'
'168'
['3', '88', '5', '164', '168']
parsed_discourse_facet ['method_citation']
<S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'137'", "'74'", "'56'", "'149'"]
'58'
'137'
'74'
'56'
'149'
['58', '137', '74', '56', '149']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="101" ssid = "14">All components of this kernel are assigned the label NK and treated as sibling nodes.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'56'", "'101'", "'4'", "'149'"]
'80'
'56'
'101'
'4'
'149'
['80', '56', '101', '4', '149']
parsed_discourse_facet ['method_citation']
<S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'74'", "'58'", "'149'", "'80'"]
'56'
'74'
'58'
'149'
'80'
['56', '74', '58', '149', '80']
parsed_discourse_facet ['results_citation']
<S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="105" ssid = "18">In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'149'", "'56'", "'105'", "'137'", "'74'"]
'149'
'56'
'105'
'137'
'74'
['149', '56', '105', '137', '74']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
parsing: input/ref/Task1/P08-1102_aakansha.csv
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="14">As predications generated from such templates depend on the current character, we name these templates lexical-target.</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="121" ssid="32">Among other features, the 4-gram POS LM plays the most important role, removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'121'"]
'121'
['121']
parsed_discourse_facet ['method_citation']
<S sid="37" ssid="9">C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'"]
'37'
['37']
parsed_discourse_facet ['method_citation']
<S sid="73" ssid="24">For instance, if the word w appears N times in training corpus and is labelled as POS t for n times, the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'73'"]
'73'
['73']
parsed_discourse_facet ['method_citation']
<S sid="46" ssid="18">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) &#8712; X &#215; Y to a feature vector 4)(x, y) &#8712; Rd, and a parameter vector &#945;&#65533; &#8712; Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'"]
'46'
['46']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="22" ssid = "18">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here  Ci (i = L.n) denotes Chinese character  ti (i = L.m) denotes POS tag  and Cl:r (l < r) denotes character sequence ranges from Cl to Cr.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="104" ssid = "15">According to the usual practice in syntactic analysis  we choose chapters 1 − 260 (18074 sentences) as training set  chapter 271 − 300 (348 sentences) as test set and chapter 301 − 325 (350 sentences) as development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'", "'22'", "'16'", "'94'", "'104'"]
'87'
'22'
'16'
'94'
'104'
['87', '22', '16', '94', '104']
parsed_discourse_facet ['results_citation']
<S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'47'", "'48'", "'32'", "'133'"]
'69'
'47'
'48'
'32'
'133'
['69', '47', '48', '32', '133']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="99" ssid = "10">Then we trained LEX on each of the four corpora for 7 iterations.</S><S sid ="30" ssid = "2">It has comparable performance to CRFs  while with much faster training.</S><S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="122" ssid = "33">Another important feature is the labelling model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'99'", "'30'", "'101'", "'122'"]
'68'
'99'
'30'
'101'
'122'
['68', '99', '30', '101', '122']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="30" ssid = "2">It has comparable performance to CRFs  while with much faster training.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'87'", "'31'", "'94'", "'30'"]
'101'
'87'
'31'
'94'
'30'
['101', '87', '31', '94', '30']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="0" ssid = "0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'0'", "'87'", "'69'", "'48'"]
'47'
'0'
'87'
'69'
'48'
['47', '0', '87', '69', '48']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'48'", "'49'", "'133'", "'32'"]
'47'
'48'
'49'
'133'
'32'
['47', '48', '49', '133', '32']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'68'", "'94'", "'87'", "'7'"]
'16'
'68'
'94'
'87'
'7'
['16', '68', '94', '87', '7']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "19">We find that Joint S&T can also improve the segmentation accuracy.</S><S sid ="138" ssid = "1">This work was done while L. H. was visiting CAS/ICT.</S><S sid ="61" ssid = "12">In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.</S><S sid ="90" ssid = "1">We reported results from two set of experiments.</S><S sid ="107" ssid = "18">The evaluation results are shown in Table 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'138'", "'61'", "'90'", "'107'"]
'108'
'138'
'61'
'90'
'107'
['108', '138', '61', '90', '107']
parsed_discourse_facet ['method_citation']
<S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S><S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'133'", "'32'", "'69'", "'49'"]
'48'
'133'
'32'
'69'
'49'
['48', '133', '32', '69', '49']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'7'", "'49'", "'69'", "'16'"]
'47'
'7'
'49'
'69'
'16'
['47', '7', '49', '69', '16']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="54" ssid = "5">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S><S sid ="64" ssid = "15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid ="135" ssid = "6">In addition  all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus  whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'54'", "'64'", "'58'", "'135'"]
'2'
'54'
'64'
'58'
'135'
['2', '54', '64', '58', '135']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "11">Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS  CityU and MSR).</S><S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S><S sid ="121" ssid = "32">Among other features  the 4-gram POS LM plays the most important role  removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'47'", "'134'", "'32'", "'121'"]
'100'
'47'
'134'
'32'
'121'
['100', '47', '134', '32', '121']
parsed_discourse_facet ['method_citation']
<S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="52" ssid = "3">However  such features are generated dynamically during the decoding procedure so that the feature space enlarges much more rapidly.</S><S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="131" ssid = "2">Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.</S><S sid ="135" ssid = "6">In addition  all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus  whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'52'", "'2'", "'131'", "'135'"]
'133'
'52'
'2'
'131'
'135'
['133', '52', '2', '131', '135']
parsed_discourse_facet ['results_citation']
<S sid ="79" ssid = "4">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid ="73" ssid = "24">For instance  if the word w appears N times in training corpus and is labelled as POS t for n times  the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.</S><S sid ="98" ssid = "9">We found that LEX outperforms NON-LEX with a margin of about 0.002 at each iteration  and its learning curve reaches a tableland at iteration 7.</S><S sid ="37" ssid = "9">C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).</S><S sid ="70" ssid = "21">Given a training corpus with POS tags  we can train a word-POS co-occurrence model to approximate the probability that the word sequence of the labelled result co-exists with its corresponding POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'", "'73'", "'98'", "'37'", "'70'"]
'79'
'73'
'98'
'37'
'70'
['79', '73', '98', '37', '70']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="103" ssid = "14">We turned to experiments on CTB 5.0 to test the performance of the cascaded model.</S><S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'87'", "'103'", "'68'", "'101'"]
'16'
'87'
'103'
'68'
'101'
['16', '87', '103', '68', '101']
parsed_discourse_facet ['method_citation']
<S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="22" ssid = "18">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here  Ci (i = L.n) denotes Chinese character  ti (i = L.m) denotes POS tag  and Cl:r (l < r) denotes character sequence ranges from Cl to Cr.</S><S sid ="91" ssid = "2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2  including the Academia Sinica Corpus (AS)  the Hong Kong City University Corpus (CityU)  the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'", "'22'", "'91'", "'45'", "'16'"]
'31'
'22'
'91'
'45'
'16'
['31', '22', '91', '45', '16']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/A97-1014_sweta.csv
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["168'"]
168'
['168']
parsed_discourse_facet ['method_citation']
<S sid="165" ssid="7">In addition the approach provides empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linea.rizations, and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.</S>
original cit marker offset is 0
new cit marker offset is 0



["165'"]
165'
['165']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="26">This amount of data suffices as training material to reliably assign the grammatical functions if the user determines the elements of a phrase and its type (step 1 of the list above).</S>
original cit marker offset is 0
new cit marker offset is 0



["145'"]
145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.<
original cit marker offset is 0
new cit marker offset is 0



["15'"]
15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="31">Apart from this rather technical problem, two further arguments speak against phrase structure as the structural pivot of the annotation scheme: Finally, the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies, which has proved tedious.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
<S sid="47" ssid="37">Argument structure can be represented in terms of unordered trees (with crossing branches).</S>
original cit marker offset is 0
new cit marker offset is 0



["47'"]
47'
['47']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
 <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["160'"]
160'
['160']
parsed_discourse_facet ['method_citation']
<S sid="166" ssid="8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
<S sid="166" ssid="8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="24">Sentences annotated in previous steps are used as training material for further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
  <S sid="71" ssid="16">However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["71'"]
71'
['71']
parsed_discourse_facet ['method_citation']
  <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A97-1014.csv
<S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="81" ssid = "26">During the second annotation stage  the annotation is enriched with information about thematic roles  quantifier scope and anaphoric reference.</S><S sid ="79" ssid = "24">PM stands for morphological particle  a label for German infinitival Z7t and superlative am.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'81'", "'79'", "'137'", "'162'"]
'4'
'81'
'79'
'137'
'162'
['4', '81', '79', '137', '162']
parsed_discourse_facet ['results_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="79" ssid = "24">PM stands for morphological particle  a label for German infinitival Z7t and superlative am.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="140" ssid = "21">For the implementation  we used Tcl/Tk Version 4.1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'4'", "'79'", "'58'", "'140'"]
'80'
'4'
'79'
'58'
'140'
['80', '4', '79', '58', '140']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'4'", "'137'", "'56'", "'149'"]
'80'
'4'
'137'
'56'
'149'
['80', '4', '137', '56', '149']
parsed_discourse_facet ['method_citation']
<S sid ="129" ssid = "10">In the second phase  secondary links and additional structural functions are supported.</S><S sid ="140" ssid = "21">For the implementation  we used Tcl/Tk Version 4.1.</S><S sid ="141" ssid = "22">The corpus is stored in a SQL database.</S><S sid ="116" ssid = "29">This extra marking makes it easy to distinguish between 'normal' and coordinated categories.</S><S sid ="105" ssid = "18">In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'129'", "'140'", "'141'", "'116'", "'105'"]
'129'
'140'
'141'
'116'
'105'
['129', '140', '141', '116', '105']
parsed_discourse_facet ['method_citation']
<S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="128" ssid = "9">In the first phase  the main functionality for building and displaying unordered trees is supplied.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="105" ssid = "18">In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'137'", "'128'", "'56'", "'105'"]
'74'
'137'
'128'
'56'
'105'
['74', '137', '128', '56', '105']
parsed_discourse_facet ['method_citation']
<S sid ="55" ssid = "45">A uniform representation of local and non-local dependencies makes the structure more transparent'.</S><S sid ="53" ssid = "43">A tree meeting these requirements is given below: Adv V NP NP V CPL NP V damn wird ihn Anna erkennen  dais er 'vein!</S><S sid ="120" ssid = "1">The development of linguistically interpreted corpora presents a laborious and time-consuming task.</S><S sid ="65" ssid = "10">The tree resembles traditional constituent structures.</S><S sid ="14" ssid = "4">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'53'", "'120'", "'65'", "'14'"]
'55'
'53'
'120'
'65'
'14'
['55', '53', '120', '65', '14']
parsed_discourse_facet ['method_citation']
<S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'58'", "'80'", "'149'", "'56'"]
'74'
'58'
'80'
'149'
'56'
['74', '58', '80', '149', '56']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'74'", "'56'", "'58'", "'149'"]
'80'
'74'
'56'
'58'
'149'
['80', '74', '56', '58', '149']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'56'", "'58'", "'137'", "'149'"]
'80'
'56'
'58'
'137'
'149'
['80', '56', '58', '137', '149']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.</S><S sid ="88" ssid = "1">As theory-independence is one of our objectives  the annotation scheme incorporates a number of widely accepted linguistic analyses  especially in the area of verbal  adverbial and adjectival syntax.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="164" ssid = "6">As modern linguistics is also becoming more aware of the importance of larger sets of naturally occurring data.  interpreted corpora. are a. valuable resource for theoretical and descriptive linguistic research.</S><S sid ="168" ssid = "10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'88'", "'5'", "'164'", "'168'"]
'3'
'88'
'5'
'164'
'168'
['3', '88', '5', '164', '168']
parsed_discourse_facet ['method_citation']
<S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'137'", "'74'", "'56'", "'149'"]
'58'
'137'
'74'
'56'
'149'
['58', '137', '74', '56', '149']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="101" ssid = "14">All components of this kernel are assigned the label NK and treated as sibling nodes.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'56'", "'101'", "'4'", "'149'"]
'80'
'56'
'101'
'4'
'149'
['80', '56', '101', '4', '149']
parsed_discourse_facet ['method_citation']
<S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'74'", "'58'", "'149'", "'80'"]
'56'
'74'
'58'
'149'
'80'
['56', '74', '58', '149', '80']
parsed_discourse_facet ['results_citation']
<S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="105" ssid = "18">In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'149'", "'56'", "'105'", "'137'", "'74'"]
'149'
'56'
'105'
'137'
'74'
['149', '56', '105', '137', '74']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
parsing: input/ref/Task1/A00-2030_vardha.csv
 <S sid="11" ssid="1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="10">Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'"]
'50'
['50']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
 <S sid="106" ssid="3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
 <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
 <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="2">The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
 <S sid="3" ssid="1">Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'"]
'3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="15">Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'"]
'32'
['32']
parsed_discourse_facet ['method_citation']
 <S sid="10" ssid="8">Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="13">Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
    <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
 <S sid="26" ssid="9">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'"]
'26'
['26']
parsed_discourse_facet ['method_citation']
    <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="2">Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="49" ssid = "9">By necessity  we adopted the strategy of hand marking only the semantics.</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="70" ssid = "11">Post-modifier constituents for the PER/NP.</S><S sid ="73" ssid = "14">We now briefly summarize the probability structure of the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'", "'49'", "'68'", "'70'", "'73'"]
'28'
'49'
'68'
'70'
'73'
['28', '49', '68', '70', '73']
parsed_discourse_facet ['results_citation']
<S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="76" ssid = "17">Finally  word features  fm  for modifiers are predicted based on the modifier  cm  the partof-speech tag of the modifier word   t„„ the part-of-speech tag of the head word th  the head word itself  wh  and whether or not the modifier head word  w„„ is known or unknown.</S><S sid ="39" ssid = "7">For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.</S><S sid ="30" ssid = "13">Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'101'", "'76'", "'39'", "'30'"]
'61'
'101'
'76'
'39'
'30'
['61', '101', '76', '39', '30']
parsed_discourse_facet ['method_citation']
<S sid ="33" ssid = "1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S sid ="41" ssid = "1">To train our integrated model  we required a large corpus of augmented parse trees.</S><S sid ="64" ssid = "5">Word features are introduced primarily to help with unknown words  as in (Weischedel et al. 1993).</S><S sid ="2" ssid = "2">In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'41'", "'64'", "'2'", "'23'"]
'33'
'41'
'64'
'2'
'23'
['33', '41', '64', '2', '23']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'101'", "'81'", "'74'", "'0'"]
'61'
'101'
'81'
'74'
'0'
['61', '101', '81', '74', '0']
parsed_discourse_facet ['method_citation']
<S sid ="60" ssid = "1">In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).</S><S sid ="72" ssid = "13">This generation process is continued until the entire tree has been produced.</S><S sid ="77" ssid = "18">The probability of a complete tree is the product of the probabilities of generating each element in the tree.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'72'", "'77'", "'88'", "'28'"]
'60'
'72'
'77'
'88'
'28'
['60', '72', '77', '88', '28']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="54" ssid = "3">These steps are given below:</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'46'", "'57'", "'54'", "'106'"]
'0'
'46'
'57'
'54'
'106'
['0', '46', '57', '54', '106']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="94" ssid = "13">Given a new sentence  the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S><S sid ="30" ssid = "13">Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.</S><S sid ="60" ssid = "1">In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).</S><S sid ="39" ssid = "7">For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'94'", "'30'", "'60'", "'39'"]
'61'
'94'
'30'
'60'
'39'
['61', '94', '30', '60', '39']
parsed_discourse_facet ['method_citation']
<S sid ="110" ssid = "2">Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.</S><S sid ="41" ssid = "1">To train our integrated model  we required a large corpus of augmented parse trees.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'", "'41'", "'57'", "'51'", "'46'"]
'110'
'41'
'57'
'51'
'46'
['110', '41', '57', '51', '46']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'81'", "'0'", "'61'", "'74'"]
'51'
'81'
'0'
'61'
'74'
['51', '81', '0', '61', '74']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "15">Because generative statistical models had already proven successful for each of the first three stages  we were optimistic that some of their properties — especially their ability to learn from large amounts of data  and their robustness when presented with unexpected inputs — would also benefit semantic analysis.</S><S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="19" ssid = "2">Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'61'", "'19'", "'101'", "'51'"]
'32'
'61'
'19'
'101'
'51'
['32', '61', '19', '101', '51']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="54" ssid = "3">These steps are given below:</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'54'", "'81'", "'57'", "'46'"]
'0'
'54'
'81'
'57'
'46'
['0', '54', '81', '57', '46']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "13">Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'46'", "'105'", "'106'", "'11'"]
'30'
'46'
'105'
'106'
'11'
['30', '46', '105', '106', '11']
parsed_discourse_facet ['method_citation']
<S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="54" ssid = "3">These steps are given below:</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'", "'0'", "'46'", "'57'", "'54'"]
'105'
'0'
'46'
'57'
'54'
['105', '0', '46', '57', '54']
parsed_discourse_facet ['results_citation']
<S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'0'", "'57'", "'51'", "'105'"]
'46'
'0'
'57'
'51'
'105'
['46', '0', '57', '51', '105']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'57'", "'46'", "'51'", "'81'"]
'0'
'57'
'46'
'51'
'81'
['0', '57', '46', '51', '81']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="32" ssid = "15">Because generative statistical models had already proven successful for each of the first three stages  we were optimistic that some of their properties — especially their ability to learn from large amounts of data  and their robustness when presented with unexpected inputs — would also benefit semantic analysis.</S><S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S><S sid ="26" ssid = "9">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993)  and more recently  had begun using a generative statistical model for name finding (Bikel et al.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'32'", "'61'", "'11'", "'26'"]
'51'
'32'
'61'
'11'
'26'
['51', '32', '61', '11', '26']
parsed_discourse_facet ['method_citation']
<S sid ="110" ssid = "2">Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'", "'106'", "'46'", "'57'", "'105'"]
'110'
'106'
'46'
'57'
'105'
['110', '106', '46', '57', '105']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "3">These steps are given below:</S><S sid ="55" ssid = "1">syntactic modifier of the other  the inserted node serves to indicate the relation as well as the argument.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="7" ssid = "5">The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'55'", "'0'", "'62'", "'7'"]
'54'
'55'
'0'
'62'
'7'
['54', '55', '0', '62', '7']
parsed_discourse_facet ['method_citation']
<S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="110" ssid = "2">Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'110'", "'46'", "'105'", "'57'"]
'106'
'110'
'46'
'105'
'57'
['106', '110', '46', '105', '57']
parsed_discourse_facet ['aim_citation', 'results_citation']
parsing: input/ref/Task1/P11-1061_swastika.csv
    <S sid="144" ssid="7">For comparison, the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%, and goes up to 88.7% with a treebank dictionary.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
    <S sid="44" ssid="10">Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.</S
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
parsed_discourse_facet ['method_citation']
    <S sid="16" ssid="12">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (&#167;3), and then use graph label propagation to project syntactic information from English to the foreign language (&#167;4).</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="44" ssid="10">Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.</S
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
parsed_discourse_facet ['method_citation']
<S sid="110" ssid="10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S>
original cit marker offset is 0
new cit marker offset is 0



['110']
110
['110']
parsed_discourse_facet ['aim_citation']
    <S sid="115" ssid="15">The taggers were trained on datasets labeled with the universal tags.</S>
original cit marker offset is 0
new cit marker offset is 0



['115']
115
['115']
parsed_discourse_facet ['method_citation']
    <S sid="115" ssid="15">The taggers were trained on datasets labeled with the universal tags.</S>
original cit marker offset is 0
new cit marker offset is 0



['115']
115
['115']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['158']
158
['158']
parsed_discourse_facet ['result_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



['23']
23
['23']
parsed_discourse_facet ['result_citation']
<S sid="24" ssid="1">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['24']
24
['24']
parsed_discourse_facet ['aim_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['???']
???
['???']
parsed_discourse_facet ['method_citation']
    <S sid="16" ssid="12">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (&#167;3), and then use graph label propagation to project syntactic information from English to the foreign language (&#167;4).</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



['23']
23
['23']
parsed_discourse_facet ['result_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['158']
158
['158']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
<S sid="56" ssid="22">To define a similarity function between the English and the foreign vertices, we rely on high-confidence word alignments.</S>
original cit marker offset is 0
new cit marker offset is 0



['56']
56
['56']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



['161']
161
['161']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P11-1061.csv
<S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="62" ssid = "28">Note that since all English vertices were extracted from the parallel text  we will have an initial label distribution for all vertices in Ve.</S><S sid ="68" ssid = "34">In the label propagation stage  we propagate the automatic English tags to the aligned Italian trigram types  followed by further propagation solely among the Italian vertices. the Italian vertices are connected to an automatically labeled English vertex.</S><S sid ="42" ssid = "8">The foreign language vertices (denoted by Vf) correspond to foreign trigram types  exactly as in Subramanya et al. (2010).</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'62'", "'68'", "'42'", "'160'"]
'19'
'62'
'68'
'42'
'160'
['19', '62', '68', '42', '160']
parsed_discourse_facet ['results_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid ="30" ssid = "7">To initialize the graph we tag the English side of the parallel text using a supervised model.</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'113'", "'30'", "'102'", "'25'"]
'26'
'113'
'30'
'102'
'25'
['26', '113', '30', '102', '25']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'113'", "'25'", "'3'", "'19'"]
'16'
'113'
'25'
'3'
'19'
['16', '113', '25', '3', '19']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="119" ssid = "19">To provide a thorough analysis  we evaluated three baselines and two oracles in addition to two variants of our graph-based approach.</S><S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="82" ssid = "13">We formulate the update as follows: where ∀ui ∈ Vf \ Vfl  γi(y) and κi are defined as: We ran this procedure for 10 iterations.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'119'", "'54'", "'82'", "'28'"]
'134'
'119'
'54'
'82'
'28'
['134', '119', '54', '82', '28']
parsed_discourse_facet ['method_citation']
<S sid ="71" ssid = "2">We use label propagation in two stages to generate soft labels on all the vertices in the graph.</S><S sid ="61" ssid = "27">These tag distributions are used to initialize the label distributions over the English vertices in the graph.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="86" ssid = "17">For a sentence x and a state sequence z  a first order Markov model defines a distribution: (9) where Val(X) corresponds to the entire vocabulary.</S><S sid ="124" ssid = "24">We tried two versions of our graph-based approach: feature after the first stage of label propagation (Eq.</S>
original cit marker offset is 0
new cit marker offset is 0



["'71'", "'61'", "'3'", "'86'", "'124'"]
'71'
'61'
'3'
'86'
'124'
['71', '61', '3', '86', '124']
parsed_discourse_facet ['method_citation']
<S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="57" ssid = "23">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'26'", "'57'", "'97'", "'28'"]
'89'
'26'
'57'
'97'
'28'
['89', '26', '57', '97', '28']
parsed_discourse_facet ['method_citation']
<S sid ="93" ssid = "24">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'93'", "'26'", "'28'", "'97'", "'89'"]
'93'
'26'
'28'
'97'
'89'
['93', '26', '28', '97', '89']
parsed_discourse_facet ['method_citation']
<S sid ="92" ssid = "23">To optimize this function  we used L-BFGS  a quasi-Newton method (Liu and Nocedal  1989).</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="49" ssid = "15">We define a symmetric similarity function K(uZ7 uj) over two foreign language vertices uZ7 uj E Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1.</S><S sid ="142" ssid = "5">The “No LP” model does not outperform direct projection for German and Greek  but performs better for six out of eight languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'102'", "'19'", "'49'", "'142'"]
'92'
'102'
'19'
'49'
'142'
['92', '102', '19', '49', '142']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="159" ssid = "2">Because we are interested in applying our techniques to languages for which no labeled resources are available  we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'159'", "'97'", "'89'", "'28'"]
'26'
'159'
'97'
'89'
'28'
['26', '159', '97', '89', '28']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'97'", "'102'", "'28'", "'83'"]
'134'
'97'
'102'
'28'
'83'
['134', '97', '102', '28', '83']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="72" ssid = "3">In the first stage  we run a single step of label propagation  which transfers the label distributions from the English vertices to the connected foreign language vertices (say  Vf�) at the periphery of the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'97'", "'28'", "'102'", "'72'"]
'26'
'97'
'28'
'102'
'72'
['26', '97', '28', '102', '72']
parsed_discourse_facet ['method_citation']
<S sid ="95" ssid = "26">This feature ft incorporates information from the smoothed graph and prunes hidden states that are inconsistent with the thresholded vector tx.</S><S sid ="15" ssid = "11">First  we use a novel graph-based framework for projecting syntactic information across language boundaries.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid ="41" ssid = "7">Because the information flow in our graph is asymmetric (from English to the foreign language)  we use different types of vertices for each language.</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'15'", "'25'", "'41'", "'113'"]
'95'
'15'
'25'
'41'
'113'
['95', '15', '25', '41', '113']
parsed_discourse_facet ['method_citation']
<S sid ="21" ssid = "17">These universal POS categories not only facilitate the transfer of POS information from one language to another  but also relieve us from using controversial evaluation metrics 2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S><S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="10" ssid = "6">To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S><S sid ="36" ssid = "2">Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand  using individual words as the vertices throws away the context necessary for disambiguation; on the other hand  it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'16'", "'10'", "'29'", "'36'"]
'21'
'16'
'10'
'29'
'36'
['21', '16', '10', '29', '36']
parsed_discourse_facet ['results_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S><S sid ="35" ssid = "1">In graph-based learning approaches one constructs a graph whose vertices are labeled and unlabeled examples  and whose weighted edges encode the degree to which the examples they link have the same label (Zhu et al.  2003).</S><S sid ="159" ssid = "2">Because we are interested in applying our techniques to languages for which no labeled resources are available  we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'54'", "'89'", "'35'", "'159'"]
'26'
'54'
'89'
'35'
'159'
['26', '54', '89', '35', '159']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'28'", "'134'", "'97'", "'89'"]
'26'
'28'
'134'
'97'
'89'
['26', '28', '134', '97', '89']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="21" ssid = "17">These universal POS categories not only facilitate the transfer of POS information from one language to another  but also relieve us from using controversial evaluation metrics 2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'21'", "'160'", "'113'", "'29'"]
'16'
'21'
'160'
'113'
'29'
['16', '21', '160', '113', '29']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'28'", "'97'", "'26'", "'89'"]
'134'
'28'
'97'
'26'
'89'
['134', '28', '97', '26', '89']
parsed_discourse_facet ['method_citation']
<S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="49" ssid = "15">We define a symmetric similarity function K(uZ7 uj) over two foreign language vertices uZ7 uj E Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1.</S><S sid ="136" ssid = "36">For graph propagation  the hyperparameter v was set to 2 x 10−6 and was not tuned.</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'102'", "'49'", "'136'", "'83'"]
'97'
'102'
'49'
'136'
'83'
['97', '102', '49', '136', '83']
parsed_discourse_facet ['method_citation']



P11-1061
P12-3012
0
method_citation
['results_citation']
parsing: input/ref/Task1/W99-0623_vardha.csv
    <S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="117" ssid="46">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'117'"]
'117'
['117']
parsed_discourse_facet ['method_citation']
<S sid="72" ssid="1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
    <S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'"]
'120'
['120']
parsed_discourse_facet ['method_citation']
  <S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'139'"]
'139'
['139']
parsed_discourse_facet ['method_citation']
 <S sid="84" ssid="13">The first shows how constituent features and context do not help in deciding which parser to trust.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'"]
'84'
['84']
parsed_discourse_facet ['method_citation']
    <S sid="76" ssid="5">The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'"]
'76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'"]
'38'
['38']
parsed_discourse_facet ['method_citation']
  <S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
 <S sid="72" ssid="1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
    <S sid="87" ssid="16">It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'"]
'87'
['87']
parsed_discourse_facet ['method_citation']
    <S sid="51" ssid="37">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
    <S sid="79" ssid="8">Precision is the portion of hypothesized constituents that are correct and recall is the portion of the Treebank constituents that are hypothesized.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
  <S sid="27" ssid="13">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'"]
'27'
['27']
parsed_discourse_facet ['method_citation']
 <S sid="77" ssid="6">Each parse is converted into a set of constituents represented as a tuples: (label, start, end).</S>
original cit marker offset is 0
new cit marker offset is 0



["'77'"]
'77'
['77']
parsed_discourse_facet ['method_citation']
  <S sid="11" ssid="7">Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
  <S sid="116" ssid="45">The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'116'"]
'116'
['116']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0623.csv
<S sid ="142" ssid = "4">Both of the switching techniques  as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.</S><S sid ="141" ssid = "3">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'141'", "'18'", "'81'", "'57'"]
'142'
'141'
'18'
'81'
'57'
['142', '141', '18', '81', '57']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'61'", "'81'", "'78'", "'139'"]
'112'
'61'
'81'
'78'
'139'
['112', '61', '81', '78', '139']
Error in Discourse Facet
<S sid ="62" ssid = "48">This is the parse that is closest to the centroid of the observed parses under the similarity metric.</S><S sid ="88" ssid = "17">For example  one parser could be more accurate at predicting noun phrases than the other parsers.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid ="115" ssid = "44">It is the performance we could achieve if an omniscient observer told us which parser to pick for each of the sentences.</S><S sid ="22" ssid = "8">If enough parsers suggest that a particular constituent belongs in the parse  we include it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'", "'88'", "'27'", "'115'", "'22'"]
'62'
'88'
'27'
'115'
'22'
['62', '88', '27', '115', '22']
Error in Discourse Facet
<S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="125" ssid = "54">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S><S sid ="41" ssid = "27">IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.</S><S sid ="44" ssid = "30">Each of the constituents must have received at least 1 votes from the k parsers  so a > I1 and 2 — 2k±-1 b > ri-5-111.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'21'", "'125'", "'41'", "'44'"]
'38'
'21'
'125'
'41'
'44'
['38', '21', '125', '41', '44']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'81'", "'15'", "'78'", "'139'"]
'112'
'81'
'15'
'78'
'139'
['112', '81', '15', '78', '139']
Error in Discourse Facet
<S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="136" ssid = "65">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S><S sid ="131" ssid = "60">It was then tested on section 22 of the Treebank in conjunction with the other parsers.</S><S sid ="103" ssid = "32">The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.</S><S sid ="132" ssid = "61">The results of this experiment can be seen in Table 5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'136'", "'131'", "'103'", "'132'"]
'21'
'136'
'131'
'103'
'132'
['21', '136', '131', '103', '132']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'78'", "'81'", "'61'", "'130'"]
'112'
'78'
'81'
'61'
'130'
['112', '78', '81', '61', '130']
Error in Discourse Facet
<S sid ="125" ssid = "54">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="1" ssid = "1">Three state-of-the-art statistical parsers are combined to produce more accurate parses  as well as new bounds on achievable Treebank parsing accuracy.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'27'", "'21'", "'1'", "'38'"]
'125'
'27'
'21'
'1'
'38'
['125', '27', '21', '1', '38']
Error in Discourse Facet
<S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="51" ssid = "37">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes  a completely flat structure.</S><S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'12'", "'51'", "'18'", "'57'"]
'55'
'12'
'51'
'18'
'57'
['55', '12', '51', '18', '57']
Error in Discourse Facet
<S sid ="142" ssid = "4">Both of the switching techniques  as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.</S><S sid ="127" ssid = "56">Parser 3  the most accurate parser  was chosen 71% of the time  and Parser 1  the least accurate parser was chosen 16% of the time.</S><S sid ="141" ssid = "3">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="140" ssid = "2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'127'", "'141'", "'12'", "'140'"]
'142'
'127'
'141'
'12'
'140'
['142', '127', '141', '12', '140']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'78'", "'61'", "'130'", "'15'"]
'112'
'78'
'61'
'130'
'15'
['112', '78', '61', '130', '15']
Error in Discourse Facet
<S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="51" ssid = "37">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes  a completely flat structure.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S><S sid ="26" ssid = "12">This technique has the advantage of requiring no training  but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'51'", "'52'", "'50'", "'26'"]
'55'
'51'
'52'
'50'
'26'
['55', '51', '52', '50', '26']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="114" ssid = "43">The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'81'", "'78'", "'61'", "'114'"]
'112'
'81'
'78'
'61'
'114'
['112', '81', '78', '61', '114']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'81'", "'12'", "'15'", "'130'"]
'112'
'81'
'12'
'15'
'130'
['112', '81', '12', '15', '130']
Error in Discourse Facet
<S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="114" ssid = "43">The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.</S><S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S>
original cit marker offset is 0
new cit marker offset is 0



["'78'", "'61'", "'114'", "'112'", "'81'"]
'78'
'61'
'114'
'112'
'81'
['78', '61', '114', '112', '81']
Error in Discourse Facet
<S sid ="129" ssid = "58">In the interest of testing the robustness of these combining techniques  we added a fourth  simple nonlexicalized PCFG parser.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'129'", "'11'", "'130'", "'15'", "'9'"]
'129'
'11'
'130'
'15'
'9'
['129', '11', '130', '15', '9']
Error in Discourse Facet
<S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S><S sid ="114" ssid = "43">The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.</S><S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["'81'", "'57'", "'114'", "'112'", "'78'"]
'81'
'57'
'114'
'112'
'78'
['81', '57', '114', '112', '78']
Error in Discourse Facet
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



W99-0623
N06-2033
0
method_citation



W99-0623
P01-1005
0
method_citation
parsing: input/ref/Task1/D09-1092_swastika.csv
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="114" ssid="63">Ideally, the &#8220;glue&#8221; documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.</S>
original cit marker offset is 0
new cit marker offset is 0



['114']
114
['114']
parsed_discourse_facet ['aim_citation']
<S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



['138']
138
['138']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



['55']
55
['55']
parsed_discourse_facet ['method_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['192']
192
['192']
parsed_discourse_facet ['result_citation']
<S sid="119" ssid="68">The lower the divergence, the more similar the distributions are to each other.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['result_citation']
<S sid="148" ssid="97">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="163" ssid="112">Performance continues to improve with longer documents, most likely due to better topic inference.</S>
original cit marker offset is 0
new cit marker offset is 0



['163']
163
['163']
parsed_discourse_facet ['result_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="148" ssid="97">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['192']
192
['192']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['method_citation']
    <S sid="10" ssid="6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'4'", "'134'", "'54'", "'35'"]
'131'
'4'
'134'
'54'
'35'
['131', '4', '134', '54', '35']
parsed_discourse_facet ['results_citation']
<S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="6" ssid = "2">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'148'", "'146'", "'4'", "'131'", "'6'"]
'148'
'146'
'4'
'131'
'6'
['148', '146', '4', '131', '6']
parsed_discourse_facet ['method_citation']
<S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of “objective.” The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S>
original cit marker offset is 0
new cit marker offset is 0



["'146'", "'131'", "'148'", "'25'", "'69'"]
'146'
'131'
'148'
'25'
'69'
['146', '131', '148', '25', '69']
parsed_discourse_facet ['method_citation']
<S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="195" ssid = "4">Additionally  PLTM can support the creation of bilingual lexica for low resource language pairs  providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'146'", "'195'", "'4'", "'19'"]
'25'
'146'
'195'
'4'
'19'
['25', '146', '195', '4', '19']
parsed_discourse_facet ['method_citation']
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="47" ssid = "13">We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="86" ssid = "35">Given a set of training document tuples  PLTM can be used to obtain posterior estimates of Φ'  ...   ΦL and αm.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'47'", "'148'", "'86'", "'35'"]
'131'
'47'
'148'
'86'
'35'
['131', '47', '148', '86', '35']
parsed_discourse_facet ['method_citation']
<S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S><S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="182" ssid = "16">As with EuroParl  we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.</S><S sid ="37" ssid = "3">PLTM assumes that the documents in a tuple share the same tuple-specific distribution over topics.</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'161'", "'59'", "'182'", "'37'", "'91'"]
'161'
'59'
'182'
'37'
'91'
['161', '59', '182', '37', '91']
parsed_discourse_facet ['method_citation']
<S sid ="158" ssid = "107">Results averaged over all query/target language pairs are shown in figure 7 for Jensen-Shannon divergence.</S><S sid ="187" ssid = "21">To demonstrate the wide variation in topics  we calculated the proportion of tokens in each language assigned to each topic.</S><S sid ="95" ssid = "44">Additionally  the predictive ability of PLTM is consistently slightly worse than that of (monolingual) LDA.</S><S sid ="72" ssid = "21">To answer this question  we compute the posterior probability of each topic in each tuple under the trained model.</S><S sid ="154" ssid = "103">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al.  2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'", "'187'", "'95'", "'72'", "'154'"]
'158'
'187'
'95'
'72'
'154'
['158', '187', '95', '72', '154']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'131'", "'35'", "'54'", "'91'"]
'134'
'131'
'35'
'54'
'91'
['134', '131', '35', '54', '91']
parsed_discourse_facet ['method_citation']
<S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="86" ssid = "35">Given a set of training document tuples  PLTM can be used to obtain posterior estimates of Φ'  ...   ΦL and αm.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S><S sid ="46" ssid = "12">  ΦL and αm from P(Φ1  ...   ΦL  αm  |W'  β) or by evaluating a point estimate.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'", "'86'", "'131'", "'91'", "'46'"]
'35'
'86'
'131'
'91'
'46'
['35', '86', '131', '91', '46']
parsed_discourse_facet ['method_citation']
<S sid ="67" ssid = "16">(Interestingly  all languages except Greek and Finnish use closely related words for “youth” or “young” in a separate topic.)</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S><S sid ="36" ssid = "2">Each tuple is a set of documents that are loosely equivalent to each other  but written in different languages  e.g.  corresponding Wikipedia articles in French  English and German.</S><S sid ="65" ssid = "14">This topic provides an illustration of the variation in technical terminology captured by PLTM  including the wide array of acronyms used by different languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'54'", "'38'", "'36'", "'65'"]
'67'
'54'
'38'
'36'
'65'
['67', '54', '38', '36', '65']
parsed_discourse_facet ['method_citation']
<S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="143" ssid = "92">We also do not count morphological variants: the model finds EN “rules” and DE “vorschriften ” but the lexicon contains only “rule” and “vorschrift.” Results remain strong as we increase K. With K = 3  T = 800  1349 of the 7200 candidate pairs for Spanish appeared in the lexicon. topic in different languages translations of each other?</S><S sid ="39" ssid = "5">Additionally  PLTM assumes that each “topic” consists of a set of discrete distributions over words—one for each language l = 1  ...   L. In other words  rather than using a single set of topics Φ = {φ1  ...   φT}  as in LDA  there are L sets of language-specific topics  Φ1  ...   ΦL  each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter βl.</S><S sid ="78" ssid = "27">Although the model does not distinguish between topic assignment variables within a given document tuple (so it is technically incorrect to speak of different posterior distributions over topics for different documents in a given tuple)  we can nevertheless divide topic assignment variables between languages and use them to estimate a Dirichlet-multinomial posterior distribution for each language in each tuple.</S><S sid ="40" ssid = "6">Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter α and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl ∼ P(wl  |zl Φl) = 11n φlwl |zl .</S>
original cit marker offset is 0
new cit marker offset is 0



["'148'", "'143'", "'39'", "'78'", "'40'"]
'148'
'143'
'39'
'78'
'40'
['148', '143', '39', '78', '40']
parsed_discourse_facet ['method_citation']
<S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="68" ssid = "17">The third topic demonstrates differences in inflectional variation.</S><S sid ="193" ssid = "2">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="129" ssid = "78">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'", "'68'", "'193'", "'25'", "'129'"]
'52'
'68'
'193'
'25'
'129'
['52', '68', '193', '25', '129']
parsed_discourse_facet ['method_citation']
<S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="92" ssid = "41">We perform five estimation runs for each document and then calculate standard errors using a bootstrap method.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'43'", "'52'", "'25'", "'92'"]
'147'
'43'
'52'
'25'
'92'
['147', '43', '52', '25', '92']
parsed_discourse_facet ['results_citation']
<S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="47" ssid = "13">We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'43'", "'148'", "'47'", "'131'"]
'25'
'43'
'148'
'47'
'131'
['25', '43', '148', '47', '131']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="55" ssid = "4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish  German  Greek  English  Spanish  Finnish  French  Italian  Dutch  Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'35'", "'134'", "'131'", "'55'"]
'4'
'35'
'134'
'131'
'55'
['4', '35', '134', '131', '55']
parsed_discourse_facet ['method_citation']
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'134'", "'4'", "'35'", "'25'"]
'131'
'134'
'4'
'35'
'25'
['131', '134', '4', '35', '25']
parsed_discourse_facet ['method_citation']
<S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S><S sid ="164" ssid = "113">Results vary by language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'161'", "'134'", "'91'", "'164'"]
'59'
'161'
'134'
'91'
'164'
['59', '161', '134', '91', '164']
parsed_discourse_facet ['method_citation']
<S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="195" ssid = "4">Additionally  PLTM can support the creation of bilingual lexica for low resource language pairs  providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S><S sid ="24" ssid = "20">By linking topics across languages  polylingual topic models can increase cross-cultural understanding by providing readers with the ability to characterize the contents of collections in unfamiliar languages and identify trends in topic prevalence.</S><S sid ="83" ssid = "32">The vast majority of divergences are relatively low (1.0 indicates no overlap in topics between languages in a given document tuple) indicating that  for each tuple  the model is not simply assigning all tokens in a particular language to a single topic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'146'", "'195'", "'24'", "'83'"]
'25'
'146'
'195'
'24'
'83'
['25', '146', '195', '24', '83']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid ="47" ssid = "13">We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="120" ssid = "69">From the results in figure 4  we know that leaving all document tuples intact should result in a mean JS divergence of less than 0.1.</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'47'", "'25'", "'120'", "'148'"]
'43'
'47'
'25'
'120'
'148'
['43', '47', '25', '120', '148']
parsed_discourse_facet ['aim_citation', 'results_citation']



D09-1092
D10-1025
0
method_citation
['method_citation']
parsing: input/ref/Task1/D09-1092_sweta.csv
 <S sid="32" ssid="8">Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="5">Additionally, PLTM assumes that each &#8220;topic&#8221; consists of a set of discrete distributions over words&#8212;one for each language l = 1, ... , L. In other words, rather than using a single set of topics &#934; = {&#966;1, ... , &#966;T}, as in LDA, there are L sets of language-specific topics, &#934;1, ... , &#934;L, each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter &#946;l.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



["138'"]
138'
['138']
parsed_discourse_facet ['method_citation']
 <S sid="196" ssid="5">When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>
original cit marker offset is 0
new cit marker offset is 0



["196'"]
196'
['196']
parsed_discourse_facet ['method_citation']
<S sid="111" ssid="60">In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts &#8211; i.e., we put each of these documents in a single-document tuple.</S>
original cit marker offset is 0
new cit marker offset is 0



["111'"]
111'
['111']
parsed_discourse_facet ['method_citation']
<S sid="128" ssid="77">Although the PLTM is clearly not a substitute for a machine translation system&#8212;it has no way to represent syntax or even multi-word phrases&#8212;it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.</S>
original cit marker offset is 0
new cit marker offset is 0



["128'"]
128'
['128']
parsed_discourse_facet ['method_citation']
<S sid="122" ssid="71">Divergence drops significantly when the proportion of &#8220;glue&#8221; tuples increases from 0.01 to 0.25.</S>
original cit marker offset is 0
new cit marker offset is 0



["122'"]
122'
['122']
parsed_discourse_facet ['method_citation']
 <S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
 <S sid="110" ssid="59">Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S>
original cit marker offset is 0
new cit marker offset is 0



["110'"]
110'
['110']
parsed_discourse_facet ['method_citation']
<S sid="30" ssid="6">However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["30'"]
30'
['30']
parsed_discourse_facet ['method_citation']
    <S sid="146" ssid="95">In addition to enhancing lexicons by aligning topic-specific vocabulary, PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["146'"]
146'
['146']
parsed_discourse_facet ['method_citation']
<S sid="77" ssid="26">Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.</
original cit marker offset is 0
new cit marker offset is 0



["77'"]
77'
['77']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="105">We use both Jensen-Shannon divergence and cosine distance.</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="7">They also provide little analysis of the differences between polylingual and single-language topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>
original cit marker offset is 0
new cit marker offset is 0



["19'"]
19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="131" ssid="80">We evaluate sets of high-probability words in each topic and multilingual &#8220;synsets&#8221; by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["131'"]
131'
['131']
parsed_discourse_facet ['method_citation']
<S sid="196" ssid="5">When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>
original cit marker offset is 0
new cit marker offset is 0



["196'"]
196'
['196']
parsed_discourse_facet ['method_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["192'"]
192'
['192']
parsed_discourse_facet ['method_citation']
<S sid="195" ssid="4">Additionally, PLTM can support the creation of bilingual lexica for low resource language pairs, providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["195'"]
195'
['195']
parsed_discourse_facet ['method_citation']
<S sid="6" ssid="2">Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["6'"]
6'
['6']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'4'", "'134'", "'54'", "'35'"]
'131'
'4'
'134'
'54'
'35'
['131', '4', '134', '54', '35']
parsed_discourse_facet ['results_citation']
<S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="6" ssid = "2">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'148'", "'146'", "'4'", "'131'", "'6'"]
'148'
'146'
'4'
'131'
'6'
['148', '146', '4', '131', '6']
parsed_discourse_facet ['method_citation']
<S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of “objective.” The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S>
original cit marker offset is 0
new cit marker offset is 0



["'146'", "'131'", "'148'", "'25'", "'69'"]
'146'
'131'
'148'
'25'
'69'
['146', '131', '148', '25', '69']
parsed_discourse_facet ['method_citation']
<S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="195" ssid = "4">Additionally  PLTM can support the creation of bilingual lexica for low resource language pairs  providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'146'", "'195'", "'4'", "'19'"]
'25'
'146'
'195'
'4'
'19'
['25', '146', '195', '4', '19']
parsed_discourse_facet ['method_citation']
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="47" ssid = "13">We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="86" ssid = "35">Given a set of training document tuples  PLTM can be used to obtain posterior estimates of Φ'  ...   ΦL and αm.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'47'", "'148'", "'86'", "'35'"]
'131'
'47'
'148'
'86'
'35'
['131', '47', '148', '86', '35']
parsed_discourse_facet ['method_citation']
<S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S><S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="182" ssid = "16">As with EuroParl  we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.</S><S sid ="37" ssid = "3">PLTM assumes that the documents in a tuple share the same tuple-specific distribution over topics.</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'161'", "'59'", "'182'", "'37'", "'91'"]
'161'
'59'
'182'
'37'
'91'
['161', '59', '182', '37', '91']
parsed_discourse_facet ['method_citation']
<S sid ="158" ssid = "107">Results averaged over all query/target language pairs are shown in figure 7 for Jensen-Shannon divergence.</S><S sid ="187" ssid = "21">To demonstrate the wide variation in topics  we calculated the proportion of tokens in each language assigned to each topic.</S><S sid ="95" ssid = "44">Additionally  the predictive ability of PLTM is consistently slightly worse than that of (monolingual) LDA.</S><S sid ="72" ssid = "21">To answer this question  we compute the posterior probability of each topic in each tuple under the trained model.</S><S sid ="154" ssid = "103">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al.  2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'", "'187'", "'95'", "'72'", "'154'"]
'158'
'187'
'95'
'72'
'154'
['158', '187', '95', '72', '154']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'131'", "'35'", "'54'", "'91'"]
'134'
'131'
'35'
'54'
'91'
['134', '131', '35', '54', '91']
parsed_discourse_facet ['method_citation']
<S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="86" ssid = "35">Given a set of training document tuples  PLTM can be used to obtain posterior estimates of Φ'  ...   ΦL and αm.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S><S sid ="46" ssid = "12">  ΦL and αm from P(Φ1  ...   ΦL  αm  |W'  β) or by evaluating a point estimate.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'", "'86'", "'131'", "'91'", "'46'"]
'35'
'86'
'131'
'91'
'46'
['35', '86', '131', '91', '46']
parsed_discourse_facet ['method_citation']
<S sid ="67" ssid = "16">(Interestingly  all languages except Greek and Finnish use closely related words for “youth” or “young” in a separate topic.)</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S><S sid ="36" ssid = "2">Each tuple is a set of documents that are loosely equivalent to each other  but written in different languages  e.g.  corresponding Wikipedia articles in French  English and German.</S><S sid ="65" ssid = "14">This topic provides an illustration of the variation in technical terminology captured by PLTM  including the wide array of acronyms used by different languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'54'", "'38'", "'36'", "'65'"]
'67'
'54'
'38'
'36'
'65'
['67', '54', '38', '36', '65']
parsed_discourse_facet ['method_citation']
<S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="143" ssid = "92">We also do not count morphological variants: the model finds EN “rules” and DE “vorschriften ” but the lexicon contains only “rule” and “vorschrift.” Results remain strong as we increase K. With K = 3  T = 800  1349 of the 7200 candidate pairs for Spanish appeared in the lexicon. topic in different languages translations of each other?</S><S sid ="39" ssid = "5">Additionally  PLTM assumes that each “topic” consists of a set of discrete distributions over words—one for each language l = 1  ...   L. In other words  rather than using a single set of topics Φ = {φ1  ...   φT}  as in LDA  there are L sets of language-specific topics  Φ1  ...   ΦL  each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter βl.</S><S sid ="78" ssid = "27">Although the model does not distinguish between topic assignment variables within a given document tuple (so it is technically incorrect to speak of different posterior distributions over topics for different documents in a given tuple)  we can nevertheless divide topic assignment variables between languages and use them to estimate a Dirichlet-multinomial posterior distribution for each language in each tuple.</S><S sid ="40" ssid = "6">Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter α and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl ∼ P(wl  |zl Φl) = 11n φlwl |zl .</S>
original cit marker offset is 0
new cit marker offset is 0



["'148'", "'143'", "'39'", "'78'", "'40'"]
'148'
'143'
'39'
'78'
'40'
['148', '143', '39', '78', '40']
parsed_discourse_facet ['method_citation']
<S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="68" ssid = "17">The third topic demonstrates differences in inflectional variation.</S><S sid ="193" ssid = "2">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="129" ssid = "78">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'", "'68'", "'193'", "'25'", "'129'"]
'52'
'68'
'193'
'25'
'129'
['52', '68', '193', '25', '129']
parsed_discourse_facet ['method_citation']
<S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="92" ssid = "41">We perform five estimation runs for each document and then calculate standard errors using a bootstrap method.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'43'", "'52'", "'25'", "'92'"]
'147'
'43'
'52'
'25'
'92'
['147', '43', '52', '25', '92']
parsed_discourse_facet ['results_citation']
<S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="47" ssid = "13">We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'43'", "'148'", "'47'", "'131'"]
'25'
'43'
'148'
'47'
'131'
['25', '43', '148', '47', '131']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="55" ssid = "4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish  German  Greek  English  Spanish  Finnish  French  Italian  Dutch  Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'35'", "'134'", "'131'", "'55'"]
'4'
'35'
'134'
'131'
'55'
['4', '35', '134', '131', '55']
parsed_discourse_facet ['method_citation']
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'134'", "'4'", "'35'", "'25'"]
'131'
'134'
'4'
'35'
'25'
['131', '134', '4', '35', '25']
parsed_discourse_facet ['method_citation']
<S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S><S sid ="164" ssid = "113">Results vary by language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'161'", "'134'", "'91'", "'164'"]
'59'
'161'
'134'
'91'
'164'
['59', '161', '134', '91', '164']
parsed_discourse_facet ['method_citation']
<S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="195" ssid = "4">Additionally  PLTM can support the creation of bilingual lexica for low resource language pairs  providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S><S sid ="24" ssid = "20">By linking topics across languages  polylingual topic models can increase cross-cultural understanding by providing readers with the ability to characterize the contents of collections in unfamiliar languages and identify trends in topic prevalence.</S><S sid ="83" ssid = "32">The vast majority of divergences are relatively low (1.0 indicates no overlap in topics between languages in a given document tuple) indicating that  for each tuple  the model is not simply assigning all tokens in a particular language to a single topic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'146'", "'195'", "'24'", "'83'"]
'25'
'146'
'195'
'24'
'83'
['25', '146', '195', '24', '83']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid ="47" ssid = "13">We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="120" ssid = "69">From the results in figure 4  we know that leaving all document tuples intact should result in a mean JS divergence of less than 0.1.</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'47'", "'25'", "'120'", "'148'"]
'43'
'47'
'25'
'120'
'148'
['43', '47', '25', '120', '148']
parsed_discourse_facet ['aim_citation', 'results_citation']



D09-1092
W12-3117
0
method_citation
['method_citation']
parsing: input/ref/Task1/W99-0613_vardha.csv
    <S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="35" ssid="29">AdaBoost finds a weighted combination of simple (weak) classifiers, where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
    <S sid="134" ssid="1">This section describes an algorithm based on boosting algorithms, which were previously developed for supervised machine learning problems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'"]
'134'
['134']
parsed_discourse_facet ['method_citation']
    <S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'"]
'236'
['236']
parsed_discourse_facet ['method_citation']
    <S sid="8" ssid="2">Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
    <S sid="42" ssid="36">(Riloff and Shepherd 97) describe a bootstrapping approach for acquiring nouns in particular categories (such as &amp;quot;vehicle&amp;quot; or &amp;quot;weapon&amp;quot; categories).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
    <S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'"]
'236'
['236']
parsed_discourse_facet ['method_citation']
    <S sid="222" ssid="1">The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'222'"]
'222'
['222']
parsed_discourse_facet ['method_citation']
    <S sid="30" ssid="24">(Blum and Mitchell 98) offer a promising formulation of redundancy, also prove some results about how the use of can help classification, and suggest an objective function when training with unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'"]
'30'
['30']
parsed_discourse_facet ['method_citation']
 <S sid="26" ssid="20">We present two algorithms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'"]
'26'
['26']
parsed_discourse_facet ['method_citation']
    <S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="26">The algorithm can be viewed as heuristically optimizing an objective function suggested by (Blum and Mitchell 98); empirically it is shown to be quite successful in optimizing this criterion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'"]
'32'
['32']
parsed_discourse_facet ['method_citation']
    <S sid="47" ssid="1">971,746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'"]
'47'
['47']
parsed_discourse_facet ['method_citation']
    <S sid="127" ssid="60">The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'"]
'127'
['127']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'228'", "'0'", "'75'", "'77'"]
'142'
'228'
'0'
'75'
'77'
['142', '228', '0', '75', '77']
parsed_discourse_facet ['results_citation']
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="102" ssid = "35">(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'77'", "'102'", "'52'", "'75'"]
'142'
'77'
'102'
'52'
'75'
['142', '77', '102', '52', '75']
parsed_discourse_facet ['method_citation']
<S sid ="225" ssid = "4">We again assume a training set of n examples {x1 . xri} where the first m examples have labels {y1 ... yin}  and the last (n — m) examples are unlabeled.</S><S sid ="203" ssid = "70">Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).</S><S sid ="140" ssid = "7">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S><S sid ="79" ssid = "12">2 We now introduce a new algorithm for learning from unlabeled examples  which we will call DLCoTrain (DL stands for decision list  the term Cotrain is taken from (Blum and Mitchell 98)).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'225'", "'203'", "'140'", "'79'", "'31'"]
'225'
'203'
'140'
'79'
'31'
['225', '203', '140', '79', '31']
parsed_discourse_facet ['method_citation']
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="102" ssid = "35">(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'102'", "'77'", "'52'", "'228'"]
'142'
'102'
'77'
'52'
'228'
['142', '102', '77', '52', '228']
parsed_discourse_facet ['method_citation']
<S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="207" ssid = "74">Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S><S sid ="166" ssid = "33">The first m pairs have labels yi  whereas for i = m + 1    n the pairs are unlabeled.</S><S sid ="84" ssid = "17">For each label (Per s on  organization and Location)  take the n contextual rules with the highest value of Count' (x) whose unsmoothed3 strength is above some threshold pmin.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'", "'207'", "'248'", "'166'", "'84'"]
'236'
'207'
'248'
'166'
'84'
['236', '207', '248', '166', '84']
parsed_discourse_facet ['method_citation']
<S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'228'", "'142'", "'75'", "'77'", "'0'"]
'228'
'142'
'75'
'77'
'0'
['228', '142', '75', '77', '0']
parsed_discourse_facet ['method_citation']
<S sid ="207" ssid = "74">Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S><S sid ="86" ssid = "19">Thus at each iteration the method induces at most n x k rules  where k is the number of possible labels (k = 3 in the experiments in this paper). step 3.</S><S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="10" ssid = "4">The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'248'", "'86'", "'204'", "'10'"]
'207'
'248'
'86'
'204'
'10'
['207', '248', '86', '204', '10']
parsed_discourse_facet ['method_citation']
<S sid ="202" ssid = "69">The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels  and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.</S><S sid ="124" ssid = "57">In particular  it may not be possible to learn functions fi (x f2(x2 t) for i = m + 1...n: either because there is some noise in the data  or because it is just not realistic to expect to learn perfect classifiers given the features used for representation.</S><S sid ="121" ssid = "54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).</S><S sid ="50" ssid = "4">It is a sequence of proper nouns within an NP; its last word Cooper is the head of the NP; and the NP has an appositive modifier (a vice president at S.&P.) whose head is a singular noun (president).</S><S sid ="25" ssid = "19">The unlabeled data gives many such &quot;hints&quot; that two features should predict the same label  and these hints turn out to be surprisingly useful when building a classifier.</S>
original cit marker offset is 0
new cit marker offset is 0



["'202'", "'124'", "'121'", "'50'", "'25'"]
'202'
'124'
'121'
'50'
'25'
['202', '124', '121', '50', '25']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="12" ssid = "6">The approach uses both spelling and contextual rules.</S><S sid ="150" ssid = "17">Pseudo-code describing the generalized boosting algorithm of Schapire and Singer is given in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'52'", "'75'", "'12'", "'150'"]
'157'
'52'
'75'
'12'
'150'
['157', '52', '75', '12', '150']
parsed_discourse_facet ['method_citation']
<S sid ="63" ssid = "17">In this case nonalpha is the string formed by removing all upper/lower case letters from the spelling (e.g.  for Thomas E. Petry nonalpha= .</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="47" ssid = "1">971 746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'52'", "'157'", "'77'", "'47'"]
'63'
'52'
'157'
'77'
'47'
['63', '52', '157', '77', '47']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="239" ssid = "6">Of these cases  38 were temporal expressions (either a day of the week or month of the year).</S><S sid ="93" ssid = "26">To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.</S><S sid ="249" ssid = "16">The test accuracy more or less asymptotes.</S><S sid ="197" ssid = "64">Note that in our formalism a weakhypothesis can abstain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'239'", "'93'", "'249'", "'197'"]
'204'
'239'
'93'
'249'
'197'
['204', '239', '93', '249', '197']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S><S sid ="93" ssid = "26">To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'75'", "'228'", "'0'", "'93'"]
'204'
'75'
'228'
'0'
'93'
['204', '75', '228', '0', '93']
parsed_discourse_facet ['method_citation']
<S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="193" ssid = "60">In Input: {(x1 i  Initialize: Vi  j : e(xi) = 0.</S><S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'", "'157'", "'77'", "'193'", "'142'"]
'52'
'157'
'77'
'193'
'142'
['52', '157', '77', '193', '142']
parsed_discourse_facet ['results_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



W99-0613
C02-1154
0
method_citation
['method_citation']
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W99-0613_sweta.csv
<S sid="121" ssid="54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page, and other pages pointing to the page).</S>
original cit marker offset is 0
new cit marker offset is 0



["121'"]
121'
['121']
parsed_discourse_facet ['method_citation']
<S sid="252" ssid="3">The method uses a &amp;quot;soft&amp;quot; measure of the agreement between two classifiers as an objective function; we described an algorithm which directly optimizes this function.</S>
original cit marker offset is 0
new cit marker offset is 0



["252'"]
252'
['252']
parsed_discourse_facet ['method_citation']
 <S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



["213'"]
213'
['213']
parsed_discourse_facet ['method_citation']
 <S sid="250" ssid="1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S>
original cit marker offset is 0
new cit marker offset is 0



["250'"]
250'
['250']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="33">(Brin 98) ,describes a system for extracting (author, book-title) pairs from the World Wide Web using an approach that bootstraps from an initial seed set of examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="202" ssid="69">The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels, and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.</S>
original cit marker offset is 0
new cit marker offset is 0



["202'"]
202'
['202']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="15">The following features were used: full-string=x The full string (e.g., for Maury Cooper, full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word, this feature applies for any words that the string contains (e.g., Maury Cooper contributes two such features, contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g., IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods, and contains at least one period.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="176" ssid="43">(7) is at 0 when: 1) Vi : sign(gi (xi)) = sign(g2 (xi)); 2) Ig3(xi)l oo; and 3) sign(gi (xi)) = yi for i = 1, , m. In fact, Zco provides a bound on the sum of the classification error of the labeled examples and the number of disagreements between the two classifiers on the unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["176'"]
176'
['176']
parsed_discourse_facet ['method_citation']
 <S sid="108" ssid="41">In the cotraining case, (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples, and (2) must agree with each other on the unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S>
    <S sid="28" ssid="22">(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'", "'28'"]
27'
'28'
['27', '28']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["7'"]
7'
['7']
parsed_discourse_facet ['method_citation']
 <S sid="172" ssid="39">To see this, note thai the first two terms in the above equation correspond to the function that AdaBoost attempts to minimize in the standard supervised setting (Equ.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["85'"]
85'
['85']
parsed_discourse_facet ['method_citation']
 <S sid="214" ssid="81">This modification brings the method closer to the DL-CoTrain algorithm described earlier, and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples, preventing one label from dominating &#8212; this deserves more theoretical investigation.</S>
original cit marker offset is 0
new cit marker offset is 0



["214'"]
214'
['214']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'228'", "'0'", "'75'", "'77'"]
'142'
'228'
'0'
'75'
'77'
['142', '228', '0', '75', '77']
parsed_discourse_facet ['results_citation']
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="102" ssid = "35">(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'77'", "'102'", "'52'", "'75'"]
'142'
'77'
'102'
'52'
'75'
['142', '77', '102', '52', '75']
parsed_discourse_facet ['method_citation']
<S sid ="225" ssid = "4">We again assume a training set of n examples {x1 . xri} where the first m examples have labels {y1 ... yin}  and the last (n — m) examples are unlabeled.</S><S sid ="203" ssid = "70">Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).</S><S sid ="140" ssid = "7">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S><S sid ="79" ssid = "12">2 We now introduce a new algorithm for learning from unlabeled examples  which we will call DLCoTrain (DL stands for decision list  the term Cotrain is taken from (Blum and Mitchell 98)).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'225'", "'203'", "'140'", "'79'", "'31'"]
'225'
'203'
'140'
'79'
'31'
['225', '203', '140', '79', '31']
parsed_discourse_facet ['method_citation']
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="102" ssid = "35">(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'102'", "'77'", "'52'", "'228'"]
'142'
'102'
'77'
'52'
'228'
['142', '102', '77', '52', '228']
parsed_discourse_facet ['method_citation']
<S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="207" ssid = "74">Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S><S sid ="166" ssid = "33">The first m pairs have labels yi  whereas for i = m + 1    n the pairs are unlabeled.</S><S sid ="84" ssid = "17">For each label (Per s on  organization and Location)  take the n contextual rules with the highest value of Count' (x) whose unsmoothed3 strength is above some threshold pmin.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'", "'207'", "'248'", "'166'", "'84'"]
'236'
'207'
'248'
'166'
'84'
['236', '207', '248', '166', '84']
parsed_discourse_facet ['method_citation']
<S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'228'", "'142'", "'75'", "'77'", "'0'"]
'228'
'142'
'75'
'77'
'0'
['228', '142', '75', '77', '0']
parsed_discourse_facet ['method_citation']
<S sid ="207" ssid = "74">Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S><S sid ="86" ssid = "19">Thus at each iteration the method induces at most n x k rules  where k is the number of possible labels (k = 3 in the experiments in this paper). step 3.</S><S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="10" ssid = "4">The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'248'", "'86'", "'204'", "'10'"]
'207'
'248'
'86'
'204'
'10'
['207', '248', '86', '204', '10']
parsed_discourse_facet ['method_citation']
<S sid ="202" ssid = "69">The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels  and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.</S><S sid ="124" ssid = "57">In particular  it may not be possible to learn functions fi (x f2(x2 t) for i = m + 1...n: either because there is some noise in the data  or because it is just not realistic to expect to learn perfect classifiers given the features used for representation.</S><S sid ="121" ssid = "54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).</S><S sid ="50" ssid = "4">It is a sequence of proper nouns within an NP; its last word Cooper is the head of the NP; and the NP has an appositive modifier (a vice president at S.&P.) whose head is a singular noun (president).</S><S sid ="25" ssid = "19">The unlabeled data gives many such &quot;hints&quot; that two features should predict the same label  and these hints turn out to be surprisingly useful when building a classifier.</S>
original cit marker offset is 0
new cit marker offset is 0



["'202'", "'124'", "'121'", "'50'", "'25'"]
'202'
'124'
'121'
'50'
'25'
['202', '124', '121', '50', '25']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="12" ssid = "6">The approach uses both spelling and contextual rules.</S><S sid ="150" ssid = "17">Pseudo-code describing the generalized boosting algorithm of Schapire and Singer is given in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'52'", "'75'", "'12'", "'150'"]
'157'
'52'
'75'
'12'
'150'
['157', '52', '75', '12', '150']
parsed_discourse_facet ['method_citation']
<S sid ="63" ssid = "17">In this case nonalpha is the string formed by removing all upper/lower case letters from the spelling (e.g.  for Thomas E. Petry nonalpha= .</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="47" ssid = "1">971 746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'52'", "'157'", "'77'", "'47'"]
'63'
'52'
'157'
'77'
'47'
['63', '52', '157', '77', '47']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="239" ssid = "6">Of these cases  38 were temporal expressions (either a day of the week or month of the year).</S><S sid ="93" ssid = "26">To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.</S><S sid ="249" ssid = "16">The test accuracy more or less asymptotes.</S><S sid ="197" ssid = "64">Note that in our formalism a weakhypothesis can abstain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'239'", "'93'", "'249'", "'197'"]
'204'
'239'
'93'
'249'
'197'
['204', '239', '93', '249', '197']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S><S sid ="93" ssid = "26">To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'75'", "'228'", "'0'", "'93'"]
'204'
'75'
'228'
'0'
'93'
['204', '75', '228', '0', '93']
parsed_discourse_facet ['method_citation']
<S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="193" ssid = "60">In Input: {(x1 i  Initialize: Vi  j : e(xi) = 0.</S><S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'", "'157'", "'77'", "'193'", "'142'"]
'52'
'157'
'77'
'193'
'142'
['52', '157', '77', '193', '142']
parsed_discourse_facet ['results_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
parsing: input/ref/Task1/D10-1044_aakansha.csv
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S>
    <S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'96'"]
'95'
'96'
['95', '96']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'"]
'75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="28">For comparison to information-retrieval inspired baselines, eg (L&#168;u et al., 2007), we select sentences from OUT using language model perplexities from IN.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'"]
'22'
['22']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="23">The 2nd block contains the IR system, which was tuned by selecting text in multiples of the size of the EMEA training corpus, according to dev set performance.</S>
    <S sid="120" ssid="24">This significantly underperforms log-linear combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'", "'120'"]
'119'
'120'
['119', '120']
parsed_discourse_facet ['result_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
    <S sid="24" ssid="21">Sentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="4">We focus here instead on adapting the two most important features: the language model (LM), which estimates the probability p(wIh) of a target word w following an ngram h; and the translation models (TM) p(slt) and p(t1s), which give the probability of source phrase s translating to target phrase t, and vice versa.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D10-1044.csv
<S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daum´e’s (2007) work that splits each feature into domain-specific and general copies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'128'", "'140'", "'10'", "'4'", "'136'"]
'128'
'140'
'10'
'4'
'136'
['128', '140', '10', '4', '136']
parsed_discourse_facet ['results_citation']
<S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="82" ssid = "19">This is not unreasonable given the application to phrase pairs from OUT  but it suggests that an interesting alternative might be to use a plain log-linear weighting function exp(Ei Aifi(s  t))  with outputs in [0  oo].</S><S sid ="142" ssid = "11">There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'38'", "'82'", "'142'", "'130'"]
'143'
'38'
'82'
'142'
'130'
['143', '38', '82', '142', '130']
parsed_discourse_facet ['method_citation']
<S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S><S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="31" ssid = "28">For comparison to information-retrieval inspired baselines  eg (L¨u et al.  2007)  we select sentences from OUT using language model perplexities from IN.</S><S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'7'", "'31'", "'128'", "'78'"]
'140'
'7'
'31'
'128'
'78'
['140', '7', '31', '128', '78']
parsed_discourse_facet ['method_citation']
<S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="102" ssid = "6">The dev corpus was taken from the NIST05 evaluation set  augmented with some randomly-selected material reserved from the training set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'127'", "'143'", "'65'", "'102'"]
'130'
'127'
'143'
'65'
'102'
['130', '127', '143', '65', '102']
parsed_discourse_facet ['method_citation']
<S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="55" ssid = "19">This suggests a direct parallel to (1): where ˜p(s  t) is a joint empirical distribution extracted from the IN dev set using the standard procedure.2 An alternative form of linear combination is a maximum a posteriori (MAP) combination (Bacchiani et al.  2004).</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'21'", "'49'", "'55'", "'127'"]
'75'
'21'
'49'
'55'
'127'
['75', '21', '49', '55', '127']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'130'", "'49'", "'38'", "'143'"]
'127'
'130'
'49'
'38'
'143'
['127', '130', '49', '38', '143']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'20'", "'38'", "'130'", "'49'"]
'127'
'20'
'38'
'130'
'49'
['127', '20', '38', '130', '49']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S><S sid ="37" ssid = "1">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features  interpreted as log probabilities  many of which have their own internal parameters and objectives.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'49'", "'111'", "'37'", "'21'"]
'127'
'49'
'111'
'37'
'21'
['127', '49', '111', '37', '21']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'81'", "'151'", "'75'", "'143'"]
'7'
'81'
'151'
'75'
'143'
['7', '81', '151', '75', '143']
parsed_discourse_facet ['method_citation']
<S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'75'", "'127'", "'151'", "'143'"]
'21'
'75'
'127'
'151'
'143'
['21', '75', '127', '151', '143']
parsed_discourse_facet ['method_citation']
<S sid ="97" ssid = "1">We carried out translation experiments in two different settings.</S><S sid ="53" ssid = "17">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).</S><S sid ="106" ssid = "10">The corpora for both settings are summarized in table 1.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="26" ssid = "23">Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009)  who weight sentences according to sub-corpus and genre membership.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'53'", "'106'", "'81'", "'26'"]
'97'
'53'
'106'
'81'
'26'
['97', '53', '106', '81', '26']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="31" ssid = "28">For comparison to information-retrieval inspired baselines  eg (L¨u et al.  2007)  we select sentences from OUT using language model perplexities from IN.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'81'", "'31'", "'78'", "'140'"]
'7'
'81'
'31'
'78'
'140'
['7', '81', '31', '78', '140']
parsed_discourse_facet ['method_citation']
<S sid ="60" ssid = "24">The matching sentence pairs are then added to the IN corpus  and the system is re-trained.</S><S sid ="30" ssid = "27">A similar maximumlikelihood approach was used by Foster and Kuhn (2007)  but for language models only.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="145" ssid = "2">Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'30'", "'24'", "'65'", "'145'"]
'60'
'30'
'24'
'65'
'145'
['60', '30', '24', '65', '145']
parsed_discourse_facet ['results_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="22" ssid = "19">Within this framework  we use features intended to capture degree of generality  including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'20'", "'130'", "'49'", "'22'"]
'127'
'20'
'130'
'49'
'22'
['127', '20', '130', '49', '22']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="119" ssid = "23">The 2nd block contains the IR system  which was tuned by selecting text in multiples of the size of the EMEA training corpus  according to dev set performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'130'", "'20'", "'49'", "'119'"]
'127'
'130'
'20'
'49'
'119'
['127', '130', '20', '49', '119']
parsed_discourse_facet ['method_citation']
<S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S><S sid ="66" ssid = "3">The weight on each sentence is a value in [0  1] computed by a perceptron with Boolean features that indicate collection and genre membership.</S><S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="113" ssid = "17">The corpus was wordaligned using both HMM and IBM2 models  and the phrase table was the union of phrases extracted from these separate alignments  with a length limit of 7.</S><S sid ="102" ssid = "6">The dev corpus was taken from the NIST05 evaluation set  augmented with some randomly-selected material reserved from the training set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'66'", "'75'", "'113'", "'102'"]
'143'
'66'
'75'
'113'
'102'
['143', '66', '75', '113', '102']
parsed_discourse_facet ['method_citation']
<S sid ="44" ssid = "8">When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.</S><S sid ="79" ssid = "16">This combination generalizes (2) and (3): we use either at = a to obtain a fixed-weight linear combination  or at = cI(t)/(cI(t) + 0) to obtain a MAP combination.</S><S sid ="62" ssid = "26">To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S><S sid ="115" ssid = "19">Table 2 shows results for both settings and all methods described in sections 2 and 3.</S><S sid ="13" ssid = "10">The techniques we develop can be extended in a relatively straightforward manner to the more general case when OUT consists of multiple sub-domains.</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'79'", "'62'", "'115'", "'13'"]
'44'
'79'
'62'
'115'
'13'
['44', '79', '62', '115', '13']
parsed_discourse_facet ['method_citation']
<S sid ="106" ssid = "10">The corpora for both settings are summarized in table 1.</S><S sid ="60" ssid = "24">The matching sentence pairs are then added to the IN corpus  and the system is re-trained.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="53" ssid = "17">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'60'", "'81'", "'38'", "'53'"]
'106'
'60'
'81'
'38'
'53'
['106', '60', '81', '38', '53']
parsed_discourse_facet ['method_citation']
<S sid ="95" ssid = "32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S><S sid ="50" ssid = "14">Linear weights are difficult to incorporate into the standard MERT procedure because they are “hidden” within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="59" ssid = "23">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid ="51" ssid = "15">For the LM  adaptive weights are set as follows: where α is a weight vector containing an element αi for each domain (just IN and OUT in our case)  pi are the corresponding domain-specific models  and ˜p(w  h) is an empirical distribution from a targetlanguage training corpus—we used the IN dev set for this.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'50'", "'21'", "'59'", "'51'"]
'95'
'50'
'21'
'59'
'51'
['95', '50', '21', '59', '51']
parsed_discourse_facet ['aim_citation', 'results_citation']



D10-1044
P12-1099
0
method_citation
['method_citation']
parsing: input/ref/Task1/P11-1061_aakansha.csv
<S sid="40" ssid="6">We extend Subramanya et al.&#8217;s intuitions to our bilingual setup.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="11">First, we use a novel graph-based framework for projecting syntactic information across language boundaries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="29" ssid="6">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'"]
'158'
['158']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'"]
'158'
['158']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="1">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="17" ssid="13">Second, we treat the projected labels as features in an unsupervised model (&#167;5), rather than using them directly for supervised training.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="111" ssid="11">We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).</S>
original cit marker offset is 0
new cit marker offset is 0



["'111'"]
'111'
['111']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="29" ssid="6">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'161'"]
'161'
['161']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1061.csv
<S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="62" ssid = "28">Note that since all English vertices were extracted from the parallel text  we will have an initial label distribution for all vertices in Ve.</S><S sid ="68" ssid = "34">In the label propagation stage  we propagate the automatic English tags to the aligned Italian trigram types  followed by further propagation solely among the Italian vertices. the Italian vertices are connected to an automatically labeled English vertex.</S><S sid ="42" ssid = "8">The foreign language vertices (denoted by Vf) correspond to foreign trigram types  exactly as in Subramanya et al. (2010).</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'62'", "'68'", "'42'", "'160'"]
'19'
'62'
'68'
'42'
'160'
['19', '62', '68', '42', '160']
parsed_discourse_facet ['results_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid ="30" ssid = "7">To initialize the graph we tag the English side of the parallel text using a supervised model.</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'113'", "'30'", "'102'", "'25'"]
'26'
'113'
'30'
'102'
'25'
['26', '113', '30', '102', '25']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'113'", "'25'", "'3'", "'19'"]
'16'
'113'
'25'
'3'
'19'
['16', '113', '25', '3', '19']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="119" ssid = "19">To provide a thorough analysis  we evaluated three baselines and two oracles in addition to two variants of our graph-based approach.</S><S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="82" ssid = "13">We formulate the update as follows: where ∀ui ∈ Vf \ Vfl  γi(y) and κi are defined as: We ran this procedure for 10 iterations.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'119'", "'54'", "'82'", "'28'"]
'134'
'119'
'54'
'82'
'28'
['134', '119', '54', '82', '28']
parsed_discourse_facet ['method_citation']
<S sid ="71" ssid = "2">We use label propagation in two stages to generate soft labels on all the vertices in the graph.</S><S sid ="61" ssid = "27">These tag distributions are used to initialize the label distributions over the English vertices in the graph.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="86" ssid = "17">For a sentence x and a state sequence z  a first order Markov model defines a distribution: (9) where Val(X) corresponds to the entire vocabulary.</S><S sid ="124" ssid = "24">We tried two versions of our graph-based approach: feature after the first stage of label propagation (Eq.</S>
original cit marker offset is 0
new cit marker offset is 0



["'71'", "'61'", "'3'", "'86'", "'124'"]
'71'
'61'
'3'
'86'
'124'
['71', '61', '3', '86', '124']
parsed_discourse_facet ['method_citation']
<S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="57" ssid = "23">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'26'", "'57'", "'97'", "'28'"]
'89'
'26'
'57'
'97'
'28'
['89', '26', '57', '97', '28']
parsed_discourse_facet ['method_citation']
<S sid ="93" ssid = "24">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'93'", "'26'", "'28'", "'97'", "'89'"]
'93'
'26'
'28'
'97'
'89'
['93', '26', '28', '97', '89']
parsed_discourse_facet ['method_citation']
<S sid ="92" ssid = "23">To optimize this function  we used L-BFGS  a quasi-Newton method (Liu and Nocedal  1989).</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="49" ssid = "15">We define a symmetric similarity function K(uZ7 uj) over two foreign language vertices uZ7 uj E Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1.</S><S sid ="142" ssid = "5">The “No LP” model does not outperform direct projection for German and Greek  but performs better for six out of eight languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'102'", "'19'", "'49'", "'142'"]
'92'
'102'
'19'
'49'
'142'
['92', '102', '19', '49', '142']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="159" ssid = "2">Because we are interested in applying our techniques to languages for which no labeled resources are available  we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'159'", "'97'", "'89'", "'28'"]
'26'
'159'
'97'
'89'
'28'
['26', '159', '97', '89', '28']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'97'", "'102'", "'28'", "'83'"]
'134'
'97'
'102'
'28'
'83'
['134', '97', '102', '28', '83']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="72" ssid = "3">In the first stage  we run a single step of label propagation  which transfers the label distributions from the English vertices to the connected foreign language vertices (say  Vf�) at the periphery of the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'97'", "'28'", "'102'", "'72'"]
'26'
'97'
'28'
'102'
'72'
['26', '97', '28', '102', '72']
parsed_discourse_facet ['method_citation']
<S sid ="95" ssid = "26">This feature ft incorporates information from the smoothed graph and prunes hidden states that are inconsistent with the thresholded vector tx.</S><S sid ="15" ssid = "11">First  we use a novel graph-based framework for projecting syntactic information across language boundaries.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid ="41" ssid = "7">Because the information flow in our graph is asymmetric (from English to the foreign language)  we use different types of vertices for each language.</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'15'", "'25'", "'41'", "'113'"]
'95'
'15'
'25'
'41'
'113'
['95', '15', '25', '41', '113']
parsed_discourse_facet ['method_citation']
<S sid ="21" ssid = "17">These universal POS categories not only facilitate the transfer of POS information from one language to another  but also relieve us from using controversial evaluation metrics 2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S><S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="10" ssid = "6">To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S><S sid ="36" ssid = "2">Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand  using individual words as the vertices throws away the context necessary for disambiguation; on the other hand  it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'16'", "'10'", "'29'", "'36'"]
'21'
'16'
'10'
'29'
'36'
['21', '16', '10', '29', '36']
parsed_discourse_facet ['results_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S><S sid ="35" ssid = "1">In graph-based learning approaches one constructs a graph whose vertices are labeled and unlabeled examples  and whose weighted edges encode the degree to which the examples they link have the same label (Zhu et al.  2003).</S><S sid ="159" ssid = "2">Because we are interested in applying our techniques to languages for which no labeled resources are available  we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'54'", "'89'", "'35'", "'159'"]
'26'
'54'
'89'
'35'
'159'
['26', '54', '89', '35', '159']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'28'", "'134'", "'97'", "'89'"]
'26'
'28'
'134'
'97'
'89'
['26', '28', '134', '97', '89']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="21" ssid = "17">These universal POS categories not only facilitate the transfer of POS information from one language to another  but also relieve us from using controversial evaluation metrics 2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'21'", "'160'", "'113'", "'29'"]
'16'
'21'
'160'
'113'
'29'
['16', '21', '160', '113', '29']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'28'", "'97'", "'26'", "'89'"]
'134'
'28'
'97'
'26'
'89'
['134', '28', '97', '26', '89']
parsed_discourse_facet ['method_citation']
<S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="49" ssid = "15">We define a symmetric similarity function K(uZ7 uj) over two foreign language vertices uZ7 uj E Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1.</S><S sid ="136" ssid = "36">For graph propagation  the hyperparameter v was set to 2 x 10−6 and was not tuned.</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'102'", "'49'", "'136'", "'83'"]
'97'
'102'
'49'
'136'
'83'
['97', '102', '49', '136', '83']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P05-1013_aakansha.csv
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="17">However, the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech, with a best performance of 73% UAS (Holan, 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="86" ssid="13">As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'"]
'86'
['86']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="7">As shown in Table 3, the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.</S>
    <S sid="81"  ssid="8">However, the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'81'"]
'80'
'81'
['80', '81']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'"]
'49'
['49']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="10">While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P05-1013.csv
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'48'", "'21'", "'36'"]
'50'
'75'
'48'
'21'
'36'
['50', '75', '48', '21', '36']
parsed_discourse_facet ['results_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="62" ssid = "1">In the experiments below  we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs 3 previously tested on Swedish (Nivre et al.  2004) and English (Nivre and Scholz  2004).</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="91" ssid = "2">Table 5 shows the overall parsing accuracy attained with the three different encoding schemes  compared to the baseline (no special arc labels) and to training directly on non-projective dependency graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'75'", "'62'", "'21'", "'91'"]
'48'
'75'
'62'
'21'
'91'
['48', '75', '62', '21', '91']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="77" ssid = "4">The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'77'", "'48'", "'21'"]
'50'
'75'
'77'
'48'
'21'
['50', '75', '77', '48', '21']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'71'", "'21'", "'45'"]
'50'
'75'
'71'
'21'
'45'
['50', '75', '71', '21', '45']
parsed_discourse_facet ['method_citation']
<S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'50'", "'48'", "'45'", "'21'"]
'75'
'50'
'48'
'45'
'21'
['75', '50', '48', '45', '21']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p↓.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'51'", "'36'", "'21'", "'48'"]
'50'
'51'
'36'
'21'
'48'
['50', '51', '36', '21', '48']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'48'", "'21'", "'36'", "'75'"]
'50'
'48'
'21'
'36'
'75'
['50', '48', '21', '36', '75']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p↓.</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'21'", "'51'", "'48'", "'75'"]
'50'
'21'
'51'
'48'
'75'
['50', '21', '51', '48', '75']
parsed_discourse_facet ['method_citation']
<S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p↓.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="23" ssid = "19">By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'", "'51'", "'21'", "'48'", "'23'"]
'36'
'51'
'21'
'48'
'23'
['36', '51', '21', '48', '23']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'48'", "'21'", "'45'"]
'50'
'75'
'48'
'21'
'45'
['50', '75', '48', '21', '45']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="83" ssid = "10">It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="6" ssid = "2">However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.</S><S sid ="35" ssid = "6">The dependency graph in Figure 1 satisfies all the defining conditions above  but it fails to satisfy the condition ofprojectivity (Kahane et al.  1998): The arc connecting the head jedna (one) to the dependent Z (out-of) spans the token je (is)  which is not dominated by jedna.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'83'", "'7'", "'6'", "'35'"]
'2'
'83'
'7'
'6'
'35'
['2', '83', '7', '6', '35']
parsed_discourse_facet ['method_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'75'", "'45'", "'21'", "'20'"]
'48'
'75'
'45'
'21'
'20'
['48', '75', '45', '21', '20']
parsed_discourse_facet ['method_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S><S sid ="73" ssid = "12">More details on the memory-based prediction can be found in Nivre et al. (2004) and Nivre and Scholz (2004).</S><S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'45'", "'73'", "'20'", "'106'"]
'48'
'45'
'73'
'20'
'106'
['48', '45', '73', '20', '106']
parsed_discourse_facet ['results_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="52" ssid = "23">Thus  the arc from je to jedna will be labeled 5b↓ (to indicate that there is a syntactic head below it).</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'48'", "'75'", "'36'", "'52'"]
'50'
'48'
'75'
'36'
'52'
['50', '48', '75', '36', '52']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="84" ssid = "11">In the second part of the experiment  we applied the inverse transformation based on breadth-first search under the three different encoding schemes.</S><S sid ="79" ssid = "6">In the first part of the experiment  dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'48'", "'21'", "'84'", "'79'"]
'50'
'48'
'21'
'84'
'79'
['50', '48', '21', '84', '79']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "8">For each token  three types of features may be taken into account: the word form; the part-of-speech assigned by an automatic tagger; and labels on previously assigned dependency arcs involving the token – the arc from its head and the arcs to its leftmost and rightmost dependent  respectively.</S><S sid ="23" ssid = "19">By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S><S sid ="64" ssid = "3">At each point during the derivation  the parser has a choice between pushing the next input token onto the stack – with or without adding an arc from the token on top of the stack to the token pushed – and popping a token from the stack – with or without adding an arc from the next input token to the token popped.</S><S sid ="59" ssid = "30">The details of the transformation procedure are slightly different depending on the encoding schemes: d↑h let the linear head be the syntactic head). target arc must have the form wl −→ wm; if no target arc is found  Head is used as backoff. must have the form wl −→ wm and no outgoing arcs of the form wm p'↓ −→ wo; no backoff.</S><S sid ="46" ssid = "17">In principle  it would be possible to encode the exact position of the syntactic head in the label of the arc from the linear head  but this would give a potentially infinite set of arc labels and would make the training of the parser very hard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'23'", "'64'", "'59'", "'46'"]
'69'
'23'
'64'
'59'
'46'
['69', '23', '64', '59', '46']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'71'", "'21'", "'45'"]
'50'
'75'
'71'
'21'
'45'
['50', '75', '71', '21', '45']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "18">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 15–25%  the total proportion of non-projective arcs is normally only 1–2%.</S><S sid ="101" ssid = "12">However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.</S><S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S><S sid ="87" ssid = "14">However  it can be noted that the results for the least informative encoding  Path  are almost comparable  while the third encoding  Head  gives substantially worse results for both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'14'", "'101'", "'89'", "'87'"]
'107'
'14'
'101'
'89'
'87'
['107', '14', '101', '89', '87']
parsed_discourse_facet ['method_citation']
<S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 15–25%  the total proportion of non-projective arcs is normally only 1–2%.</S><S sid ="88" ssid = "15">We also see that the increase in the size of the label sets for Head and Head+Path is far below the theoretical upper bounds given in Table 1.</S><S sid ="28" ssid = "24">First  in section 4  we evaluate the graph transformation techniques in themselves  with data from the Prague Dependency Treebank and the Danish Dependency Treebank.</S><S sid ="103" ssid = "14">On the other hand  given that all schemes have similar parsing accuracy overall  this means that the Path scheme is the least likely to introduce errors on projective arcs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'14'", "'88'", "'28'", "'103'"]
'89'
'14'
'88'
'28'
'103'
['89', '14', '88', '28', '103']
parsed_discourse_facet ['aim_citation', 'results_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'50'", "'75'", "'21'", "'36'"]
'48'
'50'
'75'
'21'
'36'
['48', '50', '75', '21', '36']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



P05-1013
N07-1050
0
method_citation
['method_citation']



P05-1013
P11-2121
0
method_citation
['method_citation']



P05-1013
D08-1008
0
method_citation
['method_citation']
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P08-1043_aakansha.csv
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="69" ssid="1">We represent all morphological analyses of a given utterance using a lattice structure.</S>
    <S sid="70" ssid="2">Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'70'"]
'69'
'70'
['69', '70']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="17">The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty, 2006) and (Cohen and Smith, 2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="11">Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
    <S sid="134" ssid="12">Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training 	similar to (Cohen and Smith, 2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'134'"]
'133'
'134'
['133', '134']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="17">The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="155" ssid="33">Evaluation We use 8 different measures to evaluate the performance of our system on the joint disambiguation task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="31" ssid = "10">Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'92'", "'86'", "'192'", "'31'"]
'183'
'92'
'86'
'192'
'31'
['183', '92', '86', '192', '31']
parsed_discourse_facet ['results_citation']
<S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="173" ssid = "11">The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter (α) which is tuned separately for each of the tasks.</S><S sid ="77" ssid = "9">Segments with the same surface form but different PoS tags are treated as different lexemes  and are represented as separate arcs (e.g. the two arcs labeled neim from node 6 to 7).</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'192'", "'173'", "'77'", "'133'"]
'183'
'192'
'173'
'77'
'133'
['183', '192', '173', '77', '133']
parsed_discourse_facet ['method_citation']
<S sid ="141" ssid = "19">This analyzer setting is similar to that of (Cohen and Smith  2007)  and models using it are denoted nohsp  Parser and Grammar We used BitPar (Schmid  2004)  an efficient general purpose parser 10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.</S><S sid ="89" ssid = "21">Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree π s.t.</S><S sid ="51" ssid = "9">Tsarfaty (2006) used a morphological analyzer (Segal  2000)  a PoS tagger (Bar-Haim et al.  2005)  and a general purpose parser (Schmid  2000) in an integrated framework in which morphological and syntactic components interact to share information  leading to improved performance on the joint task.</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="82" ssid = "14">In sequential tagging models such as (Adler and Elhadad  2006; Bar-Haim et al.  2007; Smith et al.  2005) weights are assigned according to a language model The input for the joint task is a sequence W = w1  ...   wn of space-delimited tokens.</S>
original cit marker offset is 0
new cit marker offset is 0



["'141'", "'89'", "'51'", "'156'", "'82'"]
'141'
'89'
'51'
'156'
'82'
['141', '89', '51', '156', '82']
parsed_discourse_facet ['method_citation']
<S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'183'", "'86'", "'0'", "'92'"]
'192'
'183'
'86'
'0'
'92'
['192', '183', '86', '0', '92']
parsed_discourse_facet ['method_citation']
<S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'183'", "'0'", "'86'", "'92'"]
'192'
'183'
'0'
'86'
'92'
['192', '183', '0', '86', '92']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="89" ssid = "21">Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree π s.t.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'86'", "'192'", "'89'", "'92'"]
'183'
'86'
'192'
'89'
'92'
['183', '86', '192', '89', '92']
parsed_discourse_facet ['method_citation']
<S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'86'", "'57'", "'192'", "'183'"]
'92'
'86'
'57'
'192'
'183'
['92', '86', '57', '192', '183']
parsed_discourse_facet ['method_citation']
<S sid ="31" ssid = "10">Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="186" ssid = "24">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'", "'92'", "'133'", "'0'", "'186'"]
'31'
'92'
'133'
'0'
'186'
['31', '92', '133', '0', '186']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'92'", "'192'", "'183'", "'133'"]
'86'
'92'
'192'
'183'
'133'
['86', '92', '192', '183', '133']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'192'", "'183'", "'0'", "'92'"]
'86'
'192'
'183'
'0'
'92'
['86', '192', '183', '0', '92']
parsed_discourse_facet ['method_citation']
<S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="123" ssid = "1">Previous work on morphological and syntactic disambiguation in Hebrew used different sets of data  different splits  differing annotation schemes  and different evaluation measures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'57'", "'192'", "'86'", "'123'"]
'92'
'57'
'192'
'86'
'123'
['92', '57', '192', '86', '123']
parsed_discourse_facet ['method_citation']
<S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="69" ssid = "1">We represent all morphological analyses of a given utterance using a lattice structure.</S><S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="113" ssid = "45">The remaining arcs are marked OOV.</S><S sid ="50" ssid = "8">The joint morphological and syntactic hypothesis was first discussed in (Tsarfaty  2006; Tsarfaty and Sima’an  2004) and empirically explored in (Tsarfaty  2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'", "'69'", "'14'", "'113'", "'50'"]
'57'
'69'
'14'
'113'
'50'
['57', '69', '14', '113', '50']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'92'", "'192'", "'183'", "'57'"]
'86'
'92'
'192'
'183'
'57'
['86', '92', '192', '183', '57']
parsed_discourse_facet ['results_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'192'", "'0'", "'92'", "'183'"]
'86'
'192'
'0'
'92'
'183'
['86', '192', '0', '92', '183']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'183'", "'192'", "'92'", "'0'"]
'86'
'183'
'192'
'92'
'0'
['86', '183', '192', '92', '0']
parsed_discourse_facet ['method_citation']
<S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S><S sid ="18" ssid = "14">Cohen and Smith (2007) followed up on these results and proposed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'92'", "'183'", "'133'", "'18'"]
'192'
'92'
'183'
'133'
'18'
['192', '92', '183', '133', '18']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/J01-2004_aakansha.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'"]
'372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="17" ssid="5">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



["'302'"]
'302'
['302']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="37">This underspecification of the nonterminal predictions (e.g., VP-VBD in the example in Figure 2, as opposed to NP), allows lexical items to become part of the left context, and so be used to condition production probabilities, even the production probabilities of constituents that dominate them in the unfactored tree.</S>
    <S sid="80" ssid="38">It also brings words further downstream into the look-ahead at the point of specification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'", "'80'"]
'79'
'80'
['79', '80']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="42">Our approach is found to yield very accurate parses efficiently, and, in addition, to lend itself straightforwardly to estimating word probabilities on-line, that is, in a single pass from left to right.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="291" ssid="47">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'291'"]
'291'
['291']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['method_citation']
<S sid="209" ssid="113">This parser is essentially a stochastic version of the top-down parser described in Aho, Sethi, and Ullman (1986).</S>
    <S sid="210" ssid="114">It uses a PCFG with a conditional probability model of the sort defined in the previous section.</S>
original cit marker offset is 0
new cit marker offset is 0



["'209'", "'210'"]
'209'
'210'
['209', '210']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'"]
'372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="8">Two features of our top-down parsing approach will emerge as key to its success.</S>
    <S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S><S sid="32" ssid="20">A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'21'", "'32'"]
'20'
'21'
'32'
['20', '21', '32']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="21">Because the rooted partial derivation is fully connected, all of the conditioning information that might be extracted from the top-down left context has already been specified, and a conditional probability model built on this information will not impose any additional burden on the search.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'"]
'33'
['33']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="168" ssid = "72">For example  in Figure 2  constituent (NP-DT-NN) is simply NP.</S><S sid ="356" ssid = "112">The DARPA '93 HUB1 test setup consists of 213 utterances read from the Wall Street Journal  a total of 3 446 words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'224'", "'322'", "'358'", "'168'", "'356'"]
'224'
'322'
'358'
'168'
'356'
['224', '322', '358', '168', '356']
parsed_discourse_facet ['results_citation']
<S sid ="17" ssid = "5">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S sid ="102" ssid = "6">One approach to syntactic language modeling is to use this distribution directly as a language model.</S><S sid ="340" ssid = "96">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid ="113" ssid = "17">In empirical trials  Goddeau used the top two stack entries to condition the word probability.</S><S sid ="12" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'102'", "'340'", "'113'", "'12'"]
'17'
'102'
'340'
'113'
'12'
['17', '102', '340', '113', '12']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="95" ssid = "53">For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'95'", "'302'", "'224'", "'104'"]
'134'
'95'
'302'
'224'
'104'
['134', '95', '302', '224', '104']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="167" ssid = "71">In order to avoid any confusions in identifying the nonterminal label of a particular rule production in either its factored or rionfactored version  we introduce the function constituent (A) for every nonterminal in the factored grammar Gf  which is simply the label of the constituent whose factorization results in A.</S><S sid ="112" ssid = "16">Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state  and conditioning on those entries in a way similar to an n-gram.</S><S sid ="85" ssid = "43">The simple definition of c-command that we will be using in this paper is the following: a node A c-commands a node B if and only if (i) A does not dominate B; and (ii) the lowest branching node (i.e.  non-unary node) that dominates A also dominates B.'</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'167'", "'112'", "'85'", "'390'"]
'134'
'167'
'112'
'85'
'390'
['134', '167', '112', '85', '390']
parsed_discourse_facet ['method_citation']
<S sid ="321" ssid = "77">In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.</S><S sid ="95" ssid = "53">For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].</S><S sid ="199" ssid = "103">Table 1 gives a breakdown of the different levels of conditioning information used in the empirical trials  with a mnemonic label that will be used when presenting results.</S><S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="11" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'321'", "'95'", "'199'", "'5'", "'11'"]
'321'
'95'
'199'
'5'
'11'
['321', '95', '199', '5', '11']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="87" ssid = "45">Notice that the subject NP c-commands the object NP  but not vice versa  since the lowest branching node that dominates the object NP is the VP  which does not dominate the subject NP.</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S><S sid ="85" ssid = "43">The simple definition of c-command that we will be using in this paper is the following: a node A c-commands a node B if and only if (i) A does not dominate B; and (ii) the lowest branching node (i.e.  non-unary node) that dominates A also dominates B.'</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'87'", "'302'", "'104'", "'85'"]
'134'
'87'
'302'
'104'
'85'
['134', '87', '302', '104', '85']
parsed_discourse_facet ['method_citation']
<S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="52" ssid = "10">Consider  for example  the parse tree shown in (a) in Figure 1: the start symbol is St  which expands into an S. The S node expands into an NP followed by a VP.</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S>
original cit marker offset is 0
new cit marker offset is 0



["'224'", "'134'", "'52'", "'322'", "'306'"]
'224'
'134'
'52'
'322'
'306'
['224', '134', '52', '322', '306']
parsed_discourse_facet ['method_citation']
<S sid ="112" ssid = "16">Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state  and conditioning on those entries in a way similar to an n-gram.</S><S sid ="129" ssid = "33">The shift-reduce parser will have  perhaps  built the structure shown  and the stack state will have an NP entry with the head &quot;cat&quot; at the top of the stack  and a VBD entry with the head &quot;chased&quot; second on the stack.</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="87" ssid = "45">Notice that the subject NP c-commands the object NP  but not vice versa  since the lowest branching node that dominates the object NP is the VP  which does not dominate the subject NP.</S><S sid ="109" ssid = "13">A shift-reduce parser operates from left to right using a stack and a pointer to the next word in the input string.9 Each stack entry consists minimally of a nonterminal label.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'129'", "'302'", "'87'", "'109'"]
'112'
'129'
'302'
'87'
'109'
['112', '129', '302', '87', '109']
parsed_discourse_facet ['method_citation']
<S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="213" ssid = "117">The parser takes an input string 4  a PCFG G  and a priority queue of candidate analyses.</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'358'", "'306'", "'224'", "'213'", "'322'"]
'358'
'306'
'224'
'213'
'322'
['358', '306', '224', '213', '322']
parsed_discourse_facet ['method_citation']
<S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="59" ssid = "17">A PCFG is a CFG with a probability assigned to each rule; specifically  each righthand side has a probability given the left-hand side of the rule.</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="52" ssid = "10">Consider  for example  the parse tree shown in (a) in Figure 1: the start symbol is St  which expands into an S. The S node expands into an NP followed by a VP.</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'302'", "'59'", "'134'", "'52'", "'104'"]
'302'
'59'
'134'
'52'
'104'
['302', '59', '134', '52', '104']
parsed_discourse_facet ['method_citation']
<S sid ="210" ssid = "114">It uses a PCFG with a conditional probability model of the sort defined in the previous section.</S><S sid ="142" ssid = "46">A simple PCFG conditions rule probabilities on the left-hand side of the rule.</S><S sid ="323" ssid = "79">They trained and tested the SLM on a modified  more &quot;speech-like&quot; version of the Penn Treebank.</S><S sid ="0" ssid = "0">Probabilistic Top-Down Parsing and Language Modeling</S><S sid ="370" ssid = "126">We followed Chelba (2000) in using an LM weight of 16 for the lattice trigram.</S>
original cit marker offset is 0
new cit marker offset is 0



["'210'", "'142'", "'323'", "'0'", "'370'"]
'210'
'142'
'323'
'0'
'370'
['210', '142', '323', '0', '370']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S><S sid ="154" ssid = "58">We use the relative frequency estimator  and smooth our production probabilities by interpolating the relative frequency estimates with those obtained by &quot;annotating&quot; less contextual information.</S><S sid ="0" ssid = "0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'224'", "'104'", "'154'", "'0'"]
'134'
'224'
'104'
'154'
'0'
['134', '224', '104', '154', '0']
parsed_discourse_facet ['method_citation']
<S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="95" ssid = "53">For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].</S><S sid ="124" ssid = "28">Each distinct derivation path within the beam has a probability and a stack state associated with it.</S><S sid ="147" ssid = "51">One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).</S><S sid ="168" ssid = "72">For example  in Figure 2  constituent (NP-DT-NN) is simply NP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'302'", "'95'", "'124'", "'147'", "'168'"]
'302'
'95'
'124'
'147'
'168'
['302', '95', '124', '147', '168']
parsed_discourse_facet ['results_citation']
<S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S>
original cit marker offset is 0
new cit marker offset is 0



["'358'", "'306'", "'322'", "'224'", "'134'"]
'358'
'306'
'322'
'224'
'134'
['358', '306', '322', '224', '134']
parsed_discourse_facet ['method_citation']
<S sid ="289" ssid = "45">These results  achieved using very straightforward conditioning events and considering only the left context  are within one to four points of the best published Observed running time on Section 23 of the Penn Treebank  with the full conditional probability model and beam of 10-11  using one 300 MHz UltraSPARC processor and 256MB of RAM of a Sun Enterprise 450. accuracies cited above.'</S><S sid ="25" ssid = "13">A parser that is not left to right  but which has rooted derivations  e.g.  a headfirst parser  will be able to calculate generative joint probabilities for entire strings; however  it will not be able to calculate probabilities for each word conditioned on previously generated words  unless each derivation generates the words in the string in exactly the same order.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="141" ssid = "45">We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.</S><S sid ="22" ssid = "10">A left-toright parser whose derivations are not rooted  i.e.  with derivations that can consist of disconnected tree fragments  such as an LR or shift-reduce parser  cannot incrementally calculate the probability of each prefix string being generated by the probabilistic grammar  because their derivations include probability mass from unrooted structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'289'", "'25'", "'390'", "'141'", "'22'"]
'289'
'25'
'390'
'141'
'22'
['289', '25', '390', '141', '22']
parsed_discourse_facet ['method_citation']
<S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'224'", "'306'", "'358'", "'134'", "'322'"]
'224'
'306'
'358'
'134'
'322'
['224', '306', '358', '134', '322']
parsed_discourse_facet ['method_citation']
<S sid ="147" ssid = "51">One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="278" ssid = "34">Section 22 (41 817 words  1 700 sentences) served as the development corpus  on which the parser was tested until stable versions were ready to run on the test data  to avoid developing the parser to fit the specific test data.</S><S sid ="141" ssid = "45">We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'302'", "'278'", "'141'", "'390'"]
'147'
'302'
'278'
'141'
'390'
['147', '302', '278', '141', '390']
parsed_discourse_facet ['method_citation']



J01-2004
P08-1013
0
method_citation
['method_citation']
parsing: input/ref/Task1/P08-1102_swastika.csv
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="10">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="8">All feature templates and their instances are shown in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



['36']
36
['36']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
<S sid="97" ssid="8">Figure 3 shows their learning curves depicting the F-measure on the development set after 1 to 10 training iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



['97']
97
['97']
parsed_discourse_facet ['result_citation']
<S sid="91" ssid="2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['aim_citation']
<S sid="16" ssid="12">Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



['34']
34
['34']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="22" ssid = "18">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here  Ci (i = L.n) denotes Chinese character  ti (i = L.m) denotes POS tag  and Cl:r (l < r) denotes character sequence ranges from Cl to Cr.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="104" ssid = "15">According to the usual practice in syntactic analysis  we choose chapters 1 − 260 (18074 sentences) as training set  chapter 271 − 300 (348 sentences) as test set and chapter 301 − 325 (350 sentences) as development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'", "'22'", "'16'", "'94'", "'104'"]
'87'
'22'
'16'
'94'
'104'
['87', '22', '16', '94', '104']
parsed_discourse_facet ['results_citation']
<S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'47'", "'48'", "'32'", "'133'"]
'69'
'47'
'48'
'32'
'133'
['69', '47', '48', '32', '133']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="99" ssid = "10">Then we trained LEX on each of the four corpora for 7 iterations.</S><S sid ="30" ssid = "2">It has comparable performance to CRFs  while with much faster training.</S><S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="122" ssid = "33">Another important feature is the labelling model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'99'", "'30'", "'101'", "'122'"]
'68'
'99'
'30'
'101'
'122'
['68', '99', '30', '101', '122']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="30" ssid = "2">It has comparable performance to CRFs  while with much faster training.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'87'", "'31'", "'94'", "'30'"]
'101'
'87'
'31'
'94'
'30'
['101', '87', '31', '94', '30']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="0" ssid = "0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'0'", "'87'", "'69'", "'48'"]
'47'
'0'
'87'
'69'
'48'
['47', '0', '87', '69', '48']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'48'", "'49'", "'133'", "'32'"]
'47'
'48'
'49'
'133'
'32'
['47', '48', '49', '133', '32']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'68'", "'94'", "'87'", "'7'"]
'16'
'68'
'94'
'87'
'7'
['16', '68', '94', '87', '7']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "19">We find that Joint S&T can also improve the segmentation accuracy.</S><S sid ="138" ssid = "1">This work was done while L. H. was visiting CAS/ICT.</S><S sid ="61" ssid = "12">In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.</S><S sid ="90" ssid = "1">We reported results from two set of experiments.</S><S sid ="107" ssid = "18">The evaluation results are shown in Table 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'138'", "'61'", "'90'", "'107'"]
'108'
'138'
'61'
'90'
'107'
['108', '138', '61', '90', '107']
parsed_discourse_facet ['method_citation']
<S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S><S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'133'", "'32'", "'69'", "'49'"]
'48'
'133'
'32'
'69'
'49'
['48', '133', '32', '69', '49']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'7'", "'49'", "'69'", "'16'"]
'47'
'7'
'49'
'69'
'16'
['47', '7', '49', '69', '16']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="54" ssid = "5">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S><S sid ="64" ssid = "15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid ="135" ssid = "6">In addition  all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus  whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'54'", "'64'", "'58'", "'135'"]
'2'
'54'
'64'
'58'
'135'
['2', '54', '64', '58', '135']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "11">Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS  CityU and MSR).</S><S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S><S sid ="121" ssid = "32">Among other features  the 4-gram POS LM plays the most important role  removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'47'", "'134'", "'32'", "'121'"]
'100'
'47'
'134'
'32'
'121'
['100', '47', '134', '32', '121']
parsed_discourse_facet ['method_citation']
<S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="52" ssid = "3">However  such features are generated dynamically during the decoding procedure so that the feature space enlarges much more rapidly.</S><S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="131" ssid = "2">Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.</S><S sid ="135" ssid = "6">In addition  all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus  whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'52'", "'2'", "'131'", "'135'"]
'133'
'52'
'2'
'131'
'135'
['133', '52', '2', '131', '135']
parsed_discourse_facet ['results_citation']
<S sid ="79" ssid = "4">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid ="73" ssid = "24">For instance  if the word w appears N times in training corpus and is labelled as POS t for n times  the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.</S><S sid ="98" ssid = "9">We found that LEX outperforms NON-LEX with a margin of about 0.002 at each iteration  and its learning curve reaches a tableland at iteration 7.</S><S sid ="37" ssid = "9">C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).</S><S sid ="70" ssid = "21">Given a training corpus with POS tags  we can train a word-POS co-occurrence model to approximate the probability that the word sequence of the labelled result co-exists with its corresponding POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'", "'73'", "'98'", "'37'", "'70'"]
'79'
'73'
'98'
'37'
'70'
['79', '73', '98', '37', '70']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="103" ssid = "14">We turned to experiments on CTB 5.0 to test the performance of the cascaded model.</S><S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'87'", "'103'", "'68'", "'101'"]
'16'
'87'
'103'
'68'
'101'
['16', '87', '103', '68', '101']
parsed_discourse_facet ['method_citation']
<S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="22" ssid = "18">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here  Ci (i = L.n) denotes Chinese character  ti (i = L.m) denotes POS tag  and Cl:r (l < r) denotes character sequence ranges from Cl to Cr.</S><S sid ="91" ssid = "2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2  including the Academia Sinica Corpus (AS)  the Hong Kong City University Corpus (CityU)  the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'", "'22'", "'91'", "'45'", "'16'"]
'31'
'22'
'91'
'45'
'16'
['31', '22', '91', '45', '16']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W11-2123_aakansha.csv
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="136" ssid="8">We offer a state function s(wn1) = wn&#65533; where substring wn&#65533; is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'"]
'136'
['136']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="3">Queries take the form p(wn|wn&#8722;1 1 ) where wn1 is an n-gram.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="205" ssid="24">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al., 2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'205'"]
'205'
['205']
parsed_discourse_facet ['method_citation']
<S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
<S sid="204" ssid="23">For RandLM, we used the settings in the documentation: 8 bits per value and false positive probability 1 256.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'"]
'204'
['204']
parsed_discourse_facet ['method_citation']
<S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="229" ssid="48">Then we ran binary search to determine the least amount of memory with which it would run.</S>
original cit marker offset is 0
new cit marker offset is 0



["'229'"]
'229'
['229']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="93" ssid="71">The cost of storing these averages, in bits, is Because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.</S>
original cit marker offset is 0
new cit marker offset is 0



["'93'"]
'93'
['93']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W11-2123.csv
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="27" ssid = "5">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'183'", "'69'", "'27'", "'231'"]
'0'
'183'
'69'
'27'
'231'
['0', '183', '69', '27', '231']
parsed_discourse_facet ['results_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="149" ssid = "21">RandLM and SRILM also remove context that will not extend  but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.</S><S sid ="193" ssid = "12">It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'183'", "'69'", "'149'", "'193'"]
'0'
'183'
'69'
'149'
'193'
['0', '183', '69', '149', '193']
parsed_discourse_facet ['method_citation']
<S sid ="62" ssid = "40">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S><S sid ="142" ssid = "14">Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn−1 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.</S><S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="216" ssid = "35">Though we are not able to calculate their memory usage on our model  results reported in their paper suggest lower memory consumption than TRIE on large-scale models  at the expense of CPU time.</S><S sid ="50" ssid = "28">Given counts cn1 where e.g. c1 is the vocabulary size  total memory consumption  in bits  is Our PROBING data structure places all n-grams of the same order into a single giant hash table.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'", "'142'", "'3'", "'216'", "'50'"]
'62'
'142'
'3'
'216'
'50'
['62', '142', '3', '216', '50']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="137" ssid = "9">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="157" ssid = "29">Each visited entry wni stores backoff b(wni ).</S><S sid ="27" ssid = "5">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'137'", "'183'", "'157'", "'27'"]
'0'
'137'
'183'
'157'
'27'
['0', '137', '183', '157', '27']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="233" ssid = "52">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid ="193" ssid = "12">It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'3'", "'233'", "'193'", "'69'"]
'183'
'3'
'233'
'193'
'69'
['183', '3', '233', '193', '69']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="38" ssid = "16">The ratio of buckets to entries is controlled by space multiplier m > 1.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="71" ssid = "49">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'0'", "'38'", "'88'", "'71'"]
'183'
'0'
'38'
'88'
'71'
['183', '0', '38', '88', '71']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="71" ssid = "49">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'0'", "'14'", "'71'", "'88'"]
'69'
'0'
'14'
'71'
'88'
['69', '0', '14', '71', '88']
parsed_discourse_facet ['method_citation']
<S sid ="124" ssid = "28">Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="227" ssid = "46">The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.</S><S sid ="21" ssid = "16">Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'69'", "'227'", "'21'", "'13'"]
'124'
'69'
'227'
'21'
'13'
['124', '69', '227', '21', '13']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="137" ssid = "9">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid ="116" ssid = "20">Both implementations employ a state object  opaque to the application  that carries information from one query to the next; we discuss both further in Section 4.2.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'0'", "'137'", "'116'", "'231'"]
'183'
'0'
'137'
'116'
'231'
['183', '0', '137', '116', '231']
parsed_discourse_facet ['method_citation']
<S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="118" ssid = "22">The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'183'", "'69'", "'118'", "'14'"]
'88'
'183'
'69'
'118'
'14'
['88', '183', '69', '118', '14']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="227" ssid = "46">The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.</S><S sid ="124" ssid = "28">Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'231'", "'183'", "'227'", "'124'"]
'69'
'231'
'183'
'227'
'124'
['69', '231', '183', '227', '124']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "29">Each visited entry wni stores backoff b(wni ).</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="53" ssid = "31">Sorted arrays store key-value pairs in an array sorted by key  incurring no space overhead.</S><S sid ="27" ssid = "5">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S><S sid ="118" ssid = "22">The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'0'", "'53'", "'27'", "'118'"]
'157'
'0'
'53'
'27'
'118'
['157', '0', '53', '27', '118']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="195" ssid = "14">Interpolation search has a more expensive pivot but performs less pivoting and reads  so it is slow on small data and faster on large data.</S><S sid ="116" ssid = "20">Both implementations employ a state object  opaque to the application  that carries information from one query to the next; we discuss both further in Section 4.2.</S><S sid ="137" ssid = "9">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid ="62" ssid = "40">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'195'", "'116'", "'137'", "'62'"]
'183'
'195'
'116'
'137'
'62'
['183', '195', '116', '137', '62']
parsed_discourse_facet ['results_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S><S sid ="233" ssid = "52">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid ="193" ssid = "12">It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'69'", "'231'", "'233'", "'193'"]
'183'
'69'
'231'
'233'
'193'
['183', '69', '231', '233', '193']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="201" ssid = "20">Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'69'", "'88'", "'183'", "'201'"]
'0'
'69'
'88'
'183'
'201'
['0', '69', '88', '183', '201']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="62" ssid = "40">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S><S sid ="102" ssid = "6">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie)  allocating memory all at once (eliminating the need for full pointers)  and being easy to compile.</S><S sid ="179" ssid = "51">We do not experiment with models larger than physical memory in this paper because TPT is unreleased  factors such as disk speed are hard to replicate  and in such situations we recommend switching to a more compact representation  such as RandLM.</S><S sid ="142" ssid = "14">Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn−1 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'62'", "'102'", "'179'", "'142'"]
'3'
'62'
'102'
'179'
'142'
['3', '62', '102', '179', '142']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="2" ssid = "2">The structure uses linear probing hash tables and is designed for speed.</S><S sid ="129" ssid = "1">In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.</S><S sid ="71" ssid = "49">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S><S sid ="201" ssid = "20">Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'2'", "'129'", "'71'", "'201'"]
'69'
'2'
'129'
'71'
'201'
['69', '2', '129', '71', '201']
parsed_discourse_facet ['method_citation']
<S sid ="199" ssid = "18">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid ="205" ssid = "24">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al.  2009).</S><S sid ="182" ssid = "1">This section measures performance on shared tasks in order of increasing complexity: sparse lookups  evaluating perplexity of a large file  and translation with Moses.</S><S sid ="67" ssid = "45">While sorted arrays could be used to implement the same data structure as PROBING  effectively making m = 1  we abandoned this implementation because it is slower and larger than a trie implementation.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'", "'205'", "'182'", "'67'", "'45'"]
'199'
'205'
'182'
'67'
'45'
['199', '205', '182', '67', '45']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="149" ssid = "21">RandLM and SRILM also remove context that will not extend  but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.</S><S sid ="201" ssid = "20">Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'183'", "'69'", "'149'", "'201'"]
'0'
'183'
'69'
'149'
'201'
['0', '183', '69', '149', '201']
parsed_discourse_facet ['aim_citation', 'results_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="118" ssid = "22">The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'0'", "'69'", "'88'", "'118'"]
'183'
'0'
'69'
'88'
'118'
['183', '0', '69', '88', '118']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P87-1015_vardha.csv
<S sid="165" ssid="50">This class of formalisms have the properties that their derivation trees are local sets, and manipulate objects, using a finite number of composition operations that use a finite number of symbols.</S>
original cit marker offset is 0
new cit marker offset is 0



["'165'"]
'165'
['165']
parsed_discourse_facet ['method_citation']
 <S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'"]
'119'
['119']
parsed_discourse_facet ['method_citation']
    <S sid="3" ssid="1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'"]
'3'
['3']
parsed_discourse_facet ['method_citation']
    <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
    <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
    <S sid="133" ssid="18">To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'"]
'133'
['133']
parsed_discourse_facet ['method_citation']
 <S sid="138" ssid="23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
    <S sid="156" ssid="41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'"]
'156'
['156']
parsed_discourse_facet ['method_citation']
 <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
 <S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'"]
'119'
['119']
parsed_discourse_facet ['method_citation']
  <S sid="133" ssid="18">To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'"]
'133'
['133']
parsed_discourse_facet ['method_citation']
 <S sid="164" ssid="49">Although embedding this version of LCFRS's in the framework of ILFP developed by Rounds (1985) is straightforward, our motivation was to capture properties shared by a family of grammatical systems and generalize them defining a class of related formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'164'"]
'164'
['164']
parsed_discourse_facet ['method_citation']
 <S sid="204" ssid="10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's, HG's, TAG's, and MCTAG's are all local sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'"]
'204'
['204']
parsed_discourse_facet ['method_citation']
    <S sid="9" ssid="7">We examine both the complexity of the paths of trees in the tree sets, and the kinds of dependencies that the formalisms can impose between paths.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="28" ssid="13">A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
 <S sid="138" ssid="23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P87-1015.csv
<S sid ="115" ssid = "21">From the point of view of recognition  independent paths in the derivation structures suggests that a top-down parser (for example) can work on each branch independently  which may lead to efficient parsing using an algorithm based on the Divide and Conquer technique.</S><S sid ="222" ssid = "28">However  in order to capture the properties of various grammatical systems under consideration  our notation is more restrictive that ILFP  which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="67" ssid = "52">On the one hand  the definition of composition in Steedman (1985)  which technically permits composition of functions with unbounded number of arguments  generates tree sets with dependent paths such as those shown in Figure 6.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'222'", "'195'", "'67'", "'153'"]
'115'
'222'
'195'
'67'
'153'
['115', '222', '195', '67', '153']
parsed_discourse_facet ['results_citation']
<S sid ="37" ssid = "22">Like CFG's  the choice is predetermined by a finite number of rules encapsulated in the grammar.</S><S sid ="231" ssid = "37">In this paper  our goal has been to use the notion of LCFRS's to classify grammatical systems on the basis of their strong generative capacity.</S><S sid ="197" ssid = "3">We address the question of whether or not a formalism can generate only structural descriptions with independent paths.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S><S sid ="92" ssid = "77">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'231'", "'197'", "'116'", "'92'"]
'37'
'231'
'197'
'116'
'92'
['37', '231', '197', '116', '92']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.</S><S sid ="131" ssid = "16">The composition operations in the case of CFG's are parameterized by the productions.</S><S sid ="65" ssid = "50">While the generative power of CG's is greater that of CFG's  it appears to be highly constrained.</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="179" ssid = "64">It can be seen that M performs a top-down recognition of the input al ... nin logspace.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'131'", "'65'", "'227'", "'179'"]
'204'
'131'
'65'
'227'
'179'
['204', '131', '65', '227', '179']
parsed_discourse_facet ['method_citation']
<S sid ="194" ssid = "79">Thus  M works in logspace and recognition can be done on a deterministic TM in polynomial tape.</S><S sid ="92" ssid = "77">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S><S sid ="65" ssid = "50">While the generative power of CG's is greater that of CFG's  it appears to be highly constrained.</S><S sid ="20" ssid = "5">As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.</S><S sid ="131" ssid = "16">The composition operations in the case of CFG's are parameterized by the productions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'92'", "'65'", "'20'", "'131'"]
'194'
'92'
'65'
'20'
'131'
['194', '92', '65', '20', '131']
parsed_discourse_facet ['method_citation']
<S sid ="155" ssid = "40">CFG's  TAG's  MCTAG's and HG's are all members of this class since they satisfy these restrictions.</S><S sid ="37" ssid = "22">Like CFG's  the choice is predetermined by a finite number of rules encapsulated in the grammar.</S><S sid ="117" ssid = "2">Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.</S><S sid ="92" ssid = "77">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S><S sid ="48" ssid = "33">There has been recent interest in the application of Indexed Grammars (IG's) to natural languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'", "'37'", "'117'", "'92'", "'48'"]
'155'
'37'
'117'
'92'
'48'
['155', '37', '117', '92', '48']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "5">As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.</S><S sid ="216" ssid = "22">Having defined LCFRS's  in Section 4.2 we established the semilinearity (and hence constant growth property) of the languages generated.</S><S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="179" ssid = "64">It can be seen that M performs a top-down recognition of the input al ... nin logspace.</S><S sid ="131" ssid = "16">The composition operations in the case of CFG's are parameterized by the productions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'216'", "'59'", "'179'", "'131'"]
'20'
'216'
'59'
'179'
'131'
['20', '216', '59', '179', '131']
parsed_discourse_facet ['method_citation']
<S sid ="162" ssid = "47">Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="219" ssid = "25">The composition operations are mapped onto operations that use concatenation to define the substrings spanned by the resulting structures.</S><S sid ="0" ssid = "0">CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*</S><S sid ="193" ssid = "78">Since the work tapes store integers (which can be written in binary) that never exceed the size of the input  no configuration has space exceeding 0(log n).</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'", "'195'", "'219'", "'0'", "'193'"]
'162'
'195'
'219'
'0'
'193'
['162', '195', '219', '0', '193']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "9">By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.</S><S sid ="115" ssid = "21">From the point of view of recognition  independent paths in the derivation structures suggests that a top-down parser (for example) can work on each branch independently  which may lead to efficient parsing using an algorithm based on the Divide and Conquer technique.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="1" ssid = "1">We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'115'", "'153'", "'1'", "'195'"]
'11'
'115'
'153'
'1'
'195'
['11', '115', '153', '1', '195']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*</S><S sid ="219" ssid = "25">The composition operations are mapped onto operations that use concatenation to define the substrings spanned by the resulting structures.</S><S sid ="125" ssid = "10">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S sid ="58" ssid = "43">Unification is used in LFG's to produce structures having two dependent spines of unbounded length as in Figure 5.</S><S sid ="192" ssid = "77">Thus  the ATM has no more than 6km&quot; + 1 work tapes  where km&quot; is the maximum number of substrings spanned by a derived structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'219'", "'125'", "'58'", "'192'"]
'0'
'219'
'125'
'58'
'192'
['0', '219', '125', '58', '192']
parsed_discourse_facet ['method_citation']
<S sid ="193" ssid = "78">Since the work tapes store integers (which can be written in binary) that never exceed the size of the input  no configuration has space exceeding 0(log n).</S><S sid ="129" ssid = "14">Frontier nodes are annotated by zero arty functions corresponding to elementary structures.</S><S sid ="127" ssid = "12">Nodes are annotated by the name of the composition operation used at that step in the derivation.</S><S sid ="172" ssid = "57">A step of an ATM consists of reading a symbol from each tape and optionally moving each head to the left or right one tape cell.</S><S sid ="173" ssid = "58">A configuration of M consists of a state of the finite control  the nonblank contents of the input tape and k work tapes  and the position of each head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'193'", "'129'", "'127'", "'172'", "'173'"]
'193'
'129'
'127'
'172'
'173'
['193', '129', '127', '172', '173']
parsed_discourse_facet ['method_citation']
<S sid ="189" ssid = "74">For rules p : A fpo such that fp is constant function  giving an elementary structure  fp is defined such that fp() = (Si ... xi() where each z is a constant string.</S><S sid ="183" ssid = "68">We assume that M is in an existential state qA  with integers i1 and i2 representing zi in the (2i — 1)th and 22th work tape  for 1 < i < k. For each rule p : A fp(B  C) such that fp is mapped onto the function fp defined by the following rule. jp((xi .. •  rnt)  (1ii  • • • • Yn3))= (Zi   • • •   Zk) M breaks xi   zk into substrings xi    xn  and yi ... y&quot; conforming to the definition of fp.</S><S sid ="160" ssid = "45">A derived structure will be mapped onto a sequence zi of substrings (not necessarily contiguous in the input)  and the composition operations will be mapped onto functions that can defined as follows3. f((zi • • •   zni)  (m. • • •  Yn3)) = (Z1  • • •   Zn3) where each z  is the concatenation of strings from z 's and yk's.</S><S sid ="148" ssid = "33">If 0(A) gives the number of occurrences of each terminal in the structure named by A  then  given the constraints imposed on the formalism  for each rule A --. fp(Ai    An) we have the equality where c„ is some constant.</S><S sid ="201" ssid = "7">It is interesting to note  however  that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG  FUG and IG's.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'183'", "'160'", "'148'", "'201'"]
'189'
'183'
'160'
'148'
'201'
['189', '183', '160', '148', '201']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*</S><S sid ="162" ssid = "47">Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="196" ssid = "2">We contrasted formalisms such as CFG's  HG's  TAG's and MCTAG's  with formalisms such as IG's and unificational systems such as LFG's and FUG's.</S><S sid ="204" ssid = "10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'162'", "'227'", "'196'", "'204'"]
'0'
'162'
'227'
'196'
'204'
['0', '162', '227', '196', '204']
parsed_discourse_facet ['method_citation']
<S sid ="58" ssid = "43">Unification is used in LFG's to produce structures having two dependent spines of unbounded length as in Figure 5.</S><S sid ="214" ssid = "20">LCFRS's share several properties possessed by the class of mildly context-sensitive formalisms discussed by Joshi (1983/85).</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="196" ssid = "2">We contrasted formalisms such as CFG's  HG's  TAG's and MCTAG's  with formalisms such as IG's and unificational systems such as LFG's and FUG's.</S><S sid ="163" ssid = "48">This representation of structures by substrings and the composition operation by its effect on substrings is related to the work of Rounds (1985).</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'214'", "'227'", "'196'", "'163'"]
'58'
'214'
'227'
'196'
'163'
['58', '214', '227', '196', '163']
parsed_discourse_facet ['results_citation']
<S sid ="50" ssid = "35">The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.</S><S sid ="202" ssid = "8">As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.</S><S sid ="7" ssid = "5">The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG's and IG's.</S><S sid ="230" ssid = "36">LCFRS's have only been loosely defined in this paper; we have yet to provide a complete set of formal properties associated with members of this class.</S><S sid ="110" ssid = "16">The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'202'", "'7'", "'230'", "'110'"]
'50'
'202'
'7'
'230'
'110'
['50', '202', '7', '230', '110']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "42">For example  in TAG's a derived auxiliary tree spans two substrings (to the left and right of the foot node)  and the adjunction operation inserts another substring (spanned by the subtree under the node where adjunction takes place) between them (see Figure 3).</S><S sid ="162" ssid = "47">Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .</S><S sid ="83" ssid = "68">The following context-free production captures the derivation step of the grammar shown in Figure 7  in which the trees in the auxiliary tree set are adjoined into themselves at the root node (address c).</S><S sid ="67" ssid = "52">On the one hand  the definition of composition in Steedman (1985)  which technically permits composition of functions with unbounded number of arguments  generates tree sets with dependent paths such as those shown in Figure 6.</S><S sid ="154" ssid = "39">Since each composition operation is linear and nonerasing  a bounded sequences of substrings associated with the resulting structure is obtained by combining the substrings in each of its arguments using only the concatenation operation  including each substring exactly once.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'162'", "'83'", "'67'", "'154'"]
'157'
'162'
'83'
'67'
'154'
['157', '162', '83', '67', '154']
parsed_discourse_facet ['method_citation']
<S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="156" ssid = "41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S><S sid ="204" ssid = "10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.</S><S sid ="213" ssid = "19">By sharing stacks (in IG's) or by using nonlinear equations over f-structures (in FUG's and LFG's)  structures with unbounded dependencies between paths can be generated.</S><S sid ="125" ssid = "10">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'227'", "'156'", "'204'", "'213'", "'125'"]
'227'
'156'
'204'
'213'
'125'
['227', '156', '204', '213', '125']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/A00-2018_akanksha.csv
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'"]
'90'
'91'
['90', '91']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="17">Maximum-entropy models have two benefits for a parser builder.</S>
    <S sid="49" ssid="18">First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &amp;quot;features&amp;quot; suggests that the probability model should be easily changeable &#8212; just change the set of features used.</S>
    <S sid="51" ssid="20">Second, and this is a point we have not yet mentioned, the features used in these models need have no particular independence of one another.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'49'", "'51'"]
'48'
'49'
'51'
['48', '49', '51']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
    <S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
    <S sid="93" ssid="4">This allows the second pass to see expansions not present in the training corpus.</S>
    <S sid="94" ssid="5">We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'", "'92'", "'93'", "'94'"]
'90'
'91'
'92'
'93'
'94'
['90', '91', '92', '93', '94']
parsed_discourse_facet ['method_citation']
NA
original cit marker offset is 0
new cit marker offset is 0



['0']
0
['0']
Error in Reference Offset
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
    <S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
    <S sid="93" ssid="4">This allows the second pass to see expansions not present in the training corpus.</S>
    <S sid="94" ssid="5">We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'", "'92'", "'93'", "'94'"]
'90'
'91'
'92'
'93'
'94'
['90', '91', '92', '93', '94']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="7">To compute a probability in a log-linear model one first defines a set of &amp;quot;features&amp;quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.</S>
    <S sid="39" ssid="8">In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.</S>
    <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'39'", "'40'"]
'38'
'39'
'40'
['38', '39', '40']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['result_citation']
<S sid="85" ssid="54">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models, this simplifies the model significantly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="32">As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &amp;quot;before&amp;quot;), and the label of the grandparent of c (la).</S><S sid="143" ssid="34">The first is simply that if we first guess the pre-terminal, when we go to guess the head the first thing we can condition upon is the pre-terminal, i.e., we compute p(h I t).</S><S sid="146" ssid="37">The second major reason why first guessing the pre-terminal makes so much difference is that it can be used when backing off the lexical head in computing the probability of the rule expansion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'143'", "'146'"]
'63'
'143'
'146'
['63', '143', '146']
parsed_discourse_facet ['method_citation']
???<S sid="78" ssid="47">With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.</S>                                        <S sid="79" ssid="48">We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.</S>
original cit marker offset is 0
new cit marker offset is 0



["'78'", "'79'"]
'78'
'79'
['78', '79']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/A00-2018.csv
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'23'", "'62'", "'33'", "'27'"]
'30'
'23'
'62'
'33'
'27'
['30', '23', '62', '33', '27']
parsed_discourse_facet ['results_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'27'", "'62'", "'33'", "'23'"]
'30'
'27'
'62'
'33'
'23'
['30', '27', '62', '33', '23']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'62'", "'33'", "'89'", "'23'"]
'30'
'62'
'33'
'89'
'23'
['30', '62', '33', '89', '23']
parsed_discourse_facet ['method_citation']
<S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="20" ssid = "9">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="29" ssid = "18">So  for example  in a second-order Markov PCFG  L2 would be conditioned on L1 and M. In our complete model  of course  the probability of each label in the expansions is also conditioned on other material as specified in Equation 1  e.g.  p(e t  h  H).</S><S sid ="91" ssid = "2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'", "'20'", "'174'", "'29'", "'91'"]
'27'
'20'
'174'
'29'
'91'
['27', '20', '174', '29', '91']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'23'", "'62'", "'27'", "'33'"]
'30'
'23'
'62'
'27'
'33'
['30', '23', '62', '27', '33']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="63" ssid = "32">As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'1'", "'5'", "'27'", "'63'"]
'101'
'1'
'5'
'27'
'63'
['101', '1', '5', '27', '63']
parsed_discourse_facet ['method_citation']
<S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'", "'30'", "'62'", "'89'", "'23'"]
'27'
'30'
'62'
'89'
'23'
['27', '30', '62', '89', '23']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'62'", "'23'", "'89'", "'33'"]
'30'
'62'
'23'
'89'
'33'
['30', '62', '23', '89', '33']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "7">To compute a probability in a log-linear model one first defines a set of &quot;features&quot;  functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.</S><S sid ="91" ssid = "2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="20" ssid = "9">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid ="26" ssid = "15">Thus an expansion e(c) looks like: The expansion is generated by guessing first M  then in order L1 through L „.+1 (= A)  and similarly for RI through In a pure Markov PCFG we are given the left-hand side label 1 and then probabilistically generate the right-hand side conditioning on no information other than 1 and (possibly) previously generated pieces of the right-hand side itself.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'91'", "'174'", "'20'", "'26'"]
'38'
'91'
'174'
'20'
'26'
['38', '91', '174', '20', '26']
parsed_discourse_facet ['method_citation']
<S sid ="29" ssid = "18">So  for example  in a second-order Markov PCFG  L2 would be conditioned on L1 and M. In our complete model  of course  the probability of each label in the expansions is also conditioned on other material as specified in Equation 1  e.g.  p(e t  h  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="91" ssid = "2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'", "'27'", "'174'", "'91'", "'33'"]
'29'
'27'
'174'
'91'
'33'
['29', '27', '174', '91', '33']
parsed_discourse_facet ['method_citation']
<S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="101" ssid = "12">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S><S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'", "'27'", "'89'", "'101'", "'30'"]
'62'
'27'
'89'
'101'
'30'
['62', '27', '89', '101', '30']
parsed_discourse_facet ['method_citation']
<S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="115" ssid = "6">As noted in [5]  that system is based upon a &quot;tree-bank grammar&quot; - a grammar read directly off the training corpus.</S><S sid ="141" ssid = "32">(For example  part-ofspeech tagging using the most probable preterminal for each word is 90% accurate [8].)</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="184" ssid = "11">The first is the slight  but important  improvement achieved by using this model over conventional deleted interpolation  as indicated in Figure 2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'115'", "'141'", "'62'", "'184'"]
'23'
'115'
'141'
'62'
'184'
['23', '115', '141', '62', '184']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "9">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid ="76" ssid = "45">This requires finding the appropriate Ais for Equation 3  which is accomplished using an algorithm such as iterative scaling [II] in which values for the Ai are initially &quot;guessed&quot; and then modified until they converge on stable values.</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="110" ssid = "1">In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach  the aspect of the parser that is most novel.</S><S sid ="101" ssid = "12">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'76'", "'27'", "'110'", "'101'"]
'20'
'76'
'27'
'110'
'101'
['20', '76', '27', '110', '101']
parsed_discourse_facet ['results_citation']
<S sid ="74" ssid = "43">Suppose we were  in fact  going to compute a true maximum entropy model based upon the features used in Equation 7  Ii (t 1)  f2(t 1 1p)  f3(t 1 lp) .</S><S sid ="126" ssid = "17">This is indicated in Figure 2  where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%  0.4% lower than the results on the official test corpus.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'126'", "'125'", "'174'", "'27'"]
'74'
'126'
'125'
'174'
'27'
['74', '126', '125', '174', '27']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="42" ssid = "11">This feature is obviously composed of two sub-features  one recognizing t  the other 1.</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'89'", "'23'", "'42'", "'62'"]
'30'
'89'
'23'
'42'
'62'
['30', '89', '23', '42', '62']
parsed_discourse_facet ['method_citation']
<S sid ="176" ssid = "3">That the previous three best parsers on this test [5 9 17] all perform within a percentage point of each other  despite quite different basic mechanisms  led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training  and to conjecture that perhaps we were at it.</S><S sid ="58" ssid = "27">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid ="180" ssid = "7">From our perspective  perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.</S><S sid ="44" ssid = "13">Now consider computing a conditional probability p(a H) with a set of features h that connect a to the history H. In a log-linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability  the higher the absolute value of the associated A.</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'", "'58'", "'180'", "'44'", "'13'"]
'176'
'58'
'180'
'44'
'13'
['176', '58', '180', '44', '13']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



A00-2018
P05-1065
0
method_citation
['method_citation']
parsing: input/ref/Task1/A00-2030_aakansha.csv
<S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="9">By necessity, we adopted the strategy of hand marking only the semantics.</S>
    <S sid="50" ssid="10">Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'50'"]
'49'
'50'
['49', '50']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="8">Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'"]
'16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S><S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'", "'33'", "'34'"]
'23'
'24'
'33'
'34'
['23', '24', '33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="60" ssid="1">In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).</S>
    <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'61'"]
'60'
'61'
['60', '61']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'"]
'33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).</S>
    <S sid="12" ssid="2">The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).</S><S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'12'", "'16'"]
'11'
'12'
'16'
['11', '12', '16']
parsed_discourse_facet ['method_citation']
<S sid="105" ssid="2">A single model proved capable of performing all necessary sentential processing, both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'"]
'105'
['105']
parsed_discourse_facet ['result_citation']
<S sid="60" ssid="1">In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).</S>
    <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'61'"]
'60'
'61'
['60', '61']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'"]
'16'
['16']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="49" ssid = "9">By necessity  we adopted the strategy of hand marking only the semantics.</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="70" ssid = "11">Post-modifier constituents for the PER/NP.</S><S sid ="73" ssid = "14">We now briefly summarize the probability structure of the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'", "'49'", "'68'", "'70'", "'73'"]
'28'
'49'
'68'
'70'
'73'
['28', '49', '68', '70', '73']
parsed_discourse_facet ['results_citation']
<S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="76" ssid = "17">Finally  word features  fm  for modifiers are predicted based on the modifier  cm  the partof-speech tag of the modifier word   t„„ the part-of-speech tag of the head word th  the head word itself  wh  and whether or not the modifier head word  w„„ is known or unknown.</S><S sid ="39" ssid = "7">For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.</S><S sid ="30" ssid = "13">Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'101'", "'76'", "'39'", "'30'"]
'61'
'101'
'76'
'39'
'30'
['61', '101', '76', '39', '30']
parsed_discourse_facet ['method_citation']
<S sid ="33" ssid = "1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S sid ="41" ssid = "1">To train our integrated model  we required a large corpus of augmented parse trees.</S><S sid ="64" ssid = "5">Word features are introduced primarily to help with unknown words  as in (Weischedel et al. 1993).</S><S sid ="2" ssid = "2">In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'41'", "'64'", "'2'", "'23'"]
'33'
'41'
'64'
'2'
'23'
['33', '41', '64', '2', '23']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'101'", "'81'", "'74'", "'0'"]
'61'
'101'
'81'
'74'
'0'
['61', '101', '81', '74', '0']
parsed_discourse_facet ['method_citation']
<S sid ="60" ssid = "1">In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).</S><S sid ="72" ssid = "13">This generation process is continued until the entire tree has been produced.</S><S sid ="77" ssid = "18">The probability of a complete tree is the product of the probabilities of generating each element in the tree.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'72'", "'77'", "'88'", "'28'"]
'60'
'72'
'77'
'88'
'28'
['60', '72', '77', '88', '28']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="54" ssid = "3">These steps are given below:</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'46'", "'57'", "'54'", "'106'"]
'0'
'46'
'57'
'54'
'106'
['0', '46', '57', '54', '106']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="94" ssid = "13">Given a new sentence  the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S><S sid ="30" ssid = "13">Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.</S><S sid ="60" ssid = "1">In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).</S><S sid ="39" ssid = "7">For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'94'", "'30'", "'60'", "'39'"]
'61'
'94'
'30'
'60'
'39'
['61', '94', '30', '60', '39']
parsed_discourse_facet ['method_citation']
<S sid ="110" ssid = "2">Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.</S><S sid ="41" ssid = "1">To train our integrated model  we required a large corpus of augmented parse trees.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'", "'41'", "'57'", "'51'", "'46'"]
'110'
'41'
'57'
'51'
'46'
['110', '41', '57', '51', '46']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'81'", "'0'", "'61'", "'74'"]
'51'
'81'
'0'
'61'
'74'
['51', '81', '0', '61', '74']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "15">Because generative statistical models had already proven successful for each of the first three stages  we were optimistic that some of their properties — especially their ability to learn from large amounts of data  and their robustness when presented with unexpected inputs — would also benefit semantic analysis.</S><S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="19" ssid = "2">Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'61'", "'19'", "'101'", "'51'"]
'32'
'61'
'19'
'101'
'51'
['32', '61', '19', '101', '51']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="54" ssid = "3">These steps are given below:</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'54'", "'81'", "'57'", "'46'"]
'0'
'54'
'81'
'57'
'46'
['0', '54', '81', '57', '46']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "13">Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'46'", "'105'", "'106'", "'11'"]
'30'
'46'
'105'
'106'
'11'
['30', '46', '105', '106', '11']
parsed_discourse_facet ['method_citation']
<S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="54" ssid = "3">These steps are given below:</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'", "'0'", "'46'", "'57'", "'54'"]
'105'
'0'
'46'
'57'
'54'
['105', '0', '46', '57', '54']
parsed_discourse_facet ['results_citation']
<S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'0'", "'57'", "'51'", "'105'"]
'46'
'0'
'57'
'51'
'105'
['46', '0', '57', '51', '105']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'57'", "'46'", "'51'", "'81'"]
'0'
'57'
'46'
'51'
'81'
['0', '57', '46', '51', '81']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="32" ssid = "15">Because generative statistical models had already proven successful for each of the first three stages  we were optimistic that some of their properties — especially their ability to learn from large amounts of data  and their robustness when presented with unexpected inputs — would also benefit semantic analysis.</S><S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S><S sid ="26" ssid = "9">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993)  and more recently  had begun using a generative statistical model for name finding (Bikel et al.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'32'", "'61'", "'11'", "'26'"]
'51'
'32'
'61'
'11'
'26'
['51', '32', '61', '11', '26']
parsed_discourse_facet ['method_citation']
<S sid ="110" ssid = "2">Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'", "'106'", "'46'", "'57'", "'105'"]
'110'
'106'
'46'
'57'
'105'
['110', '106', '46', '57', '105']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "3">These steps are given below:</S><S sid ="55" ssid = "1">syntactic modifier of the other  the inserted node serves to indicate the relation as well as the argument.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="7" ssid = "5">The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'55'", "'0'", "'62'", "'7'"]
'54'
'55'
'0'
'62'
'7'
['54', '55', '0', '62', '7']
parsed_discourse_facet ['method_citation']
<S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="110" ssid = "2">Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'110'", "'46'", "'105'", "'57'"]
'106'
'110'
'46'
'105'
'57'
['106', '110', '46', '105', '57']
parsed_discourse_facet ['aim_citation', 'results_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W06-3114_sweta.csv
 <S sid="108" ssid="1">The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="27">For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="44">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages, as demonstrated by the results for English-German and English-French.</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
 <S sid="172" ssid="3">Due to many similarly performing systems, we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
 <S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["36'"]
36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="103" ssid="19">Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["103'"]
103'
['103']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="60">One annotator suggested that this was the case for as much as 10% of our test sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["102'"]
102'
['102']
parsed_discourse_facet ['method_citation']
<S sid="123" ssid="16">For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.</S>
original cit marker offset is 0
new cit marker offset is 0



["123'"]
123'
['123']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="27">For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="1">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="19">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["126'"]
126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="173" ssid="4">The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["173'"]
173'
['173']
parsed_discourse_facet ['method_citation']
 <S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["170'"]
170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="84" ssid="23">The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["84'"]
84'
['84']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="80" ssid = "19">We collected around 300–400 judgements per judgement type (adequacy or fluency)  per system  per language pair.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'126'", "'80'", "'136'", "'0'"]
'134'
'126'
'80'
'136'
'0'
['134', '126', '80', '136', '0']
parsed_discourse_facet ['results_citation']
<S sid ="159" ssid = "52">(b) does the translation have the same meaning  including connotations?</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'159'", "'0'", "'84'", "'17'", "'170'"]
'159'
'0'
'84'
'17'
'170'
['159', '0', '84', '17', '170']
parsed_discourse_facet ['method_citation']
<S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'", "'18'", "'16'", "'126'", "'84'"]
'170'
'18'
'16'
'126'
'84'
['170', '18', '16', '126', '84']
parsed_discourse_facet ['method_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'17'", "'170'", "'84'", "'38'"]
'126'
'17'
'170'
'84'
'38'
['126', '17', '170', '84', '38']
parsed_discourse_facet ['method_citation']
<S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'", "'38'", "'126'", "'17'", "'84'"]
'170'
'38'
'126'
'17'
'84'
['170', '38', '126', '17', '84']
parsed_discourse_facet ['method_citation']
<S sid ="52" ssid = "18">Pairwise comparison: We can use the same method to assess the statistical significance of one system outperforming another.</S><S sid ="21" ssid = "14">The out-of-domain test set differs from the Europarl data in various ways.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="129" ssid = "22">All systems (except for Systran  which was not tuned to Europarl) did considerably worse on outof-domain training data.</S><S sid ="35" ssid = "1">For the automatic evaluation  we used BLEU  since it is the most established metric in the field.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'", "'21'", "'28'", "'129'", "'35'"]
'52'
'21'
'28'
'129'
'35'
['52', '21', '28', '129', '35']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="113" ssid = "6">The confidence intervals are computed by bootstrap resampling for BLEU  and by standard significance testing for the manual scores  as described earlier in the paper.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'113'", "'126'", "'28'", "'0'"]
'16'
'113'
'126'
'28'
'0'
['16', '113', '126', '28', '0']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="109" ssid = "2">The scores and confidence intervals are detailed first in the Figures 7–10 in table form (including ranks)  and then in graphical form in Figures 11–16.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'16'", "'38'", "'84'", "'109'"]
'18'
'16'
'38'
'84'
'109'
['18', '16', '38', '84', '109']
parsed_discourse_facet ['method_citation']
<S sid ="9" ssid = "2">Training and testing is based on the Europarl corpus.</S><S sid ="44" ssid = "10">We computed BLEU scores for each submission with a single reference translation.</S><S sid ="143" ssid = "36">For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.</S><S sid ="25" ssid = "18">We received submissions from 14 groups from 11 institutions  as listed in Figure 2.</S><S sid ="35" ssid = "1">For the automatic evaluation  we used BLEU  since it is the most established metric in the field.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'44'", "'143'", "'25'", "'35'"]
'9'
'44'
'143'
'25'
'35'
['9', '44', '143', '25', '35']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="68" ssid = "7">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'68'", "'0'", "'28'", "'136'"]
'134'
'68'
'0'
'28'
'136'
['134', '68', '0', '28', '136']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="68" ssid = "7">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'0'", "'68'", "'126'", "'17'"]
'134'
'0'
'68'
'126'
'17'
['134', '0', '68', '126', '17']
parsed_discourse_facet ['method_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="159" ssid = "52">(b) does the translation have the same meaning  including connotations?</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'134'", "'17'", "'84'", "'159'"]
'126'
'134'
'17'
'84'
'159'
['126', '134', '17', '84', '159']
parsed_discourse_facet ['method_citation']
<S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'38'", "'155'", "'170'", "'6'"]
'84'
'38'
'155'
'170'
'6'
['84', '38', '155', '170', '6']
parsed_discourse_facet ['results_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'134'", "'0'", "'17'", "'84'"]
'126'
'134'
'0'
'17'
'84'
['126', '134', '0', '17', '84']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="68" ssid = "7">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'0'", "'126'", "'68'", "'17'"]
'134'
'0'
'126'
'68'
'17'
['134', '0', '126', '68', '17']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'170'", "'84'", "'126'", "'17'"]
'0'
'170'
'84'
'126'
'17'
['0', '170', '84', '126', '17']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "19">We collected around 300–400 judgements per judgement type (adequacy or fluency)  per system  per language pair.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="150" ssid = "43">For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French)  at least as measured by BLEU.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'84'", "'150'", "'16'", "'155'"]
'80'
'84'
'150'
'16'
'155'
['80', '84', '150', '16', '155']
parsed_discourse_facet ['method_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'84'", "'38'", "'17'", "'170'"]
'126'
'84'
'38'
'17'
'170'
['126', '84', '38', '17', '170']
parsed_discourse_facet ['method_citation']



W06-3114
C08-1074
0
method_citation
['method_citation']



W06-3114
P07-1108
0
method_citation
['method_citation']



W06-3114
E12-3010
0
method_citation
['method_citation']
parsing: input/ref/Task1/J01-2004_swastika.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="3">In the past few years, however, some improvements have been made over these language models through the use of statistical methods of natural language processing, and the development of innovative, linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
<S sid="215" ssid="119">The first word in the string remaining to be parsed, w1, we will call the look-ahead word.</S>
original cit marker offset is 0
new cit marker offset is 0



['215']
215
['215']
parsed_discourse_facet ['method_citation']
    <S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



['302']
302
['302']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="100" ssid="4">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['method_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="168" ssid = "72">For example  in Figure 2  constituent (NP-DT-NN) is simply NP.</S><S sid ="356" ssid = "112">The DARPA '93 HUB1 test setup consists of 213 utterances read from the Wall Street Journal  a total of 3 446 words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'224'", "'322'", "'358'", "'168'", "'356'"]
'224'
'322'
'358'
'168'
'356'
['224', '322', '358', '168', '356']
parsed_discourse_facet ['results_citation']
<S sid ="17" ssid = "5">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S sid ="102" ssid = "6">One approach to syntactic language modeling is to use this distribution directly as a language model.</S><S sid ="340" ssid = "96">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid ="113" ssid = "17">In empirical trials  Goddeau used the top two stack entries to condition the word probability.</S><S sid ="12" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'102'", "'340'", "'113'", "'12'"]
'17'
'102'
'340'
'113'
'12'
['17', '102', '340', '113', '12']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="95" ssid = "53">For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'95'", "'302'", "'224'", "'104'"]
'134'
'95'
'302'
'224'
'104'
['134', '95', '302', '224', '104']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="167" ssid = "71">In order to avoid any confusions in identifying the nonterminal label of a particular rule production in either its factored or rionfactored version  we introduce the function constituent (A) for every nonterminal in the factored grammar Gf  which is simply the label of the constituent whose factorization results in A.</S><S sid ="112" ssid = "16">Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state  and conditioning on those entries in a way similar to an n-gram.</S><S sid ="85" ssid = "43">The simple definition of c-command that we will be using in this paper is the following: a node A c-commands a node B if and only if (i) A does not dominate B; and (ii) the lowest branching node (i.e.  non-unary node) that dominates A also dominates B.'</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'167'", "'112'", "'85'", "'390'"]
'134'
'167'
'112'
'85'
'390'
['134', '167', '112', '85', '390']
parsed_discourse_facet ['method_citation']
<S sid ="321" ssid = "77">In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.</S><S sid ="95" ssid = "53">For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].</S><S sid ="199" ssid = "103">Table 1 gives a breakdown of the different levels of conditioning information used in the empirical trials  with a mnemonic label that will be used when presenting results.</S><S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="11" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'321'", "'95'", "'199'", "'5'", "'11'"]
'321'
'95'
'199'
'5'
'11'
['321', '95', '199', '5', '11']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="87" ssid = "45">Notice that the subject NP c-commands the object NP  but not vice versa  since the lowest branching node that dominates the object NP is the VP  which does not dominate the subject NP.</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S><S sid ="85" ssid = "43">The simple definition of c-command that we will be using in this paper is the following: a node A c-commands a node B if and only if (i) A does not dominate B; and (ii) the lowest branching node (i.e.  non-unary node) that dominates A also dominates B.'</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'87'", "'302'", "'104'", "'85'"]
'134'
'87'
'302'
'104'
'85'
['134', '87', '302', '104', '85']
parsed_discourse_facet ['method_citation']
<S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="52" ssid = "10">Consider  for example  the parse tree shown in (a) in Figure 1: the start symbol is St  which expands into an S. The S node expands into an NP followed by a VP.</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S>
original cit marker offset is 0
new cit marker offset is 0



["'224'", "'134'", "'52'", "'322'", "'306'"]
'224'
'134'
'52'
'322'
'306'
['224', '134', '52', '322', '306']
parsed_discourse_facet ['method_citation']
<S sid ="112" ssid = "16">Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state  and conditioning on those entries in a way similar to an n-gram.</S><S sid ="129" ssid = "33">The shift-reduce parser will have  perhaps  built the structure shown  and the stack state will have an NP entry with the head &quot;cat&quot; at the top of the stack  and a VBD entry with the head &quot;chased&quot; second on the stack.</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="87" ssid = "45">Notice that the subject NP c-commands the object NP  but not vice versa  since the lowest branching node that dominates the object NP is the VP  which does not dominate the subject NP.</S><S sid ="109" ssid = "13">A shift-reduce parser operates from left to right using a stack and a pointer to the next word in the input string.9 Each stack entry consists minimally of a nonterminal label.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'129'", "'302'", "'87'", "'109'"]
'112'
'129'
'302'
'87'
'109'
['112', '129', '302', '87', '109']
parsed_discourse_facet ['method_citation']
<S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="213" ssid = "117">The parser takes an input string 4  a PCFG G  and a priority queue of candidate analyses.</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'358'", "'306'", "'224'", "'213'", "'322'"]
'358'
'306'
'224'
'213'
'322'
['358', '306', '224', '213', '322']
parsed_discourse_facet ['method_citation']
<S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="59" ssid = "17">A PCFG is a CFG with a probability assigned to each rule; specifically  each righthand side has a probability given the left-hand side of the rule.</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="52" ssid = "10">Consider  for example  the parse tree shown in (a) in Figure 1: the start symbol is St  which expands into an S. The S node expands into an NP followed by a VP.</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'302'", "'59'", "'134'", "'52'", "'104'"]
'302'
'59'
'134'
'52'
'104'
['302', '59', '134', '52', '104']
parsed_discourse_facet ['method_citation']
<S sid ="210" ssid = "114">It uses a PCFG with a conditional probability model of the sort defined in the previous section.</S><S sid ="142" ssid = "46">A simple PCFG conditions rule probabilities on the left-hand side of the rule.</S><S sid ="323" ssid = "79">They trained and tested the SLM on a modified  more &quot;speech-like&quot; version of the Penn Treebank.</S><S sid ="0" ssid = "0">Probabilistic Top-Down Parsing and Language Modeling</S><S sid ="370" ssid = "126">We followed Chelba (2000) in using an LM weight of 16 for the lattice trigram.</S>
original cit marker offset is 0
new cit marker offset is 0



["'210'", "'142'", "'323'", "'0'", "'370'"]
'210'
'142'
'323'
'0'
'370'
['210', '142', '323', '0', '370']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S><S sid ="154" ssid = "58">We use the relative frequency estimator  and smooth our production probabilities by interpolating the relative frequency estimates with those obtained by &quot;annotating&quot; less contextual information.</S><S sid ="0" ssid = "0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'224'", "'104'", "'154'", "'0'"]
'134'
'224'
'104'
'154'
'0'
['134', '224', '104', '154', '0']
parsed_discourse_facet ['method_citation']
<S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="95" ssid = "53">For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].</S><S sid ="124" ssid = "28">Each distinct derivation path within the beam has a probability and a stack state associated with it.</S><S sid ="147" ssid = "51">One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).</S><S sid ="168" ssid = "72">For example  in Figure 2  constituent (NP-DT-NN) is simply NP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'302'", "'95'", "'124'", "'147'", "'168'"]
'302'
'95'
'124'
'147'
'168'
['302', '95', '124', '147', '168']
parsed_discourse_facet ['results_citation']
<S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S>
original cit marker offset is 0
new cit marker offset is 0



["'358'", "'306'", "'322'", "'224'", "'134'"]
'358'
'306'
'322'
'224'
'134'
['358', '306', '322', '224', '134']
parsed_discourse_facet ['method_citation']
<S sid ="289" ssid = "45">These results  achieved using very straightforward conditioning events and considering only the left context  are within one to four points of the best published Observed running time on Section 23 of the Penn Treebank  with the full conditional probability model and beam of 10-11  using one 300 MHz UltraSPARC processor and 256MB of RAM of a Sun Enterprise 450. accuracies cited above.'</S><S sid ="25" ssid = "13">A parser that is not left to right  but which has rooted derivations  e.g.  a headfirst parser  will be able to calculate generative joint probabilities for entire strings; however  it will not be able to calculate probabilities for each word conditioned on previously generated words  unless each derivation generates the words in the string in exactly the same order.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="141" ssid = "45">We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.</S><S sid ="22" ssid = "10">A left-toright parser whose derivations are not rooted  i.e.  with derivations that can consist of disconnected tree fragments  such as an LR or shift-reduce parser  cannot incrementally calculate the probability of each prefix string being generated by the probabilistic grammar  because their derivations include probability mass from unrooted structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'289'", "'25'", "'390'", "'141'", "'22'"]
'289'
'25'
'390'
'141'
'22'
['289', '25', '390', '141', '22']
parsed_discourse_facet ['method_citation']
<S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'224'", "'306'", "'358'", "'134'", "'322'"]
'224'
'306'
'358'
'134'
'322'
['224', '306', '358', '134', '322']
parsed_discourse_facet ['method_citation']
<S sid ="147" ssid = "51">One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="278" ssid = "34">Section 22 (41 817 words  1 700 sentences) served as the development corpus  on which the parser was tested until stable versions were ready to run on the test data  to avoid developing the parser to fit the specific test data.</S><S sid ="141" ssid = "45">We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'302'", "'278'", "'141'", "'390'"]
'147'
'302'
'278'
'141'
'390'
['147', '302', '278', '141', '390']
parsed_discourse_facet ['method_citation']



J01-2004
P05-1022
0
method_citation
['method_citation']
parsing: input/ref/Task1/D10-1044_swastika.csv
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
<S sid="71" ssid="8">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c&#955;(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



['71']
71
['71']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
<S sid="71" ssid="8">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c&#955;(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



['71']
71
['71']
parsed_discourse_facet ['result_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="9">An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (Och, 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



['45']
45
['45']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



['75']
75
['75']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
  <S sid="42" ssid="6">The natural baseline approach is to concatenate data from IN and OUT.</S>
original cit marker offset is 0
new cit marker offset is 0



['42']
42
['42']
parsed_discourse_facet ['aim_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/D10-1044.csv
<S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daum´e’s (2007) work that splits each feature into domain-specific and general copies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'128'", "'140'", "'10'", "'4'", "'136'"]
'128'
'140'
'10'
'4'
'136'
['128', '140', '10', '4', '136']
parsed_discourse_facet ['results_citation']
<S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="82" ssid = "19">This is not unreasonable given the application to phrase pairs from OUT  but it suggests that an interesting alternative might be to use a plain log-linear weighting function exp(Ei Aifi(s  t))  with outputs in [0  oo].</S><S sid ="142" ssid = "11">There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'38'", "'82'", "'142'", "'130'"]
'143'
'38'
'82'
'142'
'130'
['143', '38', '82', '142', '130']
parsed_discourse_facet ['method_citation']
<S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S><S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="31" ssid = "28">For comparison to information-retrieval inspired baselines  eg (L¨u et al.  2007)  we select sentences from OUT using language model perplexities from IN.</S><S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'7'", "'31'", "'128'", "'78'"]
'140'
'7'
'31'
'128'
'78'
['140', '7', '31', '128', '78']
parsed_discourse_facet ['method_citation']
<S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="102" ssid = "6">The dev corpus was taken from the NIST05 evaluation set  augmented with some randomly-selected material reserved from the training set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'127'", "'143'", "'65'", "'102'"]
'130'
'127'
'143'
'65'
'102'
['130', '127', '143', '65', '102']
parsed_discourse_facet ['method_citation']
<S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="55" ssid = "19">This suggests a direct parallel to (1): where ˜p(s  t) is a joint empirical distribution extracted from the IN dev set using the standard procedure.2 An alternative form of linear combination is a maximum a posteriori (MAP) combination (Bacchiani et al.  2004).</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'21'", "'49'", "'55'", "'127'"]
'75'
'21'
'49'
'55'
'127'
['75', '21', '49', '55', '127']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'130'", "'49'", "'38'", "'143'"]
'127'
'130'
'49'
'38'
'143'
['127', '130', '49', '38', '143']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'20'", "'38'", "'130'", "'49'"]
'127'
'20'
'38'
'130'
'49'
['127', '20', '38', '130', '49']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S><S sid ="37" ssid = "1">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features  interpreted as log probabilities  many of which have their own internal parameters and objectives.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'49'", "'111'", "'37'", "'21'"]
'127'
'49'
'111'
'37'
'21'
['127', '49', '111', '37', '21']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'81'", "'151'", "'75'", "'143'"]
'7'
'81'
'151'
'75'
'143'
['7', '81', '151', '75', '143']
parsed_discourse_facet ['method_citation']
<S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'75'", "'127'", "'151'", "'143'"]
'21'
'75'
'127'
'151'
'143'
['21', '75', '127', '151', '143']
parsed_discourse_facet ['method_citation']
<S sid ="97" ssid = "1">We carried out translation experiments in two different settings.</S><S sid ="53" ssid = "17">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).</S><S sid ="106" ssid = "10">The corpora for both settings are summarized in table 1.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="26" ssid = "23">Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009)  who weight sentences according to sub-corpus and genre membership.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'53'", "'106'", "'81'", "'26'"]
'97'
'53'
'106'
'81'
'26'
['97', '53', '106', '81', '26']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="31" ssid = "28">For comparison to information-retrieval inspired baselines  eg (L¨u et al.  2007)  we select sentences from OUT using language model perplexities from IN.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'81'", "'31'", "'78'", "'140'"]
'7'
'81'
'31'
'78'
'140'
['7', '81', '31', '78', '140']
parsed_discourse_facet ['method_citation']
<S sid ="60" ssid = "24">The matching sentence pairs are then added to the IN corpus  and the system is re-trained.</S><S sid ="30" ssid = "27">A similar maximumlikelihood approach was used by Foster and Kuhn (2007)  but for language models only.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="145" ssid = "2">Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'30'", "'24'", "'65'", "'145'"]
'60'
'30'
'24'
'65'
'145'
['60', '30', '24', '65', '145']
parsed_discourse_facet ['results_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="22" ssid = "19">Within this framework  we use features intended to capture degree of generality  including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'20'", "'130'", "'49'", "'22'"]
'127'
'20'
'130'
'49'
'22'
['127', '20', '130', '49', '22']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="119" ssid = "23">The 2nd block contains the IR system  which was tuned by selecting text in multiples of the size of the EMEA training corpus  according to dev set performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'130'", "'20'", "'49'", "'119'"]
'127'
'130'
'20'
'49'
'119'
['127', '130', '20', '49', '119']
parsed_discourse_facet ['method_citation']
<S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S><S sid ="66" ssid = "3">The weight on each sentence is a value in [0  1] computed by a perceptron with Boolean features that indicate collection and genre membership.</S><S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="113" ssid = "17">The corpus was wordaligned using both HMM and IBM2 models  and the phrase table was the union of phrases extracted from these separate alignments  with a length limit of 7.</S><S sid ="102" ssid = "6">The dev corpus was taken from the NIST05 evaluation set  augmented with some randomly-selected material reserved from the training set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'66'", "'75'", "'113'", "'102'"]
'143'
'66'
'75'
'113'
'102'
['143', '66', '75', '113', '102']
parsed_discourse_facet ['method_citation']
<S sid ="44" ssid = "8">When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.</S><S sid ="79" ssid = "16">This combination generalizes (2) and (3): we use either at = a to obtain a fixed-weight linear combination  or at = cI(t)/(cI(t) + 0) to obtain a MAP combination.</S><S sid ="62" ssid = "26">To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S><S sid ="115" ssid = "19">Table 2 shows results for both settings and all methods described in sections 2 and 3.</S><S sid ="13" ssid = "10">The techniques we develop can be extended in a relatively straightforward manner to the more general case when OUT consists of multiple sub-domains.</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'79'", "'62'", "'115'", "'13'"]
'44'
'79'
'62'
'115'
'13'
['44', '79', '62', '115', '13']
parsed_discourse_facet ['method_citation']
<S sid ="106" ssid = "10">The corpora for both settings are summarized in table 1.</S><S sid ="60" ssid = "24">The matching sentence pairs are then added to the IN corpus  and the system is re-trained.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="53" ssid = "17">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'60'", "'81'", "'38'", "'53'"]
'106'
'60'
'81'
'38'
'53'
['106', '60', '81', '38', '53']
parsed_discourse_facet ['method_citation']
<S sid ="95" ssid = "32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S><S sid ="50" ssid = "14">Linear weights are difficult to incorporate into the standard MERT procedure because they are “hidden” within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="59" ssid = "23">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid ="51" ssid = "15">For the LM  adaptive weights are set as follows: where α is a weight vector containing an element αi for each domain (just IN and OUT in our case)  pi are the corresponding domain-specific models  and ˜p(w  h) is an empirical distribution from a targetlanguage training corpus—we used the IN dev set for this.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'50'", "'21'", "'59'", "'51'"]
'95'
'50'
'21'
'59'
'51'
['95', '50', '21', '59', '51']
parsed_discourse_facet ['aim_citation', 'results_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



D10-1044
P12-1099
0
method_citation
['method_citation']
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P11-1060_sweta.csv
<S sid="8" ssid="4">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
 <S sid="45" ssid="21">The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["45'"]
45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="27">It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent&#8217;s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
 <S sid="166" ssid="51">Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language&#8212;think grounded al., 2010). compositional semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
 <S sid="8" ssid="4">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
 <S sid="11" ssid="7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["11'"]
11'
['11']
parsed_discourse_facet ['method_citation']
  <S sid="115" ssid="91">After training, given a new utterance x, our system outputs the most likely y, summing out the latent logical form z: argmaxy p&#952;(T)(y  |x, z &#8712; &#732;ZL,&#952;(T)).</S>
original cit marker offset is 0
new cit marker offset is 0



["115'"]
115'
['115']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="70">We now turn to the task of mapping natural language For the example in Figure 4(b), the de- utterances to DCS trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["94'"]
94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="157" ssid="42">Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["157'"]
157'
['157']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["106'"]
106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="52">In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="23">Next, we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S>
original cit marker offset is 0
new cit marker offset is 0



["138'"]
138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="16">The CSP has two types of constraints: (i) x &#8712; w(p) for each node x labeled with predicate p &#8712; P; and (ii) xj = yj0 (the j-th component of x must equal the j'-th component of y) for each edge (x, y) labeled with j0j &#8712; R. A solution to the CSP is an assignment of nodes to values that satisfies all the constraints.</S>
original cit marker offset is 0
new cit marker offset is 0



["40'"]
40'
['40']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["171'"]
171'
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="158" ssid = "43">5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation  DCS  which offers a new perspective lexical semantics.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="23" ssid = "19">We trained our model using an EM-like algorithm (Section 3) on two benchmarks  GEO and JOBS (Section 4).</S><S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'158'", "'21'", "'23'", "'132'"]
'26'
'158'
'21'
'23'
'132'
['26', '158', '21', '23', '132']
parsed_discourse_facet ['results_citation']
<S sid ="39" ssid = "15">Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.</S><S sid ="56" ssid = "32">The basic version of DCS described thus far handles a core subset of language.</S><S sid ="4" ssid = "4">On two stansemantic parsing benchmarks our system obtains the highest published accuracies  despite requiring no annotated logical forms.</S><S sid ="136" ssid = "21">In fact  DCS performs comparably to even the version of SEMRESP trained using logical forms.</S><S sid ="83" ssid = "59">It suffices to define Xi(d) for a single column i.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'56'", "'4'", "'136'", "'83'"]
'39'
'56'
'4'
'136'
'83'
['39', '56', '4', '136', '83']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S><S sid ="148" ssid = "33">This bootstrapping behavior occurs naturally: The “easy” examples are processed first  where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'100'", "'115'", "'88'", "'148'"]
'26'
'100'
'115'
'88'
'148'
['26', '100', '115', '88', '148']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="138" ssid = "23">Next  we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="53" ssid = "29">Having instantiated s as a value  everything above this node is an ordinary CSP: s constrains the count node  which in turns constrains the root node to |s|.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'138'", "'26'", "'53'"]
'100'
'21'
'138'
'26'
'53'
['100', '21', '138', '26', '53']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S><S sid ="94" ssid = "70">We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'94'", "'134'", "'8'", "'58'"]
'18'
'94'
'134'
'8'
'58'
['18', '94', '134', '8', '58']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'21'", "'100'", "'115'", "'42'"]
'26'
'21'
'100'
'115'
'42'
['26', '21', '100', '115', '42']
parsed_discourse_facet ['method_citation']
<S sid ="87" ssid = "63">For example  in Figure 4(a)  before execution  the denotation of the DCS tree is hh{[(CA  OR)  (OR)] ... }; ø; (E  Qhstatei�w  ø)ii; after applying X1  we have hh{[(OR)]  ... }; øii.</S><S sid ="113" ssid = "89">Formally  let ˜O(θ  θ') be the objective function O(θ) with ZL(x) ˜ZL θI(x).</S><S sid ="20" ssid = "16">At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="74" ssid = "50">Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'", "'113'", "'20'", "'154'", "'74'"]
'87'
'113'
'20'
'154'
'74'
['87', '113', '20', '154', '74']
parsed_discourse_facet ['method_citation']
<S sid ="117" ssid = "2">In each dataset  each sentence x is annotated with a Prolog logical form  which we use only to evaluate and get an answer y.</S><S sid ="1" ssid = "1">Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'117'", "'1'", "'15'", "'88'", "'62'"]
'117'
'1'
'15'
'88'
'62'
['117', '1', '15', '88', '62']
parsed_discourse_facet ['method_citation']
<S sid ="83" ssid = "59">It suffices to define Xi(d) for a single column i.</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="101" ssid = "77">Formally  pθ(z  |x) ∝ eφ(x z)Tθ  where θ and φ(x  z) are parameter and feature vectors  respectively.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'169'", "'8'", "'101'", "'134'"]
'83'
'169'
'8'
'101'
'134'
['83', '169', '8', '101', '134']
parsed_discourse_facet ['method_citation']
<S sid ="1" ssid = "1">Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid ="99" ssid = "75">To further reduce the search space  F imposes a few additional constraints  e.g.  limiting the number of marked nodes to 2 and only allowing trace predicates between arity 1 predicates.</S><S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S><S sid ="129" ssid = "14">We also define an augmented lexicon L+ which includes a prototype word x for each predicate appearing in (iii) above (e.g.  (large  size))  which cancels the predicates triggered by x’s POS tag.</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'99'", "'132'", "'129'", "'26'"]
'1'
'99'
'132'
'129'
'26'
['1', '99', '132', '129', '26']
parsed_discourse_facet ['method_citation']
<S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="54" ssid = "30">A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S><S sid ="166" ssid = "51">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'54'", "'42'", "'166'", "'132'"]
'21'
'54'
'42'
'166'
'132'
['21', '54', '42', '166', '132']
parsed_discourse_facet ['method_citation']
<S sid ="59" ssid = "35">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S><S sid ="54" ssid = "30">A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="45" ssid = "21">The logical forms in DCS are called DCS trees  where nodes are labeled with predicates  and edges are labeled with relations.</S><S sid ="43" ssid = "19">Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter  2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'54'", "'154'", "'45'", "'43'"]
'59'
'54'
'154'
'45'
'43'
['59', '54', '154', '45', '43']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "30">A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="74" ssid = "50">Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S><S sid ="107" ssid = "83">Our model is arc-factored  so we can sum over all DCS trees in ZL(x) using dynamic programming.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'42'", "'21'", "'74'", "'107'"]
'54'
'42'
'21'
'74'
'107'
['54', '42', '21', '74', '107']
parsed_discourse_facet ['results_citation']
<S sid ="59" ssid = "35">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S><S sid ="148" ssid = "33">This bootstrapping behavior occurs naturally: The “easy” examples are processed first  where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S><S sid ="47" ssid = "23">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.</S><S sid ="103" ssid = "79">To define the features  we technically need to augment each tree z ∈ ZL(x) with alignment information—namely  for each predicate in z  the span in x (if any) that triggered it.</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'148'", "'47'", "'103'", "'62'"]
'59'
'148'
'47'
'103'
'62'
['59', '148', '47', '103', '62']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="23" ssid = "19">We trained our model using an EM-like algorithm (Section 3) on two benchmarks  GEO and JOBS (Section 4).</S><S sid ="86" ssid = "62">Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  −(i  ø)]{α1 = ø}.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'23'", "'86'", "'115'", "'100'"]
'26'
'23'
'86'
'115'
'100'
['26', '23', '86', '115', '100']
parsed_discourse_facet ['method_citation']
<S sid ="113" ssid = "89">Formally  let ˜O(θ  θ') be the objective function O(θ) with ZL(x) ˜ZL θI(x).</S><S sid ="78" ssid = "54">The stores are also concatenated (α + α').</S><S sid ="74" ssid = "50">Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="111" ssid = "87">Let ˜ZL θ(x) be this approximation of ZL(x).</S>
original cit marker offset is 0
new cit marker offset is 0



["'113'", "'78'", "'74'", "'154'", "'111'"]
'113'
'78'
'74'
'154'
'111'
['113', '78', '74', '154', '111']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="63" ssid = "39">Think of d as consisting of n columns  one for each active node according to a pre-order traversal of z.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="86" ssid = "62">Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  −(i  ø)]{α1 = ø}.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'100'", "'63'", "'115'", "'86'"]
'26'
'100'
'63'
'115'
'86'
['26', '100', '63', '115', '86']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'26'", "'115'", "'42'"]
'100'
'21'
'26'
'115'
'42'
['100', '21', '26', '115', '42']
parsed_discourse_facet ['method_citation']
<S sid ="151" ssid = "36">Inspecting the final parameters calculus formulae.</S><S sid ="56" ssid = "32">The basic version of DCS described thus far handles a core subset of language.</S><S sid ="16" ssid = "12">Which one should we use?</S><S sid ="118" ssid = "3">This evaluation is done with respect to a world w. Recall that a world w maps each predicate p ∈ P to a set of tuples w(p).</S><S sid ="33" ssid = "9">As another example  w(average) = {(S  ¯x) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'56'", "'16'", "'118'", "'33'"]
'151'
'56'
'16'
'118'
'33'
['151', '56', '16', '118', '33']
parsed_discourse_facet ['aim_citation', 'results_citation']



P11-1060
P12-1045
0
method_citation
['method_citation']
parsing: input/ref/Task1/W06-3114_aakansha.csv
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="2">Training and testing is based on the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="2">Training and testing is based on the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'"]
'102'
['102']
parsed_discourse_facet ['method_citation']
<S sid="84" ssid="23">The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'"]
'84'
['84']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="4">To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="19">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="6">Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="3">&#8226; We evaluated translation from English, in addition to into English.</S>
    <S sid="6" ssid="4">English was again paired with German, French, and Spanish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'6'"]
'5'
'6'
['5', '6']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="80" ssid = "19">We collected around 300–400 judgements per judgement type (adequacy or fluency)  per system  per language pair.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'126'", "'80'", "'136'", "'0'"]
'134'
'126'
'80'
'136'
'0'
['134', '126', '80', '136', '0']
parsed_discourse_facet ['results_citation']
<S sid ="159" ssid = "52">(b) does the translation have the same meaning  including connotations?</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'159'", "'0'", "'84'", "'17'", "'170'"]
'159'
'0'
'84'
'17'
'170'
['159', '0', '84', '17', '170']
parsed_discourse_facet ['method_citation']
<S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'", "'18'", "'16'", "'126'", "'84'"]
'170'
'18'
'16'
'126'
'84'
['170', '18', '16', '126', '84']
parsed_discourse_facet ['method_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'17'", "'170'", "'84'", "'38'"]
'126'
'17'
'170'
'84'
'38'
['126', '17', '170', '84', '38']
parsed_discourse_facet ['method_citation']
<S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'", "'38'", "'126'", "'17'", "'84'"]
'170'
'38'
'126'
'17'
'84'
['170', '38', '126', '17', '84']
parsed_discourse_facet ['method_citation']
<S sid ="52" ssid = "18">Pairwise comparison: We can use the same method to assess the statistical significance of one system outperforming another.</S><S sid ="21" ssid = "14">The out-of-domain test set differs from the Europarl data in various ways.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="129" ssid = "22">All systems (except for Systran  which was not tuned to Europarl) did considerably worse on outof-domain training data.</S><S sid ="35" ssid = "1">For the automatic evaluation  we used BLEU  since it is the most established metric in the field.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'", "'21'", "'28'", "'129'", "'35'"]
'52'
'21'
'28'
'129'
'35'
['52', '21', '28', '129', '35']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="113" ssid = "6">The confidence intervals are computed by bootstrap resampling for BLEU  and by standard significance testing for the manual scores  as described earlier in the paper.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'113'", "'126'", "'28'", "'0'"]
'16'
'113'
'126'
'28'
'0'
['16', '113', '126', '28', '0']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="109" ssid = "2">The scores and confidence intervals are detailed first in the Figures 7–10 in table form (including ranks)  and then in graphical form in Figures 11–16.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'16'", "'38'", "'84'", "'109'"]
'18'
'16'
'38'
'84'
'109'
['18', '16', '38', '84', '109']
parsed_discourse_facet ['method_citation']
<S sid ="9" ssid = "2">Training and testing is based on the Europarl corpus.</S><S sid ="44" ssid = "10">We computed BLEU scores for each submission with a single reference translation.</S><S sid ="143" ssid = "36">For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.</S><S sid ="25" ssid = "18">We received submissions from 14 groups from 11 institutions  as listed in Figure 2.</S><S sid ="35" ssid = "1">For the automatic evaluation  we used BLEU  since it is the most established metric in the field.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'44'", "'143'", "'25'", "'35'"]
'9'
'44'
'143'
'25'
'35'
['9', '44', '143', '25', '35']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="68" ssid = "7">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'68'", "'0'", "'28'", "'136'"]
'134'
'68'
'0'
'28'
'136'
['134', '68', '0', '28', '136']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="68" ssid = "7">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'0'", "'68'", "'126'", "'17'"]
'134'
'0'
'68'
'126'
'17'
['134', '0', '68', '126', '17']
parsed_discourse_facet ['method_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="159" ssid = "52">(b) does the translation have the same meaning  including connotations?</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'134'", "'17'", "'84'", "'159'"]
'126'
'134'
'17'
'84'
'159'
['126', '134', '17', '84', '159']
parsed_discourse_facet ['method_citation']
<S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'38'", "'155'", "'170'", "'6'"]
'84'
'38'
'155'
'170'
'6'
['84', '38', '155', '170', '6']
parsed_discourse_facet ['results_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'134'", "'0'", "'17'", "'84'"]
'126'
'134'
'0'
'17'
'84'
['126', '134', '0', '17', '84']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="68" ssid = "7">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'0'", "'126'", "'68'", "'17'"]
'134'
'0'
'126'
'68'
'17'
['134', '0', '126', '68', '17']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'170'", "'84'", "'126'", "'17'"]
'0'
'170'
'84'
'126'
'17'
['0', '170', '84', '126', '17']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "19">We collected around 300–400 judgements per judgement type (adequacy or fluency)  per system  per language pair.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="150" ssid = "43">For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French)  at least as measured by BLEU.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'84'", "'150'", "'16'", "'155'"]
'80'
'84'
'150'
'16'
'155'
['80', '84', '150', '16', '155']
parsed_discourse_facet ['method_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'84'", "'38'", "'17'", "'170'"]
'126'
'84'
'38'
'17'
'170'
['126', '84', '38', '17', '170']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P11-1061_sweta.csv
 <S sid="9" ssid="5">Unfortunately, the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al., 2010), making its practical usability questionable at best.</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="47" ssid="13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["47'"]
47'
['47']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["10'"]
10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="1">Given the bilingual graph described in the previous section, we can use label propagation to project the English POS labels to the foreign language.</S>
original cit marker offset is 0
new cit marker offset is 0



["70'"]
70'
['70']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="18">This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
 <S sid="83" ssid="14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value &#964;: We describe how we choose &#964; in &#167;6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["83'"]
83'
['83']
parsed_discourse_facet ['method_citation']
<S sid="113" ssid="13">For each language under consideration, Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["113'"]
113'
['113']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
 <S sid="13" ssid="9">(2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available.</S>
original cit marker offset is 0
new cit marker offset is 0



["13'"]
13'
['13']
parsed_discourse_facet ['method_citation']
 <S sid="3" ssid="3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="20">We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Syntactic universals are a well studied concept in linguistics (Carnie, 2002; Newmeyer, 2005), and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["19'"]
19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="16">Figure 2 shows an excerpt of a sentence from the Italian test set and the tags assigned by four different models, as well as the gold tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



["161'"]
161'
['161']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["23'"]
23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["23'"]
23'
['23']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1061.csv
<S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="62" ssid = "28">Note that since all English vertices were extracted from the parallel text  we will have an initial label distribution for all vertices in Ve.</S><S sid ="68" ssid = "34">In the label propagation stage  we propagate the automatic English tags to the aligned Italian trigram types  followed by further propagation solely among the Italian vertices. the Italian vertices are connected to an automatically labeled English vertex.</S><S sid ="42" ssid = "8">The foreign language vertices (denoted by Vf) correspond to foreign trigram types  exactly as in Subramanya et al. (2010).</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'62'", "'68'", "'42'", "'160'"]
'19'
'62'
'68'
'42'
'160'
['19', '62', '68', '42', '160']
parsed_discourse_facet ['results_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid ="30" ssid = "7">To initialize the graph we tag the English side of the parallel text using a supervised model.</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'113'", "'30'", "'102'", "'25'"]
'26'
'113'
'30'
'102'
'25'
['26', '113', '30', '102', '25']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'113'", "'25'", "'3'", "'19'"]
'16'
'113'
'25'
'3'
'19'
['16', '113', '25', '3', '19']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="119" ssid = "19">To provide a thorough analysis  we evaluated three baselines and two oracles in addition to two variants of our graph-based approach.</S><S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="82" ssid = "13">We formulate the update as follows: where ∀ui ∈ Vf \ Vfl  γi(y) and κi are defined as: We ran this procedure for 10 iterations.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'119'", "'54'", "'82'", "'28'"]
'134'
'119'
'54'
'82'
'28'
['134', '119', '54', '82', '28']
parsed_discourse_facet ['method_citation']
<S sid ="71" ssid = "2">We use label propagation in two stages to generate soft labels on all the vertices in the graph.</S><S sid ="61" ssid = "27">These tag distributions are used to initialize the label distributions over the English vertices in the graph.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="86" ssid = "17">For a sentence x and a state sequence z  a first order Markov model defines a distribution: (9) where Val(X) corresponds to the entire vocabulary.</S><S sid ="124" ssid = "24">We tried two versions of our graph-based approach: feature after the first stage of label propagation (Eq.</S>
original cit marker offset is 0
new cit marker offset is 0



["'71'", "'61'", "'3'", "'86'", "'124'"]
'71'
'61'
'3'
'86'
'124'
['71', '61', '3', '86', '124']
parsed_discourse_facet ['method_citation']
<S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="57" ssid = "23">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'26'", "'57'", "'97'", "'28'"]
'89'
'26'
'57'
'97'
'28'
['89', '26', '57', '97', '28']
parsed_discourse_facet ['method_citation']
<S sid ="93" ssid = "24">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'93'", "'26'", "'28'", "'97'", "'89'"]
'93'
'26'
'28'
'97'
'89'
['93', '26', '28', '97', '89']
parsed_discourse_facet ['method_citation']
<S sid ="92" ssid = "23">To optimize this function  we used L-BFGS  a quasi-Newton method (Liu and Nocedal  1989).</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="49" ssid = "15">We define a symmetric similarity function K(uZ7 uj) over two foreign language vertices uZ7 uj E Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1.</S><S sid ="142" ssid = "5">The “No LP” model does not outperform direct projection for German and Greek  but performs better for six out of eight languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'102'", "'19'", "'49'", "'142'"]
'92'
'102'
'19'
'49'
'142'
['92', '102', '19', '49', '142']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="159" ssid = "2">Because we are interested in applying our techniques to languages for which no labeled resources are available  we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'159'", "'97'", "'89'", "'28'"]
'26'
'159'
'97'
'89'
'28'
['26', '159', '97', '89', '28']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'97'", "'102'", "'28'", "'83'"]
'134'
'97'
'102'
'28'
'83'
['134', '97', '102', '28', '83']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="72" ssid = "3">In the first stage  we run a single step of label propagation  which transfers the label distributions from the English vertices to the connected foreign language vertices (say  Vf�) at the periphery of the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'97'", "'28'", "'102'", "'72'"]
'26'
'97'
'28'
'102'
'72'
['26', '97', '28', '102', '72']
parsed_discourse_facet ['method_citation']
<S sid ="95" ssid = "26">This feature ft incorporates information from the smoothed graph and prunes hidden states that are inconsistent with the thresholded vector tx.</S><S sid ="15" ssid = "11">First  we use a novel graph-based framework for projecting syntactic information across language boundaries.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid ="41" ssid = "7">Because the information flow in our graph is asymmetric (from English to the foreign language)  we use different types of vertices for each language.</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'15'", "'25'", "'41'", "'113'"]
'95'
'15'
'25'
'41'
'113'
['95', '15', '25', '41', '113']
parsed_discourse_facet ['method_citation']
<S sid ="21" ssid = "17">These universal POS categories not only facilitate the transfer of POS information from one language to another  but also relieve us from using controversial evaluation metrics 2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S><S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="10" ssid = "6">To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S><S sid ="36" ssid = "2">Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand  using individual words as the vertices throws away the context necessary for disambiguation; on the other hand  it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'16'", "'10'", "'29'", "'36'"]
'21'
'16'
'10'
'29'
'36'
['21', '16', '10', '29', '36']
parsed_discourse_facet ['results_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S><S sid ="35" ssid = "1">In graph-based learning approaches one constructs a graph whose vertices are labeled and unlabeled examples  and whose weighted edges encode the degree to which the examples they link have the same label (Zhu et al.  2003).</S><S sid ="159" ssid = "2">Because we are interested in applying our techniques to languages for which no labeled resources are available  we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'54'", "'89'", "'35'", "'159'"]
'26'
'54'
'89'
'35'
'159'
['26', '54', '89', '35', '159']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'28'", "'134'", "'97'", "'89'"]
'26'
'28'
'134'
'97'
'89'
['26', '28', '134', '97', '89']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="21" ssid = "17">These universal POS categories not only facilitate the transfer of POS information from one language to another  but also relieve us from using controversial evaluation metrics 2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'21'", "'160'", "'113'", "'29'"]
'16'
'21'
'160'
'113'
'29'
['16', '21', '160', '113', '29']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'28'", "'97'", "'26'", "'89'"]
'134'
'28'
'97'
'26'
'89'
['134', '28', '97', '26', '89']
parsed_discourse_facet ['method_citation']
<S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="49" ssid = "15">We define a symmetric similarity function K(uZ7 uj) over two foreign language vertices uZ7 uj E Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1.</S><S sid ="136" ssid = "36">For graph propagation  the hyperparameter v was set to 2 x 10−6 and was not tuned.</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'102'", "'49'", "'136'", "'83'"]
'97'
'102'
'49'
'136'
'83'
['97', '102', '49', '136', '83']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/E03-1005_aakansha.csv
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'"]
'105'
['105']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'"]
'145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'"]
'80'
['80']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'"]
'143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid="141" ssid="6">This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'141'"]
'140'
'141'
['140', '141']
parsed_discourse_facet ['result_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="102" ssid="5">In case the shortest derivation is not unique, Bod (2000b) proposes to back off to a frequency ordering of the subtrees.</S>
    <S sid="103" ssid="6">That is, all subtrees of each root label are assigned a rank according to their frequency in the treebank: the most frequent subtree (or subtrees) of each root label gets rank 1, the second most frequent subtree gets rank 2, etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'", "'103'"]
'102'
'103'
['102', '103']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="46" ssid="43">Most previous notions of best parse tree in DOP1 were based on a probabilistic metric, with Bod (2000b) as a notable exception, who used a simplicity metric based on the shortest derivation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'"]
'46'
['46']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="37">For example, Bod (2001) samples a fixed number of subtrees of each depth, which has the effect of assigning roughly equal weight to each node in the training data, and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S>
    <S sid="86" ssid="38">Bod reports state-of-the-art results with this method, and observes no decrease in parse accuracy when larger subtrees are included (using subtrees up to depth 14).</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'", "'86'"]
'85'
'86'
['85', '86']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="115" ssid="18">The only thing that needs to be changed for Simplicity-DOP is that all subtrees should be assigned equal probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="71" ssid = "23">For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'64'", "'99'", "'71'", "'39'"]
'7'
'64'
'99'
'71'
'39'
['7', '64', '99', '71', '39']
parsed_discourse_facet ['results_citation']
<S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="27" ssid = "24">Goodman (1996  1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set  thus converting the exponential number of subtrees to a compact grammar.</S><S sid ="88" ssid = "40">In this paper  we will test a simple extension of Goodman's compact PCFG-reduction of DOP which has the same property as the normalization proposed in Bod (2001) in that it assigns roughly equal weight to each node in the training data.</S><S sid ="69" ssid = "21">Thus  rather than using the large  explicit DOP1 model  one can also use this small PCFG that generates isomorphic derivations  with identical probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'130'", "'27'", "'88'", "'69'"]
'140'
'130'
'27'
'88'
'69'
['140', '130', '27', '88', '69']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "2">The probability of a parse tree is computed from the occurrencefrequencies of the subtrees in the treebank.</S><S sid ="76" ssid = "28">Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.</S><S sid ="46" ssid = "43">Most previous notions of best parse tree in DOP1 were based on a probabilistic metric  with Bod (2000b) as a notable exception  who used a simplicity metric based on the shortest derivation.</S><S sid ="91" ssid = "43">It easy to see that this is equivalent to reducing the probability of a tree by a factor of four for each pair of nonterminals it contains  resulting in the PCFG reduction in figure 4.</S><S sid ="52" ssid = "4">The probability of a parse tree T is the sum of the probabilities of its distinct derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'76'", "'46'", "'91'", "'52'"]
'50'
'76'
'46'
'91'
'52'
['50', '76', '46', '91', '52']
parsed_discourse_facet ['method_citation']
<S sid ="74" ssid = "26">Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.</S><S sid ="124" ssid = "5">We employed the same unknown (category) word model as in Bod (2001)  based on statistics on word-endings  hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 87).</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="96" ssid = "48">However  as mentioned above  efficient parsing does not necessarily mean efficient disambiguation: the exact computation of the most probable parse remains exponential.</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'124'", "'40'", "'13'", "'96'"]
'74'
'124'
'40'
'13'
'96'
['74', '124', '40', '13', '96']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">An Efficient Implementation of a New DOP Model</S><S sid ="120" ssid = "1">For our experiments we used the standard division of the WSJ (Marcus et al. 1993)  with sections 2 through 21 for training (approx.</S><S sid ="34" ssid = "31">But even with cross-validation  ML-DOP is outperformed by the much simpler DOP1 model on both the ATIS and OVIS treebanks (Bod 2000b).</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'120'", "'34'", "'15'", "'40'"]
'0'
'120'
'34'
'15'
'40'
['0', '120', '34', '15', '40']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="3" ssid = "3">Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models  and an average processing time of 3.6 seconds per WSJ sentence.</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'3'", "'37'", "'39'", "'85'"]
'7'
'3'
'37'
'39'
'85'
['7', '3', '37', '39', '85']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'64'", "'136'", "'99'", "'140'"]
'7'
'64'
'136'
'99'
'140'
['7', '64', '136', '99', '140']
parsed_discourse_facet ['method_citation']
<S sid ="145" ssid = "10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S><S sid ="46" ssid = "43">Most previous notions of best parse tree in DOP1 were based on a probabilistic metric  with Bod (2000b) as a notable exception  who used a simplicity metric based on the shortest derivation.</S><S sid ="124" ssid = "5">We employed the same unknown (category) word model as in Bod (2001)  based on statistics on word-endings  hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 87).</S><S sid ="105" ssid = "8">The derivation with the smallest sum  or highest rank  is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP  its results are still rather impressive for such a simple model.</S><S sid ="41" ssid = "38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'", "'46'", "'124'", "'105'", "'41'"]
'145'
'46'
'124'
'105'
'41'
['145', '46', '124', '105', '41']
parsed_discourse_facet ['method_citation']
<S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="42" ssid = "39">We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'64'", "'99'", "'42'", "'136'"]
'140'
'64'
'99'
'42'
'136'
['140', '64', '99', '42', '136']
parsed_discourse_facet ['method_citation']
<S sid ="142" ssid = "7">Compared to the reranking technique in Collins (2000)  who obtained an LP of 89.9% and an LR of 89.6%  our results show a 9% relative error rate reduction.</S><S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="141" ssid = "6">This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S><S sid ="59" ssid = "11">A new nonterminal is created for each node in the training data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'7'", "'141'", "'136'", "'59'"]
'142'
'7'
'141'
'136'
'59'
['142', '7', '141', '136', '59']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="116" ssid = "19">Then the shortest derivation is equal to the most probable derivation and can be computed by standard Viterbi optimization  which can be seen as follows: if each subtree has a probability p then the probability of a derivation involving n subtrees is equal to pn  and since 0<p<1  the derivation with the fewest subtrees has the greatest probability.</S><S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'116'", "'85'", "'37'", "'39'"]
'7'
'116'
'85'
'37'
'39'
['7', '116', '85', '37', '39']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="103" ssid = "6">That is  all subtrees of each root label are assigned a rank according to their frequency in the treebank: the most frequent subtree (or subtrees) of each root label gets rank 1  the second most frequent subtree gets rank 2  etc.</S><S sid ="134" ssid = "15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'37'", "'99'", "'103'", "'134'"]
'7'
'37'
'99'
'103'
'134'
['7', '37', '99', '103', '134']
parsed_discourse_facet ['method_citation']
<S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S><S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="88" ssid = "40">In this paper  we will test a simple extension of Goodman's compact PCFG-reduction of DOP which has the same property as the normalization proposed in Bod (2001) in that it assigns roughly equal weight to each node in the training data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'85'", "'7'", "'39'", "'88'"]
'37'
'85'
'7'
'39'
'88'
['37', '85', '7', '39', '88']
parsed_discourse_facet ['results_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'37'", "'140'", "'39'", "'64'"]
'7'
'37'
'140'
'39'
'64'
['7', '37', '140', '39', '64']
parsed_discourse_facet ['method_citation']
<S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S><S sid ="42" ssid = "39">We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="141" ssid = "6">This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'99'", "'136'", "'42'", "'140'", "'141'"]
'99'
'136'
'42'
'140'
'141'
['99', '136', '42', '140', '141']
parsed_discourse_facet ['method_citation']



E03-1005
P07-1051
0
method_citation
['results_citation']



E03-1005
W04-0305
0
result_citation
['method_citation']



E03-1005
W06-2905
0
result_citation
['method_citation']
parsing: input/ref/Task1/E03-1005_sweta.csv
<S sid="20" ssid="17">Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).</S>
original cit marker offset is 0
new cit marker offset is 0



["20'"]
20'
['20']
parsed_discourse_facet ['method_citation']
 <S sid="74" ssid="26">Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.</S>
original cit marker offset is 0
new cit marker offset is 0



["74'"]
74'
['74']
parsed_discourse_facet ['method_citation']
<S sid="44" ssid="41">But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["44'"]
44'
['44']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["145'"]
145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S>
original cit marker offset is 0
new cit marker offset is 0



["22'"]
22'
['22']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="14">It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).</S>
original cit marker offset is 0
new cit marker offset is 0



["133'"]
133'
['133']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier &amp; Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="35">Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
 <S sid="100" ssid="3">In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="29">However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
 <S sid="130" ssid="11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["130'"]
130'
['130']
parsed_discourse_facet ['method_citation']
 <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["140'"]
140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="24">Goodman (1996, 1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'"]
27'
['27']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="71" ssid = "23">For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'64'", "'99'", "'71'", "'39'"]
'7'
'64'
'99'
'71'
'39'
['7', '64', '99', '71', '39']
parsed_discourse_facet ['results_citation']
<S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="27" ssid = "24">Goodman (1996  1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set  thus converting the exponential number of subtrees to a compact grammar.</S><S sid ="88" ssid = "40">In this paper  we will test a simple extension of Goodman's compact PCFG-reduction of DOP which has the same property as the normalization proposed in Bod (2001) in that it assigns roughly equal weight to each node in the training data.</S><S sid ="69" ssid = "21">Thus  rather than using the large  explicit DOP1 model  one can also use this small PCFG that generates isomorphic derivations  with identical probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'130'", "'27'", "'88'", "'69'"]
'140'
'130'
'27'
'88'
'69'
['140', '130', '27', '88', '69']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "2">The probability of a parse tree is computed from the occurrencefrequencies of the subtrees in the treebank.</S><S sid ="76" ssid = "28">Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.</S><S sid ="46" ssid = "43">Most previous notions of best parse tree in DOP1 were based on a probabilistic metric  with Bod (2000b) as a notable exception  who used a simplicity metric based on the shortest derivation.</S><S sid ="91" ssid = "43">It easy to see that this is equivalent to reducing the probability of a tree by a factor of four for each pair of nonterminals it contains  resulting in the PCFG reduction in figure 4.</S><S sid ="52" ssid = "4">The probability of a parse tree T is the sum of the probabilities of its distinct derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'76'", "'46'", "'91'", "'52'"]
'50'
'76'
'46'
'91'
'52'
['50', '76', '46', '91', '52']
parsed_discourse_facet ['method_citation']
<S sid ="74" ssid = "26">Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.</S><S sid ="124" ssid = "5">We employed the same unknown (category) word model as in Bod (2001)  based on statistics on word-endings  hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 87).</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="96" ssid = "48">However  as mentioned above  efficient parsing does not necessarily mean efficient disambiguation: the exact computation of the most probable parse remains exponential.</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'124'", "'40'", "'13'", "'96'"]
'74'
'124'
'40'
'13'
'96'
['74', '124', '40', '13', '96']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">An Efficient Implementation of a New DOP Model</S><S sid ="120" ssid = "1">For our experiments we used the standard division of the WSJ (Marcus et al. 1993)  with sections 2 through 21 for training (approx.</S><S sid ="34" ssid = "31">But even with cross-validation  ML-DOP is outperformed by the much simpler DOP1 model on both the ATIS and OVIS treebanks (Bod 2000b).</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'120'", "'34'", "'15'", "'40'"]
'0'
'120'
'34'
'15'
'40'
['0', '120', '34', '15', '40']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="3" ssid = "3">Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models  and an average processing time of 3.6 seconds per WSJ sentence.</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'3'", "'37'", "'39'", "'85'"]
'7'
'3'
'37'
'39'
'85'
['7', '3', '37', '39', '85']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'64'", "'136'", "'99'", "'140'"]
'7'
'64'
'136'
'99'
'140'
['7', '64', '136', '99', '140']
parsed_discourse_facet ['method_citation']
<S sid ="145" ssid = "10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S><S sid ="46" ssid = "43">Most previous notions of best parse tree in DOP1 were based on a probabilistic metric  with Bod (2000b) as a notable exception  who used a simplicity metric based on the shortest derivation.</S><S sid ="124" ssid = "5">We employed the same unknown (category) word model as in Bod (2001)  based on statistics on word-endings  hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 87).</S><S sid ="105" ssid = "8">The derivation with the smallest sum  or highest rank  is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP  its results are still rather impressive for such a simple model.</S><S sid ="41" ssid = "38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'", "'46'", "'124'", "'105'", "'41'"]
'145'
'46'
'124'
'105'
'41'
['145', '46', '124', '105', '41']
parsed_discourse_facet ['method_citation']
<S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="42" ssid = "39">We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'64'", "'99'", "'42'", "'136'"]
'140'
'64'
'99'
'42'
'136'
['140', '64', '99', '42', '136']
parsed_discourse_facet ['method_citation']
<S sid ="142" ssid = "7">Compared to the reranking technique in Collins (2000)  who obtained an LP of 89.9% and an LR of 89.6%  our results show a 9% relative error rate reduction.</S><S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="141" ssid = "6">This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S><S sid ="59" ssid = "11">A new nonterminal is created for each node in the training data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'7'", "'141'", "'136'", "'59'"]
'142'
'7'
'141'
'136'
'59'
['142', '7', '141', '136', '59']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="116" ssid = "19">Then the shortest derivation is equal to the most probable derivation and can be computed by standard Viterbi optimization  which can be seen as follows: if each subtree has a probability p then the probability of a derivation involving n subtrees is equal to pn  and since 0<p<1  the derivation with the fewest subtrees has the greatest probability.</S><S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'116'", "'85'", "'37'", "'39'"]
'7'
'116'
'85'
'37'
'39'
['7', '116', '85', '37', '39']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="103" ssid = "6">That is  all subtrees of each root label are assigned a rank according to their frequency in the treebank: the most frequent subtree (or subtrees) of each root label gets rank 1  the second most frequent subtree gets rank 2  etc.</S><S sid ="134" ssid = "15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'37'", "'99'", "'103'", "'134'"]
'7'
'37'
'99'
'103'
'134'
['7', '37', '99', '103', '134']
parsed_discourse_facet ['method_citation']
<S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S><S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="88" ssid = "40">In this paper  we will test a simple extension of Goodman's compact PCFG-reduction of DOP which has the same property as the normalization proposed in Bod (2001) in that it assigns roughly equal weight to each node in the training data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'85'", "'7'", "'39'", "'88'"]
'37'
'85'
'7'
'39'
'88'
['37', '85', '7', '39', '88']
parsed_discourse_facet ['results_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'37'", "'140'", "'39'", "'64'"]
'7'
'37'
'140'
'39'
'64'
['7', '37', '140', '39', '64']
parsed_discourse_facet ['method_citation']
<S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S><S sid ="42" ssid = "39">We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="141" ssid = "6">This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'99'", "'136'", "'42'", "'140'", "'141'"]
'99'
'136'
'42'
'140'
'141'
['99', '136', '42', '140', '141']
parsed_discourse_facet ['method_citation']



E03-1005
W04-0305
0
method_citation
['method_citation']
parsing: input/ref/Task1/A00-2018_sweta.csv
 <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["17'"]
17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
 <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["17'"]
17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="11">Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &amp;quot;correct&amp;quot;, and statistics were collected on the resulting parses.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="10">(It is &amp;quot;soft&amp;quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.)</S>
original cit marker offset is 0
new cit marker offset is 0



["119'"]
119'
['119']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.</S>
original cit marker offset is 0
new cit marker offset is 0



["95'"]
95'
['95']
parsed_discourse_facet ['method_citation']
 <S sid="175" ssid="2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["175'"]
175'
['175']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
original cit marker offset is 0
new cit marker offset is 0



["92'"]
92'
['92']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="17">This is indicated in Figure 2, where the model labeled &amp;quot;Best&amp;quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["126'"]
126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="1">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c, t(c) (t for &amp;quot;tag&amp;quot;), then the lexical head of c, h(c), and then the expansion of c into further constituents e(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["174'"]
174'
['174']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="32">As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &amp;quot;before&amp;quot;), and the label of the grandparent of c (la).</S>
original cit marker offset is 0
new cit marker offset is 0



["63'"]
63'
['63']
parsed_discourse_facet ['method_citation']
<S sid="78" ssid="47">With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.</S>
original cit marker offset is 0
new cit marker offset is 0



["78'"]
78'
['78']
parsed_discourse_facet ['method_citation']
 <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="7">From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.</S>
original cit marker offset is 0
new cit marker offset is 0



["180'"]
180'
['180']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2018.csv
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'23'", "'62'", "'33'", "'27'"]
'30'
'23'
'62'
'33'
'27'
['30', '23', '62', '33', '27']
parsed_discourse_facet ['results_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'27'", "'62'", "'33'", "'23'"]
'30'
'27'
'62'
'33'
'23'
['30', '27', '62', '33', '23']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'62'", "'33'", "'89'", "'23'"]
'30'
'62'
'33'
'89'
'23'
['30', '62', '33', '89', '23']
parsed_discourse_facet ['method_citation']
<S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="20" ssid = "9">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="29" ssid = "18">So  for example  in a second-order Markov PCFG  L2 would be conditioned on L1 and M. In our complete model  of course  the probability of each label in the expansions is also conditioned on other material as specified in Equation 1  e.g.  p(e t  h  H).</S><S sid ="91" ssid = "2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'", "'20'", "'174'", "'29'", "'91'"]
'27'
'20'
'174'
'29'
'91'
['27', '20', '174', '29', '91']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'23'", "'62'", "'27'", "'33'"]
'30'
'23'
'62'
'27'
'33'
['30', '23', '62', '27', '33']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="63" ssid = "32">As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'1'", "'5'", "'27'", "'63'"]
'101'
'1'
'5'
'27'
'63'
['101', '1', '5', '27', '63']
parsed_discourse_facet ['method_citation']
<S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'", "'30'", "'62'", "'89'", "'23'"]
'27'
'30'
'62'
'89'
'23'
['27', '30', '62', '89', '23']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'62'", "'23'", "'89'", "'33'"]
'30'
'62'
'23'
'89'
'33'
['30', '62', '23', '89', '33']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "7">To compute a probability in a log-linear model one first defines a set of &quot;features&quot;  functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.</S><S sid ="91" ssid = "2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="20" ssid = "9">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid ="26" ssid = "15">Thus an expansion e(c) looks like: The expansion is generated by guessing first M  then in order L1 through L „.+1 (= A)  and similarly for RI through In a pure Markov PCFG we are given the left-hand side label 1 and then probabilistically generate the right-hand side conditioning on no information other than 1 and (possibly) previously generated pieces of the right-hand side itself.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'91'", "'174'", "'20'", "'26'"]
'38'
'91'
'174'
'20'
'26'
['38', '91', '174', '20', '26']
parsed_discourse_facet ['method_citation']
<S sid ="29" ssid = "18">So  for example  in a second-order Markov PCFG  L2 would be conditioned on L1 and M. In our complete model  of course  the probability of each label in the expansions is also conditioned on other material as specified in Equation 1  e.g.  p(e t  h  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="91" ssid = "2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'", "'27'", "'174'", "'91'", "'33'"]
'29'
'27'
'174'
'91'
'33'
['29', '27', '174', '91', '33']
parsed_discourse_facet ['method_citation']
<S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="101" ssid = "12">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S><S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'", "'27'", "'89'", "'101'", "'30'"]
'62'
'27'
'89'
'101'
'30'
['62', '27', '89', '101', '30']
parsed_discourse_facet ['method_citation']
<S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="115" ssid = "6">As noted in [5]  that system is based upon a &quot;tree-bank grammar&quot; - a grammar read directly off the training corpus.</S><S sid ="141" ssid = "32">(For example  part-ofspeech tagging using the most probable preterminal for each word is 90% accurate [8].)</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="184" ssid = "11">The first is the slight  but important  improvement achieved by using this model over conventional deleted interpolation  as indicated in Figure 2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'115'", "'141'", "'62'", "'184'"]
'23'
'115'
'141'
'62'
'184'
['23', '115', '141', '62', '184']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "9">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid ="76" ssid = "45">This requires finding the appropriate Ais for Equation 3  which is accomplished using an algorithm such as iterative scaling [II] in which values for the Ai are initially &quot;guessed&quot; and then modified until they converge on stable values.</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="110" ssid = "1">In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach  the aspect of the parser that is most novel.</S><S sid ="101" ssid = "12">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'76'", "'27'", "'110'", "'101'"]
'20'
'76'
'27'
'110'
'101'
['20', '76', '27', '110', '101']
parsed_discourse_facet ['results_citation']
<S sid ="74" ssid = "43">Suppose we were  in fact  going to compute a true maximum entropy model based upon the features used in Equation 7  Ii (t 1)  f2(t 1 1p)  f3(t 1 lp) .</S><S sid ="126" ssid = "17">This is indicated in Figure 2  where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%  0.4% lower than the results on the official test corpus.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'126'", "'125'", "'174'", "'27'"]
'74'
'126'
'125'
'174'
'27'
['74', '126', '125', '174', '27']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="42" ssid = "11">This feature is obviously composed of two sub-features  one recognizing t  the other 1.</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'89'", "'23'", "'42'", "'62'"]
'30'
'89'
'23'
'42'
'62'
['30', '89', '23', '42', '62']
parsed_discourse_facet ['method_citation']
<S sid ="176" ssid = "3">That the previous three best parsers on this test [5 9 17] all perform within a percentage point of each other  despite quite different basic mechanisms  led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training  and to conjecture that perhaps we were at it.</S><S sid ="58" ssid = "27">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid ="180" ssid = "7">From our perspective  perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.</S><S sid ="44" ssid = "13">Now consider computing a conditional probability p(a H) with a set of features h that connect a to the history H. In a log-linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability  the higher the absolute value of the associated A.</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'", "'58'", "'180'", "'44'", "'13'"]
'176'
'58'
'180'
'44'
'13'
['176', '58', '180', '44', '13']
parsed_discourse_facet ['method_citation']



A00-2018
P04-1042
0
method_citation
['method_citation']
parsing: input/ref/Task1/W11-2123_swastika.csv
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



['52']
52
['52']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="177" ssid="49">However, TRIE partitions storage by n-gram length, so walking the trie reads N disjoint pages.</S>
original cit marker offset is 0
new cit marker offset is 0



['177']
177
['177']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="278" ssid="5">We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns.</S>
original cit marker offset is 0
new cit marker offset is 0



['278']
278
['278']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/W11-2123.csv
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="27" ssid = "5">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'183'", "'69'", "'27'", "'231'"]
'0'
'183'
'69'
'27'
'231'
['0', '183', '69', '27', '231']
parsed_discourse_facet ['results_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="149" ssid = "21">RandLM and SRILM also remove context that will not extend  but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.</S><S sid ="193" ssid = "12">It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'183'", "'69'", "'149'", "'193'"]
'0'
'183'
'69'
'149'
'193'
['0', '183', '69', '149', '193']
parsed_discourse_facet ['method_citation']
<S sid ="62" ssid = "40">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S><S sid ="142" ssid = "14">Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn−1 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.</S><S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="216" ssid = "35">Though we are not able to calculate their memory usage on our model  results reported in their paper suggest lower memory consumption than TRIE on large-scale models  at the expense of CPU time.</S><S sid ="50" ssid = "28">Given counts cn1 where e.g. c1 is the vocabulary size  total memory consumption  in bits  is Our PROBING data structure places all n-grams of the same order into a single giant hash table.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'", "'142'", "'3'", "'216'", "'50'"]
'62'
'142'
'3'
'216'
'50'
['62', '142', '3', '216', '50']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="137" ssid = "9">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="157" ssid = "29">Each visited entry wni stores backoff b(wni ).</S><S sid ="27" ssid = "5">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'137'", "'183'", "'157'", "'27'"]
'0'
'137'
'183'
'157'
'27'
['0', '137', '183', '157', '27']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="233" ssid = "52">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid ="193" ssid = "12">It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'3'", "'233'", "'193'", "'69'"]
'183'
'3'
'233'
'193'
'69'
['183', '3', '233', '193', '69']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="38" ssid = "16">The ratio of buckets to entries is controlled by space multiplier m > 1.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="71" ssid = "49">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'0'", "'38'", "'88'", "'71'"]
'183'
'0'
'38'
'88'
'71'
['183', '0', '38', '88', '71']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="71" ssid = "49">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'0'", "'14'", "'71'", "'88'"]
'69'
'0'
'14'
'71'
'88'
['69', '0', '14', '71', '88']
parsed_discourse_facet ['method_citation']
<S sid ="124" ssid = "28">Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="227" ssid = "46">The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.</S><S sid ="21" ssid = "16">Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'69'", "'227'", "'21'", "'13'"]
'124'
'69'
'227'
'21'
'13'
['124', '69', '227', '21', '13']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="137" ssid = "9">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid ="116" ssid = "20">Both implementations employ a state object  opaque to the application  that carries information from one query to the next; we discuss both further in Section 4.2.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'0'", "'137'", "'116'", "'231'"]
'183'
'0'
'137'
'116'
'231'
['183', '0', '137', '116', '231']
parsed_discourse_facet ['method_citation']
<S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="118" ssid = "22">The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'183'", "'69'", "'118'", "'14'"]
'88'
'183'
'69'
'118'
'14'
['88', '183', '69', '118', '14']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="227" ssid = "46">The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.</S><S sid ="124" ssid = "28">Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'231'", "'183'", "'227'", "'124'"]
'69'
'231'
'183'
'227'
'124'
['69', '231', '183', '227', '124']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "29">Each visited entry wni stores backoff b(wni ).</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="53" ssid = "31">Sorted arrays store key-value pairs in an array sorted by key  incurring no space overhead.</S><S sid ="27" ssid = "5">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S><S sid ="118" ssid = "22">The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'0'", "'53'", "'27'", "'118'"]
'157'
'0'
'53'
'27'
'118'
['157', '0', '53', '27', '118']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="195" ssid = "14">Interpolation search has a more expensive pivot but performs less pivoting and reads  so it is slow on small data and faster on large data.</S><S sid ="116" ssid = "20">Both implementations employ a state object  opaque to the application  that carries information from one query to the next; we discuss both further in Section 4.2.</S><S sid ="137" ssid = "9">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid ="62" ssid = "40">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'195'", "'116'", "'137'", "'62'"]
'183'
'195'
'116'
'137'
'62'
['183', '195', '116', '137', '62']
parsed_discourse_facet ['results_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S><S sid ="233" ssid = "52">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid ="193" ssid = "12">It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'69'", "'231'", "'233'", "'193'"]
'183'
'69'
'231'
'233'
'193'
['183', '69', '231', '233', '193']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="201" ssid = "20">Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'69'", "'88'", "'183'", "'201'"]
'0'
'69'
'88'
'183'
'201'
['0', '69', '88', '183', '201']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="62" ssid = "40">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S><S sid ="102" ssid = "6">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie)  allocating memory all at once (eliminating the need for full pointers)  and being easy to compile.</S><S sid ="179" ssid = "51">We do not experiment with models larger than physical memory in this paper because TPT is unreleased  factors such as disk speed are hard to replicate  and in such situations we recommend switching to a more compact representation  such as RandLM.</S><S sid ="142" ssid = "14">Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn−1 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'62'", "'102'", "'179'", "'142'"]
'3'
'62'
'102'
'179'
'142'
['3', '62', '102', '179', '142']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="2" ssid = "2">The structure uses linear probing hash tables and is designed for speed.</S><S sid ="129" ssid = "1">In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.</S><S sid ="71" ssid = "49">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S><S sid ="201" ssid = "20">Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'2'", "'129'", "'71'", "'201'"]
'69'
'2'
'129'
'71'
'201'
['69', '2', '129', '71', '201']
parsed_discourse_facet ['method_citation']
<S sid ="199" ssid = "18">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid ="205" ssid = "24">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al.  2009).</S><S sid ="182" ssid = "1">This section measures performance on shared tasks in order of increasing complexity: sparse lookups  evaluating perplexity of a large file  and translation with Moses.</S><S sid ="67" ssid = "45">While sorted arrays could be used to implement the same data structure as PROBING  effectively making m = 1  we abandoned this implementation because it is slower and larger than a trie implementation.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'", "'205'", "'182'", "'67'", "'45'"]
'199'
'205'
'182'
'67'
'45'
['199', '205', '182', '67', '45']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="149" ssid = "21">RandLM and SRILM also remove context that will not extend  but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.</S><S sid ="201" ssid = "20">Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'183'", "'69'", "'149'", "'201'"]
'0'
'183'
'69'
'149'
'201'
['0', '183', '69', '149', '201']
parsed_discourse_facet ['aim_citation', 'results_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="118" ssid = "22">The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'0'", "'69'", "'88'", "'118'"]
'183'
'0'
'69'
'88'
'118'
['183', '0', '69', '88', '118']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P08-1043_sweta.csv
<S sid="156" ssid="34">To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["4'"]
4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="2">Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.</S>
original cit marker offset is 0
new cit marker offset is 0



["70'"]
70'
['70']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="3">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="9">Tsarfaty (2006) used a morphological analyzer (Segal, 2000), a PoS tagger (Bar-Haim et al., 2005), and a general purpose parser (Schmid, 2000) in an integrated framework in which morphological and syntactic components interact to share information, leading to improved performance on the joint task.</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="107" ssid="39">Firstly, Hebrew unknown tokens are doubly unknown: each unknown token may correspond to several segmentation possibilities, and each segment in such sequences may be able to admit multiple PoS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["107'"]
107'
['107']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="10">The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["14'"]
14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="12">The current work treats both segmental and super-segmental phenomena, yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg, 2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="53" ssid="11">Both (Tsarfaty, 2006; Cohen and Smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has shown a single generative model which handles both tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["53'"]
53'
['53']
parsed_discourse_facet ['method_citation']
 <S sid="48" ssid="6">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["48'"]
48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="141" ssid="19">This analyzer setting is similar to that of (Cohen and Smith, 2007), and models using it are denoted nohsp, Parser and Grammar We used BitPar (Schmid, 2004), an efficient general purpose parser,10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["141'"]
141'
['141']
parsed_discourse_facet ['method_citation']
<S sid="188" ssid="2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



["188'"]
188'
['188']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["94'"]
94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="12">Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training data similar to (Cohen and Smith, 2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
    <S sid="156" ssid="34">To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth, &#8216;tokens&#8217;) that constitute the unanalyzed surface forms (utterances).</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="31" ssid = "10">Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'92'", "'86'", "'192'", "'31'"]
'183'
'92'
'86'
'192'
'31'
['183', '92', '86', '192', '31']
parsed_discourse_facet ['results_citation']
<S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="173" ssid = "11">The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter (α) which is tuned separately for each of the tasks.</S><S sid ="77" ssid = "9">Segments with the same surface form but different PoS tags are treated as different lexemes  and are represented as separate arcs (e.g. the two arcs labeled neim from node 6 to 7).</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'192'", "'173'", "'77'", "'133'"]
'183'
'192'
'173'
'77'
'133'
['183', '192', '173', '77', '133']
parsed_discourse_facet ['method_citation']
<S sid ="141" ssid = "19">This analyzer setting is similar to that of (Cohen and Smith  2007)  and models using it are denoted nohsp  Parser and Grammar We used BitPar (Schmid  2004)  an efficient general purpose parser 10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.</S><S sid ="89" ssid = "21">Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree π s.t.</S><S sid ="51" ssid = "9">Tsarfaty (2006) used a morphological analyzer (Segal  2000)  a PoS tagger (Bar-Haim et al.  2005)  and a general purpose parser (Schmid  2000) in an integrated framework in which morphological and syntactic components interact to share information  leading to improved performance on the joint task.</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="82" ssid = "14">In sequential tagging models such as (Adler and Elhadad  2006; Bar-Haim et al.  2007; Smith et al.  2005) weights are assigned according to a language model The input for the joint task is a sequence W = w1  ...   wn of space-delimited tokens.</S>
original cit marker offset is 0
new cit marker offset is 0



["'141'", "'89'", "'51'", "'156'", "'82'"]
'141'
'89'
'51'
'156'
'82'
['141', '89', '51', '156', '82']
parsed_discourse_facet ['method_citation']
<S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'183'", "'86'", "'0'", "'92'"]
'192'
'183'
'86'
'0'
'92'
['192', '183', '86', '0', '92']
parsed_discourse_facet ['method_citation']
<S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'183'", "'0'", "'86'", "'92'"]
'192'
'183'
'0'
'86'
'92'
['192', '183', '0', '86', '92']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="89" ssid = "21">Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree π s.t.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'86'", "'192'", "'89'", "'92'"]
'183'
'86'
'192'
'89'
'92'
['183', '86', '192', '89', '92']
parsed_discourse_facet ['method_citation']
<S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'86'", "'57'", "'192'", "'183'"]
'92'
'86'
'57'
'192'
'183'
['92', '86', '57', '192', '183']
parsed_discourse_facet ['method_citation']
<S sid ="31" ssid = "10">Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="186" ssid = "24">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'", "'92'", "'133'", "'0'", "'186'"]
'31'
'92'
'133'
'0'
'186'
['31', '92', '133', '0', '186']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'92'", "'192'", "'183'", "'133'"]
'86'
'92'
'192'
'183'
'133'
['86', '92', '192', '183', '133']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'192'", "'183'", "'0'", "'92'"]
'86'
'192'
'183'
'0'
'92'
['86', '192', '183', '0', '92']
parsed_discourse_facet ['method_citation']
<S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="123" ssid = "1">Previous work on morphological and syntactic disambiguation in Hebrew used different sets of data  different splits  differing annotation schemes  and different evaluation measures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'57'", "'192'", "'86'", "'123'"]
'92'
'57'
'192'
'86'
'123'
['92', '57', '192', '86', '123']
parsed_discourse_facet ['method_citation']
<S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="69" ssid = "1">We represent all morphological analyses of a given utterance using a lattice structure.</S><S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="113" ssid = "45">The remaining arcs are marked OOV.</S><S sid ="50" ssid = "8">The joint morphological and syntactic hypothesis was first discussed in (Tsarfaty  2006; Tsarfaty and Sima’an  2004) and empirically explored in (Tsarfaty  2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'", "'69'", "'14'", "'113'", "'50'"]
'57'
'69'
'14'
'113'
'50'
['57', '69', '14', '113', '50']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'92'", "'192'", "'183'", "'57'"]
'86'
'92'
'192'
'183'
'57'
['86', '92', '192', '183', '57']
parsed_discourse_facet ['results_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'192'", "'0'", "'92'", "'183'"]
'86'
'192'
'0'
'92'
'183'
['86', '192', '0', '92', '183']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'183'", "'192'", "'92'", "'0'"]
'86'
'183'
'192'
'92'
'0'
['86', '183', '192', '92', '0']
parsed_discourse_facet ['method_citation']
<S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S><S sid ="18" ssid = "14">Cohen and Smith (2007) followed up on these results and proposed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'92'", "'183'", "'133'", "'18'"]
'192'
'92'
'183'
'133'
'18'
['192', '92', '183', '133', '18']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P04-1036_sweta.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="82" ssid="11">We also calculate the WSD accuracy that would be obtained on SemCor, when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["82'"]
82'
['82']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="20">We briefly summarise the two measures here; for a more detailed summary see (Patwardhan et al., 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="172" ssid="20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="1">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="165" ssid="13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus, whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S>
original cit marker offset is 0
new cit marker offset is 0



["165'"]
165'
['165']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["171'"]
171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["89'"]
89'
['89']
parsed_discourse_facet ['method_citation']
<S sid="172" ssid="20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="12">Additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["189'"]
189'
['189']
parsed_discourse_facet ['method_citation']
<S sid="87" ssid="16">Again, the automatic ranking outperforms this by a large margin.</S>
original cit marker offset is 0
new cit marker offset is 0



["87'"]
87'
['87']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["89'"]
89'
['89']
parsed_discourse_facet ['method_citation']
 <S sid="137" ssid="14">Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["137'"]
137'
['137']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="19">We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline, but because of space limitations give results for the two which perform the best.</S>
original cit marker offset is 0
new cit marker offset is 0



["63'"]
63'
['63']
parsed_discourse_facet ['method_citation']
<S sid="159" ssid="7">Magnini and Cavagli`a (2000) have identified WordNet word senses with particular domains, and this has proven useful for high precision WSD (Magnini et al., 2001); indeed in section 5 we used these domain labels for evaluation.</S>
original cit marker offset is 0
new cit marker offset is 0



["159'"]
159'
['159']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="115" ssid = "13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor  which is very encouraging given that our method only uses raw text  with no manual labelling.</S><S sid ="12" ssid = "5">The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor  from those that do not (without HTD).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'123'", "'14'", "'115'", "'12'"]
'8'
'123'
'14'
'115'
'12'
['8', '123', '14', '115', '12']
parsed_discourse_facet ['results_citation']
<S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="105" ssid = "3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents  rather than a lexical sample task  where the target words are manually determined and the results will depend on the skew of the words in the sample.</S><S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="145" ssid = "22">It is not always intuitively clear which of the senses to expect as predominant sense for either a particular domain or for the BNC  but the first senses of words like division and goal shift towards the more specific senses (league and score respectively).</S><S sid ="155" ssid = "3">A major benefit of our work  rather than reliance on hand-tagged training data such as SemCor  is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'105'", "'165'", "'145'", "'155'"]
'15'
'105'
'165'
'145'
'155'
['15', '105', '165', '145', '155']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="174" ssid = "22">We have restricted ourselves to nouns in this work  since this PoS is perhaps most affected by domain.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of “nearest neighbours”.</S><S sid ="133" ssid = "10">We acquired thesauruses for these corpora using the procedure described in section 2.1.</S><S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'174'", "'34'", "'133'", "'7'"]
'100'
'174'
'34'
'133'
'7'
['100', '174', '34', '133', '7']
parsed_discourse_facet ['method_citation']
<S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="70" ssid = "26">Jiang and Conrath specify a distance measure:   where the third class ( ) is the most informative  or most specific  superordinate synset of the two senses and .</S><S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="71" ssid = "27">This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal:</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'50'", "'70'", "'61'", "'71'"]
'44'
'50'
'70'
'61'
'71'
['44', '50', '70', '61', '71']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S><S sid ="72" ssid = "1">In order to evaluate our method we use the data in SemCor as a gold-standard.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'72'", "'91'", "'10'", "'45'"]
'30'
'72'
'91'
'10'
'45'
['30', '72', '91', '10', '45']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S><S sid ="81" ssid = "10">4 We calculate the accuracy of finding the predominant sense  when there is indeed one sense with a higher frequency than the others for this word in SemCor ( ).</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'50'", "'82'", "'81'", "'44'"]
'69'
'50'
'82'
'81'
'44'
['69', '50', '82', '81', '44']
parsed_discourse_facet ['method_citation']
<S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S><S sid ="58" ssid = "14">A noun    is thus described by a set of co-occurrence triples and associated frequencies  where is a grammatical relation and is a possible cooccurrence with in that relation.</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S><S sid ="56" ssid = "12">For each noun we considered the co-occurring verbs in the direct object and subject relation  the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations.</S><S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'58'", "'45'", "'56'", "'7'"]
'55'
'58'
'45'
'56'
'7'
['55', '58', '45', '56', '7']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S><S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="53" ssid = "9">This weight is the WordNet similarity score ( ) between the target sense ( ) and the sense of ( ) that maximises this score  divided by the sum of all such WordNet similarity scores for and .</S><S sid ="52" ssid = "8">For each sense of ( ) we obtain a ranking score by summing over the of each neighbour ( ) multiplied by a weight.</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'136'", "'53'", "'52'", "'82'"]
'30'
'136'
'53'
'52'
'82'
['30', '136', '53', '52', '82']
parsed_discourse_facet ['method_citation']
<S sid ="135" ssid = "12">We therefore decided to select a limited number of words and to evaluate these words qualitatively.</S><S sid ="179" ssid = "2">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S sid ="111" ssid = "9">We give the results for this WSD task in table 2.</S><S sid ="54" ssid = "10">Thus we rank each sense using: parser (Briscoe and Carroll  2002).</S><S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'135'", "'179'", "'111'", "'54'", "'6'"]
'135'
'179'
'111'
'54'
'6'
['135', '179', '111', '54', '6']
parsed_discourse_facet ['method_citation']
<S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="156" ssid = "4">Buitelaar and Sacaleanu (2001) have previously explored ranking and selection of synsets in GermaNet for specific domains using the words in a given synset  and those related by hyponymy  and a term relevance measure taken from information retrieval.</S><S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'156'", "'61'", "'65'", "'142'"]
'137'
'156'
'61'
'65'
'142'
['137', '156', '61', '65', '142']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="71" ssid = "27">This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal:</S><S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'69'", "'44'", "'71'", "'30'"]
'50'
'69'
'44'
'71'
'30'
['50', '69', '44', '71', '30']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="70" ssid = "26">Jiang and Conrath specify a distance measure:   where the third class ( ) is the most informative  or most specific  superordinate synset of the two senses and .</S><S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'70'", "'137'", "'69'", "'44'"]
'50'
'70'
'137'
'69'
'44'
['50', '70', '137', '69', '44']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">SemCor comprises a relatively small sample of 250 000 words.</S><S sid ="161" ssid = "9">Our approach is complementary to this.</S><S sid ="29" ssid = "22">We discuss our method in the following section.</S><S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of “nearest neighbours”.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'161'", "'29'", "'6'", "'34'"]
'16'
'161'
'29'
'6'
'34'
['16', '161', '29', '6', '34']
parsed_discourse_facet ['results_citation']
<S sid ="157" ssid = "5">Buitelaar and Sacaleanu have evaluated their method on identifying domain specific concepts using human judgements on 100 items.</S><S sid ="185" ssid = "8">In the future  we will perform a large scale evaluation on domain specific corpora.</S><S sid ="193" ssid = "2">This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.</S><S sid ="39" ssid = "32">We expect that the quantity and similarity of the neighbours pertaining to different senses will reflect the dominance of the sense to which they pertain.</S><S sid ="128" ssid = "5">The Reuters corpus (Rose et al.  2002) is a collection of about 810 000 Reuters  English Language News stories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'185'", "'193'", "'39'", "'128'"]
'157'
'185'
'193'
'39'
'128'
['157', '185', '193', '39', '128']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "24">We are of course able to apply the method to other versions of WordNet. synset  is incremented with the frequency counts from the corpus of all words belonging to that synset  directly or via the hyponymy relation.</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'50'", "'65'", "'44'", "'82'"]
'68'
'50'
'65'
'44'
'82'
['68', '50', '65', '44', '82']
parsed_discourse_facet ['method_citation']
<S sid ="90" ssid = "19">From manual analysis  there are cases where the acquired first sense disagrees with SemCor  yet is intuitively plausible.</S><S sid ="175" ssid = "23">We are currently investigating the performance of the first sense heuristic  and this method  for other PoS on SENSEVAL-3 data (McCarthy et al.  2004)  although not yet with rankings from domain specific corpora.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="2" ssid = "2">The problem with using the predominant  or first sense heuristic  aside from the fact that it does not take surrounding context into account  is that it assumes some quantity of handtagged data.</S><S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'175'", "'8'", "'2'", "'1'"]
'90'
'175'
'8'
'2'
'1'
['90', '175', '8', '2', '1']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="0" ssid = "0">Finding Predominant Word Senses in Untagged Text</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'10'", "'0'", "'50'", "'30'"]
'69'
'10'
'0'
'50'
'30'
['69', '10', '0', '50', '30']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P87-1015_sweta.csv
<S sid="205" ssid="11">Independence of paths at this level reflects context freeness of rewriting and suggests why they can be recognized efficiently.</S>
original cit marker offset is 0
new cit marker offset is 0



["205'"]
205'
['205']
parsed_discourse_facet ['method_citation']
<S sid="229" ssid="35">In addition, the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars, and the hierarchy defined by Weir).</S>
original cit marker offset is 0
new cit marker offset is 0



["229'"]
229'
['229']
parsed_discourse_facet ['method_citation']
<S sid="146" ssid="31">Since every CFL is known to be semilinear (Parikh, 1966), in order to show semilinearity of some language, we need only show the existence of a letter equivalent CFL Our definition of LCFRS's insists that the composition operations are linear and nonerasing.</S>
original cit marker offset is 0
new cit marker offset is 0



["146'"]
146'
['146']
parsed_discourse_facet ['method_citation']
<S sid="201" ssid="7">It is interesting to note, however, that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG, FUG and IG's.</S>
original cit marker offset is 0
new cit marker offset is 0



["201'"]
201'
['201']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
 <S sid="222" ssid="28">However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



["222'"]
222'
['222']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="7">Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.</S>
    <S sid="23" ssid="8">He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["22'", "'23'"]
22'
'23'
['22', '23']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="221" ssid="27">Members of LCFRS whose operations have this property can be translated into the ILFP notation (Rounds, 1985).</S>
original cit marker offset is 0
new cit marker offset is 0



["221'"]
221'
['221']
parsed_discourse_facet ['method_citation']
<S sid="54" ssid="39">An IG can be viewed as a CFG in which each nonterminal is associated with a stack.</S>
original cit marker offset is 0
new cit marker offset is 0



["54'"]
54'
['54']
parsed_discourse_facet ['method_citation']
<S sid="128" ssid="13">As in the case of the derivation trees of CFG's, nodes are labeled by a member of some finite set of symbols (perhaps only implicit in the grammar as in TAG's) used to denote derived structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["128'"]
128'
['128']
parsed_discourse_facet ['method_citation']
<S sid="217" ssid="23">In considering the recognition of these languages, we were forced to be more specific regarding the relationship between the structures derived by these formalisms and the substrings they span.</S>
original cit marker offset is 0
new cit marker offset is 0



[";217'"]
;217'
[';217']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="1">From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="1">From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="214" ssid="20">LCFRS's share several properties possessed by the class of mildly context-sensitive formalisms discussed by Joshi (1983/85).</S>
original cit marker offset is 0
new cit marker offset is 0



["214'"]
214'
['214']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="20">TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish, for cross serial dependencies in Dutch subordinate clauses, and for the nestings of paired English complementizers.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P87-1015.csv
<S sid ="115" ssid = "21">From the point of view of recognition  independent paths in the derivation structures suggests that a top-down parser (for example) can work on each branch independently  which may lead to efficient parsing using an algorithm based on the Divide and Conquer technique.</S><S sid ="222" ssid = "28">However  in order to capture the properties of various grammatical systems under consideration  our notation is more restrictive that ILFP  which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="67" ssid = "52">On the one hand  the definition of composition in Steedman (1985)  which technically permits composition of functions with unbounded number of arguments  generates tree sets with dependent paths such as those shown in Figure 6.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'222'", "'195'", "'67'", "'153'"]
'115'
'222'
'195'
'67'
'153'
['115', '222', '195', '67', '153']
parsed_discourse_facet ['results_citation']
<S sid ="37" ssid = "22">Like CFG's  the choice is predetermined by a finite number of rules encapsulated in the grammar.</S><S sid ="231" ssid = "37">In this paper  our goal has been to use the notion of LCFRS's to classify grammatical systems on the basis of their strong generative capacity.</S><S sid ="197" ssid = "3">We address the question of whether or not a formalism can generate only structural descriptions with independent paths.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S><S sid ="92" ssid = "77">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'231'", "'197'", "'116'", "'92'"]
'37'
'231'
'197'
'116'
'92'
['37', '231', '197', '116', '92']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.</S><S sid ="131" ssid = "16">The composition operations in the case of CFG's are parameterized by the productions.</S><S sid ="65" ssid = "50">While the generative power of CG's is greater that of CFG's  it appears to be highly constrained.</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="179" ssid = "64">It can be seen that M performs a top-down recognition of the input al ... nin logspace.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'131'", "'65'", "'227'", "'179'"]
'204'
'131'
'65'
'227'
'179'
['204', '131', '65', '227', '179']
parsed_discourse_facet ['method_citation']
<S sid ="194" ssid = "79">Thus  M works in logspace and recognition can be done on a deterministic TM in polynomial tape.</S><S sid ="92" ssid = "77">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S><S sid ="65" ssid = "50">While the generative power of CG's is greater that of CFG's  it appears to be highly constrained.</S><S sid ="20" ssid = "5">As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.</S><S sid ="131" ssid = "16">The composition operations in the case of CFG's are parameterized by the productions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'92'", "'65'", "'20'", "'131'"]
'194'
'92'
'65'
'20'
'131'
['194', '92', '65', '20', '131']
parsed_discourse_facet ['method_citation']
<S sid ="155" ssid = "40">CFG's  TAG's  MCTAG's and HG's are all members of this class since they satisfy these restrictions.</S><S sid ="37" ssid = "22">Like CFG's  the choice is predetermined by a finite number of rules encapsulated in the grammar.</S><S sid ="117" ssid = "2">Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.</S><S sid ="92" ssid = "77">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S><S sid ="48" ssid = "33">There has been recent interest in the application of Indexed Grammars (IG's) to natural languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'", "'37'", "'117'", "'92'", "'48'"]
'155'
'37'
'117'
'92'
'48'
['155', '37', '117', '92', '48']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "5">As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.</S><S sid ="216" ssid = "22">Having defined LCFRS's  in Section 4.2 we established the semilinearity (and hence constant growth property) of the languages generated.</S><S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="179" ssid = "64">It can be seen that M performs a top-down recognition of the input al ... nin logspace.</S><S sid ="131" ssid = "16">The composition operations in the case of CFG's are parameterized by the productions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'216'", "'59'", "'179'", "'131'"]
'20'
'216'
'59'
'179'
'131'
['20', '216', '59', '179', '131']
parsed_discourse_facet ['method_citation']
<S sid ="162" ssid = "47">Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="219" ssid = "25">The composition operations are mapped onto operations that use concatenation to define the substrings spanned by the resulting structures.</S><S sid ="0" ssid = "0">CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*</S><S sid ="193" ssid = "78">Since the work tapes store integers (which can be written in binary) that never exceed the size of the input  no configuration has space exceeding 0(log n).</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'", "'195'", "'219'", "'0'", "'193'"]
'162'
'195'
'219'
'0'
'193'
['162', '195', '219', '0', '193']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "9">By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.</S><S sid ="115" ssid = "21">From the point of view of recognition  independent paths in the derivation structures suggests that a top-down parser (for example) can work on each branch independently  which may lead to efficient parsing using an algorithm based on the Divide and Conquer technique.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="1" ssid = "1">We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'115'", "'153'", "'1'", "'195'"]
'11'
'115'
'153'
'1'
'195'
['11', '115', '153', '1', '195']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*</S><S sid ="219" ssid = "25">The composition operations are mapped onto operations that use concatenation to define the substrings spanned by the resulting structures.</S><S sid ="125" ssid = "10">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S sid ="58" ssid = "43">Unification is used in LFG's to produce structures having two dependent spines of unbounded length as in Figure 5.</S><S sid ="192" ssid = "77">Thus  the ATM has no more than 6km&quot; + 1 work tapes  where km&quot; is the maximum number of substrings spanned by a derived structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'219'", "'125'", "'58'", "'192'"]
'0'
'219'
'125'
'58'
'192'
['0', '219', '125', '58', '192']
parsed_discourse_facet ['method_citation']
<S sid ="193" ssid = "78">Since the work tapes store integers (which can be written in binary) that never exceed the size of the input  no configuration has space exceeding 0(log n).</S><S sid ="129" ssid = "14">Frontier nodes are annotated by zero arty functions corresponding to elementary structures.</S><S sid ="127" ssid = "12">Nodes are annotated by the name of the composition operation used at that step in the derivation.</S><S sid ="172" ssid = "57">A step of an ATM consists of reading a symbol from each tape and optionally moving each head to the left or right one tape cell.</S><S sid ="173" ssid = "58">A configuration of M consists of a state of the finite control  the nonblank contents of the input tape and k work tapes  and the position of each head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'193'", "'129'", "'127'", "'172'", "'173'"]
'193'
'129'
'127'
'172'
'173'
['193', '129', '127', '172', '173']
parsed_discourse_facet ['method_citation']
<S sid ="189" ssid = "74">For rules p : A fpo such that fp is constant function  giving an elementary structure  fp is defined such that fp() = (Si ... xi() where each z is a constant string.</S><S sid ="183" ssid = "68">We assume that M is in an existential state qA  with integers i1 and i2 representing zi in the (2i — 1)th and 22th work tape  for 1 < i < k. For each rule p : A fp(B  C) such that fp is mapped onto the function fp defined by the following rule. jp((xi .. •  rnt)  (1ii  • • • • Yn3))= (Zi   • • •   Zk) M breaks xi   zk into substrings xi    xn  and yi ... y&quot; conforming to the definition of fp.</S><S sid ="160" ssid = "45">A derived structure will be mapped onto a sequence zi of substrings (not necessarily contiguous in the input)  and the composition operations will be mapped onto functions that can defined as follows3. f((zi • • •   zni)  (m. • • •  Yn3)) = (Z1  • • •   Zn3) where each z  is the concatenation of strings from z 's and yk's.</S><S sid ="148" ssid = "33">If 0(A) gives the number of occurrences of each terminal in the structure named by A  then  given the constraints imposed on the formalism  for each rule A --. fp(Ai    An) we have the equality where c„ is some constant.</S><S sid ="201" ssid = "7">It is interesting to note  however  that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG  FUG and IG's.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'183'", "'160'", "'148'", "'201'"]
'189'
'183'
'160'
'148'
'201'
['189', '183', '160', '148', '201']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*</S><S sid ="162" ssid = "47">Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="196" ssid = "2">We contrasted formalisms such as CFG's  HG's  TAG's and MCTAG's  with formalisms such as IG's and unificational systems such as LFG's and FUG's.</S><S sid ="204" ssid = "10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'162'", "'227'", "'196'", "'204'"]
'0'
'162'
'227'
'196'
'204'
['0', '162', '227', '196', '204']
parsed_discourse_facet ['method_citation']
<S sid ="58" ssid = "43">Unification is used in LFG's to produce structures having two dependent spines of unbounded length as in Figure 5.</S><S sid ="214" ssid = "20">LCFRS's share several properties possessed by the class of mildly context-sensitive formalisms discussed by Joshi (1983/85).</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="196" ssid = "2">We contrasted formalisms such as CFG's  HG's  TAG's and MCTAG's  with formalisms such as IG's and unificational systems such as LFG's and FUG's.</S><S sid ="163" ssid = "48">This representation of structures by substrings and the composition operation by its effect on substrings is related to the work of Rounds (1985).</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'214'", "'227'", "'196'", "'163'"]
'58'
'214'
'227'
'196'
'163'
['58', '214', '227', '196', '163']
parsed_discourse_facet ['results_citation']
<S sid ="50" ssid = "35">The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.</S><S sid ="202" ssid = "8">As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.</S><S sid ="7" ssid = "5">The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG's and IG's.</S><S sid ="230" ssid = "36">LCFRS's have only been loosely defined in this paper; we have yet to provide a complete set of formal properties associated with members of this class.</S><S sid ="110" ssid = "16">The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'202'", "'7'", "'230'", "'110'"]
'50'
'202'
'7'
'230'
'110'
['50', '202', '7', '230', '110']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "42">For example  in TAG's a derived auxiliary tree spans two substrings (to the left and right of the foot node)  and the adjunction operation inserts another substring (spanned by the subtree under the node where adjunction takes place) between them (see Figure 3).</S><S sid ="162" ssid = "47">Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .</S><S sid ="83" ssid = "68">The following context-free production captures the derivation step of the grammar shown in Figure 7  in which the trees in the auxiliary tree set are adjoined into themselves at the root node (address c).</S><S sid ="67" ssid = "52">On the one hand  the definition of composition in Steedman (1985)  which technically permits composition of functions with unbounded number of arguments  generates tree sets with dependent paths such as those shown in Figure 6.</S><S sid ="154" ssid = "39">Since each composition operation is linear and nonerasing  a bounded sequences of substrings associated with the resulting structure is obtained by combining the substrings in each of its arguments using only the concatenation operation  including each substring exactly once.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'162'", "'83'", "'67'", "'154'"]
'157'
'162'
'83'
'67'
'154'
['157', '162', '83', '67', '154']
parsed_discourse_facet ['method_citation']
<S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="156" ssid = "41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S><S sid ="204" ssid = "10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.</S><S sid ="213" ssid = "19">By sharing stacks (in IG's) or by using nonlinear equations over f-structures (in FUG's and LFG's)  structures with unbounded dependencies between paths can be generated.</S><S sid ="125" ssid = "10">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'227'", "'156'", "'204'", "'213'", "'125'"]
'227'
'156'
'204'
'213'
'125'
['227', '156', '204', '213', '125']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/D10-1044_sweta.csv
<S sid="4" ssid="1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S>
original cit marker offset is 0
new cit marker offset is 0



["4'"]
4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="1">We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="4">For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components (word-alignment model, language model, translation model, etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["7'"]
7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="14">Linear weights are difficult to incorporate into the standard MERT procedure because they are &#8220;hidden&#8221; within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn, 2007), we circumvent this problem by choosing weights to optimize corpus loglikelihood, which is roughly speaking the training criterion used by the LM and TM themselves.</S>
original cit marker offset is 0
new cit marker offset is 0



["50'"]
50'
['50']
parsed_discourse_facet ['method_citation']
<S sid="152" ssid="9">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["152'"]
152'
['152']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
 <S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="97" ssid="1">We carried out translation experiments in two different settings.</S>
original cit marker offset is 0
new cit marker offset is 0



["97'"]
97'
['97']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="12">Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="10">Finally, we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="141" ssid="10">Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L&#168;u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L&#168;u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["141'"]
141'
['141']
parsed_discourse_facet ['method_citation']
 <S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="37" ssid="1">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S>
original cit marker offset is 0
new cit marker offset is 0



["37'"]
37'
['37']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D10-1044.csv
<S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daum´e’s (2007) work that splits each feature into domain-specific and general copies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'128'", "'140'", "'10'", "'4'", "'136'"]
'128'
'140'
'10'
'4'
'136'
['128', '140', '10', '4', '136']
parsed_discourse_facet ['results_citation']
<S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="82" ssid = "19">This is not unreasonable given the application to phrase pairs from OUT  but it suggests that an interesting alternative might be to use a plain log-linear weighting function exp(Ei Aifi(s  t))  with outputs in [0  oo].</S><S sid ="142" ssid = "11">There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'38'", "'82'", "'142'", "'130'"]
'143'
'38'
'82'
'142'
'130'
['143', '38', '82', '142', '130']
parsed_discourse_facet ['method_citation']
<S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S><S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="31" ssid = "28">For comparison to information-retrieval inspired baselines  eg (L¨u et al.  2007)  we select sentences from OUT using language model perplexities from IN.</S><S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'7'", "'31'", "'128'", "'78'"]
'140'
'7'
'31'
'128'
'78'
['140', '7', '31', '128', '78']
parsed_discourse_facet ['method_citation']
<S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="102" ssid = "6">The dev corpus was taken from the NIST05 evaluation set  augmented with some randomly-selected material reserved from the training set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'127'", "'143'", "'65'", "'102'"]
'130'
'127'
'143'
'65'
'102'
['130', '127', '143', '65', '102']
parsed_discourse_facet ['method_citation']
<S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="55" ssid = "19">This suggests a direct parallel to (1): where ˜p(s  t) is a joint empirical distribution extracted from the IN dev set using the standard procedure.2 An alternative form of linear combination is a maximum a posteriori (MAP) combination (Bacchiani et al.  2004).</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'21'", "'49'", "'55'", "'127'"]
'75'
'21'
'49'
'55'
'127'
['75', '21', '49', '55', '127']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'130'", "'49'", "'38'", "'143'"]
'127'
'130'
'49'
'38'
'143'
['127', '130', '49', '38', '143']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'20'", "'38'", "'130'", "'49'"]
'127'
'20'
'38'
'130'
'49'
['127', '20', '38', '130', '49']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S><S sid ="37" ssid = "1">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features  interpreted as log probabilities  many of which have their own internal parameters and objectives.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'49'", "'111'", "'37'", "'21'"]
'127'
'49'
'111'
'37'
'21'
['127', '49', '111', '37', '21']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'81'", "'151'", "'75'", "'143'"]
'7'
'81'
'151'
'75'
'143'
['7', '81', '151', '75', '143']
parsed_discourse_facet ['method_citation']
<S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'75'", "'127'", "'151'", "'143'"]
'21'
'75'
'127'
'151'
'143'
['21', '75', '127', '151', '143']
parsed_discourse_facet ['method_citation']
<S sid ="97" ssid = "1">We carried out translation experiments in two different settings.</S><S sid ="53" ssid = "17">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).</S><S sid ="106" ssid = "10">The corpora for both settings are summarized in table 1.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="26" ssid = "23">Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009)  who weight sentences according to sub-corpus and genre membership.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'53'", "'106'", "'81'", "'26'"]
'97'
'53'
'106'
'81'
'26'
['97', '53', '106', '81', '26']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="31" ssid = "28">For comparison to information-retrieval inspired baselines  eg (L¨u et al.  2007)  we select sentences from OUT using language model perplexities from IN.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'81'", "'31'", "'78'", "'140'"]
'7'
'81'
'31'
'78'
'140'
['7', '81', '31', '78', '140']
parsed_discourse_facet ['method_citation']
<S sid ="60" ssid = "24">The matching sentence pairs are then added to the IN corpus  and the system is re-trained.</S><S sid ="30" ssid = "27">A similar maximumlikelihood approach was used by Foster and Kuhn (2007)  but for language models only.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="145" ssid = "2">Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'30'", "'24'", "'65'", "'145'"]
'60'
'30'
'24'
'65'
'145'
['60', '30', '24', '65', '145']
parsed_discourse_facet ['results_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="22" ssid = "19">Within this framework  we use features intended to capture degree of generality  including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'20'", "'130'", "'49'", "'22'"]
'127'
'20'
'130'
'49'
'22'
['127', '20', '130', '49', '22']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="119" ssid = "23">The 2nd block contains the IR system  which was tuned by selecting text in multiples of the size of the EMEA training corpus  according to dev set performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'130'", "'20'", "'49'", "'119'"]
'127'
'130'
'20'
'49'
'119'
['127', '130', '20', '49', '119']
parsed_discourse_facet ['method_citation']
<S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S><S sid ="66" ssid = "3">The weight on each sentence is a value in [0  1] computed by a perceptron with Boolean features that indicate collection and genre membership.</S><S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="113" ssid = "17">The corpus was wordaligned using both HMM and IBM2 models  and the phrase table was the union of phrases extracted from these separate alignments  with a length limit of 7.</S><S sid ="102" ssid = "6">The dev corpus was taken from the NIST05 evaluation set  augmented with some randomly-selected material reserved from the training set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'66'", "'75'", "'113'", "'102'"]
'143'
'66'
'75'
'113'
'102'
['143', '66', '75', '113', '102']
parsed_discourse_facet ['method_citation']
<S sid ="44" ssid = "8">When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.</S><S sid ="79" ssid = "16">This combination generalizes (2) and (3): we use either at = a to obtain a fixed-weight linear combination  or at = cI(t)/(cI(t) + 0) to obtain a MAP combination.</S><S sid ="62" ssid = "26">To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S><S sid ="115" ssid = "19">Table 2 shows results for both settings and all methods described in sections 2 and 3.</S><S sid ="13" ssid = "10">The techniques we develop can be extended in a relatively straightforward manner to the more general case when OUT consists of multiple sub-domains.</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'79'", "'62'", "'115'", "'13'"]
'44'
'79'
'62'
'115'
'13'
['44', '79', '62', '115', '13']
parsed_discourse_facet ['method_citation']
<S sid ="106" ssid = "10">The corpora for both settings are summarized in table 1.</S><S sid ="60" ssid = "24">The matching sentence pairs are then added to the IN corpus  and the system is re-trained.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="53" ssid = "17">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'60'", "'81'", "'38'", "'53'"]
'106'
'60'
'81'
'38'
'53'
['106', '60', '81', '38', '53']
parsed_discourse_facet ['method_citation']
<S sid ="95" ssid = "32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S><S sid ="50" ssid = "14">Linear weights are difficult to incorporate into the standard MERT procedure because they are “hidden” within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="59" ssid = "23">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid ="51" ssid = "15">For the LM  adaptive weights are set as follows: where α is a weight vector containing an element αi for each domain (just IN and OUT in our case)  pi are the corresponding domain-specific models  and ˜p(w  h) is an empirical distribution from a targetlanguage training corpus—we used the IN dev set for this.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'50'", "'21'", "'59'", "'51'"]
'95'
'50'
'21'
'59'
'51'
['95', '50', '21', '59', '51']
parsed_discourse_facet ['aim_citation', 'results_citation']



D10-1044
P11-2074
0
method_citation
['results_citation']



D10-1044
E12-1055
0
method_citation
['method_citation']



D10-1044
D12-1129
0
method_citation
['method_citation']
parsing: input/ref/Task1/P08-1028_swastika.csv
<S sid="21" ssid="17">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="23">Our results show that the multiplicative models are superior and correlate significantly with behavioral data.</S>
original cit marker offset is 0
new cit marker offset is 0



['27']
27
['27']
parsed_discourse_facet ['result_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['189']
189
['189']
parsed_discourse_facet ['aim_citation']
<S sid="53" ssid="1">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.</S>
original cit marker offset is 0
new cit marker offset is 0



['53']
53
['53']
parsed_discourse_facet ['method_citation']
<S sid="76" ssid="24">The models considered so far assume that components do not &#8216;interfere&#8217; with each other, i.e., that It is also possible to re-introduce the dependence on K into the model of vector composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['76']
76
['76']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['57']
57
['57']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['57']
57
['57']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="60" ssid="8">To derive specific models from this general framework requires the identification of appropriate constraints to narrow the space of functions being considered.</S>
original cit marker offset is 0
new cit marker offset is 0



['60']
60
['60']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="73" ssid="21">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware, since semantically important constituents can participate more actively in the composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['73']
73
['73']
parsed_discourse_facet ['result_citation']
<S sid="99" ssid="12">In order to establish an independent measure of sentence similarity, we assembled a set of experimental materials and elicited similarity ratings from human subjects.</S>
original cit marker offset is 0
new cit marker offset is 0



['99']
99
['99']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['189']
189
['189']
parsed_discourse_facet ['aim_citation']
<S sid="191" ssid="3">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S>
original cit marker offset is 0
new cit marker offset is 0



['191']
191
['191']
parsed_discourse_facet ['result_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P08-1028.csv
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="140" ssid = "53">Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid ="40" ssid = "13">Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.</S><S sid ="155" ssid = "68">Secondly  we optimized the weightings in the combined model (11) with a similar grid search over its three parameters.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'140'", "'51'", "'40'", "'155'"]
'194'
'140'
'51'
'40'
'155'
['194', '140', '51', '40', '155']
parsed_discourse_facet ['results_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="58" ssid = "6">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="10" ssid = "6">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'58'", "'59'", "'10'", "'51'"]
'194'
'58'
'59'
'10'
'51'
['194', '58', '59', '10', '51']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="38" ssid = "11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S><S sid ="141" ssid = "54">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S><S sid ="36" ssid = "9">To overcome this problem  other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'194'", "'38'", "'141'", "'36'"]
'108'
'194'
'38'
'141'
'36'
['108', '194', '38', '141', '36']
parsed_discourse_facet ['method_citation']
<S sid ="160" ssid = "73">Specifically  m was set to 20 and m to 1.</S><S sid ="156" ssid = "69">This yielded a weighted sum consisting of 95% verb  0% noun and 5% of their multiplicative combination.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="190" ssid = "2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid ="117" ssid = "30">This yielded a reduced set of experimental items (120 in total) consisting of 15 reference verbs  each with 4 nouns  and 2 landmarks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'", "'156'", "'34'", "'190'", "'117'"]
'160'
'156'
'34'
'190'
'117'
['160', '156', '34', '190', '117']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">Vector-based Models of Semantic Composition</S><S sid ="36" ssid = "9">To overcome this problem  other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid ="142" ssid = "55">We experimented with a variety of dimensions (ranging from 50 to 500 000)  vector component definitions (e.g.  pointwise mutual information or log likelihood ratio) and similarity measures (e.g.  cosine or confusion probability).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'36'", "'51'", "'142'", "'194'"]
'0'
'36'
'51'
'142'
'194'
['0', '36', '51', '142', '194']
parsed_discourse_facet ['method_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid ="58" ssid = "6">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S><S sid ="201" ssid = "13">We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al.  1988) and inductive inference (Heit and Rubinstein  1994).</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'59'", "'51'", "'58'", "'201'"]
'194'
'59'
'51'
'58'
'201'
['194', '59', '51', '58', '201']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "3">For the hierarchical structure of natural language this binding problem becomes particularly acute.</S><S sid ="96" ssid = "9">Any adequate model of composition must be able to represent argument-verb meaning.</S><S sid ="1" ssid = "1">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid ="24" ssid = "20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S><S sid ="44" ssid = "17">For example  assuming that individual words are represented by vectors  we can compute the meaning of a sentence by taking their mean (Foltz et al.  1998; Landauer and Dumais  1997).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'96'", "'1'", "'24'", "'44'"]
'30'
'96'
'1'
'24'
'44'
['30', '96', '1', '24', '44']
parsed_discourse_facet ['method_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="37" ssid = "10">Holographic reduced representations (Plate  1991) are one implementation of this idea where the tensor product is projected back onto the space of its components.</S><S sid ="141" ssid = "54">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'3'", "'101'", "'37'", "'141'"]
'194'
'3'
'101'
'37'
'141'
['194', '3', '101', '37', '141']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid ="140" ssid = "53">Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.</S><S sid ="144" ssid = "57">We obtained best results with a model using a context window of five words on either side of the target word  the cosine measure  and 2 000 vector components.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="37" ssid = "10">Holographic reduced representations (Plate  1991) are one implementation of this idea where the tensor product is projected back onto the space of its components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'140'", "'144'", "'194'", "'37'"]
'39'
'140'
'144'
'194'
'37'
['39', '140', '144', '194', '37']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="168" ssid = "2">These included three additive models  i.e.  simple addition (equation (5)  Add)  weighted addition (equation (7)  WeightAdd)  and Kintsch’s (2001) model (equation (10)  Kintsch)  a multiplicative model (equation (6)  Multiply)  and also a model which combines multiplication with addition (equation (11)  Combined).</S><S sid ="35" ssid = "8">The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely  the tensor product has dimensionality m x n).</S><S sid ="144" ssid = "57">We obtained best results with a model using a context window of five words on either side of the target word  the cosine measure  and 2 000 vector components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'194'", "'168'", "'35'", "'144'"]
'108'
'194'
'168'
'35'
'144'
['108', '194', '168', '35', '144']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid ="117" ssid = "30">This yielded a reduced set of experimental items (120 in total) consisting of 15 reference verbs  each with 4 nouns  and 2 landmarks.</S><S sid ="35" ssid = "8">The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely  the tensor product has dimensionality m x n).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'117'", "'35'", "'194'", "'108'"]
'39'
'117'
'35'
'194'
'108'
['39', '117', '35', '194', '108']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="153" ssid = "66">Specifically  we considered eleven models  varying in their weightings  in steps of 10%  from 100% noun through 50% of both verb and noun to 100% verb.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'101'", "'3'", "'153'", "'194'"]
'108'
'101'
'3'
'153'
'194'
['108', '101', '3', '153', '194']
parsed_discourse_facet ['method_citation']
<S sid ="148" ssid = "61">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S><S sid ="177" ssid = "11">The difference between High and Low similarity values estimated by these models are statistically significant (p < 0.01 using the Wilcoxon rank sum test).</S><S sid ="4" ssid = "4">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S><S sid ="127" ssid = "40">Analysis of Similarity Ratings The reliability of the collected judgments is important for our evaluation experiments; we therefore performed several tests to validate the quality of the ratings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'148'", "'164'", "'177'", "'4'", "'127'"]
'148'
'164'
'177'
'4'
'127'
['148', '164', '177', '4', '127']
parsed_discourse_facet ['results_citation']
<S sid ="115" ssid = "28">Each cell recorded the number of times our subjects selected the landmark as compatible with the noun or not.</S><S sid ="131" ssid = "44">A Wilcoxon rank sum test confirmed that the difference is statistically significant (p < 0.01).</S><S sid ="147" ssid = "60">This configuration gave high correlations with the WordSim353 similarity judgments using the cosine measure.</S><S sid ="173" ssid = "7">Model similarities have been estimated using cosine which ranges from 0 to 1  whereas our subjects rated the sentences on a scale from 1 to 7.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'131'", "'147'", "'173'", "'164'"]
'115'
'131'
'147'
'173'
'164'
['115', '131', '147', '173', '164']
parsed_discourse_facet ['method_citation']
<S sid ="140" ssid = "53">Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.</S><S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="2" ssid = "2">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S><S sid ="139" ssid = "52">The semantic space we used in our experiments was built on a lemmatised version of the BNC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'3'", "'101'", "'2'", "'139'"]
'140'
'3'
'101'
'2'
'139'
['140', '3', '101', '2', '139']
parsed_discourse_facet ['method_citation']
<S sid ="115" ssid = "28">Each cell recorded the number of times our subjects selected the landmark as compatible with the noun or not.</S><S sid ="147" ssid = "60">This configuration gave high correlations with the WordSim353 similarity judgments using the cosine measure.</S><S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid ="173" ssid = "7">Model similarities have been estimated using cosine which ranges from 0 to 1  whereas our subjects rated the sentences on a scale from 1 to 7.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'147'", "'39'", "'173'", "'164'"]
'115'
'147'
'39'
'173'
'164'
['115', '147', '39', '173', '164']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S><S sid ="153" ssid = "66">Specifically  we considered eleven models  varying in their weightings  in steps of 10%  from 100% noun through 50% of both verb and noun to 100% verb.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="183" ssid = "17">The weighted additive model (p = 0.09) is not significantly different from the baseline either or Kintsch (2001) (p = 0.09).</S><S sid ="40" ssid = "13">Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'153'", "'194'", "'183'", "'40'"]
'38'
'153'
'194'
'183'
'40'
['38', '153', '194', '183', '40']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P11-1060_aakansha.csv
<S sid="11" ssid="7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'"]
'2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="5">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'"]
'112'
['112']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="11">Although a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">However, we still model the logical form (now as a latent variable) to capture the complexities of language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="12">It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="21">The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="88" ssid="64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets, a restrictor A and a nuclear scope B.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'"]
'88'
['88']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="158" ssid = "43">5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation  DCS  which offers a new perspective lexical semantics.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="23" ssid = "19">We trained our model using an EM-like algorithm (Section 3) on two benchmarks  GEO and JOBS (Section 4).</S><S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'158'", "'21'", "'23'", "'132'"]
'26'
'158'
'21'
'23'
'132'
['26', '158', '21', '23', '132']
parsed_discourse_facet ['results_citation']
<S sid ="39" ssid = "15">Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.</S><S sid ="56" ssid = "32">The basic version of DCS described thus far handles a core subset of language.</S><S sid ="4" ssid = "4">On two stansemantic parsing benchmarks our system obtains the highest published accuracies  despite requiring no annotated logical forms.</S><S sid ="136" ssid = "21">In fact  DCS performs comparably to even the version of SEMRESP trained using logical forms.</S><S sid ="83" ssid = "59">It suffices to define Xi(d) for a single column i.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'56'", "'4'", "'136'", "'83'"]
'39'
'56'
'4'
'136'
'83'
['39', '56', '4', '136', '83']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S><S sid ="148" ssid = "33">This bootstrapping behavior occurs naturally: The “easy” examples are processed first  where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'100'", "'115'", "'88'", "'148'"]
'26'
'100'
'115'
'88'
'148'
['26', '100', '115', '88', '148']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="138" ssid = "23">Next  we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="53" ssid = "29">Having instantiated s as a value  everything above this node is an ordinary CSP: s constrains the count node  which in turns constrains the root node to |s|.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'138'", "'26'", "'53'"]
'100'
'21'
'138'
'26'
'53'
['100', '21', '138', '26', '53']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S><S sid ="94" ssid = "70">We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'94'", "'134'", "'8'", "'58'"]
'18'
'94'
'134'
'8'
'58'
['18', '94', '134', '8', '58']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'21'", "'100'", "'115'", "'42'"]
'26'
'21'
'100'
'115'
'42'
['26', '21', '100', '115', '42']
parsed_discourse_facet ['method_citation']
<S sid ="87" ssid = "63">For example  in Figure 4(a)  before execution  the denotation of the DCS tree is hh{[(CA  OR)  (OR)] ... }; ø; (E  Qhstatei�w  ø)ii; after applying X1  we have hh{[(OR)]  ... }; øii.</S><S sid ="113" ssid = "89">Formally  let ˜O(θ  θ') be the objective function O(θ) with ZL(x) ˜ZL θI(x).</S><S sid ="20" ssid = "16">At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="74" ssid = "50">Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'", "'113'", "'20'", "'154'", "'74'"]
'87'
'113'
'20'
'154'
'74'
['87', '113', '20', '154', '74']
parsed_discourse_facet ['method_citation']
<S sid ="117" ssid = "2">In each dataset  each sentence x is annotated with a Prolog logical form  which we use only to evaluate and get an answer y.</S><S sid ="1" ssid = "1">Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'117'", "'1'", "'15'", "'88'", "'62'"]
'117'
'1'
'15'
'88'
'62'
['117', '1', '15', '88', '62']
parsed_discourse_facet ['method_citation']
<S sid ="83" ssid = "59">It suffices to define Xi(d) for a single column i.</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="101" ssid = "77">Formally  pθ(z  |x) ∝ eφ(x z)Tθ  where θ and φ(x  z) are parameter and feature vectors  respectively.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'169'", "'8'", "'101'", "'134'"]
'83'
'169'
'8'
'101'
'134'
['83', '169', '8', '101', '134']
parsed_discourse_facet ['method_citation']
<S sid ="1" ssid = "1">Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid ="99" ssid = "75">To further reduce the search space  F imposes a few additional constraints  e.g.  limiting the number of marked nodes to 2 and only allowing trace predicates between arity 1 predicates.</S><S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S><S sid ="129" ssid = "14">We also define an augmented lexicon L+ which includes a prototype word x for each predicate appearing in (iii) above (e.g.  (large  size))  which cancels the predicates triggered by x’s POS tag.</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'99'", "'132'", "'129'", "'26'"]
'1'
'99'
'132'
'129'
'26'
['1', '99', '132', '129', '26']
parsed_discourse_facet ['method_citation']
<S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="54" ssid = "30">A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S><S sid ="166" ssid = "51">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'54'", "'42'", "'166'", "'132'"]
'21'
'54'
'42'
'166'
'132'
['21', '54', '42', '166', '132']
parsed_discourse_facet ['method_citation']
<S sid ="59" ssid = "35">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S><S sid ="54" ssid = "30">A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="45" ssid = "21">The logical forms in DCS are called DCS trees  where nodes are labeled with predicates  and edges are labeled with relations.</S><S sid ="43" ssid = "19">Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter  2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'54'", "'154'", "'45'", "'43'"]
'59'
'54'
'154'
'45'
'43'
['59', '54', '154', '45', '43']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "30">A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="74" ssid = "50">Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S><S sid ="107" ssid = "83">Our model is arc-factored  so we can sum over all DCS trees in ZL(x) using dynamic programming.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'42'", "'21'", "'74'", "'107'"]
'54'
'42'
'21'
'74'
'107'
['54', '42', '21', '74', '107']
parsed_discourse_facet ['results_citation']
<S sid ="59" ssid = "35">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S><S sid ="148" ssid = "33">This bootstrapping behavior occurs naturally: The “easy” examples are processed first  where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S><S sid ="47" ssid = "23">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.</S><S sid ="103" ssid = "79">To define the features  we technically need to augment each tree z ∈ ZL(x) with alignment information—namely  for each predicate in z  the span in x (if any) that triggered it.</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'148'", "'47'", "'103'", "'62'"]
'59'
'148'
'47'
'103'
'62'
['59', '148', '47', '103', '62']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="23" ssid = "19">We trained our model using an EM-like algorithm (Section 3) on two benchmarks  GEO and JOBS (Section 4).</S><S sid ="86" ssid = "62">Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  −(i  ø)]{α1 = ø}.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'23'", "'86'", "'115'", "'100'"]
'26'
'23'
'86'
'115'
'100'
['26', '23', '86', '115', '100']
parsed_discourse_facet ['method_citation']
<S sid ="113" ssid = "89">Formally  let ˜O(θ  θ') be the objective function O(θ) with ZL(x) ˜ZL θI(x).</S><S sid ="78" ssid = "54">The stores are also concatenated (α + α').</S><S sid ="74" ssid = "50">Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="111" ssid = "87">Let ˜ZL θ(x) be this approximation of ZL(x).</S>
original cit marker offset is 0
new cit marker offset is 0



["'113'", "'78'", "'74'", "'154'", "'111'"]
'113'
'78'
'74'
'154'
'111'
['113', '78', '74', '154', '111']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="63" ssid = "39">Think of d as consisting of n columns  one for each active node according to a pre-order traversal of z.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="86" ssid = "62">Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  −(i  ø)]{α1 = ø}.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'100'", "'63'", "'115'", "'86'"]
'26'
'100'
'63'
'115'
'86'
['26', '100', '63', '115', '86']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'26'", "'115'", "'42'"]
'100'
'21'
'26'
'115'
'42'
['100', '21', '26', '115', '42']
parsed_discourse_facet ['method_citation']
<S sid ="151" ssid = "36">Inspecting the final parameters calculus formulae.</S><S sid ="56" ssid = "32">The basic version of DCS described thus far handles a core subset of language.</S><S sid ="16" ssid = "12">Which one should we use?</S><S sid ="118" ssid = "3">This evaluation is done with respect to a world w. Recall that a world w maps each predicate p ∈ P to a set of tuples w(p).</S><S sid ="33" ssid = "9">As another example  w(average) = {(S  ¯x) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'56'", "'16'", "'118'", "'33'"]
'151'
'56'
'16'
'118'
'33'
['151', '56', '16', '118', '33']
parsed_discourse_facet ['aim_citation', 'results_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W06-3114_swastika.csv
<S sid="47" ssid="13">Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data.</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
    <S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="37">Our suspicion is that BLEU is very sensitive to jargon, to selecting exactly the right words, and not synonyms that human judges may appreciate as equally good.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
<S sid="145" ssid="38">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S>
original cit marker offset is 0
new cit marker offset is 0



['145']
145
['145']
parsed_discourse_facet ['result_citation']
    <S sid="103" ssid="19">Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['103']
103
['103']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="16">Following this method, we repeatedly &#8212; say, 1000 times &#8212; sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.</S>
original cit marker offset is 0
new cit marker offset is 0



['50']
50
['50']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="7">We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.</S>
original cit marker offset is 0
new cit marker offset is 0



['68']
68
['68']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="8">The way judgements are collected, human judges tend to use the scores to rank systems against each other.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['result_citation']
<S sid="145" ssid="38">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S>
original cit marker offset is 0
new cit marker offset is 0



['145']
145
['145']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="80" ssid = "19">We collected around 300–400 judgements per judgement type (adequacy or fluency)  per system  per language pair.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'126'", "'80'", "'136'", "'0'"]
'134'
'126'
'80'
'136'
'0'
['134', '126', '80', '136', '0']
parsed_discourse_facet ['results_citation']
<S sid ="159" ssid = "52">(b) does the translation have the same meaning  including connotations?</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'159'", "'0'", "'84'", "'17'", "'170'"]
'159'
'0'
'84'
'17'
'170'
['159', '0', '84', '17', '170']
parsed_discourse_facet ['method_citation']
<S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'", "'18'", "'16'", "'126'", "'84'"]
'170'
'18'
'16'
'126'
'84'
['170', '18', '16', '126', '84']
parsed_discourse_facet ['method_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'17'", "'170'", "'84'", "'38'"]
'126'
'17'
'170'
'84'
'38'
['126', '17', '170', '84', '38']
parsed_discourse_facet ['method_citation']
<S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'", "'38'", "'126'", "'17'", "'84'"]
'170'
'38'
'126'
'17'
'84'
['170', '38', '126', '17', '84']
parsed_discourse_facet ['method_citation']
<S sid ="52" ssid = "18">Pairwise comparison: We can use the same method to assess the statistical significance of one system outperforming another.</S><S sid ="21" ssid = "14">The out-of-domain test set differs from the Europarl data in various ways.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="129" ssid = "22">All systems (except for Systran  which was not tuned to Europarl) did considerably worse on outof-domain training data.</S><S sid ="35" ssid = "1">For the automatic evaluation  we used BLEU  since it is the most established metric in the field.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'", "'21'", "'28'", "'129'", "'35'"]
'52'
'21'
'28'
'129'
'35'
['52', '21', '28', '129', '35']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="113" ssid = "6">The confidence intervals are computed by bootstrap resampling for BLEU  and by standard significance testing for the manual scores  as described earlier in the paper.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'113'", "'126'", "'28'", "'0'"]
'16'
'113'
'126'
'28'
'0'
['16', '113', '126', '28', '0']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="109" ssid = "2">The scores and confidence intervals are detailed first in the Figures 7–10 in table form (including ranks)  and then in graphical form in Figures 11–16.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'16'", "'38'", "'84'", "'109'"]
'18'
'16'
'38'
'84'
'109'
['18', '16', '38', '84', '109']
parsed_discourse_facet ['method_citation']
<S sid ="9" ssid = "2">Training and testing is based on the Europarl corpus.</S><S sid ="44" ssid = "10">We computed BLEU scores for each submission with a single reference translation.</S><S sid ="143" ssid = "36">For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.</S><S sid ="25" ssid = "18">We received submissions from 14 groups from 11 institutions  as listed in Figure 2.</S><S sid ="35" ssid = "1">For the automatic evaluation  we used BLEU  since it is the most established metric in the field.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'44'", "'143'", "'25'", "'35'"]
'9'
'44'
'143'
'25'
'35'
['9', '44', '143', '25', '35']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="68" ssid = "7">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'68'", "'0'", "'28'", "'136'"]
'134'
'68'
'0'
'28'
'136'
['134', '68', '0', '28', '136']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="68" ssid = "7">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'0'", "'68'", "'126'", "'17'"]
'134'
'0'
'68'
'126'
'17'
['134', '0', '68', '126', '17']
parsed_discourse_facet ['method_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="159" ssid = "52">(b) does the translation have the same meaning  including connotations?</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'134'", "'17'", "'84'", "'159'"]
'126'
'134'
'17'
'84'
'159'
['126', '134', '17', '84', '159']
parsed_discourse_facet ['method_citation']
<S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'38'", "'155'", "'170'", "'6'"]
'84'
'38'
'155'
'170'
'6'
['84', '38', '155', '170', '6']
parsed_discourse_facet ['results_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'134'", "'0'", "'17'", "'84'"]
'126'
'134'
'0'
'17'
'84'
['126', '134', '0', '17', '84']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="68" ssid = "7">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'0'", "'126'", "'68'", "'17'"]
'134'
'0'
'126'
'68'
'17'
['134', '0', '126', '68', '17']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'170'", "'84'", "'126'", "'17'"]
'0'
'170'
'84'
'126'
'17'
['0', '170', '84', '126', '17']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "19">We collected around 300–400 judgements per judgement type (adequacy or fluency)  per system  per language pair.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="150" ssid = "43">For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French)  at least as measured by BLEU.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'84'", "'150'", "'16'", "'155'"]
'80'
'84'
'150'
'16'
'155'
['80', '84', '150', '16', '155']
parsed_discourse_facet ['method_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'84'", "'38'", "'17'", "'170'"]
'126'
'84'
'38'
'17'
'170'
['126', '84', '38', '17', '170']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



W06-3114
C08-1074
0
method_citation
['method_citation']



W06-3114
P07-1108
0
method_citation
['method_citation']



W06-3114
D07-1030
0
method_citation
['method_citation']
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W99-0623_sweta.csv
 <S sid="144" ssid="6">Combining multiple highly-accurate independent parsers yields promising results.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
 <S sid="125" ssid="54">The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["125'"]
125'
['125']
parsed_discourse_facet ['method_citation']
<S sid="125" ssid="54">The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["125'"]
125'
['125']
parsed_discourse_facet ['method_citation']
    <S sid="48" ssid="34">&#8226; Similarly, when the na&#239;ve Bayes classifier is configured such that the constituents require estimated probabilities strictly larger than 0.5 to be accepted, there is not enough probability mass remaining on crossing brackets for them to be included in the hypothesis.</S>
original cit marker offset is 0
new cit marker offset is 0



["48'"]
48'
['48']
parsed_discourse_facet ['method_citation']
 <S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="63">As seen by the drop in average individual parser performance baseline, the introduced parser does not perform very well.</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
    <S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="13" ssid="9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993).</S>
original cit marker offset is 0
new cit marker offset is 0



["13'"]
13'
['13']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="37">From this we see that a finer-grained model for parser combination, at least for the features we have examined, will not give us any additional power.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="98" ssid="27">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["98'"]
98'
['98']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="13">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'"]
27'
['27']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="9">For our experiments we also report the mean of precision and recall, which we denote by (P + R)I2 and F-measure.</S>
    <S sid="81" ssid="10">F-measure is the harmonic mean of precision and recall, 2PR/(P + R).</S>
    <S sid="82" ssid="11">It is closer to the smaller value of precision and recall when there is a large skew in their values.</S>
original cit marker offset is 0
new cit marker offset is 0



["80'", "'81'", "'82'"]
80'
'81'
'82'
['80', '81', '82']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="35">In general, the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["49'"]
49'
['49']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="7">Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["11'"]
11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="98" ssid="27">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["98'"]
98'
['98']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0623.csv
<S sid ="142" ssid = "4">Both of the switching techniques  as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.</S><S sid ="141" ssid = "3">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'141'", "'18'", "'81'", "'57'"]
'142'
'141'
'18'
'81'
'57'
['142', '141', '18', '81', '57']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'61'", "'81'", "'78'", "'139'"]
'112'
'61'
'81'
'78'
'139'
['112', '61', '81', '78', '139']
Error in Discourse Facet
<S sid ="62" ssid = "48">This is the parse that is closest to the centroid of the observed parses under the similarity metric.</S><S sid ="88" ssid = "17">For example  one parser could be more accurate at predicting noun phrases than the other parsers.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid ="115" ssid = "44">It is the performance we could achieve if an omniscient observer told us which parser to pick for each of the sentences.</S><S sid ="22" ssid = "8">If enough parsers suggest that a particular constituent belongs in the parse  we include it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'", "'88'", "'27'", "'115'", "'22'"]
'62'
'88'
'27'
'115'
'22'
['62', '88', '27', '115', '22']
Error in Discourse Facet
<S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="125" ssid = "54">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S><S sid ="41" ssid = "27">IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.</S><S sid ="44" ssid = "30">Each of the constituents must have received at least 1 votes from the k parsers  so a > I1 and 2 — 2k±-1 b > ri-5-111.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'21'", "'125'", "'41'", "'44'"]
'38'
'21'
'125'
'41'
'44'
['38', '21', '125', '41', '44']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'81'", "'15'", "'78'", "'139'"]
'112'
'81'
'15'
'78'
'139'
['112', '81', '15', '78', '139']
Error in Discourse Facet
<S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="136" ssid = "65">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S><S sid ="131" ssid = "60">It was then tested on section 22 of the Treebank in conjunction with the other parsers.</S><S sid ="103" ssid = "32">The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.</S><S sid ="132" ssid = "61">The results of this experiment can be seen in Table 5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'136'", "'131'", "'103'", "'132'"]
'21'
'136'
'131'
'103'
'132'
['21', '136', '131', '103', '132']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'78'", "'81'", "'61'", "'130'"]
'112'
'78'
'81'
'61'
'130'
['112', '78', '81', '61', '130']
Error in Discourse Facet
<S sid ="125" ssid = "54">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="1" ssid = "1">Three state-of-the-art statistical parsers are combined to produce more accurate parses  as well as new bounds on achievable Treebank parsing accuracy.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'27'", "'21'", "'1'", "'38'"]
'125'
'27'
'21'
'1'
'38'
['125', '27', '21', '1', '38']
Error in Discourse Facet
<S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="51" ssid = "37">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes  a completely flat structure.</S><S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'12'", "'51'", "'18'", "'57'"]
'55'
'12'
'51'
'18'
'57'
['55', '12', '51', '18', '57']
Error in Discourse Facet
<S sid ="142" ssid = "4">Both of the switching techniques  as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.</S><S sid ="127" ssid = "56">Parser 3  the most accurate parser  was chosen 71% of the time  and Parser 1  the least accurate parser was chosen 16% of the time.</S><S sid ="141" ssid = "3">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="140" ssid = "2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'127'", "'141'", "'12'", "'140'"]
'142'
'127'
'141'
'12'
'140'
['142', '127', '141', '12', '140']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'78'", "'61'", "'130'", "'15'"]
'112'
'78'
'61'
'130'
'15'
['112', '78', '61', '130', '15']
Error in Discourse Facet
<S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="51" ssid = "37">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes  a completely flat structure.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S><S sid ="26" ssid = "12">This technique has the advantage of requiring no training  but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'51'", "'52'", "'50'", "'26'"]
'55'
'51'
'52'
'50'
'26'
['55', '51', '52', '50', '26']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="114" ssid = "43">The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'81'", "'78'", "'61'", "'114'"]
'112'
'81'
'78'
'61'
'114'
['112', '81', '78', '61', '114']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'81'", "'12'", "'15'", "'130'"]
'112'
'81'
'12'
'15'
'130'
['112', '81', '12', '15', '130']
Error in Discourse Facet
<S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="114" ssid = "43">The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.</S><S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S>
original cit marker offset is 0
new cit marker offset is 0



["'78'", "'61'", "'114'", "'112'", "'81'"]
'78'
'61'
'114'
'112'
'81'
['78', '61', '114', '112', '81']
Error in Discourse Facet
<S sid ="129" ssid = "58">In the interest of testing the robustness of these combining techniques  we added a fourth  simple nonlexicalized PCFG parser.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'129'", "'11'", "'130'", "'15'", "'9'"]
'129'
'11'
'130'
'15'
'9'
['129', '11', '130', '15', '9']
Error in Discourse Facet
<S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S><S sid ="114" ssid = "43">The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.</S><S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["'81'", "'57'", "'114'", "'112'", "'78'"]
'81'
'57'
'114'
'112'
'78'
['81', '57', '114', '112', '78']
Error in Discourse Facet
IGNORE THIS: key error 1



W99-0623
N09-2064
0
method_citation
parsing: input/ref/Task1/P87-1015_swastika.csv
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="149" ssid="34">We can obtain a letter equivalent CFL defined by a CFG in which the for each rule as above, we have the production A &#8212;* A1 Anup where tk (up) = cp.</S>
original cit marker offset is 0
new cit marker offset is 0



['149']
149
['149']
parsed_discourse_facet ['method_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
    <S sid="2" ssid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['2']
2
['2']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="222" ssid="28">However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



['222']
222
['222']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['???']
???
['???']
parsed_discourse_facet ['aim_citation']
<S sid="207" ssid="13">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['207']
207
['207']
parsed_discourse_facet ['result_citation']
<S sid="207" ssid="13">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['207']
207
['207']
parsed_discourse_facet ['result_citation']
<S sid="19" ssid="4">It can be easily shown from Thatcher's result that the path set of every local set is a regular set.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/P87-1015.csv
<S sid ="115" ssid = "21">From the point of view of recognition  independent paths in the derivation structures suggests that a top-down parser (for example) can work on each branch independently  which may lead to efficient parsing using an algorithm based on the Divide and Conquer technique.</S><S sid ="222" ssid = "28">However  in order to capture the properties of various grammatical systems under consideration  our notation is more restrictive that ILFP  which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="67" ssid = "52">On the one hand  the definition of composition in Steedman (1985)  which technically permits composition of functions with unbounded number of arguments  generates tree sets with dependent paths such as those shown in Figure 6.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'222'", "'195'", "'67'", "'153'"]
'115'
'222'
'195'
'67'
'153'
['115', '222', '195', '67', '153']
parsed_discourse_facet ['results_citation']
<S sid ="37" ssid = "22">Like CFG's  the choice is predetermined by a finite number of rules encapsulated in the grammar.</S><S sid ="231" ssid = "37">In this paper  our goal has been to use the notion of LCFRS's to classify grammatical systems on the basis of their strong generative capacity.</S><S sid ="197" ssid = "3">We address the question of whether or not a formalism can generate only structural descriptions with independent paths.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S><S sid ="92" ssid = "77">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'231'", "'197'", "'116'", "'92'"]
'37'
'231'
'197'
'116'
'92'
['37', '231', '197', '116', '92']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.</S><S sid ="131" ssid = "16">The composition operations in the case of CFG's are parameterized by the productions.</S><S sid ="65" ssid = "50">While the generative power of CG's is greater that of CFG's  it appears to be highly constrained.</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="179" ssid = "64">It can be seen that M performs a top-down recognition of the input al ... nin logspace.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'131'", "'65'", "'227'", "'179'"]
'204'
'131'
'65'
'227'
'179'
['204', '131', '65', '227', '179']
parsed_discourse_facet ['method_citation']
<S sid ="194" ssid = "79">Thus  M works in logspace and recognition can be done on a deterministic TM in polynomial tape.</S><S sid ="92" ssid = "77">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S><S sid ="65" ssid = "50">While the generative power of CG's is greater that of CFG's  it appears to be highly constrained.</S><S sid ="20" ssid = "5">As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.</S><S sid ="131" ssid = "16">The composition operations in the case of CFG's are parameterized by the productions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'92'", "'65'", "'20'", "'131'"]
'194'
'92'
'65'
'20'
'131'
['194', '92', '65', '20', '131']
parsed_discourse_facet ['method_citation']
<S sid ="155" ssid = "40">CFG's  TAG's  MCTAG's and HG's are all members of this class since they satisfy these restrictions.</S><S sid ="37" ssid = "22">Like CFG's  the choice is predetermined by a finite number of rules encapsulated in the grammar.</S><S sid ="117" ssid = "2">Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.</S><S sid ="92" ssid = "77">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S><S sid ="48" ssid = "33">There has been recent interest in the application of Indexed Grammars (IG's) to natural languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'", "'37'", "'117'", "'92'", "'48'"]
'155'
'37'
'117'
'92'
'48'
['155', '37', '117', '92', '48']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "5">As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.</S><S sid ="216" ssid = "22">Having defined LCFRS's  in Section 4.2 we established the semilinearity (and hence constant growth property) of the languages generated.</S><S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="179" ssid = "64">It can be seen that M performs a top-down recognition of the input al ... nin logspace.</S><S sid ="131" ssid = "16">The composition operations in the case of CFG's are parameterized by the productions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'216'", "'59'", "'179'", "'131'"]
'20'
'216'
'59'
'179'
'131'
['20', '216', '59', '179', '131']
parsed_discourse_facet ['method_citation']
<S sid ="162" ssid = "47">Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="219" ssid = "25">The composition operations are mapped onto operations that use concatenation to define the substrings spanned by the resulting structures.</S><S sid ="0" ssid = "0">CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*</S><S sid ="193" ssid = "78">Since the work tapes store integers (which can be written in binary) that never exceed the size of the input  no configuration has space exceeding 0(log n).</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'", "'195'", "'219'", "'0'", "'193'"]
'162'
'195'
'219'
'0'
'193'
['162', '195', '219', '0', '193']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "9">By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.</S><S sid ="115" ssid = "21">From the point of view of recognition  independent paths in the derivation structures suggests that a top-down parser (for example) can work on each branch independently  which may lead to efficient parsing using an algorithm based on the Divide and Conquer technique.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="1" ssid = "1">We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'115'", "'153'", "'1'", "'195'"]
'11'
'115'
'153'
'1'
'195'
['11', '115', '153', '1', '195']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*</S><S sid ="219" ssid = "25">The composition operations are mapped onto operations that use concatenation to define the substrings spanned by the resulting structures.</S><S sid ="125" ssid = "10">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S sid ="58" ssid = "43">Unification is used in LFG's to produce structures having two dependent spines of unbounded length as in Figure 5.</S><S sid ="192" ssid = "77">Thus  the ATM has no more than 6km&quot; + 1 work tapes  where km&quot; is the maximum number of substrings spanned by a derived structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'219'", "'125'", "'58'", "'192'"]
'0'
'219'
'125'
'58'
'192'
['0', '219', '125', '58', '192']
parsed_discourse_facet ['method_citation']
<S sid ="193" ssid = "78">Since the work tapes store integers (which can be written in binary) that never exceed the size of the input  no configuration has space exceeding 0(log n).</S><S sid ="129" ssid = "14">Frontier nodes are annotated by zero arty functions corresponding to elementary structures.</S><S sid ="127" ssid = "12">Nodes are annotated by the name of the composition operation used at that step in the derivation.</S><S sid ="172" ssid = "57">A step of an ATM consists of reading a symbol from each tape and optionally moving each head to the left or right one tape cell.</S><S sid ="173" ssid = "58">A configuration of M consists of a state of the finite control  the nonblank contents of the input tape and k work tapes  and the position of each head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'193'", "'129'", "'127'", "'172'", "'173'"]
'193'
'129'
'127'
'172'
'173'
['193', '129', '127', '172', '173']
parsed_discourse_facet ['method_citation']
<S sid ="189" ssid = "74">For rules p : A fpo such that fp is constant function  giving an elementary structure  fp is defined such that fp() = (Si ... xi() where each z is a constant string.</S><S sid ="183" ssid = "68">We assume that M is in an existential state qA  with integers i1 and i2 representing zi in the (2i — 1)th and 22th work tape  for 1 < i < k. For each rule p : A fp(B  C) such that fp is mapped onto the function fp defined by the following rule. jp((xi .. •  rnt)  (1ii  • • • • Yn3))= (Zi   • • •   Zk) M breaks xi   zk into substrings xi    xn  and yi ... y&quot; conforming to the definition of fp.</S><S sid ="160" ssid = "45">A derived structure will be mapped onto a sequence zi of substrings (not necessarily contiguous in the input)  and the composition operations will be mapped onto functions that can defined as follows3. f((zi • • •   zni)  (m. • • •  Yn3)) = (Z1  • • •   Zn3) where each z  is the concatenation of strings from z 's and yk's.</S><S sid ="148" ssid = "33">If 0(A) gives the number of occurrences of each terminal in the structure named by A  then  given the constraints imposed on the formalism  for each rule A --. fp(Ai    An) we have the equality where c„ is some constant.</S><S sid ="201" ssid = "7">It is interesting to note  however  that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG  FUG and IG's.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'183'", "'160'", "'148'", "'201'"]
'189'
'183'
'160'
'148'
'201'
['189', '183', '160', '148', '201']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*</S><S sid ="162" ssid = "47">Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="196" ssid = "2">We contrasted formalisms such as CFG's  HG's  TAG's and MCTAG's  with formalisms such as IG's and unificational systems such as LFG's and FUG's.</S><S sid ="204" ssid = "10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'162'", "'227'", "'196'", "'204'"]
'0'
'162'
'227'
'196'
'204'
['0', '162', '227', '196', '204']
parsed_discourse_facet ['method_citation']
<S sid ="58" ssid = "43">Unification is used in LFG's to produce structures having two dependent spines of unbounded length as in Figure 5.</S><S sid ="214" ssid = "20">LCFRS's share several properties possessed by the class of mildly context-sensitive formalisms discussed by Joshi (1983/85).</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="196" ssid = "2">We contrasted formalisms such as CFG's  HG's  TAG's and MCTAG's  with formalisms such as IG's and unificational systems such as LFG's and FUG's.</S><S sid ="163" ssid = "48">This representation of structures by substrings and the composition operation by its effect on substrings is related to the work of Rounds (1985).</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'214'", "'227'", "'196'", "'163'"]
'58'
'214'
'227'
'196'
'163'
['58', '214', '227', '196', '163']
parsed_discourse_facet ['results_citation']
<S sid ="50" ssid = "35">The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.</S><S sid ="202" ssid = "8">As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.</S><S sid ="7" ssid = "5">The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG's and IG's.</S><S sid ="230" ssid = "36">LCFRS's have only been loosely defined in this paper; we have yet to provide a complete set of formal properties associated with members of this class.</S><S sid ="110" ssid = "16">The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'202'", "'7'", "'230'", "'110'"]
'50'
'202'
'7'
'230'
'110'
['50', '202', '7', '230', '110']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "42">For example  in TAG's a derived auxiliary tree spans two substrings (to the left and right of the foot node)  and the adjunction operation inserts another substring (spanned by the subtree under the node where adjunction takes place) between them (see Figure 3).</S><S sid ="162" ssid = "47">Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .</S><S sid ="83" ssid = "68">The following context-free production captures the derivation step of the grammar shown in Figure 7  in which the trees in the auxiliary tree set are adjoined into themselves at the root node (address c).</S><S sid ="67" ssid = "52">On the one hand  the definition of composition in Steedman (1985)  which technically permits composition of functions with unbounded number of arguments  generates tree sets with dependent paths such as those shown in Figure 6.</S><S sid ="154" ssid = "39">Since each composition operation is linear and nonerasing  a bounded sequences of substrings associated with the resulting structure is obtained by combining the substrings in each of its arguments using only the concatenation operation  including each substring exactly once.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'162'", "'83'", "'67'", "'154'"]
'157'
'162'
'83'
'67'
'154'
['157', '162', '83', '67', '154']
parsed_discourse_facet ['method_citation']
<S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="156" ssid = "41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S><S sid ="204" ssid = "10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.</S><S sid ="213" ssid = "19">By sharing stacks (in IG's) or by using nonlinear equations over f-structures (in FUG's and LFG's)  structures with unbounded dependencies between paths can be generated.</S><S sid ="125" ssid = "10">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'227'", "'156'", "'204'", "'213'", "'125'"]
'227'
'156'
'204'
'213'
'125'
['227', '156', '204', '213', '125']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P04-1036_swastika.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="24">We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.</S>
original cit marker offset is 0
new cit marker offset is 0



['68']
68
['68']
parsed_discourse_facet ['result_citation']
<S sid="83" ssid="12">The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.</S>
original cit marker offset is 0
new cit marker offset is 0



['83']
83
['83']
parsed_discourse_facet ['result_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
<S sid="101" ssid="30">Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.</S>
original cit marker offset is 0
new cit marker offset is 0



['101']
101
['101']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="83" ssid="12">The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.</S>
original cit marker offset is 0
new cit marker offset is 0



['83']
83
['83']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="46" ssid="2">This provides the nearest neighbours to each target word, along with the distributional similarity score between the target word and its neighbour.</S>
original cit marker offset is 0
new cit marker offset is 0



['46']
46
['46']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents, rather than a lexical sample task, where the target words are manually determined and the results will depend on the skew of the words in the sample.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
<S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['method_citation']
<S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="115" ssid = "13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor  which is very encouraging given that our method only uses raw text  with no manual labelling.</S><S sid ="12" ssid = "5">The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor  from those that do not (without HTD).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'123'", "'14'", "'115'", "'12'"]
'8'
'123'
'14'
'115'
'12'
['8', '123', '14', '115', '12']
parsed_discourse_facet ['results_citation']
<S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="105" ssid = "3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents  rather than a lexical sample task  where the target words are manually determined and the results will depend on the skew of the words in the sample.</S><S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="145" ssid = "22">It is not always intuitively clear which of the senses to expect as predominant sense for either a particular domain or for the BNC  but the first senses of words like division and goal shift towards the more specific senses (league and score respectively).</S><S sid ="155" ssid = "3">A major benefit of our work  rather than reliance on hand-tagged training data such as SemCor  is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'105'", "'165'", "'145'", "'155'"]
'15'
'105'
'165'
'145'
'155'
['15', '105', '165', '145', '155']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="174" ssid = "22">We have restricted ourselves to nouns in this work  since this PoS is perhaps most affected by domain.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of “nearest neighbours”.</S><S sid ="133" ssid = "10">We acquired thesauruses for these corpora using the procedure described in section 2.1.</S><S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'174'", "'34'", "'133'", "'7'"]
'100'
'174'
'34'
'133'
'7'
['100', '174', '34', '133', '7']
parsed_discourse_facet ['method_citation']
<S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="70" ssid = "26">Jiang and Conrath specify a distance measure:   where the third class ( ) is the most informative  or most specific  superordinate synset of the two senses and .</S><S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="71" ssid = "27">This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal:</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'50'", "'70'", "'61'", "'71'"]
'44'
'50'
'70'
'61'
'71'
['44', '50', '70', '61', '71']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S><S sid ="72" ssid = "1">In order to evaluate our method we use the data in SemCor as a gold-standard.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'72'", "'91'", "'10'", "'45'"]
'30'
'72'
'91'
'10'
'45'
['30', '72', '91', '10', '45']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S><S sid ="81" ssid = "10">4 We calculate the accuracy of finding the predominant sense  when there is indeed one sense with a higher frequency than the others for this word in SemCor ( ).</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'50'", "'82'", "'81'", "'44'"]
'69'
'50'
'82'
'81'
'44'
['69', '50', '82', '81', '44']
parsed_discourse_facet ['method_citation']
<S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S><S sid ="58" ssid = "14">A noun    is thus described by a set of co-occurrence triples and associated frequencies  where is a grammatical relation and is a possible cooccurrence with in that relation.</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S><S sid ="56" ssid = "12">For each noun we considered the co-occurring verbs in the direct object and subject relation  the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations.</S><S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'58'", "'45'", "'56'", "'7'"]
'55'
'58'
'45'
'56'
'7'
['55', '58', '45', '56', '7']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S><S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="53" ssid = "9">This weight is the WordNet similarity score ( ) between the target sense ( ) and the sense of ( ) that maximises this score  divided by the sum of all such WordNet similarity scores for and .</S><S sid ="52" ssid = "8">For each sense of ( ) we obtain a ranking score by summing over the of each neighbour ( ) multiplied by a weight.</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'136'", "'53'", "'52'", "'82'"]
'30'
'136'
'53'
'52'
'82'
['30', '136', '53', '52', '82']
parsed_discourse_facet ['method_citation']
<S sid ="135" ssid = "12">We therefore decided to select a limited number of words and to evaluate these words qualitatively.</S><S sid ="179" ssid = "2">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S sid ="111" ssid = "9">We give the results for this WSD task in table 2.</S><S sid ="54" ssid = "10">Thus we rank each sense using: parser (Briscoe and Carroll  2002).</S><S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'135'", "'179'", "'111'", "'54'", "'6'"]
'135'
'179'
'111'
'54'
'6'
['135', '179', '111', '54', '6']
parsed_discourse_facet ['method_citation']
<S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="156" ssid = "4">Buitelaar and Sacaleanu (2001) have previously explored ranking and selection of synsets in GermaNet for specific domains using the words in a given synset  and those related by hyponymy  and a term relevance measure taken from information retrieval.</S><S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'156'", "'61'", "'65'", "'142'"]
'137'
'156'
'61'
'65'
'142'
['137', '156', '61', '65', '142']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="71" ssid = "27">This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal:</S><S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'69'", "'44'", "'71'", "'30'"]
'50'
'69'
'44'
'71'
'30'
['50', '69', '44', '71', '30']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="70" ssid = "26">Jiang and Conrath specify a distance measure:   where the third class ( ) is the most informative  or most specific  superordinate synset of the two senses and .</S><S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'70'", "'137'", "'69'", "'44'"]
'50'
'70'
'137'
'69'
'44'
['50', '70', '137', '69', '44']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">SemCor comprises a relatively small sample of 250 000 words.</S><S sid ="161" ssid = "9">Our approach is complementary to this.</S><S sid ="29" ssid = "22">We discuss our method in the following section.</S><S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of “nearest neighbours”.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'161'", "'29'", "'6'", "'34'"]
'16'
'161'
'29'
'6'
'34'
['16', '161', '29', '6', '34']
parsed_discourse_facet ['results_citation']
<S sid ="157" ssid = "5">Buitelaar and Sacaleanu have evaluated their method on identifying domain specific concepts using human judgements on 100 items.</S><S sid ="185" ssid = "8">In the future  we will perform a large scale evaluation on domain specific corpora.</S><S sid ="193" ssid = "2">This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.</S><S sid ="39" ssid = "32">We expect that the quantity and similarity of the neighbours pertaining to different senses will reflect the dominance of the sense to which they pertain.</S><S sid ="128" ssid = "5">The Reuters corpus (Rose et al.  2002) is a collection of about 810 000 Reuters  English Language News stories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'185'", "'193'", "'39'", "'128'"]
'157'
'185'
'193'
'39'
'128'
['157', '185', '193', '39', '128']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "24">We are of course able to apply the method to other versions of WordNet. synset  is incremented with the frequency counts from the corpus of all words belonging to that synset  directly or via the hyponymy relation.</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'50'", "'65'", "'44'", "'82'"]
'68'
'50'
'65'
'44'
'82'
['68', '50', '65', '44', '82']
parsed_discourse_facet ['method_citation']
<S sid ="90" ssid = "19">From manual analysis  there are cases where the acquired first sense disagrees with SemCor  yet is intuitively plausible.</S><S sid ="175" ssid = "23">We are currently investigating the performance of the first sense heuristic  and this method  for other PoS on SENSEVAL-3 data (McCarthy et al.  2004)  although not yet with rankings from domain specific corpora.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="2" ssid = "2">The problem with using the predominant  or first sense heuristic  aside from the fact that it does not take surrounding context into account  is that it assumes some quantity of handtagged data.</S><S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'175'", "'8'", "'2'", "'1'"]
'90'
'175'
'8'
'2'
'1'
['90', '175', '8', '2', '1']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="0" ssid = "0">Finding Predominant Word Senses in Untagged Text</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'10'", "'0'", "'50'", "'30'"]
'69'
'10'
'0'
'50'
'30'
['69', '10', '0', '50', '30']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/W06-2932_vardha.csv
    <S sid="19" ssid="1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="5">However, in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="76" ssid="14">For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'"]
'76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="54" ssid="2">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'"]
'54'
['54']
parsed_discourse_facet ['method_citation']
    <S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="58" ssid="6">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'"]
'58'
['58']
parsed_discourse_facet ['method_citation']
    <S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'"]
'22'
['22']
parsed_discourse_facet ['method_citation']
    <S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
    <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'"]
'64'
['64']
parsed_discourse_facet ['method_citation']
    <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'"]
'43'
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S sid ="3" ssid = "3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S><S sid ="4" ssid = "4">We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.</S><S sid ="108" ssid = "5">Second  we plan on integrating any available morphological features in a more principled manner.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="106" ssid = "3">First  we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is nan
new cit marker offset is 0



["'3'", "'4'", "'108'", "'10'", "'106'"]
'3'
'4'
'108'
'10'
'106'
['3', '4', '108', '10', '106']
parsed_discourse_facet ['results_citation']
<S sid ="101" ssid = "23">For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.</S><S sid ="17" ssid = "13">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S><S sid ="16" ssid = "12">A dependency graph is represented by a set of ordered pairs (i  j) E y in which xj is a dependent and xi is the corresponding head.</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'17'", "'16'", "'2'", "'20'"]
'101'
'17'
'16'
'2'
'20'
['101', '17', '16', '2', '20']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S><S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="83" ssid = "5">In a preliminary test of this hypothesis  we looked at all of the sentences from a development set in which a main verb is incorrectly attached.</S><S sid ="50" ssid = "19">Various conjunctions of these were included based on performance on held-out data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'66'", "'88'", "'83'", "'50'"]
'61'
'66'
'88'
'83'
'50'
['61', '66', '88', '83', '50']
parsed_discourse_facet ['method_citation']
<S sid ="76" ssid = "14">For instance  sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.</S><S sid ="96" ssid = "18">Each stage by itself is relatively accurate (unlabeled accuracy is 79% and labeling accuracy3 is also 79%)  but since there is very little overlap in the kinds of errors each makes  overall labeled accuracy drops to 67%.</S><S sid ="57" ssid = "5">Performance is measured through unlabeled accuracy  which is the percentage of words that modify the correct head in the dependency graph  and labeled accuracy  which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'", "'41'", "'96'", "'57'", "'64'"]
'76'
'41'
'96'
'57'
'64'
['76', '41', '96', '57', '64']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S><S sid ="47" ssid = "16">We used the following: dependent have identical values?</S><S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="17" ssid = "13">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'47'", "'32'", "'36'", "'17'"]
'19'
'47'
'32'
'36'
'17'
['19', '47', '32', '36', '17']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "2">Based on performance from a held-out section of the training data  we used non-projective parsing algorithms for Czech  Danish  Dutch  German  Japanese  Portuguese and Slovene  and projective parsing algorithms for Arabic  Bulgarian  Chinese  Spanish  Swedish and Turkish.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="104" ssid = "1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al.  2005b; McDonald and Pereira  2006) generalizes well to languages other than English.</S><S sid ="21" ssid = "3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al.  2005a; McDonald et al.  2005b) to incorporate features over multiple edges in the dependency graph.</S><S sid ="63" ssid = "1">Our system has several components  including the ability to produce non-projective edges  sequential Japanese  Portuguese  Slovene  Spanish  Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'11'", "'104'", "'21'", "'63'"]
'54'
'11'
'104'
'21'
'63'
['54', '11', '104', '21', '63']
parsed_discourse_facet ['method_citation']
<S sid ="1" ssid = "1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S><S sid ="58" ssid = "6">These results show that the discriminative spanning tree parsing framework (McDonald et al.  2005b; McDonald and Pereira  2006) is easily adapted across all these languages.</S><S sid ="0" ssid = "0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S sid ="63" ssid = "1">Our system has several components  including the ability to produce non-projective edges  sequential Japanese  Portuguese  Slovene  Spanish  Swedish and Turkish.</S><S sid ="108" ssid = "5">Second  we plan on integrating any available morphological features in a more principled manner.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'58'", "'0'", "'63'", "'108'"]
'1'
'58'
'0'
'63'
'108'
['1', '58', '0', '63', '108']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="17" ssid = "13">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S><S sid ="53" ssid = "1">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).</S><S sid ="1" ssid = "1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'20'", "'17'", "'53'", "'1'"]
'0'
'20'
'17'
'53'
'1'
['0', '20', '17', '53', '1']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "16">We used the following: dependent have identical values?</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="19" ssid = "1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'2'", "'19'", "'20'", "'32'"]
'47'
'2'
'19'
'20'
'32'
['47', '2', '19', '20', '32']
parsed_discourse_facet ['method_citation']
<S sid ="81" ssid = "3">Other high-frequency word classes with relatively low attachment accuracy are prepositions (80%)  adverbs (82%) and subordinating conjunctions (80%)  for a total of another 23% of the test corpus.</S><S sid ="23" ssid = "5">That system uses MIRA  an online large-margin learning algorithm  to compute model parameters.</S><S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="43" ssid = "12">For score functions  we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation  we can find the highest scoring label sequence with Viterbi’s algorithm.</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'81'", "'23'", "'86'", "'43'", "'36'"]
'81'
'23'
'86'
'43'
'36'
['81', '23', '86', '43', '36']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216–220  New York City  June 2006. c�2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.</S><S sid ="76" ssid = "14">For instance  sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S><S sid ="79" ssid = "1">Although overall unlabeled accuracy is 86%  most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs  75% for the verb ser  and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'41'", "'76'", "'64'", "'79'"]
'18'
'41'
'76'
'64'
'79'
['18', '41', '76', '64', '79']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216–220  New York City  June 2006. c�2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="3" ssid = "3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S><S sid ="96" ssid = "18">Each stage by itself is relatively accurate (unlabeled accuracy is 79% and labeling accuracy3 is also 79%)  but since there is very little overlap in the kinds of errors each makes  overall labeled accuracy drops to 67%.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="95" ssid = "17">Note the difference in error between the unlabeled parser and the edge labeler: the former makes mistakes on edges into prepositions  conjunctions and verbs  and the latter makes mistakes on edges into nouns (subject/objects).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'3'", "'96'", "'11'", "'95'"]
'18'
'3'
'96'
'11'
'95'
['18', '3', '96', '11', '95']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="16" ssid = "12">A dependency graph is represented by a set of ordered pairs (i  j) E y in which xj is a dependent and xi is the corresponding head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'11'", "'41'", "'36'", "'16'"]
'32'
'11'
'41'
'36'
'16'
['32', '11', '41', '36', '16']
parsed_discourse_facet ['results_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



W06-2932
W12-3407
0
method_citation
['method_citation']



W06-2932
W08-1007
0
method_citation
['method_citation']
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W06-2932_sweta.csv
 <S sid="5" ssid="1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="5">However, in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["36'"]
36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="9">In fact, for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="76" ssid="14">For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["76'"]
76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="45" ssid="14">Furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["45'"]
45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="3">First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["106'"]
106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="58" ssid="6">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["58'"]
58'
['58']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
 <S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="2">Ideally one would like to make all parsing and labeling decisions jointly so that the shared knowledge of both decisions will help resolve any ambiguities.</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
 <S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["43'"]
43'
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S sid ="3" ssid = "3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S><S sid ="4" ssid = "4">We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.</S><S sid ="108" ssid = "5">Second  we plan on integrating any available morphological features in a more principled manner.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="106" ssid = "3">First  we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is nan
new cit marker offset is 0



["'3'", "'4'", "'108'", "'10'", "'106'"]
'3'
'4'
'108'
'10'
'106'
['3', '4', '108', '10', '106']
parsed_discourse_facet ['results_citation']
<S sid ="101" ssid = "23">For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.</S><S sid ="17" ssid = "13">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S><S sid ="16" ssid = "12">A dependency graph is represented by a set of ordered pairs (i  j) E y in which xj is a dependent and xi is the corresponding head.</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'17'", "'16'", "'2'", "'20'"]
'101'
'17'
'16'
'2'
'20'
['101', '17', '16', '2', '20']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S><S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="83" ssid = "5">In a preliminary test of this hypothesis  we looked at all of the sentences from a development set in which a main verb is incorrectly attached.</S><S sid ="50" ssid = "19">Various conjunctions of these were included based on performance on held-out data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'66'", "'88'", "'83'", "'50'"]
'61'
'66'
'88'
'83'
'50'
['61', '66', '88', '83', '50']
parsed_discourse_facet ['method_citation']
<S sid ="76" ssid = "14">For instance  sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.</S><S sid ="96" ssid = "18">Each stage by itself is relatively accurate (unlabeled accuracy is 79% and labeling accuracy3 is also 79%)  but since there is very little overlap in the kinds of errors each makes  overall labeled accuracy drops to 67%.</S><S sid ="57" ssid = "5">Performance is measured through unlabeled accuracy  which is the percentage of words that modify the correct head in the dependency graph  and labeled accuracy  which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'", "'41'", "'96'", "'57'", "'64'"]
'76'
'41'
'96'
'57'
'64'
['76', '41', '96', '57', '64']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S><S sid ="47" ssid = "16">We used the following: dependent have identical values?</S><S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="17" ssid = "13">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'47'", "'32'", "'36'", "'17'"]
'19'
'47'
'32'
'36'
'17'
['19', '47', '32', '36', '17']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "2">Based on performance from a held-out section of the training data  we used non-projective parsing algorithms for Czech  Danish  Dutch  German  Japanese  Portuguese and Slovene  and projective parsing algorithms for Arabic  Bulgarian  Chinese  Spanish  Swedish and Turkish.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="104" ssid = "1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al.  2005b; McDonald and Pereira  2006) generalizes well to languages other than English.</S><S sid ="21" ssid = "3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al.  2005a; McDonald et al.  2005b) to incorporate features over multiple edges in the dependency graph.</S><S sid ="63" ssid = "1">Our system has several components  including the ability to produce non-projective edges  sequential Japanese  Portuguese  Slovene  Spanish  Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'11'", "'104'", "'21'", "'63'"]
'54'
'11'
'104'
'21'
'63'
['54', '11', '104', '21', '63']
parsed_discourse_facet ['method_citation']
<S sid ="1" ssid = "1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S><S sid ="58" ssid = "6">These results show that the discriminative spanning tree parsing framework (McDonald et al.  2005b; McDonald and Pereira  2006) is easily adapted across all these languages.</S><S sid ="0" ssid = "0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S sid ="63" ssid = "1">Our system has several components  including the ability to produce non-projective edges  sequential Japanese  Portuguese  Slovene  Spanish  Swedish and Turkish.</S><S sid ="108" ssid = "5">Second  we plan on integrating any available morphological features in a more principled manner.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'58'", "'0'", "'63'", "'108'"]
'1'
'58'
'0'
'63'
'108'
['1', '58', '0', '63', '108']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="17" ssid = "13">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S><S sid ="53" ssid = "1">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).</S><S sid ="1" ssid = "1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'20'", "'17'", "'53'", "'1'"]
'0'
'20'
'17'
'53'
'1'
['0', '20', '17', '53', '1']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "16">We used the following: dependent have identical values?</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="19" ssid = "1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'2'", "'19'", "'20'", "'32'"]
'47'
'2'
'19'
'20'
'32'
['47', '2', '19', '20', '32']
parsed_discourse_facet ['method_citation']
<S sid ="81" ssid = "3">Other high-frequency word classes with relatively low attachment accuracy are prepositions (80%)  adverbs (82%) and subordinating conjunctions (80%)  for a total of another 23% of the test corpus.</S><S sid ="23" ssid = "5">That system uses MIRA  an online large-margin learning algorithm  to compute model parameters.</S><S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="43" ssid = "12">For score functions  we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation  we can find the highest scoring label sequence with Viterbi’s algorithm.</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'81'", "'23'", "'86'", "'43'", "'36'"]
'81'
'23'
'86'
'43'
'36'
['81', '23', '86', '43', '36']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216–220  New York City  June 2006. c�2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.</S><S sid ="76" ssid = "14">For instance  sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S><S sid ="79" ssid = "1">Although overall unlabeled accuracy is 86%  most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs  75% for the verb ser  and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'41'", "'76'", "'64'", "'79'"]
'18'
'41'
'76'
'64'
'79'
['18', '41', '76', '64', '79']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216–220  New York City  June 2006. c�2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="3" ssid = "3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S><S sid ="96" ssid = "18">Each stage by itself is relatively accurate (unlabeled accuracy is 79% and labeling accuracy3 is also 79%)  but since there is very little overlap in the kinds of errors each makes  overall labeled accuracy drops to 67%.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="95" ssid = "17">Note the difference in error between the unlabeled parser and the edge labeler: the former makes mistakes on edges into prepositions  conjunctions and verbs  and the latter makes mistakes on edges into nouns (subject/objects).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'3'", "'96'", "'11'", "'95'"]
'18'
'3'
'96'
'11'
'95'
['18', '3', '96', '11', '95']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="16" ssid = "12">A dependency graph is represented by a set of ordered pairs (i  j) E y in which xj is a dependent and xi is the corresponding head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'11'", "'41'", "'36'", "'16'"]
'32'
'11'
'41'
'36'
'16'
['32', '11', '41', '36', '16']
parsed_discourse_facet ['results_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1



W06-2932
W06-2920
0
method_citation
['method_citation']



W06-2932
W08-1007
0
method_citation
['method_citation']
parsing: input/ref/Task1/P04-1036_aakansha.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="34">In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
    <S sid="181" ssid="4">This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'181'"]
'180'
'181'
['180', '181']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="4">To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'"]
'48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="169" ssid="17">This method obtains precision of 61% and recall 51%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'"]
'169'
['169']
parsed_discourse_facet ['result_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="115" ssid="13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
    <S sid="181" ssid="4">This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'181'"]
'180'
'181'
['180', '181']
parsed_discourse_facet ['result_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'"]
'180'
['180']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="34">In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="115" ssid = "13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor  which is very encouraging given that our method only uses raw text  with no manual labelling.</S><S sid ="12" ssid = "5">The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor  from those that do not (without HTD).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'123'", "'14'", "'115'", "'12'"]
'8'
'123'
'14'
'115'
'12'
['8', '123', '14', '115', '12']
parsed_discourse_facet ['results_citation']
<S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="105" ssid = "3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents  rather than a lexical sample task  where the target words are manually determined and the results will depend on the skew of the words in the sample.</S><S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="145" ssid = "22">It is not always intuitively clear which of the senses to expect as predominant sense for either a particular domain or for the BNC  but the first senses of words like division and goal shift towards the more specific senses (league and score respectively).</S><S sid ="155" ssid = "3">A major benefit of our work  rather than reliance on hand-tagged training data such as SemCor  is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'105'", "'165'", "'145'", "'155'"]
'15'
'105'
'165'
'145'
'155'
['15', '105', '165', '145', '155']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="174" ssid = "22">We have restricted ourselves to nouns in this work  since this PoS is perhaps most affected by domain.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of “nearest neighbours”.</S><S sid ="133" ssid = "10">We acquired thesauruses for these corpora using the procedure described in section 2.1.</S><S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'174'", "'34'", "'133'", "'7'"]
'100'
'174'
'34'
'133'
'7'
['100', '174', '34', '133', '7']
parsed_discourse_facet ['method_citation']
<S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="70" ssid = "26">Jiang and Conrath specify a distance measure:   where the third class ( ) is the most informative  or most specific  superordinate synset of the two senses and .</S><S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="71" ssid = "27">This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal:</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'50'", "'70'", "'61'", "'71'"]
'44'
'50'
'70'
'61'
'71'
['44', '50', '70', '61', '71']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S><S sid ="72" ssid = "1">In order to evaluate our method we use the data in SemCor as a gold-standard.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'72'", "'91'", "'10'", "'45'"]
'30'
'72'
'91'
'10'
'45'
['30', '72', '91', '10', '45']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S><S sid ="81" ssid = "10">4 We calculate the accuracy of finding the predominant sense  when there is indeed one sense with a higher frequency than the others for this word in SemCor ( ).</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'50'", "'82'", "'81'", "'44'"]
'69'
'50'
'82'
'81'
'44'
['69', '50', '82', '81', '44']
parsed_discourse_facet ['method_citation']
<S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S><S sid ="58" ssid = "14">A noun    is thus described by a set of co-occurrence triples and associated frequencies  where is a grammatical relation and is a possible cooccurrence with in that relation.</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S><S sid ="56" ssid = "12">For each noun we considered the co-occurring verbs in the direct object and subject relation  the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations.</S><S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'58'", "'45'", "'56'", "'7'"]
'55'
'58'
'45'
'56'
'7'
['55', '58', '45', '56', '7']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S><S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="53" ssid = "9">This weight is the WordNet similarity score ( ) between the target sense ( ) and the sense of ( ) that maximises this score  divided by the sum of all such WordNet similarity scores for and .</S><S sid ="52" ssid = "8">For each sense of ( ) we obtain a ranking score by summing over the of each neighbour ( ) multiplied by a weight.</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'136'", "'53'", "'52'", "'82'"]
'30'
'136'
'53'
'52'
'82'
['30', '136', '53', '52', '82']
parsed_discourse_facet ['method_citation']
<S sid ="135" ssid = "12">We therefore decided to select a limited number of words and to evaluate these words qualitatively.</S><S sid ="179" ssid = "2">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S sid ="111" ssid = "9">We give the results for this WSD task in table 2.</S><S sid ="54" ssid = "10">Thus we rank each sense using: parser (Briscoe and Carroll  2002).</S><S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'135'", "'179'", "'111'", "'54'", "'6'"]
'135'
'179'
'111'
'54'
'6'
['135', '179', '111', '54', '6']
parsed_discourse_facet ['method_citation']
<S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="156" ssid = "4">Buitelaar and Sacaleanu (2001) have previously explored ranking and selection of synsets in GermaNet for specific domains using the words in a given synset  and those related by hyponymy  and a term relevance measure taken from information retrieval.</S><S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'156'", "'61'", "'65'", "'142'"]
'137'
'156'
'61'
'65'
'142'
['137', '156', '61', '65', '142']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="71" ssid = "27">This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal:</S><S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'69'", "'44'", "'71'", "'30'"]
'50'
'69'
'44'
'71'
'30'
['50', '69', '44', '71', '30']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="70" ssid = "26">Jiang and Conrath specify a distance measure:   where the third class ( ) is the most informative  or most specific  superordinate synset of the two senses and .</S><S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'70'", "'137'", "'69'", "'44'"]
'50'
'70'
'137'
'69'
'44'
['50', '70', '137', '69', '44']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">SemCor comprises a relatively small sample of 250 000 words.</S><S sid ="161" ssid = "9">Our approach is complementary to this.</S><S sid ="29" ssid = "22">We discuss our method in the following section.</S><S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of “nearest neighbours”.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'161'", "'29'", "'6'", "'34'"]
'16'
'161'
'29'
'6'
'34'
['16', '161', '29', '6', '34']
parsed_discourse_facet ['results_citation']
<S sid ="157" ssid = "5">Buitelaar and Sacaleanu have evaluated their method on identifying domain specific concepts using human judgements on 100 items.</S><S sid ="185" ssid = "8">In the future  we will perform a large scale evaluation on domain specific corpora.</S><S sid ="193" ssid = "2">This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.</S><S sid ="39" ssid = "32">We expect that the quantity and similarity of the neighbours pertaining to different senses will reflect the dominance of the sense to which they pertain.</S><S sid ="128" ssid = "5">The Reuters corpus (Rose et al.  2002) is a collection of about 810 000 Reuters  English Language News stories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'185'", "'193'", "'39'", "'128'"]
'157'
'185'
'193'
'39'
'128'
['157', '185', '193', '39', '128']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "24">We are of course able to apply the method to other versions of WordNet. synset  is incremented with the frequency counts from the corpus of all words belonging to that synset  directly or via the hyponymy relation.</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'50'", "'65'", "'44'", "'82'"]
'68'
'50'
'65'
'44'
'82'
['68', '50', '65', '44', '82']
parsed_discourse_facet ['method_citation']
<S sid ="90" ssid = "19">From manual analysis  there are cases where the acquired first sense disagrees with SemCor  yet is intuitively plausible.</S><S sid ="175" ssid = "23">We are currently investigating the performance of the first sense heuristic  and this method  for other PoS on SENSEVAL-3 data (McCarthy et al.  2004)  although not yet with rankings from domain specific corpora.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="2" ssid = "2">The problem with using the predominant  or first sense heuristic  aside from the fact that it does not take surrounding context into account  is that it assumes some quantity of handtagged data.</S><S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'175'", "'8'", "'2'", "'1'"]
'90'
'175'
'8'
'2'
'1'
['90', '175', '8', '2', '1']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="0" ssid = "0">Finding Predominant Word Senses in Untagged Text</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'10'", "'0'", "'50'", "'30'"]
'69'
'10'
'0'
'50'
'30'
['69', '10', '0', '50', '30']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P04-1036_vardha.csv
    <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
    <S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="66" ssid="22">It uses the glosses of semantically related (according to WordNet) senses too. jcn (Jiang and Conrath, 1997) This score uses corpus data to populate classes (synsets) in the WordNet hierarchy with frequency counts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'66'"]
'66'
['66']
parsed_discourse_facet ['method_citation']
 <S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="101" ssid="30">Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'"]
'101'
['101']
parsed_discourse_facet ['method_citation']
 <S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
 <S sid="115" ssid="13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
  <S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'"]
'89'
['89']
parsed_discourse_facet ['method_citation']
 <S sid="137" ssid="14">Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'"]
'137'
['137']
parsed_discourse_facet ['method_citation']
75 ssid="4">We generated a thesaurus entry for all polysemous nouns which occurred in SemCor with a frequency 2, and in the BNC with a frequency 10 in the grammatical relations listed in section 2.1 above.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'"]
'75'
['75']
Error in Reference Offset
 <S sid="155" ssid="3">A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
  <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
 <S sid="155" ssid="3">A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="24">We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'"]
'68'
['68']
parsed_discourse_facet ['method_citation']
    <S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'"]
'13'
['13']
parsed_discourse_facet ['method_citation']
  <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="115" ssid = "13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor  which is very encouraging given that our method only uses raw text  with no manual labelling.</S><S sid ="12" ssid = "5">The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor  from those that do not (without HTD).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'123'", "'14'", "'115'", "'12'"]
'8'
'123'
'14'
'115'
'12'
['8', '123', '14', '115', '12']
parsed_discourse_facet ['results_citation']
<S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="105" ssid = "3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents  rather than a lexical sample task  where the target words are manually determined and the results will depend on the skew of the words in the sample.</S><S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="145" ssid = "22">It is not always intuitively clear which of the senses to expect as predominant sense for either a particular domain or for the BNC  but the first senses of words like division and goal shift towards the more specific senses (league and score respectively).</S><S sid ="155" ssid = "3">A major benefit of our work  rather than reliance on hand-tagged training data such as SemCor  is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'105'", "'165'", "'145'", "'155'"]
'15'
'105'
'165'
'145'
'155'
['15', '105', '165', '145', '155']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="174" ssid = "22">We have restricted ourselves to nouns in this work  since this PoS is perhaps most affected by domain.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of “nearest neighbours”.</S><S sid ="133" ssid = "10">We acquired thesauruses for these corpora using the procedure described in section 2.1.</S><S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'174'", "'34'", "'133'", "'7'"]
'100'
'174'
'34'
'133'
'7'
['100', '174', '34', '133', '7']
parsed_discourse_facet ['method_citation']
<S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="70" ssid = "26">Jiang and Conrath specify a distance measure:   where the third class ( ) is the most informative  or most specific  superordinate synset of the two senses and .</S><S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="71" ssid = "27">This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal:</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'50'", "'70'", "'61'", "'71'"]
'44'
'50'
'70'
'61'
'71'
['44', '50', '70', '61', '71']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S><S sid ="72" ssid = "1">In order to evaluate our method we use the data in SemCor as a gold-standard.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'72'", "'91'", "'10'", "'45'"]
'30'
'72'
'91'
'10'
'45'
['30', '72', '91', '10', '45']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S><S sid ="81" ssid = "10">4 We calculate the accuracy of finding the predominant sense  when there is indeed one sense with a higher frequency than the others for this word in SemCor ( ).</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'50'", "'82'", "'81'", "'44'"]
'69'
'50'
'82'
'81'
'44'
['69', '50', '82', '81', '44']
parsed_discourse_facet ['method_citation']
<S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S><S sid ="58" ssid = "14">A noun    is thus described by a set of co-occurrence triples and associated frequencies  where is a grammatical relation and is a possible cooccurrence with in that relation.</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S><S sid ="56" ssid = "12">For each noun we considered the co-occurring verbs in the direct object and subject relation  the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations.</S><S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'58'", "'45'", "'56'", "'7'"]
'55'
'58'
'45'
'56'
'7'
['55', '58', '45', '56', '7']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S><S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="53" ssid = "9">This weight is the WordNet similarity score ( ) between the target sense ( ) and the sense of ( ) that maximises this score  divided by the sum of all such WordNet similarity scores for and .</S><S sid ="52" ssid = "8">For each sense of ( ) we obtain a ranking score by summing over the of each neighbour ( ) multiplied by a weight.</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'136'", "'53'", "'52'", "'82'"]
'30'
'136'
'53'
'52'
'82'
['30', '136', '53', '52', '82']
parsed_discourse_facet ['method_citation']
<S sid ="135" ssid = "12">We therefore decided to select a limited number of words and to evaluate these words qualitatively.</S><S sid ="179" ssid = "2">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S sid ="111" ssid = "9">We give the results for this WSD task in table 2.</S><S sid ="54" ssid = "10">Thus we rank each sense using: parser (Briscoe and Carroll  2002).</S><S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'135'", "'179'", "'111'", "'54'", "'6'"]
'135'
'179'
'111'
'54'
'6'
['135', '179', '111', '54', '6']
parsed_discourse_facet ['method_citation']
<S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="156" ssid = "4">Buitelaar and Sacaleanu (2001) have previously explored ranking and selection of synsets in GermaNet for specific domains using the words in a given synset  and those related by hyponymy  and a term relevance measure taken from information retrieval.</S><S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'156'", "'61'", "'65'", "'142'"]
'137'
'156'
'61'
'65'
'142'
['137', '156', '61', '65', '142']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="71" ssid = "27">This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal:</S><S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'69'", "'44'", "'71'", "'30'"]
'50'
'69'
'44'
'71'
'30'
['50', '69', '44', '71', '30']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="70" ssid = "26">Jiang and Conrath specify a distance measure:   where the third class ( ) is the most informative  or most specific  superordinate synset of the two senses and .</S><S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'70'", "'137'", "'69'", "'44'"]
'50'
'70'
'137'
'69'
'44'
['50', '70', '137', '69', '44']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">SemCor comprises a relatively small sample of 250 000 words.</S><S sid ="161" ssid = "9">Our approach is complementary to this.</S><S sid ="29" ssid = "22">We discuss our method in the following section.</S><S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of “nearest neighbours”.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'161'", "'29'", "'6'", "'34'"]
'16'
'161'
'29'
'6'
'34'
['16', '161', '29', '6', '34']
parsed_discourse_facet ['results_citation']
<S sid ="157" ssid = "5">Buitelaar and Sacaleanu have evaluated their method on identifying domain specific concepts using human judgements on 100 items.</S><S sid ="185" ssid = "8">In the future  we will perform a large scale evaluation on domain specific corpora.</S><S sid ="193" ssid = "2">This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.</S><S sid ="39" ssid = "32">We expect that the quantity and similarity of the neighbours pertaining to different senses will reflect the dominance of the sense to which they pertain.</S><S sid ="128" ssid = "5">The Reuters corpus (Rose et al.  2002) is a collection of about 810 000 Reuters  English Language News stories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'185'", "'193'", "'39'", "'128'"]
'157'
'185'
'193'
'39'
'128'
['157', '185', '193', '39', '128']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "24">We are of course able to apply the method to other versions of WordNet. synset  is incremented with the frequency counts from the corpus of all words belonging to that synset  directly or via the hyponymy relation.</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'50'", "'65'", "'44'", "'82'"]
'68'
'50'
'65'
'44'
'82'
['68', '50', '65', '44', '82']
parsed_discourse_facet ['method_citation']
<S sid ="90" ssid = "19">From manual analysis  there are cases where the acquired first sense disagrees with SemCor  yet is intuitively plausible.</S><S sid ="175" ssid = "23">We are currently investigating the performance of the first sense heuristic  and this method  for other PoS on SENSEVAL-3 data (McCarthy et al.  2004)  although not yet with rankings from domain specific corpora.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="2" ssid = "2">The problem with using the predominant  or first sense heuristic  aside from the fact that it does not take surrounding context into account  is that it assumes some quantity of handtagged data.</S><S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'175'", "'8'", "'2'", "'1'"]
'90'
'175'
'8'
'2'
'1'
['90', '175', '8', '2', '1']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="0" ssid = "0">Finding Predominant Word Senses in Untagged Text</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'10'", "'0'", "'50'", "'30'"]
'69'
'10'
'0'
'50'
'30'
['69', '10', '0', '50', '30']
parsed_discourse_facet ['method_citation']



P04-1036
W08-2107
0
method_citation
['method_citation']
parsing: input/ref/Task1/W99-0623_swastika.csv
<S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['aim_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['result_citation']
<S sid="91" ssid="20">Features and context were initially introduced into the models, but they refused to offer any gains in performance.</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



['139']
139
['139']
parsed_discourse_facet ['result_citation']
<S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['method_citation']
<S sid="103" ssid="32">The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.</S>
original cit marker offset is 0
new cit marker offset is 0



['103']
103
['103']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



['139']
139
['139']
parsed_discourse_facet ['result_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
<S sid="140" ssid="2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
<S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['result_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/W99-0623.csv
<S sid ="142" ssid = "4">Both of the switching techniques  as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.</S><S sid ="141" ssid = "3">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'141'", "'18'", "'81'", "'57'"]
'142'
'141'
'18'
'81'
'57'
['142', '141', '18', '81', '57']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'61'", "'81'", "'78'", "'139'"]
'112'
'61'
'81'
'78'
'139'
['112', '61', '81', '78', '139']
Error in Discourse Facet
<S sid ="62" ssid = "48">This is the parse that is closest to the centroid of the observed parses under the similarity metric.</S><S sid ="88" ssid = "17">For example  one parser could be more accurate at predicting noun phrases than the other parsers.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid ="115" ssid = "44">It is the performance we could achieve if an omniscient observer told us which parser to pick for each of the sentences.</S><S sid ="22" ssid = "8">If enough parsers suggest that a particular constituent belongs in the parse  we include it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'", "'88'", "'27'", "'115'", "'22'"]
'62'
'88'
'27'
'115'
'22'
['62', '88', '27', '115', '22']
Error in Discourse Facet
<S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="125" ssid = "54">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S><S sid ="41" ssid = "27">IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.</S><S sid ="44" ssid = "30">Each of the constituents must have received at least 1 votes from the k parsers  so a > I1 and 2 — 2k±-1 b > ri-5-111.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'21'", "'125'", "'41'", "'44'"]
'38'
'21'
'125'
'41'
'44'
['38', '21', '125', '41', '44']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'81'", "'15'", "'78'", "'139'"]
'112'
'81'
'15'
'78'
'139'
['112', '81', '15', '78', '139']
Error in Discourse Facet
<S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="136" ssid = "65">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S><S sid ="131" ssid = "60">It was then tested on section 22 of the Treebank in conjunction with the other parsers.</S><S sid ="103" ssid = "32">The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.</S><S sid ="132" ssid = "61">The results of this experiment can be seen in Table 5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'136'", "'131'", "'103'", "'132'"]
'21'
'136'
'131'
'103'
'132'
['21', '136', '131', '103', '132']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'78'", "'81'", "'61'", "'130'"]
'112'
'78'
'81'
'61'
'130'
['112', '78', '81', '61', '130']
Error in Discourse Facet
<S sid ="125" ssid = "54">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="1" ssid = "1">Three state-of-the-art statistical parsers are combined to produce more accurate parses  as well as new bounds on achievable Treebank parsing accuracy.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'27'", "'21'", "'1'", "'38'"]
'125'
'27'
'21'
'1'
'38'
['125', '27', '21', '1', '38']
Error in Discourse Facet
<S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="51" ssid = "37">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes  a completely flat structure.</S><S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'12'", "'51'", "'18'", "'57'"]
'55'
'12'
'51'
'18'
'57'
['55', '12', '51', '18', '57']
Error in Discourse Facet
<S sid ="142" ssid = "4">Both of the switching techniques  as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.</S><S sid ="127" ssid = "56">Parser 3  the most accurate parser  was chosen 71% of the time  and Parser 1  the least accurate parser was chosen 16% of the time.</S><S sid ="141" ssid = "3">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="140" ssid = "2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'127'", "'141'", "'12'", "'140'"]
'142'
'127'
'141'
'12'
'140'
['142', '127', '141', '12', '140']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'78'", "'61'", "'130'", "'15'"]
'112'
'78'
'61'
'130'
'15'
['112', '78', '61', '130', '15']
Error in Discourse Facet
<S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="51" ssid = "37">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes  a completely flat structure.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S><S sid ="26" ssid = "12">This technique has the advantage of requiring no training  but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'51'", "'52'", "'50'", "'26'"]
'55'
'51'
'52'
'50'
'26'
['55', '51', '52', '50', '26']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="114" ssid = "43">The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'81'", "'78'", "'61'", "'114'"]
'112'
'81'
'78'
'61'
'114'
['112', '81', '78', '61', '114']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'81'", "'12'", "'15'", "'130'"]
'112'
'81'
'12'
'15'
'130'
['112', '81', '12', '15', '130']
Error in Discourse Facet
<S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="114" ssid = "43">The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.</S><S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S>
original cit marker offset is 0
new cit marker offset is 0



["'78'", "'61'", "'114'", "'112'", "'81'"]
'78'
'61'
'114'
'112'
'81'
['78', '61', '114', '112', '81']
Error in Discourse Facet
<S sid ="129" ssid = "58">In the interest of testing the robustness of these combining techniques  we added a fourth  simple nonlexicalized PCFG parser.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'129'", "'11'", "'130'", "'15'", "'9'"]
'129'
'11'
'130'
'15'
'9'
['129', '11', '130', '15', '9']
Error in Discourse Facet
<S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S><S sid ="114" ssid = "43">The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.</S><S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["'81'", "'57'", "'114'", "'112'", "'78'"]
'81'
'57'
'114'
'112'
'78'
['81', '57', '114', '112', '78']
Error in Discourse Facet
parsing: input/ref/Task1/W06-2932_swastika.csv
<S sid="86" ssid="8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S>
original cit marker offset is 
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="54" ssid="2">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



['54']
54
['54']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



['12']
12
['12']
parsed_discourse_facet ['method_citation']
    <S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



['41']
41
['41']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="6">Its power lies in the ability to define a rich set of features over parsing decisions, as well as surface level features relative to these decisions.</S>
original cit marker offset is 0
new cit marker offset is 0



['24']
24
['24']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



['12']
12
['12']
parsed_discourse_facet ['method_citation']
<S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



['43']
43
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S sid ="3" ssid = "3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S><S sid ="4" ssid = "4">We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.</S><S sid ="108" ssid = "5">Second  we plan on integrating any available morphological features in a more principled manner.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="106" ssid = "3">First  we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is nan
new cit marker offset is 0



["'3'", "'4'", "'108'", "'10'", "'106'"]
'3'
'4'
'108'
'10'
'106'
['3', '4', '108', '10', '106']
parsed_discourse_facet ['results_citation']
<S sid ="101" ssid = "23">For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.</S><S sid ="17" ssid = "13">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S><S sid ="16" ssid = "12">A dependency graph is represented by a set of ordered pairs (i  j) E y in which xj is a dependent and xi is the corresponding head.</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'17'", "'16'", "'2'", "'20'"]
'101'
'17'
'16'
'2'
'20'
['101', '17', '16', '2', '20']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S><S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="83" ssid = "5">In a preliminary test of this hypothesis  we looked at all of the sentences from a development set in which a main verb is incorrectly attached.</S><S sid ="50" ssid = "19">Various conjunctions of these were included based on performance on held-out data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'66'", "'88'", "'83'", "'50'"]
'61'
'66'
'88'
'83'
'50'
['61', '66', '88', '83', '50']
parsed_discourse_facet ['method_citation']
<S sid ="76" ssid = "14">For instance  sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.</S><S sid ="96" ssid = "18">Each stage by itself is relatively accurate (unlabeled accuracy is 79% and labeling accuracy3 is also 79%)  but since there is very little overlap in the kinds of errors each makes  overall labeled accuracy drops to 67%.</S><S sid ="57" ssid = "5">Performance is measured through unlabeled accuracy  which is the percentage of words that modify the correct head in the dependency graph  and labeled accuracy  which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'", "'41'", "'96'", "'57'", "'64'"]
'76'
'41'
'96'
'57'
'64'
['76', '41', '96', '57', '64']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S><S sid ="47" ssid = "16">We used the following: dependent have identical values?</S><S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="17" ssid = "13">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'47'", "'32'", "'36'", "'17'"]
'19'
'47'
'32'
'36'
'17'
['19', '47', '32', '36', '17']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "2">Based on performance from a held-out section of the training data  we used non-projective parsing algorithms for Czech  Danish  Dutch  German  Japanese  Portuguese and Slovene  and projective parsing algorithms for Arabic  Bulgarian  Chinese  Spanish  Swedish and Turkish.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="104" ssid = "1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al.  2005b; McDonald and Pereira  2006) generalizes well to languages other than English.</S><S sid ="21" ssid = "3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al.  2005a; McDonald et al.  2005b) to incorporate features over multiple edges in the dependency graph.</S><S sid ="63" ssid = "1">Our system has several components  including the ability to produce non-projective edges  sequential Japanese  Portuguese  Slovene  Spanish  Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'11'", "'104'", "'21'", "'63'"]
'54'
'11'
'104'
'21'
'63'
['54', '11', '104', '21', '63']
parsed_discourse_facet ['method_citation']
<S sid ="1" ssid = "1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S><S sid ="58" ssid = "6">These results show that the discriminative spanning tree parsing framework (McDonald et al.  2005b; McDonald and Pereira  2006) is easily adapted across all these languages.</S><S sid ="0" ssid = "0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S sid ="63" ssid = "1">Our system has several components  including the ability to produce non-projective edges  sequential Japanese  Portuguese  Slovene  Spanish  Swedish and Turkish.</S><S sid ="108" ssid = "5">Second  we plan on integrating any available morphological features in a more principled manner.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'58'", "'0'", "'63'", "'108'"]
'1'
'58'
'0'
'63'
'108'
['1', '58', '0', '63', '108']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="17" ssid = "13">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S><S sid ="53" ssid = "1">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).</S><S sid ="1" ssid = "1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'20'", "'17'", "'53'", "'1'"]
'0'
'20'
'17'
'53'
'1'
['0', '20', '17', '53', '1']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "16">We used the following: dependent have identical values?</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="19" ssid = "1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'2'", "'19'", "'20'", "'32'"]
'47'
'2'
'19'
'20'
'32'
['47', '2', '19', '20', '32']
parsed_discourse_facet ['method_citation']
<S sid ="81" ssid = "3">Other high-frequency word classes with relatively low attachment accuracy are prepositions (80%)  adverbs (82%) and subordinating conjunctions (80%)  for a total of another 23% of the test corpus.</S><S sid ="23" ssid = "5">That system uses MIRA  an online large-margin learning algorithm  to compute model parameters.</S><S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="43" ssid = "12">For score functions  we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation  we can find the highest scoring label sequence with Viterbi’s algorithm.</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'81'", "'23'", "'86'", "'43'", "'36'"]
'81'
'23'
'86'
'43'
'36'
['81', '23', '86', '43', '36']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216–220  New York City  June 2006. c�2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.</S><S sid ="76" ssid = "14">For instance  sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S><S sid ="79" ssid = "1">Although overall unlabeled accuracy is 86%  most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs  75% for the verb ser  and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'41'", "'76'", "'64'", "'79'"]
'18'
'41'
'76'
'64'
'79'
['18', '41', '76', '64', '79']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216–220  New York City  June 2006. c�2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="3" ssid = "3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S><S sid ="96" ssid = "18">Each stage by itself is relatively accurate (unlabeled accuracy is 79% and labeling accuracy3 is also 79%)  but since there is very little overlap in the kinds of errors each makes  overall labeled accuracy drops to 67%.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="95" ssid = "17">Note the difference in error between the unlabeled parser and the edge labeler: the former makes mistakes on edges into prepositions  conjunctions and verbs  and the latter makes mistakes on edges into nouns (subject/objects).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'3'", "'96'", "'11'", "'95'"]
'18'
'3'
'96'
'11'
'95'
['18', '3', '96', '11', '95']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="16" ssid = "12">A dependency graph is represented by a set of ordered pairs (i  j) E y in which xj is a dependent and xi is the corresponding head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'11'", "'41'", "'36'", "'16'"]
'32'
'11'
'41'
'36'
'16'
['32', '11', '41', '36', '16']
parsed_discourse_facet ['results_citation']



W06-2932
W12-3407
0
method_citation
['method_citation']



W06-2932
D10-1004
0
result_citation
['method_citation']
parsing: input/ref/Task1/J01-2004_sweta.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["372'"]
372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="28">The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.</S>
    <S sid="41" ssid="29">There will also be a brief review of previous work using syntactic information for language modeling, before we introduce our model in Section 4.</S>
    <S sid="42" ssid="30">Three parse trees: (a) a complete parse tree; (b) a complete parse tree with an explicit stop symbol; and (c) a partial parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["40'", "'41'", "'42'"]
40'
'41'
'42'
['40', '41', '42']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="13">A parser that is not left to right, but which has rooted derivations, e.g., a headfirst parser, will be able to calculate generative joint probabilities for entire strings; however, it will not be able to calculate probabilities for each word conditioned on previously generated words, unless each derivation generates the words in the string in exactly the same order.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="364" ssid="120">We follow Chelba (2000) in dealing with this problem: for parsing purposes, we use the Penn Treebank tokenization; for interpolation with the provided trigram model, and for evaluation, the lattice tokenization is used.</S>
original cit marker offset is 0
new cit marker offset is 0



["364'"]
364'
['364']
parsed_discourse_facet ['method_citation']
<S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



["302'"]
302'
['302']
parsed_discourse_facet ['method_citation']
 <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="231" ssid="135">Since we do not know the POS for the word, we must sum the LAP for all POS For a PCFG G, a stack S = Ao An$ (which we will write AN and a look-ahead terminal item wi, we define the look-ahead probability as follows: We recursively estimate this with two empirically observed conditional probabilities for every nonterminal A,: 13(A, w,a) and P(A, c).</S>
original cit marker offset is 0
new cit marker offset is 0



["231'"]
231'
['231']
parsed_discourse_facet ['method_citation']
<S sid="297" ssid="53">The differences between a k-best and a beam-search parser (not to mention the use of dynamic programming) make a running time difference unsurprising.</S>
original cit marker offset is 0
new cit marker offset is 0



["297'"]
297'
['297']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="37">Statistically based heuristic best-first or beam-search strategies (Caraballo and Charniak 1998; Charniak, Goldwater, and Johnson 1998; Goodman 1997) have yielded an enormous improvement in the quality and speed of parsers, even without any guarantee that the parse returned is, in fact, that with the maximum likelihood for the probability model.</S>
original cit marker offset is 0
new cit marker offset is 0



["133'"]
133'
['133']
parsed_discourse_facet ['method_citation']
<S sid="291" ssid="47">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["291'"]
291'
['291']
parsed_discourse_facet ['method_citation']
<S sid="355" ssid="111">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.</S>
original cit marker offset is 0
new cit marker offset is 0



["355'"]
355'
['355']
parsed_discourse_facet ['method_citation']
<S sid="59" ssid="17">A PCFG is a CFG with a probability assigned to each rule; specifically, each righthand side has a probability given the left-hand side of the rule.</S>
original cit marker offset is 0
new cit marker offset is 0



["59'"]
59'
['59']
parsed_discourse_facet ['method_citation']
<S sid="100" ssid="4">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="12">Another approach that uses syntactic structure for language modeling has been to use a shift-reduce parser to &amp;quot;surface&amp;quot; c-commanding phrasal headwords or part-of-speech (POS) tags from arbitrarily far back in the prefix string, for use in a trigram-like model.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="168" ssid = "72">For example  in Figure 2  constituent (NP-DT-NN) is simply NP.</S><S sid ="356" ssid = "112">The DARPA '93 HUB1 test setup consists of 213 utterances read from the Wall Street Journal  a total of 3 446 words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'224'", "'322'", "'358'", "'168'", "'356'"]
'224'
'322'
'358'
'168'
'356'
['224', '322', '358', '168', '356']
parsed_discourse_facet ['results_citation']
<S sid ="17" ssid = "5">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S sid ="102" ssid = "6">One approach to syntactic language modeling is to use this distribution directly as a language model.</S><S sid ="340" ssid = "96">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid ="113" ssid = "17">In empirical trials  Goddeau used the top two stack entries to condition the word probability.</S><S sid ="12" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'102'", "'340'", "'113'", "'12'"]
'17'
'102'
'340'
'113'
'12'
['17', '102', '340', '113', '12']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="95" ssid = "53">For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'95'", "'302'", "'224'", "'104'"]
'134'
'95'
'302'
'224'
'104'
['134', '95', '302', '224', '104']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="167" ssid = "71">In order to avoid any confusions in identifying the nonterminal label of a particular rule production in either its factored or rionfactored version  we introduce the function constituent (A) for every nonterminal in the factored grammar Gf  which is simply the label of the constituent whose factorization results in A.</S><S sid ="112" ssid = "16">Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state  and conditioning on those entries in a way similar to an n-gram.</S><S sid ="85" ssid = "43">The simple definition of c-command that we will be using in this paper is the following: a node A c-commands a node B if and only if (i) A does not dominate B; and (ii) the lowest branching node (i.e.  non-unary node) that dominates A also dominates B.'</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'167'", "'112'", "'85'", "'390'"]
'134'
'167'
'112'
'85'
'390'
['134', '167', '112', '85', '390']
parsed_discourse_facet ['method_citation']
<S sid ="321" ssid = "77">In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.</S><S sid ="95" ssid = "53">For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].</S><S sid ="199" ssid = "103">Table 1 gives a breakdown of the different levels of conditioning information used in the empirical trials  with a mnemonic label that will be used when presenting results.</S><S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="11" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'321'", "'95'", "'199'", "'5'", "'11'"]
'321'
'95'
'199'
'5'
'11'
['321', '95', '199', '5', '11']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="87" ssid = "45">Notice that the subject NP c-commands the object NP  but not vice versa  since the lowest branching node that dominates the object NP is the VP  which does not dominate the subject NP.</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S><S sid ="85" ssid = "43">The simple definition of c-command that we will be using in this paper is the following: a node A c-commands a node B if and only if (i) A does not dominate B; and (ii) the lowest branching node (i.e.  non-unary node) that dominates A also dominates B.'</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'87'", "'302'", "'104'", "'85'"]
'134'
'87'
'302'
'104'
'85'
['134', '87', '302', '104', '85']
parsed_discourse_facet ['method_citation']
<S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="52" ssid = "10">Consider  for example  the parse tree shown in (a) in Figure 1: the start symbol is St  which expands into an S. The S node expands into an NP followed by a VP.</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S>
original cit marker offset is 0
new cit marker offset is 0



["'224'", "'134'", "'52'", "'322'", "'306'"]
'224'
'134'
'52'
'322'
'306'
['224', '134', '52', '322', '306']
parsed_discourse_facet ['method_citation']
<S sid ="112" ssid = "16">Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state  and conditioning on those entries in a way similar to an n-gram.</S><S sid ="129" ssid = "33">The shift-reduce parser will have  perhaps  built the structure shown  and the stack state will have an NP entry with the head &quot;cat&quot; at the top of the stack  and a VBD entry with the head &quot;chased&quot; second on the stack.</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="87" ssid = "45">Notice that the subject NP c-commands the object NP  but not vice versa  since the lowest branching node that dominates the object NP is the VP  which does not dominate the subject NP.</S><S sid ="109" ssid = "13">A shift-reduce parser operates from left to right using a stack and a pointer to the next word in the input string.9 Each stack entry consists minimally of a nonterminal label.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'129'", "'302'", "'87'", "'109'"]
'112'
'129'
'302'
'87'
'109'
['112', '129', '302', '87', '109']
parsed_discourse_facet ['method_citation']
<S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="213" ssid = "117">The parser takes an input string 4  a PCFG G  and a priority queue of candidate analyses.</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'358'", "'306'", "'224'", "'213'", "'322'"]
'358'
'306'
'224'
'213'
'322'
['358', '306', '224', '213', '322']
parsed_discourse_facet ['method_citation']
<S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="59" ssid = "17">A PCFG is a CFG with a probability assigned to each rule; specifically  each righthand side has a probability given the left-hand side of the rule.</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="52" ssid = "10">Consider  for example  the parse tree shown in (a) in Figure 1: the start symbol is St  which expands into an S. The S node expands into an NP followed by a VP.</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'302'", "'59'", "'134'", "'52'", "'104'"]
'302'
'59'
'134'
'52'
'104'
['302', '59', '134', '52', '104']
parsed_discourse_facet ['method_citation']
<S sid ="210" ssid = "114">It uses a PCFG with a conditional probability model of the sort defined in the previous section.</S><S sid ="142" ssid = "46">A simple PCFG conditions rule probabilities on the left-hand side of the rule.</S><S sid ="323" ssid = "79">They trained and tested the SLM on a modified  more &quot;speech-like&quot; version of the Penn Treebank.</S><S sid ="0" ssid = "0">Probabilistic Top-Down Parsing and Language Modeling</S><S sid ="370" ssid = "126">We followed Chelba (2000) in using an LM weight of 16 for the lattice trigram.</S>
original cit marker offset is 0
new cit marker offset is 0



["'210'", "'142'", "'323'", "'0'", "'370'"]
'210'
'142'
'323'
'0'
'370'
['210', '142', '323', '0', '370']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S><S sid ="154" ssid = "58">We use the relative frequency estimator  and smooth our production probabilities by interpolating the relative frequency estimates with those obtained by &quot;annotating&quot; less contextual information.</S><S sid ="0" ssid = "0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'224'", "'104'", "'154'", "'0'"]
'134'
'224'
'104'
'154'
'0'
['134', '224', '104', '154', '0']
parsed_discourse_facet ['method_citation']
<S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="95" ssid = "53">For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].</S><S sid ="124" ssid = "28">Each distinct derivation path within the beam has a probability and a stack state associated with it.</S><S sid ="147" ssid = "51">One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).</S><S sid ="168" ssid = "72">For example  in Figure 2  constituent (NP-DT-NN) is simply NP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'302'", "'95'", "'124'", "'147'", "'168'"]
'302'
'95'
'124'
'147'
'168'
['302', '95', '124', '147', '168']
parsed_discourse_facet ['results_citation']
<S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S>
original cit marker offset is 0
new cit marker offset is 0



["'358'", "'306'", "'322'", "'224'", "'134'"]
'358'
'306'
'322'
'224'
'134'
['358', '306', '322', '224', '134']
parsed_discourse_facet ['method_citation']
<S sid ="289" ssid = "45">These results  achieved using very straightforward conditioning events and considering only the left context  are within one to four points of the best published Observed running time on Section 23 of the Penn Treebank  with the full conditional probability model and beam of 10-11  using one 300 MHz UltraSPARC processor and 256MB of RAM of a Sun Enterprise 450. accuracies cited above.'</S><S sid ="25" ssid = "13">A parser that is not left to right  but which has rooted derivations  e.g.  a headfirst parser  will be able to calculate generative joint probabilities for entire strings; however  it will not be able to calculate probabilities for each word conditioned on previously generated words  unless each derivation generates the words in the string in exactly the same order.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="141" ssid = "45">We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.</S><S sid ="22" ssid = "10">A left-toright parser whose derivations are not rooted  i.e.  with derivations that can consist of disconnected tree fragments  such as an LR or shift-reduce parser  cannot incrementally calculate the probability of each prefix string being generated by the probabilistic grammar  because their derivations include probability mass from unrooted structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'289'", "'25'", "'390'", "'141'", "'22'"]
'289'
'25'
'390'
'141'
'22'
['289', '25', '390', '141', '22']
parsed_discourse_facet ['method_citation']
<S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'224'", "'306'", "'358'", "'134'", "'322'"]
'224'
'306'
'358'
'134'
'322'
['224', '306', '358', '134', '322']
parsed_discourse_facet ['method_citation']
<S sid ="147" ssid = "51">One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="278" ssid = "34">Section 22 (41 817 words  1 700 sentences) served as the development corpus  on which the parser was tested until stable versions were ready to run on the test data  to avoid developing the parser to fit the specific test data.</S><S sid ="141" ssid = "45">We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'302'", "'278'", "'141'", "'390'"]
'147'
'302'
'278'
'141'
'390'
['147', '302', '278', '141', '390']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P05-1013_vardha.csv
 <S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
  <S sid="9" ssid="5">This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
  <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
    <S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
 <S sid="62" ssid="1">In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al., 2004) and English (Nivre and Scholz, 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="23" ssid="19">By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
    <S sid="96" ssid="7">With respect to exact match, the improvement is even more noticeable, which shows quite clearly that even if non-projective dependencies are rare on the token level, they are nevertheless important for getting the global syntactic structure correct.</S>
original cit marker offset is 0
new cit marker offset is 0



["'96'"]
'96'
['96']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
  <S sid="40" ssid="11">Even this may be nondeterministic, in case the graph contains several non-projective arcs whose lifts interact, but we use the following algorithm to construct a minimal projective transformation D0 = (W, A0) of a (nonprojective) dependency graph D = (W, A): The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right).</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
  <S sid="7" ssid="3">From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="14" ssid="10">While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
 <S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'"]
'49'
['49']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P05-1013.csv
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'48'", "'21'", "'36'"]
'50'
'75'
'48'
'21'
'36'
['50', '75', '48', '21', '36']
parsed_discourse_facet ['results_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="62" ssid = "1">In the experiments below  we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs 3 previously tested on Swedish (Nivre et al.  2004) and English (Nivre and Scholz  2004).</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="91" ssid = "2">Table 5 shows the overall parsing accuracy attained with the three different encoding schemes  compared to the baseline (no special arc labels) and to training directly on non-projective dependency graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'75'", "'62'", "'21'", "'91'"]
'48'
'75'
'62'
'21'
'91'
['48', '75', '62', '21', '91']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="77" ssid = "4">The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'77'", "'48'", "'21'"]
'50'
'75'
'77'
'48'
'21'
['50', '75', '77', '48', '21']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'71'", "'21'", "'45'"]
'50'
'75'
'71'
'21'
'45'
['50', '75', '71', '21', '45']
parsed_discourse_facet ['method_citation']
<S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'50'", "'48'", "'45'", "'21'"]
'75'
'50'
'48'
'45'
'21'
['75', '50', '48', '45', '21']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p↓.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'51'", "'36'", "'21'", "'48'"]
'50'
'51'
'36'
'21'
'48'
['50', '51', '36', '21', '48']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'48'", "'21'", "'36'", "'75'"]
'50'
'48'
'21'
'36'
'75'
['50', '48', '21', '36', '75']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p↓.</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'21'", "'51'", "'48'", "'75'"]
'50'
'21'
'51'
'48'
'75'
['50', '21', '51', '48', '75']
parsed_discourse_facet ['method_citation']
<S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p↓.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="23" ssid = "19">By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'", "'51'", "'21'", "'48'", "'23'"]
'36'
'51'
'21'
'48'
'23'
['36', '51', '21', '48', '23']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'48'", "'21'", "'45'"]
'50'
'75'
'48'
'21'
'45'
['50', '75', '48', '21', '45']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="83" ssid = "10">It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="6" ssid = "2">However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.</S><S sid ="35" ssid = "6">The dependency graph in Figure 1 satisfies all the defining conditions above  but it fails to satisfy the condition ofprojectivity (Kahane et al.  1998): The arc connecting the head jedna (one) to the dependent Z (out-of) spans the token je (is)  which is not dominated by jedna.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'83'", "'7'", "'6'", "'35'"]
'2'
'83'
'7'
'6'
'35'
['2', '83', '7', '6', '35']
parsed_discourse_facet ['method_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'75'", "'45'", "'21'", "'20'"]
'48'
'75'
'45'
'21'
'20'
['48', '75', '45', '21', '20']
parsed_discourse_facet ['method_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S><S sid ="73" ssid = "12">More details on the memory-based prediction can be found in Nivre et al. (2004) and Nivre and Scholz (2004).</S><S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'45'", "'73'", "'20'", "'106'"]
'48'
'45'
'73'
'20'
'106'
['48', '45', '73', '20', '106']
parsed_discourse_facet ['results_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="52" ssid = "23">Thus  the arc from je to jedna will be labeled 5b↓ (to indicate that there is a syntactic head below it).</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'48'", "'75'", "'36'", "'52'"]
'50'
'48'
'75'
'36'
'52'
['50', '48', '75', '36', '52']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="84" ssid = "11">In the second part of the experiment  we applied the inverse transformation based on breadth-first search under the three different encoding schemes.</S><S sid ="79" ssid = "6">In the first part of the experiment  dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'48'", "'21'", "'84'", "'79'"]
'50'
'48'
'21'
'84'
'79'
['50', '48', '21', '84', '79']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "8">For each token  three types of features may be taken into account: the word form; the part-of-speech assigned by an automatic tagger; and labels on previously assigned dependency arcs involving the token – the arc from its head and the arcs to its leftmost and rightmost dependent  respectively.</S><S sid ="23" ssid = "19">By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S><S sid ="64" ssid = "3">At each point during the derivation  the parser has a choice between pushing the next input token onto the stack – with or without adding an arc from the token on top of the stack to the token pushed – and popping a token from the stack – with or without adding an arc from the next input token to the token popped.</S><S sid ="59" ssid = "30">The details of the transformation procedure are slightly different depending on the encoding schemes: d↑h let the linear head be the syntactic head). target arc must have the form wl −→ wm; if no target arc is found  Head is used as backoff. must have the form wl −→ wm and no outgoing arcs of the form wm p'↓ −→ wo; no backoff.</S><S sid ="46" ssid = "17">In principle  it would be possible to encode the exact position of the syntactic head in the label of the arc from the linear head  but this would give a potentially infinite set of arc labels and would make the training of the parser very hard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'23'", "'64'", "'59'", "'46'"]
'69'
'23'
'64'
'59'
'46'
['69', '23', '64', '59', '46']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'71'", "'21'", "'45'"]
'50'
'75'
'71'
'21'
'45'
['50', '75', '71', '21', '45']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "18">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 15–25%  the total proportion of non-projective arcs is normally only 1–2%.</S><S sid ="101" ssid = "12">However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.</S><S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S><S sid ="87" ssid = "14">However  it can be noted that the results for the least informative encoding  Path  are almost comparable  while the third encoding  Head  gives substantially worse results for both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'14'", "'101'", "'89'", "'87'"]
'107'
'14'
'101'
'89'
'87'
['107', '14', '101', '89', '87']
parsed_discourse_facet ['method_citation']
<S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 15–25%  the total proportion of non-projective arcs is normally only 1–2%.</S><S sid ="88" ssid = "15">We also see that the increase in the size of the label sets for Head and Head+Path is far below the theoretical upper bounds given in Table 1.</S><S sid ="28" ssid = "24">First  in section 4  we evaluate the graph transformation techniques in themselves  with data from the Prague Dependency Treebank and the Danish Dependency Treebank.</S><S sid ="103" ssid = "14">On the other hand  given that all schemes have similar parsing accuracy overall  this means that the Path scheme is the least likely to introduce errors on projective arcs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'14'", "'88'", "'28'", "'103'"]
'89'
'14'
'88'
'28'
'103'
['89', '14', '88', '28', '103']
parsed_discourse_facet ['aim_citation', 'results_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'50'", "'75'", "'21'", "'36'"]
'48'
'50'
'75'
'21'
'36'
['48', '50', '75', '21', '36']
parsed_discourse_facet ['method_citation']



P05-1013
P11-2121
0
method_citation
['method_citation']
parsing: input/ref/Task1/W99-0613_aakansha.csv
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="30">Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'"]
'137'
['137']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="12">2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="4">The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="12">But we will show that the use of unlabeled data can drastically reduce the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
    <S sid="237" ssid="4">The numbers falling into the location, person, organization categories were 186, 289 and 402 respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'", "'237'"]
'236'
'237'
['236', '237']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
    <S sid="10" ssid="4">The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'10'"]
'9'
'10'
['9', '10']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S sid="139" ssid="6">This section describes AdaBoost, which is the basis for the CoBoost algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'39'"]
'137'
'39'
['137', '39']
parsed_discourse_facet ['method_citation']
<S sid="26" ssid="20">We present two algorithms.</S>
    <S sid="27" ssid="21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'27'"]
'26'
'27'
['26', '27']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="12">But we will show that the use of unlabeled data can drastically reduce the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="2">Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S>
    <S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'9'"]
'8'
'9'
['8', '9']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'228'", "'0'", "'75'", "'77'"]
'142'
'228'
'0'
'75'
'77'
['142', '228', '0', '75', '77']
parsed_discourse_facet ['results_citation']
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="102" ssid = "35">(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'77'", "'102'", "'52'", "'75'"]
'142'
'77'
'102'
'52'
'75'
['142', '77', '102', '52', '75']
parsed_discourse_facet ['method_citation']
<S sid ="225" ssid = "4">We again assume a training set of n examples {x1 . xri} where the first m examples have labels {y1 ... yin}  and the last (n — m) examples are unlabeled.</S><S sid ="203" ssid = "70">Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).</S><S sid ="140" ssid = "7">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S><S sid ="79" ssid = "12">2 We now introduce a new algorithm for learning from unlabeled examples  which we will call DLCoTrain (DL stands for decision list  the term Cotrain is taken from (Blum and Mitchell 98)).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'225'", "'203'", "'140'", "'79'", "'31'"]
'225'
'203'
'140'
'79'
'31'
['225', '203', '140', '79', '31']
parsed_discourse_facet ['method_citation']
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="102" ssid = "35">(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'102'", "'77'", "'52'", "'228'"]
'142'
'102'
'77'
'52'
'228'
['142', '102', '77', '52', '228']
parsed_discourse_facet ['method_citation']
<S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="207" ssid = "74">Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S><S sid ="166" ssid = "33">The first m pairs have labels yi  whereas for i = m + 1    n the pairs are unlabeled.</S><S sid ="84" ssid = "17">For each label (Per s on  organization and Location)  take the n contextual rules with the highest value of Count' (x) whose unsmoothed3 strength is above some threshold pmin.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'", "'207'", "'248'", "'166'", "'84'"]
'236'
'207'
'248'
'166'
'84'
['236', '207', '248', '166', '84']
parsed_discourse_facet ['method_citation']
<S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'228'", "'142'", "'75'", "'77'", "'0'"]
'228'
'142'
'75'
'77'
'0'
['228', '142', '75', '77', '0']
parsed_discourse_facet ['method_citation']
<S sid ="207" ssid = "74">Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S><S sid ="86" ssid = "19">Thus at each iteration the method induces at most n x k rules  where k is the number of possible labels (k = 3 in the experiments in this paper). step 3.</S><S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="10" ssid = "4">The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'248'", "'86'", "'204'", "'10'"]
'207'
'248'
'86'
'204'
'10'
['207', '248', '86', '204', '10']
parsed_discourse_facet ['method_citation']
<S sid ="202" ssid = "69">The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels  and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.</S><S sid ="124" ssid = "57">In particular  it may not be possible to learn functions fi (x f2(x2 t) for i = m + 1...n: either because there is some noise in the data  or because it is just not realistic to expect to learn perfect classifiers given the features used for representation.</S><S sid ="121" ssid = "54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).</S><S sid ="50" ssid = "4">It is a sequence of proper nouns within an NP; its last word Cooper is the head of the NP; and the NP has an appositive modifier (a vice president at S.&P.) whose head is a singular noun (president).</S><S sid ="25" ssid = "19">The unlabeled data gives many such &quot;hints&quot; that two features should predict the same label  and these hints turn out to be surprisingly useful when building a classifier.</S>
original cit marker offset is 0
new cit marker offset is 0



["'202'", "'124'", "'121'", "'50'", "'25'"]
'202'
'124'
'121'
'50'
'25'
['202', '124', '121', '50', '25']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="12" ssid = "6">The approach uses both spelling and contextual rules.</S><S sid ="150" ssid = "17">Pseudo-code describing the generalized boosting algorithm of Schapire and Singer is given in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'52'", "'75'", "'12'", "'150'"]
'157'
'52'
'75'
'12'
'150'
['157', '52', '75', '12', '150']
parsed_discourse_facet ['method_citation']
<S sid ="63" ssid = "17">In this case nonalpha is the string formed by removing all upper/lower case letters from the spelling (e.g.  for Thomas E. Petry nonalpha= .</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="47" ssid = "1">971 746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'52'", "'157'", "'77'", "'47'"]
'63'
'52'
'157'
'77'
'47'
['63', '52', '157', '77', '47']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="239" ssid = "6">Of these cases  38 were temporal expressions (either a day of the week or month of the year).</S><S sid ="93" ssid = "26">To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.</S><S sid ="249" ssid = "16">The test accuracy more or less asymptotes.</S><S sid ="197" ssid = "64">Note that in our formalism a weakhypothesis can abstain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'239'", "'93'", "'249'", "'197'"]
'204'
'239'
'93'
'249'
'197'
['204', '239', '93', '249', '197']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S><S sid ="93" ssid = "26">To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'75'", "'228'", "'0'", "'93'"]
'204'
'75'
'228'
'0'
'93'
['204', '75', '228', '0', '93']
parsed_discourse_facet ['method_citation']
<S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="193" ssid = "60">In Input: {(x1 i  Initialize: Vi  j : e(xi) = 0.</S><S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'", "'157'", "'77'", "'193'", "'142'"]
'52'
'157'
'77'
'193'
'142'
['52', '157', '77', '193', '142']
parsed_discourse_facet ['results_citation']
parsing: input/ref/Task1/P08-1043_swastika.csv
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['result_citation']
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="37">3An English sentence with ambiguous PoS assignment can be trivially represented as a lattice similar to our own, where every pair of consecutive nodes correspond to a word, and every possible PoS assignment for this word is a connecting arc.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="163" ssid="1">The accuracy results for segmentation, tagging and parsing using our different models and our standard data split are summarized in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



['163']
163
['163']
parsed_discourse_facet ['result_citation']
    <S sid="100" ssid="32">This means that the rules in our grammar are of two kinds: (a) syntactic rules relating nonterminals to a sequence of non-terminals and/or PoS tags, and (b) lexical rules relating PoS tags to lattice arcs (lexemes).</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['method_citation']
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['result_citation']
<S sid="188" ssid="2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



['188']
188
['188']
parsed_discourse_facet ['result_citation']
<S sid="86" ssid="18">A morphological analyzer M : W&#8212;* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S>
original cit marker offset is 0
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['result_citation']
<S sid="97" ssid="29">Thus our proposed model is a proper model assigning probability mass to all (7r, L) pairs, where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S>
original cit marker offset is 0
new cit marker offset is 0



['97']
97
['97']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="31" ssid = "10">Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'92'", "'86'", "'192'", "'31'"]
'183'
'92'
'86'
'192'
'31'
['183', '92', '86', '192', '31']
parsed_discourse_facet ['results_citation']
<S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="173" ssid = "11">The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter (α) which is tuned separately for each of the tasks.</S><S sid ="77" ssid = "9">Segments with the same surface form but different PoS tags are treated as different lexemes  and are represented as separate arcs (e.g. the two arcs labeled neim from node 6 to 7).</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'192'", "'173'", "'77'", "'133'"]
'183'
'192'
'173'
'77'
'133'
['183', '192', '173', '77', '133']
parsed_discourse_facet ['method_citation']
<S sid ="141" ssid = "19">This analyzer setting is similar to that of (Cohen and Smith  2007)  and models using it are denoted nohsp  Parser and Grammar We used BitPar (Schmid  2004)  an efficient general purpose parser 10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.</S><S sid ="89" ssid = "21">Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree π s.t.</S><S sid ="51" ssid = "9">Tsarfaty (2006) used a morphological analyzer (Segal  2000)  a PoS tagger (Bar-Haim et al.  2005)  and a general purpose parser (Schmid  2000) in an integrated framework in which morphological and syntactic components interact to share information  leading to improved performance on the joint task.</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="82" ssid = "14">In sequential tagging models such as (Adler and Elhadad  2006; Bar-Haim et al.  2007; Smith et al.  2005) weights are assigned according to a language model The input for the joint task is a sequence W = w1  ...   wn of space-delimited tokens.</S>
original cit marker offset is 0
new cit marker offset is 0



["'141'", "'89'", "'51'", "'156'", "'82'"]
'141'
'89'
'51'
'156'
'82'
['141', '89', '51', '156', '82']
parsed_discourse_facet ['method_citation']
<S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'183'", "'86'", "'0'", "'92'"]
'192'
'183'
'86'
'0'
'92'
['192', '183', '86', '0', '92']
parsed_discourse_facet ['method_citation']
<S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'183'", "'0'", "'86'", "'92'"]
'192'
'183'
'0'
'86'
'92'
['192', '183', '0', '86', '92']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="89" ssid = "21">Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree π s.t.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'86'", "'192'", "'89'", "'92'"]
'183'
'86'
'192'
'89'
'92'
['183', '86', '192', '89', '92']
parsed_discourse_facet ['method_citation']
<S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'86'", "'57'", "'192'", "'183'"]
'92'
'86'
'57'
'192'
'183'
['92', '86', '57', '192', '183']
parsed_discourse_facet ['method_citation']
<S sid ="31" ssid = "10">Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="186" ssid = "24">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'", "'92'", "'133'", "'0'", "'186'"]
'31'
'92'
'133'
'0'
'186'
['31', '92', '133', '0', '186']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'92'", "'192'", "'183'", "'133'"]
'86'
'92'
'192'
'183'
'133'
['86', '92', '192', '183', '133']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'192'", "'183'", "'0'", "'92'"]
'86'
'192'
'183'
'0'
'92'
['86', '192', '183', '0', '92']
parsed_discourse_facet ['method_citation']
<S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="123" ssid = "1">Previous work on morphological and syntactic disambiguation in Hebrew used different sets of data  different splits  differing annotation schemes  and different evaluation measures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'57'", "'192'", "'86'", "'123'"]
'92'
'57'
'192'
'86'
'123'
['92', '57', '192', '86', '123']
parsed_discourse_facet ['method_citation']
<S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="69" ssid = "1">We represent all morphological analyses of a given utterance using a lattice structure.</S><S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="113" ssid = "45">The remaining arcs are marked OOV.</S><S sid ="50" ssid = "8">The joint morphological and syntactic hypothesis was first discussed in (Tsarfaty  2006; Tsarfaty and Sima’an  2004) and empirically explored in (Tsarfaty  2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'", "'69'", "'14'", "'113'", "'50'"]
'57'
'69'
'14'
'113'
'50'
['57', '69', '14', '113', '50']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'92'", "'192'", "'183'", "'57'"]
'86'
'92'
'192'
'183'
'57'
['86', '92', '192', '183', '57']
parsed_discourse_facet ['results_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'192'", "'0'", "'92'", "'183'"]
'86'
'192'
'0'
'92'
'183'
['86', '192', '0', '92', '183']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'183'", "'192'", "'92'", "'0'"]
'86'
'183'
'192'
'92'
'0'
['86', '183', '192', '92', '0']
parsed_discourse_facet ['method_citation']
<S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S><S sid ="18" ssid = "14">Cohen and Smith (2007) followed up on these results and proposed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'92'", "'183'", "'133'", "'18'"]
'192'
'92'
'183'
'133'
'18'
['192', '92', '183', '133', '18']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W11-2123_vardha.csv
    <S sid="12" ssid="7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
 <S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
 <S sid="131" ssid="3">Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N &#8722; 1 preceding words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'"]
'131'
['131']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="16">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="16">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="12">Compared with SRILM, IRSTLM adds several features: lower memory consumption, a binary file format with memory mapping, caching to increase speed, and quantization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'"]
'108'
['108']
parsed_discourse_facet ['method_citation']
    <S sid="129" ssid="1">In addition to the optimizations specific to each datastructure described in Section 2, we implement several general optimizations for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'129'"]
'129'
['129']
parsed_discourse_facet ['method_citation']
    <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
    <S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
    <S sid="263" ssid="5">Quantization can be improved by jointly encoding probability and backoff.</S>
original cit marker offset is 0
new cit marker offset is 0



["'263'"]
'263'
['263']
parsed_discourse_facet ['method_citation']
    <S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
    <S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="17">If the context wnf will never extend to the right (i.e. wnf v is not present in the model for all words v) then no subsequent query will match the full context.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'"]
'145'
['145']
parsed_discourse_facet ['method_citation']
 <S sid="182" ssid="1">This section measures performance on shared tasks in order of increasing complexity: sparse lookups, evaluating perplexity of a large file, and translation with Moses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'182'"]
'182'
['182']
parsed_discourse_facet ['method_citation']
    <S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
    <S sid="12" ssid="7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
    <S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="12">We have modified Moses (Koehn et al., 2007) to keep our state with hypotheses; to conserve memory, phrases do not keep state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
 <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
 <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W11-2123.csv
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="27" ssid = "5">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'183'", "'69'", "'27'", "'231'"]
'0'
'183'
'69'
'27'
'231'
['0', '183', '69', '27', '231']
parsed_discourse_facet ['results_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="149" ssid = "21">RandLM and SRILM also remove context that will not extend  but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.</S><S sid ="193" ssid = "12">It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'183'", "'69'", "'149'", "'193'"]
'0'
'183'
'69'
'149'
'193'
['0', '183', '69', '149', '193']
parsed_discourse_facet ['method_citation']
<S sid ="62" ssid = "40">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S><S sid ="142" ssid = "14">Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn−1 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.</S><S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="216" ssid = "35">Though we are not able to calculate their memory usage on our model  results reported in their paper suggest lower memory consumption than TRIE on large-scale models  at the expense of CPU time.</S><S sid ="50" ssid = "28">Given counts cn1 where e.g. c1 is the vocabulary size  total memory consumption  in bits  is Our PROBING data structure places all n-grams of the same order into a single giant hash table.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'", "'142'", "'3'", "'216'", "'50'"]
'62'
'142'
'3'
'216'
'50'
['62', '142', '3', '216', '50']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="137" ssid = "9">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="157" ssid = "29">Each visited entry wni stores backoff b(wni ).</S><S sid ="27" ssid = "5">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'137'", "'183'", "'157'", "'27'"]
'0'
'137'
'183'
'157'
'27'
['0', '137', '183', '157', '27']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="233" ssid = "52">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid ="193" ssid = "12">It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'3'", "'233'", "'193'", "'69'"]
'183'
'3'
'233'
'193'
'69'
['183', '3', '233', '193', '69']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="38" ssid = "16">The ratio of buckets to entries is controlled by space multiplier m > 1.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="71" ssid = "49">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'0'", "'38'", "'88'", "'71'"]
'183'
'0'
'38'
'88'
'71'
['183', '0', '38', '88', '71']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="71" ssid = "49">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'0'", "'14'", "'71'", "'88'"]
'69'
'0'
'14'
'71'
'88'
['69', '0', '14', '71', '88']
parsed_discourse_facet ['method_citation']
<S sid ="124" ssid = "28">Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="227" ssid = "46">The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.</S><S sid ="21" ssid = "16">Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'69'", "'227'", "'21'", "'13'"]
'124'
'69'
'227'
'21'
'13'
['124', '69', '227', '21', '13']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="137" ssid = "9">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid ="116" ssid = "20">Both implementations employ a state object  opaque to the application  that carries information from one query to the next; we discuss both further in Section 4.2.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'0'", "'137'", "'116'", "'231'"]
'183'
'0'
'137'
'116'
'231'
['183', '0', '137', '116', '231']
parsed_discourse_facet ['method_citation']
<S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="118" ssid = "22">The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'183'", "'69'", "'118'", "'14'"]
'88'
'183'
'69'
'118'
'14'
['88', '183', '69', '118', '14']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="227" ssid = "46">The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.</S><S sid ="124" ssid = "28">Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'231'", "'183'", "'227'", "'124'"]
'69'
'231'
'183'
'227'
'124'
['69', '231', '183', '227', '124']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "29">Each visited entry wni stores backoff b(wni ).</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="53" ssid = "31">Sorted arrays store key-value pairs in an array sorted by key  incurring no space overhead.</S><S sid ="27" ssid = "5">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S><S sid ="118" ssid = "22">The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'0'", "'53'", "'27'", "'118'"]
'157'
'0'
'53'
'27'
'118'
['157', '0', '53', '27', '118']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="195" ssid = "14">Interpolation search has a more expensive pivot but performs less pivoting and reads  so it is slow on small data and faster on large data.</S><S sid ="116" ssid = "20">Both implementations employ a state object  opaque to the application  that carries information from one query to the next; we discuss both further in Section 4.2.</S><S sid ="137" ssid = "9">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid ="62" ssid = "40">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'195'", "'116'", "'137'", "'62'"]
'183'
'195'
'116'
'137'
'62'
['183', '195', '116', '137', '62']
parsed_discourse_facet ['results_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S><S sid ="233" ssid = "52">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid ="193" ssid = "12">It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'69'", "'231'", "'233'", "'193'"]
'183'
'69'
'231'
'233'
'193'
['183', '69', '231', '233', '193']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="201" ssid = "20">Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'69'", "'88'", "'183'", "'201'"]
'0'
'69'
'88'
'183'
'201'
['0', '69', '88', '183', '201']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="62" ssid = "40">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S><S sid ="102" ssid = "6">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie)  allocating memory all at once (eliminating the need for full pointers)  and being easy to compile.</S><S sid ="179" ssid = "51">We do not experiment with models larger than physical memory in this paper because TPT is unreleased  factors such as disk speed are hard to replicate  and in such situations we recommend switching to a more compact representation  such as RandLM.</S><S sid ="142" ssid = "14">Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn−1 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'62'", "'102'", "'179'", "'142'"]
'3'
'62'
'102'
'179'
'142'
['3', '62', '102', '179', '142']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="2" ssid = "2">The structure uses linear probing hash tables and is designed for speed.</S><S sid ="129" ssid = "1">In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.</S><S sid ="71" ssid = "49">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S><S sid ="201" ssid = "20">Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'2'", "'129'", "'71'", "'201'"]
'69'
'2'
'129'
'71'
'201'
['69', '2', '129', '71', '201']
parsed_discourse_facet ['method_citation']
<S sid ="199" ssid = "18">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid ="205" ssid = "24">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al.  2009).</S><S sid ="182" ssid = "1">This section measures performance on shared tasks in order of increasing complexity: sparse lookups  evaluating perplexity of a large file  and translation with Moses.</S><S sid ="67" ssid = "45">While sorted arrays could be used to implement the same data structure as PROBING  effectively making m = 1  we abandoned this implementation because it is slower and larger than a trie implementation.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'", "'205'", "'182'", "'67'", "'45'"]
'199'
'205'
'182'
'67'
'45'
['199', '205', '182', '67', '45']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="149" ssid = "21">RandLM and SRILM also remove context that will not extend  but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.</S><S sid ="201" ssid = "20">Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'183'", "'69'", "'149'", "'201'"]
'0'
'183'
'69'
'149'
'201'
['0', '183', '69', '149', '201']
parsed_discourse_facet ['aim_citation', 'results_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="118" ssid = "22">The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'0'", "'69'", "'88'", "'118'"]
'183'
'0'
'69'
'88'
'118'
['183', '0', '69', '88', '118']
parsed_discourse_facet ['method_citation']





input/ref/Task1/W99-0613_swastika.csv
input/res/Task1/W99-0613.csv
parsing: input/ref/Task1/W99-0613_swastika.csv
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
<S sid="159" ssid="26">To prevent this we &amp;quot;smooth&amp;quot; the confidence by adding a small value, e, to both W+ and W_, giving at = Plugging the value of at from Equ.</S>
original cit marker offset is 0
new cit marker offset is 0



['159']
159
['159']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S>
original cit marker offset is 0
new cit marker offset is 0



['137']
137
['137']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['method_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
 <S sid="250" ssid="1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S>
original cit marker offset is 0
new cit marker offset is 0



['250']
250
['250']
parsed_discourse_facet ['result_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
    <S sid="36" ssid="30">Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.</S>
original cit marker offset is 0
new cit marker offset is 0



['36']
36
['36']
parsed_discourse_facet ['result_citation']
<S sid="29" ssid="23">Unfortunately, Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective function.</S>
original cit marker offset is 0
new cit marker offset is 0



['29']
29
['29']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
    <S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['method_citation']
    <S sid="95" ssid="28">(Specifically, the limit n starts at 5 and increases by 5 at each iteration.)</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['method_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'228'", "'0'", "'75'", "'77'"]
'142'
'228'
'0'
'75'
'77'
['142', '228', '0', '75', '77']
parsed_discourse_facet ['results_citation']
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="102" ssid = "35">(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'77'", "'102'", "'52'", "'75'"]
'142'
'77'
'102'
'52'
'75'
['142', '77', '102', '52', '75']
parsed_discourse_facet ['method_citation']
<S sid ="225" ssid = "4">We again assume a training set of n examples {x1 . xri} where the first m examples have labels {y1 ... yin}  and the last (n — m) examples are unlabeled.</S><S sid ="203" ssid = "70">Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).</S><S sid ="140" ssid = "7">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S><S sid ="79" ssid = "12">2 We now introduce a new algorithm for learning from unlabeled examples  which we will call DLCoTrain (DL stands for decision list  the term Cotrain is taken from (Blum and Mitchell 98)).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'225'", "'203'", "'140'", "'79'", "'31'"]
'225'
'203'
'140'
'79'
'31'
['225', '203', '140', '79', '31']
parsed_discourse_facet ['method_citation']
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="102" ssid = "35">(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'102'", "'77'", "'52'", "'228'"]
'142'
'102'
'77'
'52'
'228'
['142', '102', '77', '52', '228']
parsed_discourse_facet ['method_citation']
<S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="207" ssid = "74">Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S><S sid ="166" ssid = "33">The first m pairs have labels yi  whereas for i = m + 1    n the pairs are unlabeled.</S><S sid ="84" ssid = "17">For each label (Per s on  organization and Location)  take the n contextual rules with the highest value of Count' (x) whose unsmoothed3 strength is above some threshold pmin.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'", "'207'", "'248'", "'166'", "'84'"]
'236'
'207'
'248'
'166'
'84'
['236', '207', '248', '166', '84']
parsed_discourse_facet ['method_citation']
<S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'228'", "'142'", "'75'", "'77'", "'0'"]
'228'
'142'
'75'
'77'
'0'
['228', '142', '75', '77', '0']
parsed_discourse_facet ['method_citation']
<S sid ="207" ssid = "74">Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S><S sid ="86" ssid = "19">Thus at each iteration the method induces at most n x k rules  where k is the number of possible labels (k = 3 in the experiments in this paper). step 3.</S><S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="10" ssid = "4">The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'248'", "'86'", "'204'", "'10'"]
'207'
'248'
'86'
'204'
'10'
['207', '248', '86', '204', '10']
parsed_discourse_facet ['method_citation']
<S sid ="202" ssid = "69">The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels  and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.</S><S sid ="124" ssid = "57">In particular  it may not be possible to learn functions fi (x f2(x2 t) for i = m + 1...n: either because there is some noise in the data  or because it is just not realistic to expect to learn perfect classifiers given the features used for representation.</S><S sid ="121" ssid = "54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).</S><S sid ="50" ssid = "4">It is a sequence of proper nouns within an NP; its last word Cooper is the head of the NP; and the NP has an appositive modifier (a vice president at S.&P.) whose head is a singular noun (president).</S><S sid ="25" ssid = "19">The unlabeled data gives many such &quot;hints&quot; that two features should predict the same label  and these hints turn out to be surprisingly useful when building a classifier.</S>
original cit marker offset is 0
new cit marker offset is 0



["'202'", "'124'", "'121'", "'50'", "'25'"]
'202'
'124'
'121'
'50'
'25'
['202', '124', '121', '50', '25']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="12" ssid = "6">The approach uses both spelling and contextual rules.</S><S sid ="150" ssid = "17">Pseudo-code describing the generalized boosting algorithm of Schapire and Singer is given in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'52'", "'75'", "'12'", "'150'"]
'157'
'52'
'75'
'12'
'150'
['157', '52', '75', '12', '150']
parsed_discourse_facet ['method_citation']
<S sid ="63" ssid = "17">In this case nonalpha is the string formed by removing all upper/lower case letters from the spelling (e.g.  for Thomas E. Petry nonalpha= .</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="47" ssid = "1">971 746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'52'", "'157'", "'77'", "'47'"]
'63'
'52'
'157'
'77'
'47'
['63', '52', '157', '77', '47']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="239" ssid = "6">Of these cases  38 were temporal expressions (either a day of the week or month of the year).</S><S sid ="93" ssid = "26">To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.</S><S sid ="249" ssid = "16">The test accuracy more or less asymptotes.</S><S sid ="197" ssid = "64">Note that in our formalism a weakhypothesis can abstain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'239'", "'93'", "'249'", "'197'"]
'204'
'239'
'93'
'249'
'197'
['204', '239', '93', '249', '197']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S><S sid ="93" ssid = "26">To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'75'", "'228'", "'0'", "'93'"]
'204'
'75'
'228'
'0'
'93'
['204', '75', '228', '0', '93']
parsed_discourse_facet ['method_citation']
<S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="193" ssid = "60">In Input: {(x1 i  Initialize: Vi  j : e(xi) = 0.</S><S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'", "'157'", "'77'", "'193'", "'142'"]
'52'
'157'
'77'
'193'
'142'
['52', '157', '77', '193', '142']
parsed_discourse_facet ['results_citation']
['The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.']
['2 We now introduce a new algorithm for learning from unlabeled examples  which we will call DLCoTrain (DL stands for decision list  the term Cotrain is taken from (Blum and Mitchell 98)).', "Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).", u'We again assume a training set of n examples {x1 . xri} where the first m examples have labels {y1 ... yin}  and the last (n \xe2\u20ac\u201d m) examples are unlabeled.', 'Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).', 'AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.']
['system', 'ROUGE-S*', 'Average_R:', '0.00288', '(95%-conf.int.', '0.00288', '-', '0.00288)']
['system', 'ROUGE-S*', 'Average_P:', '0.16667', '(95%-conf.int.', '0.16667', '-', '0.16667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00567', '(95%-conf.int.', '0.00567', '-', '0.00567)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:36', 'F:6']
["Unfortunately, Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective function."]
['In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.', '971 746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).', 'In this case nonalpha is the string formed by removing all upper/lower case letters from the spelling (e.g.  for Thomas E. Petry nonalpha= .', 'Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.', 'The NP is a complement to a preposition  which is the head of a PP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:120', 'F:0']
['Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.']
["For each label (Per s on  organization and Location)  take the n contextual rules with the highest value of Count' (x) whose unsmoothed3 strength is above some threshold pmin.", 'We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', 'The first m pairs have labels yi  whereas for i = m + 1    n the pairs are unlabeled.', 'With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.', 'Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.']
['system', 'ROUGE-S*', 'Average_R:', '0.00549', '(95%-conf.int.', '0.00549', '-', '0.00549)']
['system', 'ROUGE-S*', 'Average_P:', '0.12727', '(95%-conf.int.', '0.12727', '-', '0.12727)']
['system', 'ROUGE-S*', 'Average_F:', '0.01053', '(95%-conf.int.', '0.01053', '-', '0.01053)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:55', 'F:7']
['Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.']
['In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.', u'The input to AdaBoost is a set of training examples ((xi   yi)    (x\u201e.\u201e yrn)).', 'In Input: {(x1 i  Initialize: Vi  j : e(xi) = 0.', 'Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.', 'The NP is a complement to a preposition  which is the head of a PP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00672', '(95%-conf.int.', '0.00672', '-', '0.00672)']
['system', 'ROUGE-S*', 'Average_P:', '0.07273', '(95%-conf.int.', '0.07273', '-', '0.07273)']
['system', 'ROUGE-S*', 'Average_F:', '0.01231', '(95%-conf.int.', '0.01231', '-', '0.01231)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:55', 'F:4']
['Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.']
['The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.', 'Thus at each iteration the method induces at most n x k rules  where k is the number of possible labels (k = 3 in the experiments in this paper). step 3.', 'With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.', 'Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.', 'In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.']
['system', 'ROUGE-S*', 'Average_R:', '0.00376', '(95%-conf.int.', '0.00376', '-', '0.00376)']
['system', 'ROUGE-S*', 'Average_P:', '0.10909', '(95%-conf.int.', '0.10909', '-', '0.10909)']
['system', 'ROUGE-S*', 'Average_F:', '0.00727', '(95%-conf.int.', '0.00727', '-', '0.00727)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:55', 'F:6']
['This paper discusses the use of unlabeled examples for the problem of named entity classification.']
['They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).', 'The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels  and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.', 'In particular  it may not be possible to learn functions fi (x f2(x2 t) for i = m + 1...n: either because there is some noise in the data  or because it is just not realistic to expect to learn perfect classifiers given the features used for representation.', 'It is a sequence of proper nouns within an NP; its last word Cooper is the head of the NP; and the NP has an appositive modifier (a vice president at S.&P.) whose head is a singular noun (president).', 'The unlabeled data gives many such &quot;hints&quot; that two features should predict the same label  and these hints turn out to be surprisingly useful when building a classifier.']
['system', 'ROUGE-S*', 'Average_R:', '0.00038', '(95%-conf.int.', '0.00038', '-', '0.00038)']
['system', 'ROUGE-S*', 'Average_P:', '0.03571', '(95%-conf.int.', '0.03571', '-', '0.03571)']
['system', 'ROUGE-S*', 'Average_F:', '0.00075', '(95%-conf.int.', '0.00075', '-', '0.00075)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:28', 'F:1']
['To prevent this we &quot;smooth&quot; the confidence by adding a small value, e, to both W+ and W_, giving at = Plugging the value of at from Equ.']
['In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.', '(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.', 'Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).', u'The input to AdaBoost is a set of training examples ((xi   yi)    (x\u201e.\u201e yrn)).', 'The NP is a complement to a preposition  which is the head of a PP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00097', '(95%-conf.int.', '0.00097', '-', '0.00097)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00185', '(95%-conf.int.', '0.00185', '-', '0.00185)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:45', 'F:1']
['(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.']
['Unsupervised Models for Named Entity Classification Collins', 'Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).', 'To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.', 'In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.', 'Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:630', 'F:0']
['Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.']
['In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.', 'Unsupervised Models for Named Entity Classification Collins', 'Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).', u'The input to AdaBoost is a set of training examples ((xi   yi)    (x\u201e.\u201e yrn)).', 'Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).']
['system', 'ROUGE-S*', 'Average_R:', '0.00810', '(95%-conf.int.', '0.00810', '-', '0.00810)']
['system', 'ROUGE-S*', 'Average_P:', '0.10909', '(95%-conf.int.', '0.10909', '-', '0.10909)']
['system', 'ROUGE-S*', 'Average_F:', '0.01508', '(95%-conf.int.', '0.01508', '-', '0.01508)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:55', 'F:6']
0.0714199992064 0.00314444440951 0.005939999934





input/ref/Task1/E03-1005_swastika.csv
input/res/Task1/E03-1005.csv
parsing: input/ref/Task1/E03-1005_swastika.csv
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
    <S sid="41" ssid="38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



['41']
41
['41']
parsed_discourse_facet ['aim_citation']
<S sid="80" ssid="32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S>
original cit marker offset is 0
new cit marker offset is 0



['80']
80
['80']
parsed_discourse_facet ['result_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



['143']
143
['143']
parsed_discourse_facet ['method_citation']
<S sid="146" ssid="11">This paper also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic depen dencies (as e.g. in Collins 1999 and Charniak 2000).</S>
original cit marker offset is 0
new cit marker offset is 0



['146']
146
['146']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
<S sid="100" ssid="3">In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['result_citation']
    <S sid="30" ssid="27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S>
original cit marker offset is 0
new cit marker offset is 0



['30']
30
['30']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
    <S sid="30" ssid="27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S>
original cit marker offset is 0
new cit marker offset is 0



['30']
30
['30']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="71" ssid = "23">For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'64'", "'99'", "'71'", "'39'"]
'7'
'64'
'99'
'71'
'39'
['7', '64', '99', '71', '39']
parsed_discourse_facet ['results_citation']
<S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="27" ssid = "24">Goodman (1996  1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set  thus converting the exponential number of subtrees to a compact grammar.</S><S sid ="88" ssid = "40">In this paper  we will test a simple extension of Goodman's compact PCFG-reduction of DOP which has the same property as the normalization proposed in Bod (2001) in that it assigns roughly equal weight to each node in the training data.</S><S sid ="69" ssid = "21">Thus  rather than using the large  explicit DOP1 model  one can also use this small PCFG that generates isomorphic derivations  with identical probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'130'", "'27'", "'88'", "'69'"]
'140'
'130'
'27'
'88'
'69'
['140', '130', '27', '88', '69']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "2">The probability of a parse tree is computed from the occurrencefrequencies of the subtrees in the treebank.</S><S sid ="76" ssid = "28">Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.</S><S sid ="46" ssid = "43">Most previous notions of best parse tree in DOP1 were based on a probabilistic metric  with Bod (2000b) as a notable exception  who used a simplicity metric based on the shortest derivation.</S><S sid ="91" ssid = "43">It easy to see that this is equivalent to reducing the probability of a tree by a factor of four for each pair of nonterminals it contains  resulting in the PCFG reduction in figure 4.</S><S sid ="52" ssid = "4">The probability of a parse tree T is the sum of the probabilities of its distinct derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'76'", "'46'", "'91'", "'52'"]
'50'
'76'
'46'
'91'
'52'
['50', '76', '46', '91', '52']
parsed_discourse_facet ['method_citation']
<S sid ="74" ssid = "26">Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.</S><S sid ="124" ssid = "5">We employed the same unknown (category) word model as in Bod (2001)  based on statistics on word-endings  hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 87).</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="96" ssid = "48">However  as mentioned above  efficient parsing does not necessarily mean efficient disambiguation: the exact computation of the most probable parse remains exponential.</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'124'", "'40'", "'13'", "'96'"]
'74'
'124'
'40'
'13'
'96'
['74', '124', '40', '13', '96']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">An Efficient Implementation of a New DOP Model</S><S sid ="120" ssid = "1">For our experiments we used the standard division of the WSJ (Marcus et al. 1993)  with sections 2 through 21 for training (approx.</S><S sid ="34" ssid = "31">But even with cross-validation  ML-DOP is outperformed by the much simpler DOP1 model on both the ATIS and OVIS treebanks (Bod 2000b).</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'120'", "'34'", "'15'", "'40'"]
'0'
'120'
'34'
'15'
'40'
['0', '120', '34', '15', '40']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="3" ssid = "3">Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models  and an average processing time of 3.6 seconds per WSJ sentence.</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'3'", "'37'", "'39'", "'85'"]
'7'
'3'
'37'
'39'
'85'
['7', '3', '37', '39', '85']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'64'", "'136'", "'99'", "'140'"]
'7'
'64'
'136'
'99'
'140'
['7', '64', '136', '99', '140']
parsed_discourse_facet ['method_citation']
<S sid ="145" ssid = "10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S><S sid ="46" ssid = "43">Most previous notions of best parse tree in DOP1 were based on a probabilistic metric  with Bod (2000b) as a notable exception  who used a simplicity metric based on the shortest derivation.</S><S sid ="124" ssid = "5">We employed the same unknown (category) word model as in Bod (2001)  based on statistics on word-endings  hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 87).</S><S sid ="105" ssid = "8">The derivation with the smallest sum  or highest rank  is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP  its results are still rather impressive for such a simple model.</S><S sid ="41" ssid = "38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'", "'46'", "'124'", "'105'", "'41'"]
'145'
'46'
'124'
'105'
'41'
['145', '46', '124', '105', '41']
parsed_discourse_facet ['method_citation']
<S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="42" ssid = "39">We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'64'", "'99'", "'42'", "'136'"]
'140'
'64'
'99'
'42'
'136'
['140', '64', '99', '42', '136']
parsed_discourse_facet ['method_citation']
<S sid ="142" ssid = "7">Compared to the reranking technique in Collins (2000)  who obtained an LP of 89.9% and an LR of 89.6%  our results show a 9% relative error rate reduction.</S><S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="141" ssid = "6">This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S><S sid ="59" ssid = "11">A new nonterminal is created for each node in the training data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'7'", "'141'", "'136'", "'59'"]
'142'
'7'
'141'
'136'
'59'
['142', '7', '141', '136', '59']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="116" ssid = "19">Then the shortest derivation is equal to the most probable derivation and can be computed by standard Viterbi optimization  which can be seen as follows: if each subtree has a probability p then the probability of a derivation involving n subtrees is equal to pn  and since 0<p<1  the derivation with the fewest subtrees has the greatest probability.</S><S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'116'", "'85'", "'37'", "'39'"]
'7'
'116'
'85'
'37'
'39'
['7', '116', '85', '37', '39']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="103" ssid = "6">That is  all subtrees of each root label are assigned a rank according to their frequency in the treebank: the most frequent subtree (or subtrees) of each root label gets rank 1  the second most frequent subtree gets rank 2  etc.</S><S sid ="134" ssid = "15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'37'", "'99'", "'103'", "'134'"]
'7'
'37'
'99'
'103'
'134'
['7', '37', '99', '103', '134']
parsed_discourse_facet ['method_citation']
<S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S><S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="88" ssid = "40">In this paper  we will test a simple extension of Goodman's compact PCFG-reduction of DOP which has the same property as the normalization proposed in Bod (2001) in that it assigns roughly equal weight to each node in the training data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'85'", "'7'", "'39'", "'88'"]
'37'
'85'
'7'
'39'
'88'
['37', '85', '7', '39', '88']
parsed_discourse_facet ['results_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'37'", "'140'", "'39'", "'64'"]
'7'
'37'
'140'
'39'
'64'
['7', '37', '140', '39', '64']
parsed_discourse_facet ['method_citation']
<S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S><S sid ="42" ssid = "39">We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="141" ssid = "6">This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'99'", "'136'", "'42'", "'140'", "'141'"]
'99'
'136'
'42'
'140'
'141'
['99', '136', '42', '140', '141']
parsed_discourse_facet ['method_citation']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models  and an average processing time of 3.6 seconds per WSJ sentence.', 'Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.', "Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.", 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.', 'For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).']
['system', 'ROUGE-S*', 'Average_R:', '0.00064', '(95%-conf.int.', '0.00064', '-', '0.00064)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00127', '(95%-conf.int.', '0.00127', '-', '0.00127)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4656', 'P:78', 'F:3']
['The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.']
['We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.', "Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.", 'There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.', 'For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.', 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.']
['system', 'ROUGE-S*', 'Average_R:', '0.00633', '(95%-conf.int.', '0.00633', '-', '0.00633)']
['system', 'ROUGE-S*', 'Average_P:', '0.04667', '(95%-conf.int.', '0.04667', '-', '0.04667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01115', '(95%-conf.int.', '0.01115', '-', '0.01115)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:300', 'F:14']
['Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.']
["Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.", 'Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.', "In this paper  we will test a simple extension of Goodman's compact PCFG-reduction of DOP which has the same property as the normalization proposed in Bod (2001) in that it assigns roughly equal weight to each node in the training data.", 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.', 'For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).']
['system', 'ROUGE-S*', 'Average_R:', '0.00147', '(95%-conf.int.', '0.00147', '-', '0.00147)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.00288', '(95%-conf.int.', '0.00288', '-', '0.00288)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4095', 'P:78', 'F:6']
['While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.']
['The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.', "Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.", 'We employed the same unknown (category) word model as in Bod (2001)  based on statistics on word-endings  hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 87).', "Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.", 'However  as mentioned above  efficient parsing does not necessarily mean efficient disambiguation: the exact computation of the most probable parse remains exponential.']
['system', 'ROUGE-S*', 'Average_R:', '0.00746', '(95%-conf.int.', '0.00746', '-', '0.00746)']
['system', 'ROUGE-S*', 'Average_P:', '0.04558', '(95%-conf.int.', '0.04558', '-', '0.04558)']
['system', 'ROUGE-S*', 'Average_F:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:351', 'F:16']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.', "Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.", 'Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.']
['system', 'ROUGE-S*', 'Average_R:', '0.03325', '(95%-conf.int.', '0.03325', '-', '0.03325)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.06436', '(95%-conf.int.', '0.06436', '-', '0.06436)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:78', 'F:78']
['Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.']
['We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.', 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', 'As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:78', 'F:0']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['A new nonterminal is created for each node in the training data.', 'Compared to the reranking technique in Collins (2000)  who obtained an LP of 89.9% and an LR of 89.6%  our results show a 9% relative error rate reduction.', 'As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.', 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.', 'This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00363', '(95%-conf.int.', '0.00363', '-', '0.00363)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.00693', '(95%-conf.int.', '0.00693', '-', '0.00693)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:78', 'F:6']
['DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).']
['Most previous notions of best parse tree in DOP1 were based on a probabilistic metric  with Bod (2000b) as a notable exception  who used a simplicity metric based on the shortest derivation.', "Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.", 'It easy to see that this is equivalent to reducing the probability of a tree by a factor of four for each pair of nonterminals it contains  resulting in the PCFG reduction in figure 4.', 'The probability of a parse tree is computed from the occurrencefrequencies of the subtrees in the treebank.', 'The probability of a parse tree T is the sum of the probabilities of its distinct derivations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00242', '(95%-conf.int.', '0.00242', '-', '0.00242)']
['system', 'ROUGE-S*', 'Average_P:', '0.08889', '(95%-conf.int.', '0.08889', '-', '0.08889)']
['system', 'ROUGE-S*', 'Average_F:', '0.00471', '(95%-conf.int.', '0.00471', '-', '0.00471)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:45', 'F:4']
['The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.']
['We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.', 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', 'There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.', 'As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['system', 'ROUGE-S*', 'Average_R:', '0.01148', '(95%-conf.int.', '0.01148', '-', '0.01148)']
['system', 'ROUGE-S*', 'Average_P:', '0.07000', '(95%-conf.int.', '0.07000', '-', '0.07000)']
['system', 'ROUGE-S*', 'Average_F:', '0.01972', '(95%-conf.int.', '0.01972', '-', '0.01972)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:300', 'F:21']
0.16038222044 0.00740888880657 0.0137599998471





input/ref/Task1/A97-1014_swastika.csv
input/res/Task1/A97-1014.csv
parsing: input/ref/Task1/A97-1014_swastika.csv
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



['151']
151
['151']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
<S sid="47" ssid="37">Argument structure can be represented in terms of unordered trees (with crossing branches).</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



['167']
167
['167']
parsed_discourse_facet ['method_citation']
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



['160']
160
['160']
parsed_discourse_facet ['method_citation']
<S sid="127" ssid="8">As the need for certain functionalities becomes obvious with growing annotation experience, we have decided to implement the tool in two stages.</S>
original cit marker offset is 0
new cit marker offset is 0



['127']
127
['127']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



['39']
39
['39']
parsed_discourse_facet ['method_citation']
<S sid="72" ssid="17">In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.</S>
original cit marker offset is 0
new cit marker offset is 0



['72']
72
['72']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/A97-1014.csv
<S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="81" ssid = "26">During the second annotation stage  the annotation is enriched with information about thematic roles  quantifier scope and anaphoric reference.</S><S sid ="79" ssid = "24">PM stands for morphological particle  a label for German infinitival Z7t and superlative am.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'81'", "'79'", "'137'", "'162'"]
'4'
'81'
'79'
'137'
'162'
['4', '81', '79', '137', '162']
parsed_discourse_facet ['results_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="79" ssid = "24">PM stands for morphological particle  a label for German infinitival Z7t and superlative am.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="140" ssid = "21">For the implementation  we used Tcl/Tk Version 4.1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'4'", "'79'", "'58'", "'140'"]
'80'
'4'
'79'
'58'
'140'
['80', '4', '79', '58', '140']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'4'", "'137'", "'56'", "'149'"]
'80'
'4'
'137'
'56'
'149'
['80', '4', '137', '56', '149']
parsed_discourse_facet ['method_citation']
<S sid ="129" ssid = "10">In the second phase  secondary links and additional structural functions are supported.</S><S sid ="140" ssid = "21">For the implementation  we used Tcl/Tk Version 4.1.</S><S sid ="141" ssid = "22">The corpus is stored in a SQL database.</S><S sid ="116" ssid = "29">This extra marking makes it easy to distinguish between 'normal' and coordinated categories.</S><S sid ="105" ssid = "18">In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'129'", "'140'", "'141'", "'116'", "'105'"]
'129'
'140'
'141'
'116'
'105'
['129', '140', '141', '116', '105']
parsed_discourse_facet ['method_citation']
<S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="128" ssid = "9">In the first phase  the main functionality for building and displaying unordered trees is supplied.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="105" ssid = "18">In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'137'", "'128'", "'56'", "'105'"]
'74'
'137'
'128'
'56'
'105'
['74', '137', '128', '56', '105']
parsed_discourse_facet ['method_citation']
<S sid ="55" ssid = "45">A uniform representation of local and non-local dependencies makes the structure more transparent'.</S><S sid ="53" ssid = "43">A tree meeting these requirements is given below: Adv V NP NP V CPL NP V damn wird ihn Anna erkennen  dais er 'vein!</S><S sid ="120" ssid = "1">The development of linguistically interpreted corpora presents a laborious and time-consuming task.</S><S sid ="65" ssid = "10">The tree resembles traditional constituent structures.</S><S sid ="14" ssid = "4">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'53'", "'120'", "'65'", "'14'"]
'55'
'53'
'120'
'65'
'14'
['55', '53', '120', '65', '14']
parsed_discourse_facet ['method_citation']
<S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'58'", "'80'", "'149'", "'56'"]
'74'
'58'
'80'
'149'
'56'
['74', '58', '80', '149', '56']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'74'", "'56'", "'58'", "'149'"]
'80'
'74'
'56'
'58'
'149'
['80', '74', '56', '58', '149']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'56'", "'58'", "'137'", "'149'"]
'80'
'56'
'58'
'137'
'149'
['80', '56', '58', '137', '149']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.</S><S sid ="88" ssid = "1">As theory-independence is one of our objectives  the annotation scheme incorporates a number of widely accepted linguistic analyses  especially in the area of verbal  adverbial and adjectival syntax.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="164" ssid = "6">As modern linguistics is also becoming more aware of the importance of larger sets of naturally occurring data.  interpreted corpora. are a. valuable resource for theoretical and descriptive linguistic research.</S><S sid ="168" ssid = "10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'88'", "'5'", "'164'", "'168'"]
'3'
'88'
'5'
'164'
'168'
['3', '88', '5', '164', '168']
parsed_discourse_facet ['method_citation']
<S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'137'", "'74'", "'56'", "'149'"]
'58'
'137'
'74'
'56'
'149'
['58', '137', '74', '56', '149']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="101" ssid = "14">All components of this kernel are assigned the label NK and treated as sibling nodes.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'56'", "'101'", "'4'", "'149'"]
'80'
'56'
'101'
'4'
'149'
['80', '56', '101', '4', '149']
parsed_discourse_facet ['method_citation']
<S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'74'", "'58'", "'149'", "'80'"]
'56'
'74'
'58'
'149'
'80'
['56', '74', '58', '149', '80']
parsed_discourse_facet ['results_citation']
<S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="105" ssid = "18">In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'149'", "'56'", "'105'", "'137'", "'74'"]
'149'
'56'
'105'
'137'
'74'
['149', '56', '105', '137', '74']
parsed_discourse_facet ['method_citation']
['In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.']
["A uniform representation of local and non-local dependencies makes the structure more transparent'.", 'The development of linguistically interpreted corpora presents a laborious and time-consuming task.', 'The tree resembles traditional constituent structures.', "A tree meeting these requirements is given below: Adv V NP NP V CPL NP V damn wird ihn Anna erkennen  dais er 'vein!", 'Corpora annotated with syntactic structures are commonly referred to as trctbank.5.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:91', 'F:0']
['We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.']
["We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'Separable verb prefixes are labeled SVP.', 'During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.', 'Grammatical functions  encoded in edge labels  e.g.', 'During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).']
['system', 'ROUGE-S*', 'Average_R:', '0.00032', '(95%-conf.int.', '0.00032', '-', '0.00032)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00062', '(95%-conf.int.', '0.00062', '-', '0.00062)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:55', 'F:1']
['These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.']
['Separable verb prefixes are labeled SVP.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.', 'Grammatical functions  encoded in edge labels  e.g.', 'During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:21', 'F:0']
['In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.']
['Separable verb prefixes are labeled SVP.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.", 'All components of this kernel are assigned the label NK and treated as sibling nodes.', 'During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).']
['system', 'ROUGE-S*', 'Average_R:', '0.00047', '(95%-conf.int.', '0.00047', '-', '0.00047)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00092', '(95%-conf.int.', '0.00092', '-', '0.00092)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:36', 'F:1']
["The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction."]
['Separable verb prefixes are labeled SVP.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'Grammatical functions  encoded in edge labels  e.g.', 'During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.', 'During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:66', 'F:0']
["The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction."]
['During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.', 'In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).', 'During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5050', 'P:66', 'F:0']
['For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).']
['Separable verb prefixes are labeled SVP.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.', "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.", 'During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).']
['system', 'ROUGE-S*', 'Average_R:', '0.00044', '(95%-conf.int.', '0.00044', '-', '0.00044)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00086', '(95%-conf.int.', '0.00086', '-', '0.00086)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:55', 'F:1']
["Consider the German sentence (1) daran wird ihn Anna erkennen, &di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes."]
["We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.', 'The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.', 'Grammatical functions  encoded in edge labels  e.g.', 'During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).']
['system', 'ROUGE-S*', 'Average_R:', '0.00140', '(95%-conf.int.', '0.00140', '-', '0.00140)']
['system', 'ROUGE-S*', 'Average_P:', '0.01232', '(95%-conf.int.', '0.01232', '-', '0.01232)']
['system', 'ROUGE-S*', 'Average_F:', '0.00252', '(95%-conf.int.', '0.00252', '-', '0.00252)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3570', 'P:406', 'F:5']
['As the need for certain functionalities becomes obvious with growing annotation experience, we have decided to implement the tool in two stages.']
['As modern linguistics is also becoming more aware of the importance of larger sets of naturally occurring data.  interpreted corpora. are a. valuable resource for theoretical and descriptive linguistic research.', 'We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.', u'The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- \xe2\u20ac\xa2lar representational strata.', 'In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.', 'As theory-independence is one of our objectives  the annotation scheme incorporates a number of widely accepted linguistic analyses  especially in the area of verbal  adverbial and adjectival syntax.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:36', 'F:0']
['We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.']
['For the implementation  we used Tcl/Tk Version 4.1.', 'Separable verb prefixes are labeled SVP.', "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.", 'PM stands for morphological particle  a label for German infinitival Z7t and superlative am.', 'Grammatical functions  encoded in edge labels  e.g.']
['system', 'ROUGE-S*', 'Average_R:', '0.00450', '(95%-conf.int.', '0.00450', '-', '0.00450)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00832', '(95%-conf.int.', '0.00832', '-', '0.00832)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:55', 'F:3']
['Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.']
['In the second phase  secondary links and additional structural functions are supported.', "This extra marking makes it easy to distinguish between 'normal' and coordinated categories.", 'In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).', 'For the implementation  we used Tcl/Tk Version 4.1.', 'The corpus is stored in a SQL database.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:105', 'F:0']
0.0119099998917 0.000648181812289 0.00120363635269





input/ref/Task1/A00-2030_sweta.csv
input/res/Task1/A00-2030.csv
parsing: input/ref/Task1/A00-2030_sweta.csv
<S sid="18" ssid="1">Almost all approaches to information extraction &#8212; even at the sentence level &#8212; are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="100" ssid="5">Given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a While our focus throughout the project was on TE and TR, we became curious about how well the model did at part-of-speech tagging, syntactic parsing, and at name finding.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="15">Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
 <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="1">Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.<
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="49" ssid = "9">By necessity  we adopted the strategy of hand marking only the semantics.</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="70" ssid = "11">Post-modifier constituents for the PER/NP.</S><S sid ="73" ssid = "14">We now briefly summarize the probability structure of the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'", "'49'", "'68'", "'70'", "'73'"]
'28'
'49'
'68'
'70'
'73'
['28', '49', '68', '70', '73']
parsed_discourse_facet ['results_citation']
<S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="76" ssid = "17">Finally  word features  fm  for modifiers are predicted based on the modifier  cm  the partof-speech tag of the modifier word   t„„ the part-of-speech tag of the head word th  the head word itself  wh  and whether or not the modifier head word  w„„ is known or unknown.</S><S sid ="39" ssid = "7">For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.</S><S sid ="30" ssid = "13">Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'101'", "'76'", "'39'", "'30'"]
'61'
'101'
'76'
'39'
'30'
['61', '101', '76', '39', '30']
parsed_discourse_facet ['method_citation']
<S sid ="33" ssid = "1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S sid ="41" ssid = "1">To train our integrated model  we required a large corpus of augmented parse trees.</S><S sid ="64" ssid = "5">Word features are introduced primarily to help with unknown words  as in (Weischedel et al. 1993).</S><S sid ="2" ssid = "2">In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'41'", "'64'", "'2'", "'23'"]
'33'
'41'
'64'
'2'
'23'
['33', '41', '64', '2', '23']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'101'", "'81'", "'74'", "'0'"]
'61'
'101'
'81'
'74'
'0'
['61', '101', '81', '74', '0']
parsed_discourse_facet ['method_citation']
<S sid ="60" ssid = "1">In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).</S><S sid ="72" ssid = "13">This generation process is continued until the entire tree has been produced.</S><S sid ="77" ssid = "18">The probability of a complete tree is the product of the probabilities of generating each element in the tree.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'72'", "'77'", "'88'", "'28'"]
'60'
'72'
'77'
'88'
'28'
['60', '72', '77', '88', '28']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="54" ssid = "3">These steps are given below:</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'46'", "'57'", "'54'", "'106'"]
'0'
'46'
'57'
'54'
'106'
['0', '46', '57', '54', '106']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="94" ssid = "13">Given a new sentence  the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S><S sid ="30" ssid = "13">Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.</S><S sid ="60" ssid = "1">In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).</S><S sid ="39" ssid = "7">For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'94'", "'30'", "'60'", "'39'"]
'61'
'94'
'30'
'60'
'39'
['61', '94', '30', '60', '39']
parsed_discourse_facet ['method_citation']
<S sid ="110" ssid = "2">Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.</S><S sid ="41" ssid = "1">To train our integrated model  we required a large corpus of augmented parse trees.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'", "'41'", "'57'", "'51'", "'46'"]
'110'
'41'
'57'
'51'
'46'
['110', '41', '57', '51', '46']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'81'", "'0'", "'61'", "'74'"]
'51'
'81'
'0'
'61'
'74'
['51', '81', '0', '61', '74']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "15">Because generative statistical models had already proven successful for each of the first three stages  we were optimistic that some of their properties — especially their ability to learn from large amounts of data  and their robustness when presented with unexpected inputs — would also benefit semantic analysis.</S><S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="19" ssid = "2">Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'61'", "'19'", "'101'", "'51'"]
'32'
'61'
'19'
'101'
'51'
['32', '61', '19', '101', '51']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="54" ssid = "3">These steps are given below:</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'54'", "'81'", "'57'", "'46'"]
'0'
'54'
'81'
'57'
'46'
['0', '54', '81', '57', '46']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "13">Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'46'", "'105'", "'106'", "'11'"]
'30'
'46'
'105'
'106'
'11'
['30', '46', '105', '106', '11']
parsed_discourse_facet ['method_citation']
<S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="54" ssid = "3">These steps are given below:</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'", "'0'", "'46'", "'57'", "'54'"]
'105'
'0'
'46'
'57'
'54'
['105', '0', '46', '57', '54']
parsed_discourse_facet ['results_citation']
<S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'0'", "'57'", "'51'", "'105'"]
'46'
'0'
'57'
'51'
'105'
['46', '0', '57', '51', '105']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'57'", "'46'", "'51'", "'81'"]
'0'
'57'
'46'
'51'
'81'
['0', '57', '46', '51', '81']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="32" ssid = "15">Because generative statistical models had already proven successful for each of the first three stages  we were optimistic that some of their properties — especially their ability to learn from large amounts of data  and their robustness when presented with unexpected inputs — would also benefit semantic analysis.</S><S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S><S sid ="26" ssid = "9">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993)  and more recently  had begun using a generative statistical model for name finding (Bikel et al.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'32'", "'61'", "'11'", "'26'"]
'51'
'32'
'61'
'11'
'26'
['51', '32', '61', '11', '26']
parsed_discourse_facet ['method_citation']
<S sid ="110" ssid = "2">Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'", "'106'", "'46'", "'57'", "'105'"]
'110'
'106'
'46'
'57'
'105'
['110', '106', '46', '57', '105']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "3">These steps are given below:</S><S sid ="55" ssid = "1">syntactic modifier of the other  the inserted node serves to indicate the relation as well as the argument.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="7" ssid = "5">The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'55'", "'0'", "'62'", "'7'"]
'54'
'55'
'0'
'62'
'7'
['54', '55', '0', '62', '7']
parsed_discourse_facet ['method_citation']
<S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="110" ssid = "2">Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'110'", "'46'", "'105'", "'57'"]
'106'
'110'
'46'
'105'
'57'
['106', '110', '46', '105', '57']
parsed_discourse_facet ['aim_citation', 'results_citation']
['We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.']
['Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.', 'A Novel Use of Statistical Parsing to Extract Information from Text', 'David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.', 'For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:', 'To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:78', 'F:0']
['In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.']
['Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.', 'David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.', 'We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.', 'Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.', 'A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:190', 'F:0']
['We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction']
['syntactic modifier of the other  the inserted node serves to indicate the relation as well as the argument.', 'These steps are given below:', 'For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.', 'The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.', 'A Novel Use of Statistical Parsing to Extract Information from Text']
['system', 'ROUGE-S*', 'Average_R:', '0.00460', '(95%-conf.int.', '0.00460', '-', '0.00460)']
['system', 'ROUGE-S*', 'Average_P:', '0.02564', '(95%-conf.int.', '0.02564', '-', '0.02564)']
['system', 'ROUGE-S*', 'Average_F:', '0.00780', '(95%-conf.int.', '0.00780', '-', '0.00780)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:78', 'F:2']
['Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.']
['In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).', 'The probability of a complete tree is the product of the probabilities of generating each element in the tree.', 'For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).', 'Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.', 'This generation process is continued until the entire tree has been produced.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:78', 'F:0']
['The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.']
['Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.', 'A Novel Use of Statistical Parsing to Extract Information from Text', 'David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.', 'We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.', 'These steps are given below:']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:153', 'F:0']
['We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.']
['In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).', 'The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.', 'For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.', 'Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.', 'Given a new sentence  the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:78', 'F:0']
['In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.']
['A Novel Use of Statistical Parsing to Extract Information from Text', 'For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:', 'To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.', u'The categories for head constituents  cl\u201e are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c\u201e _1  and the head word of their parent  wp.', 'The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.']
['system', 'ROUGE-S*', 'Average_R:', '0.00045', '(95%-conf.int.', '0.00045', '-', '0.00045)']
['system', 'ROUGE-S*', 'Average_P:', '0.01053', '(95%-conf.int.', '0.01053', '-', '0.01053)']
['system', 'ROUGE-S*', 'Average_F:', '0.00086', '(95%-conf.int.', '0.00086', '-', '0.00086)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4465', 'P:190', 'F:2']
['The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.']
['Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.', u'Because generative statistical models had already proven successful for each of the first three stages  we were optimistic that some of their properties \u2014 especially their ability to learn from large amounts of data  and their robustness when presented with unexpected inputs \u2014 would also benefit semantic analysis.', 'We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.', 'To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.', 'The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.']
['system', 'ROUGE-S*', 'Average_R:', '0.02599', '(95%-conf.int.', '0.02599', '-', '0.02599)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.05067', '(95%-conf.int.', '0.05067', '-', '0.05067)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5886', 'P:153', 'F:153']
['In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.']
['Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.', 'David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.', 'We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.', 'Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.', 'A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:190', 'F:0']
['In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.']
['Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.', 'We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).', 'Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.', 'We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.', 'A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.']
['system', 'ROUGE-S*', 'Average_R:', '0.00539', '(95%-conf.int.', '0.00539', '-', '0.00539)']
['system', 'ROUGE-S*', 'Average_P:', '0.17778', '(95%-conf.int.', '0.17778', '-', '0.17778)']
['system', 'ROUGE-S*', 'Average_F:', '0.01046', '(95%-conf.int.', '0.01046', '-', '0.01046)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:45', 'F:8']
['We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.']
['We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).', u'Because generative statistical models had already proven successful for each of the first three stages  we were optimistic that some of their properties \u2014 especially their ability to learn from large amounts of data  and their robustness when presented with unexpected inputs \u2014 would also benefit semantic analysis.', 'We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993)  and more recently  had begun using a generative statistical model for name finding (Bikel et al.', 'To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.', 'The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.']
['system', 'ROUGE-S*', 'Average_R:', '0.00022', '(95%-conf.int.', '0.00022', '-', '0.00022)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00043', '(95%-conf.int.', '0.00043', '-', '0.00043)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4560', 'P:78', 'F:1']
['In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.']
['Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.', 'A Novel Use of Statistical Parsing to Extract Information from Text', 'David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.', 'To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.', 'A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.']
['system', 'ROUGE-S*', 'Average_R:', '0.01166', '(95%-conf.int.', '0.01166', '-', '0.01166)']
['system', 'ROUGE-S*', 'Average_P:', '0.20833', '(95%-conf.int.', '0.20833', '-', '0.20833)']
['system', 'ROUGE-S*', 'Average_F:', '0.02208', '(95%-conf.int.', '0.02208', '-', '0.02208)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:120', 'F:25']
['Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.']
['Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.', 'A Novel Use of Statistical Parsing to Extract Information from Text', 'David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.', 'These steps are given below:', 'A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.']
['system', 'ROUGE-S*', 'Average_R:', '0.00116', '(95%-conf.int.', '0.00116', '-', '0.00116)']
['system', 'ROUGE-S*', 'Average_P:', '0.00395', '(95%-conf.int.', '0.00395', '-', '0.00395)']
['system', 'ROUGE-S*', 'Average_F:', '0.00180', '(95%-conf.int.', '0.00180', '-', '0.00180)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:253', 'F:1']
0.110696152995 0.00380538458611 0.00723846148278





input/ref/Task1/P08-1028_aakansha.csv
input/res/Task1/P08-1028.csv
parsing: input/ref/Task1/P08-1028_aakansha.csv
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="185" ssid="19">The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.</S>
    <S sid="186" ssid="20">The combined model is best overall with &#961; = 0.19.</S>
    <S sid="187" ssid="21">However, the difference between the two models is not statistically significant.</S>
original cit marker offset is 0
new cit marker offset is 0



["'185'", "'186'"]
'185'
'186'
['185', '186']
parsed_discourse_facet ['result_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="21">The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'"]
'48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="53" ssid="1">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.</S><S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'53'", "'57'"]
'53'
'57'
['53', '57']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="16">Although the composition model in (5) is commonly used in the literature, from a linguistic perspective, the model in (6) is more appealing.</S>
    <S sid="69" ssid="17">Simply adding the vectors u and v lumps their contents together rather than allowing the content of one vector to pick out the relevant content of the other.</S>
    <S sid="70" ssid="18">Instead, it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v, and vice versa.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'69'", "'70'"]
'68'
'69'
'70'
['68', '69', '70']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
    <S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'", "'25'"]
'24'
'25'
['24', '25']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="12">Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'"]
'64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="176" ssid="10">The multiplicative and combined models yield means closer to the human ratings.</S>
    <S sid="177" ssid="11">The difference between High and Low similarity values estimated by these models are statistically significant (p &lt; 0.01 using the Wilcoxon rank sum test).</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'", "'177'"]
'176'
'177'
['176', '177']
parsed_discourse_facet ['method_citation']
<S sid="191" ssid="3">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'191'"]
'191'
['191']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1028.csv
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="140" ssid = "53">Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid ="40" ssid = "13">Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.</S><S sid ="155" ssid = "68">Secondly  we optimized the weightings in the combined model (11) with a similar grid search over its three parameters.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'140'", "'51'", "'40'", "'155'"]
'194'
'140'
'51'
'40'
'155'
['194', '140', '51', '40', '155']
parsed_discourse_facet ['results_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="58" ssid = "6">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="10" ssid = "6">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'58'", "'59'", "'10'", "'51'"]
'194'
'58'
'59'
'10'
'51'
['194', '58', '59', '10', '51']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="38" ssid = "11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S><S sid ="141" ssid = "54">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S><S sid ="36" ssid = "9">To overcome this problem  other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'194'", "'38'", "'141'", "'36'"]
'108'
'194'
'38'
'141'
'36'
['108', '194', '38', '141', '36']
parsed_discourse_facet ['method_citation']
<S sid ="160" ssid = "73">Specifically  m was set to 20 and m to 1.</S><S sid ="156" ssid = "69">This yielded a weighted sum consisting of 95% verb  0% noun and 5% of their multiplicative combination.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="190" ssid = "2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid ="117" ssid = "30">This yielded a reduced set of experimental items (120 in total) consisting of 15 reference verbs  each with 4 nouns  and 2 landmarks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'", "'156'", "'34'", "'190'", "'117'"]
'160'
'156'
'34'
'190'
'117'
['160', '156', '34', '190', '117']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">Vector-based Models of Semantic Composition</S><S sid ="36" ssid = "9">To overcome this problem  other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid ="142" ssid = "55">We experimented with a variety of dimensions (ranging from 50 to 500 000)  vector component definitions (e.g.  pointwise mutual information or log likelihood ratio) and similarity measures (e.g.  cosine or confusion probability).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'36'", "'51'", "'142'", "'194'"]
'0'
'36'
'51'
'142'
'194'
['0', '36', '51', '142', '194']
parsed_discourse_facet ['method_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid ="58" ssid = "6">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S><S sid ="201" ssid = "13">We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al.  1988) and inductive inference (Heit and Rubinstein  1994).</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'59'", "'51'", "'58'", "'201'"]
'194'
'59'
'51'
'58'
'201'
['194', '59', '51', '58', '201']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "3">For the hierarchical structure of natural language this binding problem becomes particularly acute.</S><S sid ="96" ssid = "9">Any adequate model of composition must be able to represent argument-verb meaning.</S><S sid ="1" ssid = "1">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid ="24" ssid = "20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S><S sid ="44" ssid = "17">For example  assuming that individual words are represented by vectors  we can compute the meaning of a sentence by taking their mean (Foltz et al.  1998; Landauer and Dumais  1997).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'96'", "'1'", "'24'", "'44'"]
'30'
'96'
'1'
'24'
'44'
['30', '96', '1', '24', '44']
parsed_discourse_facet ['method_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="37" ssid = "10">Holographic reduced representations (Plate  1991) are one implementation of this idea where the tensor product is projected back onto the space of its components.</S><S sid ="141" ssid = "54">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'3'", "'101'", "'37'", "'141'"]
'194'
'3'
'101'
'37'
'141'
['194', '3', '101', '37', '141']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid ="140" ssid = "53">Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.</S><S sid ="144" ssid = "57">We obtained best results with a model using a context window of five words on either side of the target word  the cosine measure  and 2 000 vector components.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="37" ssid = "10">Holographic reduced representations (Plate  1991) are one implementation of this idea where the tensor product is projected back onto the space of its components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'140'", "'144'", "'194'", "'37'"]
'39'
'140'
'144'
'194'
'37'
['39', '140', '144', '194', '37']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="168" ssid = "2">These included three additive models  i.e.  simple addition (equation (5)  Add)  weighted addition (equation (7)  WeightAdd)  and Kintsch’s (2001) model (equation (10)  Kintsch)  a multiplicative model (equation (6)  Multiply)  and also a model which combines multiplication with addition (equation (11)  Combined).</S><S sid ="35" ssid = "8">The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely  the tensor product has dimensionality m x n).</S><S sid ="144" ssid = "57">We obtained best results with a model using a context window of five words on either side of the target word  the cosine measure  and 2 000 vector components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'194'", "'168'", "'35'", "'144'"]
'108'
'194'
'168'
'35'
'144'
['108', '194', '168', '35', '144']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid ="117" ssid = "30">This yielded a reduced set of experimental items (120 in total) consisting of 15 reference verbs  each with 4 nouns  and 2 landmarks.</S><S sid ="35" ssid = "8">The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely  the tensor product has dimensionality m x n).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'117'", "'35'", "'194'", "'108'"]
'39'
'117'
'35'
'194'
'108'
['39', '117', '35', '194', '108']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="153" ssid = "66">Specifically  we considered eleven models  varying in their weightings  in steps of 10%  from 100% noun through 50% of both verb and noun to 100% verb.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'101'", "'3'", "'153'", "'194'"]
'108'
'101'
'3'
'153'
'194'
['108', '101', '3', '153', '194']
parsed_discourse_facet ['method_citation']
<S sid ="148" ssid = "61">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S><S sid ="177" ssid = "11">The difference between High and Low similarity values estimated by these models are statistically significant (p < 0.01 using the Wilcoxon rank sum test).</S><S sid ="4" ssid = "4">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S><S sid ="127" ssid = "40">Analysis of Similarity Ratings The reliability of the collected judgments is important for our evaluation experiments; we therefore performed several tests to validate the quality of the ratings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'148'", "'164'", "'177'", "'4'", "'127'"]
'148'
'164'
'177'
'4'
'127'
['148', '164', '177', '4', '127']
parsed_discourse_facet ['results_citation']
<S sid ="115" ssid = "28">Each cell recorded the number of times our subjects selected the landmark as compatible with the noun or not.</S><S sid ="131" ssid = "44">A Wilcoxon rank sum test confirmed that the difference is statistically significant (p < 0.01).</S><S sid ="147" ssid = "60">This configuration gave high correlations with the WordSim353 similarity judgments using the cosine measure.</S><S sid ="173" ssid = "7">Model similarities have been estimated using cosine which ranges from 0 to 1  whereas our subjects rated the sentences on a scale from 1 to 7.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'131'", "'147'", "'173'", "'164'"]
'115'
'131'
'147'
'173'
'164'
['115', '131', '147', '173', '164']
parsed_discourse_facet ['method_citation']
<S sid ="140" ssid = "53">Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.</S><S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="2" ssid = "2">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S><S sid ="139" ssid = "52">The semantic space we used in our experiments was built on a lemmatised version of the BNC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'3'", "'101'", "'2'", "'139'"]
'140'
'3'
'101'
'2'
'139'
['140', '3', '101', '2', '139']
parsed_discourse_facet ['method_citation']
<S sid ="115" ssid = "28">Each cell recorded the number of times our subjects selected the landmark as compatible with the noun or not.</S><S sid ="147" ssid = "60">This configuration gave high correlations with the WordSim353 similarity judgments using the cosine measure.</S><S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid ="173" ssid = "7">Model similarities have been estimated using cosine which ranges from 0 to 1  whereas our subjects rated the sentences on a scale from 1 to 7.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'147'", "'39'", "'173'", "'164'"]
'115'
'147'
'39'
'173'
'164'
['115', '147', '39', '173', '164']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S><S sid ="153" ssid = "66">Specifically  we considered eleven models  varying in their weightings  in steps of 10%  from 100% noun through 50% of both verb and noun to 100% verb.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="183" ssid = "17">The weighted additive model (p = 0.09) is not significantly different from the baseline either or Kintsch (2001) (p = 0.09).</S><S sid ="40" ssid = "13">Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'153'", "'194'", "'183'", "'40'"]
'38'
'153'
'194'
'183'
'40'
['38', '153', '194', '183', '40']
parsed_discourse_facet ['method_citation']
['Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
['Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.', 'Secondly  we optimized the weightings in the combined model (11) with a similar grid search over its three parameters.', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).', 'Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.', 'Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.']
['system', 'ROUGE-S*', 'Average_R:', '0.05376', '(95%-conf.int.', '0.05376', '-', '0.05376)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.10204', '(95%-conf.int.', '0.10204', '-', '0.10204)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:105', 'F:105']
['The combined model is best overall with &#961; = 0.19.', 'However, the difference between the two models is not statistically significant.', 'The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.']
['This yielded a reduced set of experimental items (120 in total) consisting of 15 reference verbs  each with 4 nouns  and 2 landmarks.', 'Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.', 'Specifically  m was set to 20 and m to 1.', 'We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.', 'This yielded a weighted sum consisting of 95% verb  0% noun and 5% of their multiplicative combination.']
['system', 'ROUGE-S*', 'Average_R:', '0.00925', '(95%-conf.int.', '0.00925', '-', '0.00925)']
['system', 'ROUGE-S*', 'Average_P:', '0.06536', '(95%-conf.int.', '0.06536', '-', '0.06536)']
['system', 'ROUGE-S*', 'Average_F:', '0.01621', '(95%-conf.int.', '0.01621', '-', '0.01621)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:153', 'F:10']
['Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
['Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.', 'Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).', 'To overcome this problem  other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.', 'We experimented with a variety of dimensions (ranging from 50 to 500 000)  vector component definitions (e.g.  pointwise mutual information or log likelihood ratio) and similarity measures (e.g.  cosine or confusion probability).']
['system', 'ROUGE-S*', 'Average_R:', '0.04895', '(95%-conf.int.', '0.04895', '-', '0.04895)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.09333', '(95%-conf.int.', '0.09333', '-', '0.09333)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:105', 'F:105']
['Although the composition model in (5) is commonly used in the literature, from a linguistic perspective, the model in (6) is more appealing.', 'Simply adding the vectors u and v lumps their contents together rather than allowing the content of one vector to pick out the relevant content of the other.', 'Instead, it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v, and vice versa.']
['Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.', 'The compression is achieved by summing along the transdiagonal elements of the tensor product.', 'Holographic reduced representations (Plate  1991) are one implementation of this idea where the tensor product is projected back onto the space of its components.', 'We obtained best results with a model using a context window of five words on either side of the target word  the cosine measure  and 2 000 vector components.', 'Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.']
['system', 'ROUGE-S*', 'Average_R:', '0.00793', '(95%-conf.int.', '0.00793', '-', '0.00793)']
['system', 'ROUGE-S*', 'Average_P:', '0.03695', '(95%-conf.int.', '0.03695', '-', '0.03695)']
['system', 'ROUGE-S*', 'Average_F:', '0.01306', '(95%-conf.int.', '0.01306', '-', '0.01306)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:406', 'F:15']
['In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.']
['Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.', 'Specifically  we considered eleven models  varying in their weightings  in steps of 10%  from 100% noun through 50% of both verb and noun to 100% verb.', 'The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.', 'Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.', 'The weighted additive model (p = 0.09) is not significantly different from the baseline either or Kintsch (2001) (p = 0.09).']
['system', 'ROUGE-S*', 'Average_R:', '0.00149', '(95%-conf.int.', '0.00149', '-', '0.00149)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00291', '(95%-conf.int.', '0.00291', '-', '0.00291)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:45', 'F:3']
['In this paper we presented a general framework for vector-based semantic composition.', 'We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).', 'Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.', 'The compression is achieved by summing along the transdiagonal elements of the tensor product.', 'This yielded a reduced set of experimental items (120 in total) consisting of 15 reference verbs  each with 4 nouns  and 2 landmarks.', u'The tensor product u \xc2\xae v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely  the tensor product has dimensionality m x n).']
['system', 'ROUGE-S*', 'Average_R:', '0.00252', '(95%-conf.int.', '0.00252', '-', '0.00252)']
['system', 'ROUGE-S*', 'Average_P:', '0.06618', '(95%-conf.int.', '0.06618', '-', '0.06618)']
['system', 'ROUGE-S*', 'Average_F:', '0.00486', '(95%-conf.int.', '0.00486', '-', '0.00486)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3570', 'P:136', 'F:9']
['The difference between High and Low similarity values estimated by these models are statistically significant (p &lt; 0.01 using the Wilcoxon rank sum test).', 'The multiplicative and combined models yield means closer to the human ratings.']
['Each cell recorded the number of times our subjects selected the landmark as compatible with the noun or not.', 'A Wilcoxon rank sum test confirmed that the difference is statistically significant (p ']
['system', 'ROUGE-S*', 'Average_R:', '0.06618', '(95%-conf.int.', '0.06618', '-', '0.06618)']
['system', 'ROUGE-S*', 'Average_P:', '0.03261', '(95%-conf.int.', '0.03261', '-', '0.03261)']
['system', 'ROUGE-S*', 'Average_F:', '0.04369', '(95%-conf.int.', '0.04369', '-', '0.04369)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:136', 'P:276', 'F:9']
['The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.']
['This paper proposes a framework for representing the meaning of phrases and sentences in vector space.', 'In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.', 'For example  assuming that individual words are represented by vectors  we can compute the meaning of a sentence by taking their mean (Foltz et al.  1998; Landauer and Dumais  1997).', 'For the hierarchical structure of natural language this binding problem becomes particularly acute.', 'Any adequate model of composition must be able to represent argument-verb meaning.']
['system', 'ROUGE-S*', 'Average_R:', '0.00266', '(95%-conf.int.', '0.00266', '-', '0.00266)']
['system', 'ROUGE-S*', 'Average_P:', '0.14286', '(95%-conf.int.', '0.14286', '-', '0.14286)']
['system', 'ROUGE-S*', 'Average_F:', '0.00522', '(95%-conf.int.', '0.00522', '-', '0.00522)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:21', 'F:3']
0.301328746233 0.0240924996988 0.0351649995604





input/ref/Task1/P05-1013_swastika.csv
input/res/Task1/P05-1013.csv
parsing: input/ref/Task1/P05-1013_swastika.csv
<S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



['49']
49
['49']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="6">In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
<S sid="38" ssid="9">Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="6">In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
Error in Reference Offset
S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
Error in Reference Offset
    <S sid="86" ssid="13">As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
 <S sid="51" ssid="22">In the second scheme, Head+Path, we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p&#8595;.</S>
original cit marker offset is 0
new cit marker offset is 0



['51']
51
['51']
parsed_discourse_facet ['method_citation']
<S sid="99" ssid="10">This may seem surprising, given the experiments reported in section 4, but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift, where the encoding of path information is often redundant.</S>
original cit marker offset is 0
new cit marker offset is 0



['99']
99
['99']
parsed_discourse_facet ['result_citation']
<S sid="7" ssid="3">From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



['2']
2
['2']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/P05-1013.csv
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'48'", "'21'", "'36'"]
'50'
'75'
'48'
'21'
'36'
['50', '75', '48', '21', '36']
parsed_discourse_facet ['results_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="62" ssid = "1">In the experiments below  we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs 3 previously tested on Swedish (Nivre et al.  2004) and English (Nivre and Scholz  2004).</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="91" ssid = "2">Table 5 shows the overall parsing accuracy attained with the three different encoding schemes  compared to the baseline (no special arc labels) and to training directly on non-projective dependency graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'75'", "'62'", "'21'", "'91'"]
'48'
'75'
'62'
'21'
'91'
['48', '75', '62', '21', '91']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="77" ssid = "4">The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'77'", "'48'", "'21'"]
'50'
'75'
'77'
'48'
'21'
['50', '75', '77', '48', '21']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'71'", "'21'", "'45'"]
'50'
'75'
'71'
'21'
'45'
['50', '75', '71', '21', '45']
parsed_discourse_facet ['method_citation']
<S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'50'", "'48'", "'45'", "'21'"]
'75'
'50'
'48'
'45'
'21'
['75', '50', '48', '45', '21']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p↓.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'51'", "'36'", "'21'", "'48'"]
'50'
'51'
'36'
'21'
'48'
['50', '51', '36', '21', '48']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'48'", "'21'", "'36'", "'75'"]
'50'
'48'
'21'
'36'
'75'
['50', '48', '21', '36', '75']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p↓.</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'21'", "'51'", "'48'", "'75'"]
'50'
'21'
'51'
'48'
'75'
['50', '21', '51', '48', '75']
parsed_discourse_facet ['method_citation']
<S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p↓.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="23" ssid = "19">By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'", "'51'", "'21'", "'48'", "'23'"]
'36'
'51'
'21'
'48'
'23'
['36', '51', '21', '48', '23']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'48'", "'21'", "'45'"]
'50'
'75'
'48'
'21'
'45'
['50', '75', '48', '21', '45']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="83" ssid = "10">It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="6" ssid = "2">However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.</S><S sid ="35" ssid = "6">The dependency graph in Figure 1 satisfies all the defining conditions above  but it fails to satisfy the condition ofprojectivity (Kahane et al.  1998): The arc connecting the head jedna (one) to the dependent Z (out-of) spans the token je (is)  which is not dominated by jedna.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'83'", "'7'", "'6'", "'35'"]
'2'
'83'
'7'
'6'
'35'
['2', '83', '7', '6', '35']
parsed_discourse_facet ['method_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'75'", "'45'", "'21'", "'20'"]
'48'
'75'
'45'
'21'
'20'
['48', '75', '45', '21', '20']
parsed_discourse_facet ['method_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S><S sid ="73" ssid = "12">More details on the memory-based prediction can be found in Nivre et al. (2004) and Nivre and Scholz (2004).</S><S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'45'", "'73'", "'20'", "'106'"]
'48'
'45'
'73'
'20'
'106'
['48', '45', '73', '20', '106']
parsed_discourse_facet ['results_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="52" ssid = "23">Thus  the arc from je to jedna will be labeled 5b↓ (to indicate that there is a syntactic head below it).</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'48'", "'75'", "'36'", "'52'"]
'50'
'48'
'75'
'36'
'52'
['50', '48', '75', '36', '52']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="84" ssid = "11">In the second part of the experiment  we applied the inverse transformation based on breadth-first search under the three different encoding schemes.</S><S sid ="79" ssid = "6">In the first part of the experiment  dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'48'", "'21'", "'84'", "'79'"]
'50'
'48'
'21'
'84'
'79'
['50', '48', '21', '84', '79']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "8">For each token  three types of features may be taken into account: the word form; the part-of-speech assigned by an automatic tagger; and labels on previously assigned dependency arcs involving the token – the arc from its head and the arcs to its leftmost and rightmost dependent  respectively.</S><S sid ="23" ssid = "19">By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S><S sid ="64" ssid = "3">At each point during the derivation  the parser has a choice between pushing the next input token onto the stack – with or without adding an arc from the token on top of the stack to the token pushed – and popping a token from the stack – with or without adding an arc from the next input token to the token popped.</S><S sid ="59" ssid = "30">The details of the transformation procedure are slightly different depending on the encoding schemes: d↑h let the linear head be the syntactic head). target arc must have the form wl −→ wm; if no target arc is found  Head is used as backoff. must have the form wl −→ wm and no outgoing arcs of the form wm p'↓ −→ wo; no backoff.</S><S sid ="46" ssid = "17">In principle  it would be possible to encode the exact position of the syntactic head in the label of the arc from the linear head  but this would give a potentially infinite set of arc labels and would make the training of the parser very hard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'23'", "'64'", "'59'", "'46'"]
'69'
'23'
'64'
'59'
'46'
['69', '23', '64', '59', '46']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'71'", "'21'", "'45'"]
'50'
'75'
'71'
'21'
'45'
['50', '75', '71', '21', '45']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "18">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 15–25%  the total proportion of non-projective arcs is normally only 1–2%.</S><S sid ="101" ssid = "12">However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.</S><S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S><S sid ="87" ssid = "14">However  it can be noted that the results for the least informative encoding  Path  are almost comparable  while the third encoding  Head  gives substantially worse results for both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'14'", "'101'", "'89'", "'87'"]
'107'
'14'
'101'
'89'
'87'
['107', '14', '101', '89', '87']
parsed_discourse_facet ['method_citation']
<S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 15–25%  the total proportion of non-projective arcs is normally only 1–2%.</S><S sid ="88" ssid = "15">We also see that the increase in the size of the label sets for Head and Head+Path is far below the theoretical upper bounds given in Table 1.</S><S sid ="28" ssid = "24">First  in section 4  we evaluate the graph transformation techniques in themselves  with data from the Prague Dependency Treebank and the Danish Dependency Treebank.</S><S sid ="103" ssid = "14">On the other hand  given that all schemes have similar parsing accuracy overall  this means that the Path scheme is the least likely to introduce errors on projective arcs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'14'", "'88'", "'28'", "'103'"]
'89'
'14'
'88'
'28'
'103'
['89', '14', '88', '28', '103']
parsed_discourse_facet ['aim_citation', 'results_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'50'", "'75'", "'21'", "'36'"]
'48'
'50'
'75'
'21'
'36'
['48', '50', '75', '21', '36']
parsed_discourse_facet ['method_citation']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.', u'In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p\u2193.', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00283', '(95%-conf.int.', '0.00283', '-', '0.00283)']
['system', 'ROUGE-S*', 'Average_P:', '0.13333', '(95%-conf.int.', '0.13333', '-', '0.13333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00554', '(95%-conf.int.', '0.00554', '-', '0.00554)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4950', 'P:105', 'F:14']
['In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00200', '(95%-conf.int.', '0.00200', '-', '0.00200)']
['system', 'ROUGE-S*', 'Average_P:', '0.13333', '(95%-conf.int.', '0.13333', '-', '0.13333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00394', '(95%-conf.int.', '0.00394', '-', '0.00394)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:45', 'F:6']
['In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', u'In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p\u2193.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00259', '(95%-conf.int.', '0.00259', '-', '0.00259)']
['system', 'ROUGE-S*', 'Average_P:', '0.15556', '(95%-conf.int.', '0.15556', '-', '0.15556)']
['system', 'ROUGE-S*', 'Average_F:', '0.00510', '(95%-conf.int.', '0.00510', '-', '0.00510)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2701', 'P:45', 'F:7']
['From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.']
['However  it can be noted that the results for the least informative encoding  Path  are almost comparable  while the third encoding  Head  gives substantially worse results for both data sets.', 'However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.', 'The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.', 'Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.', u'While the proportion of sentences containing non-projective dependencies is often 15\u201325%  the total proportion of non-projective arcs is normally only 1\u20132%.']
['system', 'ROUGE-S*', 'Average_R:', '0.00037', '(95%-conf.int.', '0.00037', '-', '0.00037)']
['system', 'ROUGE-S*', 'Average_P:', '0.00735', '(95%-conf.int.', '0.00735', '-', '0.00735)']
['system', 'ROUGE-S*', 'Average_F:', '0.00070', '(95%-conf.int.', '0.00070', '-', '0.00070)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2701', 'P:136', 'F:1']
no Reference Text in gold P05-1013
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00287', '(95%-conf.int.', '0.00287', '-', '0.00287)']
['system', 'ROUGE-S*', 'Average_P:', '0.09524', '(95%-conf.int.', '0.09524', '-', '0.09524)']
['system', 'ROUGE-S*', 'Average_F:', '0.00557', '(95%-conf.int.', '0.00557', '-', '0.00557)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:105', 'F:10']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00041', '(95%-conf.int.', '0.00041', '-', '0.00041)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00079', '(95%-conf.int.', '0.00079', '-', '0.00079)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:105', 'F:1']
['The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.']
['The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).', u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00034', '(95%-conf.int.', '0.00034', '-', '0.00034)']
['system', 'ROUGE-S*', 'Average_P:', '0.00735', '(95%-conf.int.', '0.00735', '-', '0.00735)']
['system', 'ROUGE-S*', 'Average_F:', '0.00065', '(95%-conf.int.', '0.00065', '-', '0.00065)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:136', 'F:1']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['Table 5 shows the overall parsing accuracy attained with the three different encoding schemes  compared to the baseline (no special arc labels) and to training directly on non-projective dependency graphs.', u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\xcb\u2021c  1998).', 'In the experiments below  we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs 3 previously tested on Swedish (Nivre et al.  2004) and English (Nivre and Scholz  2004).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00739', '(95%-conf.int.', '0.00739', '-', '0.00739)']
['system', 'ROUGE-S*', 'Average_P:', '0.25714', '(95%-conf.int.', '0.25714', '-', '0.25714)']
['system', 'ROUGE-S*', 'Average_F:', '0.01436', '(95%-conf.int.', '0.01436', '-', '0.01436)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3655', 'P:105', 'F:27']
['In the second scheme, Head+Path, we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p&#8595;.']
['In principle  it would be possible to encode the exact position of the syntactic head in the label of the arc from the linear head  but this would give a potentially infinite set of arc labels and would make the training of the parser very hard.', u'For each token  three types of features may be taken into account: the word form; the part-of-speech assigned by an automatic tagger; and labels on previously assigned dependency arcs involving the token \u2013 the arc from its head and the arcs to its leftmost and rightmost dependent  respectively.', u"The details of the transformation procedure are slightly different depending on the encoding schemes: d\u2191h let the linear head be the syntactic head). target arc must have the form wl \u2212\u2192 wm; if no target arc is found  Head is used as backoff. must have the form wl \u2212\u2192 wm and no outgoing arcs of the form wm p'\u2193 \u2212\u2192 wo; no backoff.", u'At each point during the derivation  the parser has a choice between pushing the next input token onto the stack \u2013 with or without adding an arc from the token on top of the stack to the token pushed \u2013 and popping a token from the stack \u2013 with or without adding an arc from the next input token to the token popped.', 'By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.']
['system', 'ROUGE-S*', 'Average_R:', '0.00429', '(95%-conf.int.', '0.00429', '-', '0.00429)']
['system', 'ROUGE-S*', 'Average_P:', '0.30000', '(95%-conf.int.', '0.30000', '-', '0.30000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00847', '(95%-conf.int.', '0.00847', '-', '0.00847)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:8385', 'P:120', 'F:36']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00344', '(95%-conf.int.', '0.00344', '-', '0.00344)']
['system', 'ROUGE-S*', 'Average_P:', '0.11429', '(95%-conf.int.', '0.11429', '-', '0.11429)']
['system', 'ROUGE-S*', 'Average_F:', '0.00668', '(95%-conf.int.', '0.00668', '-', '0.00668)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:105', 'F:12']
['The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.02467', '(95%-conf.int.', '0.02467', '-', '0.02467)']
['system', 'ROUGE-S*', 'Average_P:', '0.11606', '(95%-conf.int.', '0.11606', '-', '0.11606)']
['system', 'ROUGE-S*', 'Average_F:', '0.04069', '(95%-conf.int.', '0.04069', '-', '0.04069)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:741', 'F:86']
['This may seem surprising, given the experiments reported in section 4, but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift, where the encoding of path information is often redundant.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00333', '(95%-conf.int.', '0.00333', '-', '0.00333)']
['system', 'ROUGE-S*', 'Average_P:', '0.06536', '(95%-conf.int.', '0.06536', '-', '0.06536)']
['system', 'ROUGE-S*', 'Average_F:', '0.00634', '(95%-conf.int.', '0.00634', '-', '0.00634)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:153', 'F:10']
['Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.']
['To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', u'In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p\u2193.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.']
['system', 'ROUGE-S*', 'Average_R:', '0.00572', '(95%-conf.int.', '0.00572', '-', '0.00572)']
['system', 'ROUGE-S*', 'Average_P:', '0.42222', '(95%-conf.int.', '0.42222', '-', '0.42222)']
['system', 'ROUGE-S*', 'Average_F:', '0.01129', '(95%-conf.int.', '0.01129', '-', '0.01129)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3321', 'P:45', 'F:19']
no Reference Text in gold P05-1013
0.139749998925 0.00463461534896 0.00847076916561





input/ref/Task1/A00-2018_vardha.csv
input/res/Task1/A00-2018.csv
parsing: input/ref/Task1/A00-2018_vardha.csv
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'91'"]
'91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="8">In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'"]
'39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="162" ssid="53">Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'"]
'162'
['162']
parsed_discourse_facet ['method_citation']
 <S sid="87" ssid="56">In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17].</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'"]
'87'
['87']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'91'"]
'91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="176" ssid="3">That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'"]
'176'
['176']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['method_citation']
 <S sid="101" ssid="12">In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning).</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'"]
'101'
['101']
parsed_discourse_facet ['method_citation']
<S sid="155" ssid="46">For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
  <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="162" ssid="53">Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'"]
'162'
['162']
parsed_discourse_facet ['method_citation']
 <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
 <S sid="79" ssid="48">We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
    <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
    <S sid="175" ssid="2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["'175'"]
'175'
['175']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2018.csv
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'23'", "'62'", "'33'", "'27'"]
'30'
'23'
'62'
'33'
'27'
['30', '23', '62', '33', '27']
parsed_discourse_facet ['results_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'27'", "'62'", "'33'", "'23'"]
'30'
'27'
'62'
'33'
'23'
['30', '27', '62', '33', '23']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'62'", "'33'", "'89'", "'23'"]
'30'
'62'
'33'
'89'
'23'
['30', '62', '33', '89', '23']
parsed_discourse_facet ['method_citation']
<S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="20" ssid = "9">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="29" ssid = "18">So  for example  in a second-order Markov PCFG  L2 would be conditioned on L1 and M. In our complete model  of course  the probability of each label in the expansions is also conditioned on other material as specified in Equation 1  e.g.  p(e t  h  H).</S><S sid ="91" ssid = "2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'", "'20'", "'174'", "'29'", "'91'"]
'27'
'20'
'174'
'29'
'91'
['27', '20', '174', '29', '91']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'23'", "'62'", "'27'", "'33'"]
'30'
'23'
'62'
'27'
'33'
['30', '23', '62', '27', '33']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="63" ssid = "32">As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'1'", "'5'", "'27'", "'63'"]
'101'
'1'
'5'
'27'
'63'
['101', '1', '5', '27', '63']
parsed_discourse_facet ['method_citation']
<S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'", "'30'", "'62'", "'89'", "'23'"]
'27'
'30'
'62'
'89'
'23'
['27', '30', '62', '89', '23']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'62'", "'23'", "'89'", "'33'"]
'30'
'62'
'23'
'89'
'33'
['30', '62', '23', '89', '33']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "7">To compute a probability in a log-linear model one first defines a set of &quot;features&quot;  functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.</S><S sid ="91" ssid = "2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="20" ssid = "9">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid ="26" ssid = "15">Thus an expansion e(c) looks like: The expansion is generated by guessing first M  then in order L1 through L „.+1 (= A)  and similarly for RI through In a pure Markov PCFG we are given the left-hand side label 1 and then probabilistically generate the right-hand side conditioning on no information other than 1 and (possibly) previously generated pieces of the right-hand side itself.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'91'", "'174'", "'20'", "'26'"]
'38'
'91'
'174'
'20'
'26'
['38', '91', '174', '20', '26']
parsed_discourse_facet ['method_citation']
<S sid ="29" ssid = "18">So  for example  in a second-order Markov PCFG  L2 would be conditioned on L1 and M. In our complete model  of course  the probability of each label in the expansions is also conditioned on other material as specified in Equation 1  e.g.  p(e t  h  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="91" ssid = "2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'", "'27'", "'174'", "'91'", "'33'"]
'29'
'27'
'174'
'91'
'33'
['29', '27', '174', '91', '33']
parsed_discourse_facet ['method_citation']
<S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="101" ssid = "12">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S><S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'", "'27'", "'89'", "'101'", "'30'"]
'62'
'27'
'89'
'101'
'30'
['62', '27', '89', '101', '30']
parsed_discourse_facet ['method_citation']
<S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="115" ssid = "6">As noted in [5]  that system is based upon a &quot;tree-bank grammar&quot; - a grammar read directly off the training corpus.</S><S sid ="141" ssid = "32">(For example  part-ofspeech tagging using the most probable preterminal for each word is 90% accurate [8].)</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="184" ssid = "11">The first is the slight  but important  improvement achieved by using this model over conventional deleted interpolation  as indicated in Figure 2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'115'", "'141'", "'62'", "'184'"]
'23'
'115'
'141'
'62'
'184'
['23', '115', '141', '62', '184']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "9">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid ="76" ssid = "45">This requires finding the appropriate Ais for Equation 3  which is accomplished using an algorithm such as iterative scaling [II] in which values for the Ai are initially &quot;guessed&quot; and then modified until they converge on stable values.</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="110" ssid = "1">In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach  the aspect of the parser that is most novel.</S><S sid ="101" ssid = "12">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'76'", "'27'", "'110'", "'101'"]
'20'
'76'
'27'
'110'
'101'
['20', '76', '27', '110', '101']
parsed_discourse_facet ['results_citation']
<S sid ="74" ssid = "43">Suppose we were  in fact  going to compute a true maximum entropy model based upon the features used in Equation 7  Ii (t 1)  f2(t 1 1p)  f3(t 1 lp) .</S><S sid ="126" ssid = "17">This is indicated in Figure 2  where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%  0.4% lower than the results on the official test corpus.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'126'", "'125'", "'174'", "'27'"]
'74'
'126'
'125'
'174'
'27'
['74', '126', '125', '174', '27']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="42" ssid = "11">This feature is obviously composed of two sub-features  one recognizing t  the other 1.</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'89'", "'23'", "'42'", "'62'"]
'30'
'89'
'23'
'42'
'62'
['30', '89', '23', '42', '62']
parsed_discourse_facet ['method_citation']
<S sid ="176" ssid = "3">That the previous three best parsers on this test [5 9 17] all perform within a percentage point of each other  despite quite different basic mechanisms  led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training  and to conjecture that perhaps we were at it.</S><S sid ="58" ssid = "27">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid ="180" ssid = "7">From our perspective  perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.</S><S sid ="44" ssid = "13">Now consider computing a conditional probability p(a H) with a set of features h that connect a to the history H. In a log-linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability  the higher the absolute value of the associated A.</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'", "'58'", "'180'", "'44'", "'13'"]
'176'
'58'
'180'
'44'
'13'
['176', '58', '180', '44', '13']
parsed_discourse_facet ['method_citation']
['We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.']
['For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).', u'In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / \xe2\u20ac\u201d that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).', 'In Equation 1 we wrote this as p(t I 1  H).', 'Thus we would use p(L2 I L1  M  1  t  h  H).', 'For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).']
['system', 'ROUGE-S*', 'Average_R:', '0.00303', '(95%-conf.int.', '0.00303', '-', '0.00303)']
['system', 'ROUGE-S*', 'Average_P:', '0.00317', '(95%-conf.int.', '0.00317', '-', '0.00317)']
['system', 'ROUGE-S*', 'Average_F:', '0.00310', '(95%-conf.int.', '0.00310', '-', '0.00310)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:946', 'F:3']
['In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ', u'In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / \xe2\u20ac\u201d that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).', 'In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:55', 'F:0']
["In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17]."]
['For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).', 'In Equation 1 we wrote this as p(t I 1  H).', u'In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / \xe2\u20ac\u201d that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).', '(Actually  we use a minor variant described in [4].)', 'Thus we would use p(L2 I L1  M  1  t  h  H).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:55', 'F:0']
['In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.']
['This feature is obviously composed of two sub-features  one recognizing t  the other 1.', 'In Equation 1 we wrote this as p(t I 1  H).', 'Thus we would use p(L2 I L1  M  1  t  h  H).', '(Actually  we use a minor variant described in [4].)', 'For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).']
['system', 'ROUGE-S*', 'Average_R:', '0.00285', '(95%-conf.int.', '0.00285', '-', '0.00285)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00505', '(95%-conf.int.', '0.00505', '-', '0.00505)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:45', 'F:1']
['Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.']
['We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.', 'In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).', 'We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3240', 'P:28', 'F:0']
['As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.']
['For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).', 'In Equation 1 we wrote this as p(t I 1  H).', 'Thus we would use p(L2 I L1  M  1  t  h  H).', '(Actually  we use a minor variant described in [4].)', 'For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:231', 'F:0']
['That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.']
['As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.', 'We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ', 'To compute a probability in a log-linear model one first defines a set of &quot;features&quot;  functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.']
['system', 'ROUGE-S*', 'Average_R:', '0.00326', '(95%-conf.int.', '0.00326', '-', '0.00326)']
['system', 'ROUGE-S*', 'Average_P:', '0.03333', '(95%-conf.int.', '0.03333', '-', '0.03333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00594', '(95%-conf.int.', '0.00594', '-', '0.00594)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:210', 'F:7']
['We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.']
['For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).', u'In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / \xe2\u20ac\u201d that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).', 'In Equation 1 we wrote this as p(t I 1  H).', 'Thus we would use p(L2 I L1  M  1  t  h  H).', 'For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).']
['system', 'ROUGE-S*', 'Average_R:', '0.00303', '(95%-conf.int.', '0.00303', '-', '0.00303)']
['system', 'ROUGE-S*', 'Average_P:', '0.00317', '(95%-conf.int.', '0.00317', '-', '0.00317)']
['system', 'ROUGE-S*', 'Average_F:', '0.00310', '(95%-conf.int.', '0.00310', '-', '0.00310)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:946', 'F:3']
['This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].']
['That the previous three best parsers on this test [5 9 17] all perform within a percentage point of each other  despite quite different basic mechanisms  led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training  and to conjecture that perhaps we were at it.', "From our perspective  perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.", u'Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c \xe2\u20ac\u201d information outside c that our probability model deems important in determining the probability in question.', 'Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.', 'Now consider computing a conditional probability p(a H) with a set of features h that connect a to the history H. In a log-linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability  the higher the absolute value of the associated A.']
['system', 'ROUGE-S*', 'Average_R:', '0.00275', '(95%-conf.int.', '0.00275', '-', '0.00275)']
['system', 'ROUGE-S*', 'Average_P:', '0.25641', '(95%-conf.int.', '0.25641', '-', '0.25641)']
['system', 'ROUGE-S*', 'Average_F:', '0.00545', '(95%-conf.int.', '0.00545', '-', '0.00545)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:7260', 'P:78', 'F:20']
['Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.']
['As noted in [5]  that system is based upon a &quot;tree-bank grammar&quot; - a grammar read directly off the training corpus.', 'In Equation 1 we wrote this as p(t I 1  H).', '(For example  part-ofspeech tagging using the most probable preterminal for each word is 90% accurate [8].)', 'The first is the slight  but important  improvement achieved by using this model over conventional deleted interpolation  as indicated in Figure 2.', 'For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:28', 'F:0']
['We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ', 'Suppose we were  in fact  going to compute a true maximum entropy model based upon the features used in Equation 7  Ii (t 1)  f2(t 1 1p)  f3(t 1 lp) .', 'For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.', 'This is indicated in Figure 2  where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%  0.4% lower than the results on the official test corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00348', '(95%-conf.int.', '0.00348', '-', '0.00348)']
['system', 'ROUGE-S*', 'Average_P:', '0.05238', '(95%-conf.int.', '0.05238', '-', '0.05238)']
['system', 'ROUGE-S*', 'Average_F:', '0.00653', '(95%-conf.int.', '0.00653', '-', '0.00653)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:210', 'F:11']
0.0336981815118 0.00167272725752 0.00265181815771





input/ref/Task1/P11-1060_swastika.csv
input/res/Task1/P11-1060.csv
parsing: input/ref/Task1/P11-1060_swastika.csv
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['aim_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['aim_citation']
    <S sid="148" ssid="33">This bootstrapping behavior occurs naturally: The &#8220;easy&#8221; examples are processed first, where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
    <S sid="13" ssid="9">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['result_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="47" ssid="23">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
S sid="44" ssid="20">The recurrence is as follows: At each node, we compute the set of tuples v consistent with the predicate at that node (v &#8712; w(p)), and S(x)}, where a set of pairs S is treated as a set-valued function S(x) = {y : (x, y) &#8712; S} with domain S1 = {x : (x, y) &#8712; S}.</S>
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
Error in Reference Offset
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



['106']
106
['106']
parsed_discourse_facet ['aim_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
    <S sid="172" ssid="57">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms, we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus, and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S>
original cit marker offset is 0
new cit marker offset is 0



['172']
172
['172']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



['171']
171
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="158" ssid = "43">5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation  DCS  which offers a new perspective lexical semantics.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="23" ssid = "19">We trained our model using an EM-like algorithm (Section 3) on two benchmarks  GEO and JOBS (Section 4).</S><S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'158'", "'21'", "'23'", "'132'"]
'26'
'158'
'21'
'23'
'132'
['26', '158', '21', '23', '132']
parsed_discourse_facet ['results_citation']
<S sid ="39" ssid = "15">Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.</S><S sid ="56" ssid = "32">The basic version of DCS described thus far handles a core subset of language.</S><S sid ="4" ssid = "4">On two stansemantic parsing benchmarks our system obtains the highest published accuracies  despite requiring no annotated logical forms.</S><S sid ="136" ssid = "21">In fact  DCS performs comparably to even the version of SEMRESP trained using logical forms.</S><S sid ="83" ssid = "59">It suffices to define Xi(d) for a single column i.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'56'", "'4'", "'136'", "'83'"]
'39'
'56'
'4'
'136'
'83'
['39', '56', '4', '136', '83']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S><S sid ="148" ssid = "33">This bootstrapping behavior occurs naturally: The “easy” examples are processed first  where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'100'", "'115'", "'88'", "'148'"]
'26'
'100'
'115'
'88'
'148'
['26', '100', '115', '88', '148']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="138" ssid = "23">Next  we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="53" ssid = "29">Having instantiated s as a value  everything above this node is an ordinary CSP: s constrains the count node  which in turns constrains the root node to |s|.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'138'", "'26'", "'53'"]
'100'
'21'
'138'
'26'
'53'
['100', '21', '138', '26', '53']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S><S sid ="94" ssid = "70">We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'94'", "'134'", "'8'", "'58'"]
'18'
'94'
'134'
'8'
'58'
['18', '94', '134', '8', '58']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'21'", "'100'", "'115'", "'42'"]
'26'
'21'
'100'
'115'
'42'
['26', '21', '100', '115', '42']
parsed_discourse_facet ['method_citation']
<S sid ="87" ssid = "63">For example  in Figure 4(a)  before execution  the denotation of the DCS tree is hh{[(CA  OR)  (OR)] ... }; ø; (E  Qhstatei�w  ø)ii; after applying X1  we have hh{[(OR)]  ... }; øii.</S><S sid ="113" ssid = "89">Formally  let ˜O(θ  θ') be the objective function O(θ) with ZL(x) ˜ZL θI(x).</S><S sid ="20" ssid = "16">At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="74" ssid = "50">Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'", "'113'", "'20'", "'154'", "'74'"]
'87'
'113'
'20'
'154'
'74'
['87', '113', '20', '154', '74']
parsed_discourse_facet ['method_citation']
<S sid ="117" ssid = "2">In each dataset  each sentence x is annotated with a Prolog logical form  which we use only to evaluate and get an answer y.</S><S sid ="1" ssid = "1">Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'117'", "'1'", "'15'", "'88'", "'62'"]
'117'
'1'
'15'
'88'
'62'
['117', '1', '15', '88', '62']
parsed_discourse_facet ['method_citation']
<S sid ="83" ssid = "59">It suffices to define Xi(d) for a single column i.</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="101" ssid = "77">Formally  pθ(z  |x) ∝ eφ(x z)Tθ  where θ and φ(x  z) are parameter and feature vectors  respectively.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'169'", "'8'", "'101'", "'134'"]
'83'
'169'
'8'
'101'
'134'
['83', '169', '8', '101', '134']
parsed_discourse_facet ['method_citation']
<S sid ="1" ssid = "1">Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid ="99" ssid = "75">To further reduce the search space  F imposes a few additional constraints  e.g.  limiting the number of marked nodes to 2 and only allowing trace predicates between arity 1 predicates.</S><S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S><S sid ="129" ssid = "14">We also define an augmented lexicon L+ which includes a prototype word x for each predicate appearing in (iii) above (e.g.  (large  size))  which cancels the predicates triggered by x’s POS tag.</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'99'", "'132'", "'129'", "'26'"]
'1'
'99'
'132'
'129'
'26'
['1', '99', '132', '129', '26']
parsed_discourse_facet ['method_citation']
<S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="54" ssid = "30">A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S><S sid ="166" ssid = "51">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'54'", "'42'", "'166'", "'132'"]
'21'
'54'
'42'
'166'
'132'
['21', '54', '42', '166', '132']
parsed_discourse_facet ['method_citation']
<S sid ="59" ssid = "35">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S><S sid ="54" ssid = "30">A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="45" ssid = "21">The logical forms in DCS are called DCS trees  where nodes are labeled with predicates  and edges are labeled with relations.</S><S sid ="43" ssid = "19">Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter  2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'54'", "'154'", "'45'", "'43'"]
'59'
'54'
'154'
'45'
'43'
['59', '54', '154', '45', '43']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "30">A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="74" ssid = "50">Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S><S sid ="107" ssid = "83">Our model is arc-factored  so we can sum over all DCS trees in ZL(x) using dynamic programming.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'42'", "'21'", "'74'", "'107'"]
'54'
'42'
'21'
'74'
'107'
['54', '42', '21', '74', '107']
parsed_discourse_facet ['results_citation']
<S sid ="59" ssid = "35">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S><S sid ="148" ssid = "33">This bootstrapping behavior occurs naturally: The “easy” examples are processed first  where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S><S sid ="47" ssid = "23">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.</S><S sid ="103" ssid = "79">To define the features  we technically need to augment each tree z ∈ ZL(x) with alignment information—namely  for each predicate in z  the span in x (if any) that triggered it.</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'148'", "'47'", "'103'", "'62'"]
'59'
'148'
'47'
'103'
'62'
['59', '148', '47', '103', '62']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="23" ssid = "19">We trained our model using an EM-like algorithm (Section 3) on two benchmarks  GEO and JOBS (Section 4).</S><S sid ="86" ssid = "62">Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  −(i  ø)]{α1 = ø}.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'23'", "'86'", "'115'", "'100'"]
'26'
'23'
'86'
'115'
'100'
['26', '23', '86', '115', '100']
parsed_discourse_facet ['method_citation']
<S sid ="113" ssid = "89">Formally  let ˜O(θ  θ') be the objective function O(θ) with ZL(x) ˜ZL θI(x).</S><S sid ="78" ssid = "54">The stores are also concatenated (α + α').</S><S sid ="74" ssid = "50">Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="111" ssid = "87">Let ˜ZL θ(x) be this approximation of ZL(x).</S>
original cit marker offset is 0
new cit marker offset is 0



["'113'", "'78'", "'74'", "'154'", "'111'"]
'113'
'78'
'74'
'154'
'111'
['113', '78', '74', '154', '111']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="63" ssid = "39">Think of d as consisting of n columns  one for each active node according to a pre-order traversal of z.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="86" ssid = "62">Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  −(i  ø)]{α1 = ø}.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'100'", "'63'", "'115'", "'86'"]
'26'
'100'
'63'
'115'
'86'
['26', '100', '63', '115', '86']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'26'", "'115'", "'42'"]
'100'
'21'
'26'
'115'
'42'
['100', '21', '26', '115', '42']
parsed_discourse_facet ['method_citation']
<S sid ="151" ssid = "36">Inspecting the final parameters calculus formulae.</S><S sid ="56" ssid = "32">The basic version of DCS described thus far handles a core subset of language.</S><S sid ="16" ssid = "12">Which one should we use?</S><S sid ="118" ssid = "3">This evaluation is done with respect to a world w. Recall that a world w maps each predicate p ∈ P to a set of tuples w(p).</S><S sid ="33" ssid = "9">As another example  w(average) = {(S  ¯x) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'56'", "'16'", "'118'", "'33'"]
'151'
'56'
'16'
'118'
'33'
['151', '56', '16', '118', '33']
parsed_discourse_facet ['aim_citation', 'results_citation']
['Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.']
['This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.', 'Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.', 'The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).', u'This bootstrapping behavior occurs naturally: The \u201ceasy\u201d examples are processed first  where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.', u'To define the features  we technically need to augment each tree z \u2208 ZL(x) with alignment information\u2014namely  for each predicate in z  the span in x (if any) that triggered it.']
['system', 'ROUGE-S*', 'Average_R:', '0.00649', '(95%-conf.int.', '0.00649', '-', '0.00649)']
['system', 'ROUGE-S*', 'Average_P:', '0.06333', '(95%-conf.int.', '0.06333', '-', '0.06333)']
['system', 'ROUGE-S*', 'Average_F:', '0.01178', '(95%-conf.int.', '0.01178', '-', '0.01178)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:300', 'F:19']
['The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.', 'CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).', 'On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.', 'We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.', 'In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00218', '(95%-conf.int.', '0.00218', '-', '0.00218)']
['system', 'ROUGE-S*', 'Average_P:', '0.02857', '(95%-conf.int.', '0.02857', '-', '0.02857)']
['system', 'ROUGE-S*', 'Average_F:', '0.00405', '(95%-conf.int.', '0.00405', '-', '0.00405)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:105', 'F:3']
['Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.']
['Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.', 'To further reduce the search space  F imposes a few additional constraints  e.g.  limiting the number of marked nodes to 2 and only allowing trace predicates between arity 1 predicates.', 'We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.', u'We also define an augmented lexicon L+ which includes a prototype word x for each predicate appearing in (iii) above (e.g.  (large  size))  which cancels the predicates triggered by x\xe2\u20ac\u2122s POS tag.', 'Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00060', '(95%-conf.int.', '0.00060', '-', '0.00060)']
['system', 'ROUGE-S*', 'Average_P:', '0.00667', '(95%-conf.int.', '0.00667', '-', '0.00667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00110', '(95%-conf.int.', '0.00110', '-', '0.00110)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3321', 'P:300', 'F:2']
['This bootstrapping behavior occurs naturally: The &#8220;easy&#8221; examples are processed first, where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.']
['Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.', 'In each dataset  each sentence x is annotated with a Prolog logical form  which we use only to evaluate and get an answer y.', 'Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.', 'Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.', 'Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.']
['system', 'ROUGE-S*', 'Average_R:', '0.00212', '(95%-conf.int.', '0.00212', '-', '0.00212)']
['system', 'ROUGE-S*', 'Average_P:', '0.01905', '(95%-conf.int.', '0.01905', '-', '0.01905)']
['system', 'ROUGE-S*', 'Average_F:', '0.00381', '(95%-conf.int.', '0.00381', '-', '0.00381)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:210', 'F:4']
['This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.']
['Inspecting the final parameters calculus formulae.', u'As another example  w(average) = {(S  \xafx) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.', 'The basic version of DCS described thus far handles a core subset of language.', u'This evaluation is done with respect to a world w. Recall that a world w maps each predicate p \u2208 P to a set of tuples w(p).', 'Which one should we use?']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:190', 'F:0']
['The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
[u'After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy p\u03b8(T)(y  |x  z \u2208 \u02dcZL \u03b8(T)).', u'Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.', 'We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.', 'The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).', 'The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.05738', '(95%-conf.int.', '0.05738', '-', '0.05738)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.10853', '(95%-conf.int.', '0.10853', '-', '0.10853)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:105', 'F:105']
['We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs.']
['In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.', 'It suffices to define Xi(d) for a single column i.', u'Formally  p\u03b8(z  |x) \u221d e\u03c6(x z)T\u03b8  where \u03b8 and \u03c6(x  z) are parameter and feature vectors  respectively.', 'The combination rules are encoded in the tems  despite using no annotated logical forms.', 'On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.']
['system', 'ROUGE-S*', 'Average_R:', '0.00405', '(95%-conf.int.', '0.00405', '-', '0.00405)']
['system', 'ROUGE-S*', 'Average_P:', '0.04545', '(95%-conf.int.', '0.04545', '-', '0.04545)']
['system', 'ROUGE-S*', 'Average_F:', '0.00743', '(95%-conf.int.', '0.00743', '-', '0.00743)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:66', 'F:3']
['The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
[u'Extending this notation to denotations  let (hA; \u03b1ii[i] = hh{ai : a \u2208 A}; \u03b1iii.', u'Let \u02dcZL \u03b8(x) be this approximation of ZL(x).', 'The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).', u"Formally  let \u02dcO(\u03b8  \u03b8') be the objective function O(\u03b8) with ZL(x) \u02dcZL \u03b8I(x).", u"The stores are also concatenated (\u03b1 + \u03b1')."]
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:406', 'P:105', 'F:0']
['Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.']
['5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation  DCS  which offers a new perspective lexical semantics.', 'We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.', 'Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.', 'The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).', 'We trained our model using an EM-like algorithm (Section 3) on two benchmarks  GEO and JOBS (Section 4).']
['system', 'ROUGE-S*', 'Average_R:', '0.00093', '(95%-conf.int.', '0.00093', '-', '0.00093)']
['system', 'ROUGE-S*', 'Average_P:', '0.01000', '(95%-conf.int.', '0.01000', '-', '0.01000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00169', '(95%-conf.int.', '0.00169', '-', '0.00169)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3240', 'P:300', 'F:3']
['Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms, we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus, and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.']
[u'After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy p\u03b8(T)(y  |x  z \u2208 \u02dcZL \u03b8(T)).', 'We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.', 'Think of d as consisting of n columns  one for each active node according to a pre-order traversal of z.', u'Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  \u2212(i  \xf8)]{\u03b11 = \xf8}.', u'Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.']
['system', 'ROUGE-S*', 'Average_R:', '0.00784', '(95%-conf.int.', '0.00784', '-', '0.00784)']
['system', 'ROUGE-S*', 'Average_P:', '0.01587', '(95%-conf.int.', '0.01587', '-', '0.01587)']
['system', 'ROUGE-S*', 'Average_F:', '0.01050', '(95%-conf.int.', '0.01050', '-', '0.01050)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:630', 'F:10']
['We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.']
[u'Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.', u'After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy p\u03b8(T)(y  |x  z \u2208 \u02dcZL \u03b8(T)).', 'We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.', 'The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).', 'The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.02568', '(95%-conf.int.', '0.02568', '-', '0.02568)']
['system', 'ROUGE-S*', 'Average_P:', '0.27485', '(95%-conf.int.', '0.27485', '-', '0.27485)']
['system', 'ROUGE-S*', 'Average_F:', '0.04698', '(95%-conf.int.', '0.04698', '-', '0.04698)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:171', 'F:47']
0.133071816972 0.00975181809317 0.0178063634745





input/ref/Task1/P08-1028_sweta.csv
input/res/Task1/P08-1028.csv
parsing: input/ref/Task1/P08-1028_sweta.csv
<S sid="65" ssid="13">So, if we assume that only the ith components of u and v contribute to the ith component of p, that these components are not dependent on i, and that the function is symmetric with regard to the interchange of u and v, we obtain a simpler instantiation of an additive model: Analogously, under the same assumptions, we obtain the following simpler multiplicative model: only the ith components of u and v contribute to the ith component of p. Another class of models can be derived by relaxing this constraint.</S>
original cit marker offset is 0
new cit marker offset is 0



["65'"]
65'
['65']
parsed_discourse_facet ['method_citation']
 <S sid="75" ssid="23">An extreme form of this differential in the contribution of constituents is where one of the vectors, say u, contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic, however it can serve as a simple baseline against which to compare more sophisticated models.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["42'"]
42'
['42']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="17">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="195" ssid="7">The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately.</S>
original cit marker offset is 0
new cit marker offset is 0



["195'"]
195'
['195']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["57'"]
57'
['57']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["190'"]
190'
['190']
parsed_discourse_facet ['method_citation']
 <S sid="29" ssid="2">While neural networks can readily represent single distinct objects, in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>
original cit marker offset is 0
new cit marker offset is 0



["29'"]
29'
['29']
parsed_discourse_facet ['method_citation']
    <S sid="38" ssid="11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="12">Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="163" ssid="76">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["163'"]
163'
['163']
parsed_discourse_facet ['method_citation']
<S sid="185" ssid="19">The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.</S>
original cit marker offset is 0
new cit marker offset is 0



["185'"]
185'
['185']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["190'"]
190'
['190']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1028.csv
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="140" ssid = "53">Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid ="40" ssid = "13">Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.</S><S sid ="155" ssid = "68">Secondly  we optimized the weightings in the combined model (11) with a similar grid search over its three parameters.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'140'", "'51'", "'40'", "'155'"]
'194'
'140'
'51'
'40'
'155'
['194', '140', '51', '40', '155']
parsed_discourse_facet ['results_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="58" ssid = "6">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="10" ssid = "6">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'58'", "'59'", "'10'", "'51'"]
'194'
'58'
'59'
'10'
'51'
['194', '58', '59', '10', '51']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="38" ssid = "11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S><S sid ="141" ssid = "54">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S><S sid ="36" ssid = "9">To overcome this problem  other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'194'", "'38'", "'141'", "'36'"]
'108'
'194'
'38'
'141'
'36'
['108', '194', '38', '141', '36']
parsed_discourse_facet ['method_citation']
<S sid ="160" ssid = "73">Specifically  m was set to 20 and m to 1.</S><S sid ="156" ssid = "69">This yielded a weighted sum consisting of 95% verb  0% noun and 5% of their multiplicative combination.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="190" ssid = "2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid ="117" ssid = "30">This yielded a reduced set of experimental items (120 in total) consisting of 15 reference verbs  each with 4 nouns  and 2 landmarks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'", "'156'", "'34'", "'190'", "'117'"]
'160'
'156'
'34'
'190'
'117'
['160', '156', '34', '190', '117']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">Vector-based Models of Semantic Composition</S><S sid ="36" ssid = "9">To overcome this problem  other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid ="142" ssid = "55">We experimented with a variety of dimensions (ranging from 50 to 500 000)  vector component definitions (e.g.  pointwise mutual information or log likelihood ratio) and similarity measures (e.g.  cosine or confusion probability).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'36'", "'51'", "'142'", "'194'"]
'0'
'36'
'51'
'142'
'194'
['0', '36', '51', '142', '194']
parsed_discourse_facet ['method_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid ="58" ssid = "6">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S><S sid ="201" ssid = "13">We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al.  1988) and inductive inference (Heit and Rubinstein  1994).</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'59'", "'51'", "'58'", "'201'"]
'194'
'59'
'51'
'58'
'201'
['194', '59', '51', '58', '201']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "3">For the hierarchical structure of natural language this binding problem becomes particularly acute.</S><S sid ="96" ssid = "9">Any adequate model of composition must be able to represent argument-verb meaning.</S><S sid ="1" ssid = "1">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid ="24" ssid = "20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S><S sid ="44" ssid = "17">For example  assuming that individual words are represented by vectors  we can compute the meaning of a sentence by taking their mean (Foltz et al.  1998; Landauer and Dumais  1997).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'96'", "'1'", "'24'", "'44'"]
'30'
'96'
'1'
'24'
'44'
['30', '96', '1', '24', '44']
parsed_discourse_facet ['method_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="37" ssid = "10">Holographic reduced representations (Plate  1991) are one implementation of this idea where the tensor product is projected back onto the space of its components.</S><S sid ="141" ssid = "54">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'3'", "'101'", "'37'", "'141'"]
'194'
'3'
'101'
'37'
'141'
['194', '3', '101', '37', '141']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid ="140" ssid = "53">Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.</S><S sid ="144" ssid = "57">We obtained best results with a model using a context window of five words on either side of the target word  the cosine measure  and 2 000 vector components.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="37" ssid = "10">Holographic reduced representations (Plate  1991) are one implementation of this idea where the tensor product is projected back onto the space of its components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'140'", "'144'", "'194'", "'37'"]
'39'
'140'
'144'
'194'
'37'
['39', '140', '144', '194', '37']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="168" ssid = "2">These included three additive models  i.e.  simple addition (equation (5)  Add)  weighted addition (equation (7)  WeightAdd)  and Kintsch’s (2001) model (equation (10)  Kintsch)  a multiplicative model (equation (6)  Multiply)  and also a model which combines multiplication with addition (equation (11)  Combined).</S><S sid ="35" ssid = "8">The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely  the tensor product has dimensionality m x n).</S><S sid ="144" ssid = "57">We obtained best results with a model using a context window of five words on either side of the target word  the cosine measure  and 2 000 vector components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'194'", "'168'", "'35'", "'144'"]
'108'
'194'
'168'
'35'
'144'
['108', '194', '168', '35', '144']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid ="117" ssid = "30">This yielded a reduced set of experimental items (120 in total) consisting of 15 reference verbs  each with 4 nouns  and 2 landmarks.</S><S sid ="35" ssid = "8">The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely  the tensor product has dimensionality m x n).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'117'", "'35'", "'194'", "'108'"]
'39'
'117'
'35'
'194'
'108'
['39', '117', '35', '194', '108']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="153" ssid = "66">Specifically  we considered eleven models  varying in their weightings  in steps of 10%  from 100% noun through 50% of both verb and noun to 100% verb.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'101'", "'3'", "'153'", "'194'"]
'108'
'101'
'3'
'153'
'194'
['108', '101', '3', '153', '194']
parsed_discourse_facet ['method_citation']
<S sid ="148" ssid = "61">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S><S sid ="177" ssid = "11">The difference between High and Low similarity values estimated by these models are statistically significant (p < 0.01 using the Wilcoxon rank sum test).</S><S sid ="4" ssid = "4">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S><S sid ="127" ssid = "40">Analysis of Similarity Ratings The reliability of the collected judgments is important for our evaluation experiments; we therefore performed several tests to validate the quality of the ratings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'148'", "'164'", "'177'", "'4'", "'127'"]
'148'
'164'
'177'
'4'
'127'
['148', '164', '177', '4', '127']
parsed_discourse_facet ['results_citation']
<S sid ="115" ssid = "28">Each cell recorded the number of times our subjects selected the landmark as compatible with the noun or not.</S><S sid ="131" ssid = "44">A Wilcoxon rank sum test confirmed that the difference is statistically significant (p < 0.01).</S><S sid ="147" ssid = "60">This configuration gave high correlations with the WordSim353 similarity judgments using the cosine measure.</S><S sid ="173" ssid = "7">Model similarities have been estimated using cosine which ranges from 0 to 1  whereas our subjects rated the sentences on a scale from 1 to 7.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'131'", "'147'", "'173'", "'164'"]
'115'
'131'
'147'
'173'
'164'
['115', '131', '147', '173', '164']
parsed_discourse_facet ['method_citation']
<S sid ="140" ssid = "53">Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.</S><S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="2" ssid = "2">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S><S sid ="139" ssid = "52">The semantic space we used in our experiments was built on a lemmatised version of the BNC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'3'", "'101'", "'2'", "'139'"]
'140'
'3'
'101'
'2'
'139'
['140', '3', '101', '2', '139']
parsed_discourse_facet ['method_citation']
<S sid ="115" ssid = "28">Each cell recorded the number of times our subjects selected the landmark as compatible with the noun or not.</S><S sid ="147" ssid = "60">This configuration gave high correlations with the WordSim353 similarity judgments using the cosine measure.</S><S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid ="173" ssid = "7">Model similarities have been estimated using cosine which ranges from 0 to 1  whereas our subjects rated the sentences on a scale from 1 to 7.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'147'", "'39'", "'173'", "'164'"]
'115'
'147'
'39'
'173'
'164'
['115', '147', '39', '173', '164']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S><S sid ="153" ssid = "66">Specifically  we considered eleven models  varying in their weightings  in steps of 10%  from 100% noun through 50% of both verb and noun to 100% verb.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="183" ssid = "17">The weighted additive model (p = 0.09) is not significantly different from the baseline either or Kintsch (2001) (p = 0.09).</S><S sid ="40" ssid = "13">Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'153'", "'194'", "'183'", "'40'"]
'38'
'153'
'194'
'183'
'40'
['38', '153', '194', '183', '40']
parsed_discourse_facet ['method_citation']
['An extreme form of this differential in the contribution of constituents is where one of the vectors, say u, contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic, however it can serve as a simple baseline against which to compare more sophisticated models.']
['Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.', 'Secondly  we optimized the weightings in the combined model (11) with a similar grid search over its three parameters.', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).', 'Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.', 'Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.']
['system', 'ROUGE-S*', 'Average_R:', '0.00307', '(95%-conf.int.', '0.00307', '-', '0.00307)']
['system', 'ROUGE-S*', 'Average_P:', '0.03509', '(95%-conf.int.', '0.03509', '-', '0.03509)']
['system', 'ROUGE-S*', 'Average_F:', '0.00565', '(95%-conf.int.', '0.00565', '-', '0.00565)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:171', 'F:6']
['The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately.']
['This yielded a reduced set of experimental items (120 in total) consisting of 15 reference verbs  each with 4 nouns  and 2 landmarks.', 'Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.', 'Specifically  m was set to 20 and m to 1.', 'We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.', 'This yielded a weighted sum consisting of 95% verb  0% noun and 5% of their multiplicative combination.']
['system', 'ROUGE-S*', 'Average_R:', '0.00185', '(95%-conf.int.', '0.00185', '-', '0.00185)']
['system', 'ROUGE-S*', 'Average_P:', '0.02564', '(95%-conf.int.', '0.02564', '-', '0.02564)']
['system', 'ROUGE-S*', 'Average_F:', '0.00345', '(95%-conf.int.', '0.00345', '-', '0.00345)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:78', 'F:2']
['We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).']
['Each cell recorded the number of times our subjects selected the landmark as compatible with the noun or not.', 'A Wilcoxon rank sum test confirmed that the difference is statistically significant (p ']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:136', 'P:55', 'F:0']
['We present a general framework for vector-based composition which allows us to consider different classes of models.']
['Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.', 'Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).', 'To overcome this problem  other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.', 'We experimented with a variety of dimensions (ranging from 50 to 500 000)  vector component definitions (e.g.  pointwise mutual information or log likelihood ratio) and similarity measures (e.g.  cosine or confusion probability).']
['system', 'ROUGE-S*', 'Average_R:', '0.00420', '(95%-conf.int.', '0.00420', '-', '0.00420)']
['system', 'ROUGE-S*', 'Average_P:', '0.32143', '(95%-conf.int.', '0.32143', '-', '0.32143)']
['system', 'ROUGE-S*', 'Average_F:', '0.00828', '(95%-conf.int.', '0.00828', '-', '0.00828)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:28', 'F:9']
['We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.', 'The compression is achieved by summing along the transdiagonal elements of the tensor product.', 'Holographic reduced representations (Plate  1991) are one implementation of this idea where the tensor product is projected back onto the space of its components.', 'We obtained best results with a model using a context window of five words on either side of the target word  the cosine measure  and 2 000 vector components.', 'Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.']
['system', 'ROUGE-S*', 'Average_R:', '0.00529', '(95%-conf.int.', '0.00529', '-', '0.00529)']
['system', 'ROUGE-S*', 'Average_P:', '0.27778', '(95%-conf.int.', '0.27778', '-', '0.27778)']
['system', 'ROUGE-S*', 'Average_F:', '0.01038', '(95%-conf.int.', '0.01038', '-', '0.01038)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:36', 'F:10']
['We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.', 'Specifically  we considered eleven models  varying in their weightings  in steps of 10%  from 100% noun through 50% of both verb and noun to 100% verb.', 'The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.', 'Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.', 'The weighted additive model (p = 0.09) is not significantly different from the baseline either or Kintsch (2001) (p = 0.09).']
['system', 'ROUGE-S*', 'Average_R:', '0.00694', '(95%-conf.int.', '0.00694', '-', '0.00694)']
['system', 'ROUGE-S*', 'Average_P:', '0.38889', '(95%-conf.int.', '0.38889', '-', '0.38889)']
['system', 'ROUGE-S*', 'Average_F:', '0.01365', '(95%-conf.int.', '0.01365', '-', '0.01365)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:36', 'F:14']
['The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.']
['Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).', 'Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.', 'The compression is achieved by summing along the transdiagonal elements of the tensor product.', 'This yielded a reduced set of experimental items (120 in total) consisting of 15 reference verbs  each with 4 nouns  and 2 landmarks.', u'The tensor product u \xc2\xae v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely  the tensor product has dimensionality m x n).']
['system', 'ROUGE-S*', 'Average_R:', '0.00168', '(95%-conf.int.', '0.00168', '-', '0.00168)']
['system', 'ROUGE-S*', 'Average_P:', '0.10909', '(95%-conf.int.', '0.10909', '-', '0.10909)']
['system', 'ROUGE-S*', 'Average_F:', '0.00331', '(95%-conf.int.', '0.00331', '-', '0.00331)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3570', 'P:55', 'F:6']
['This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.']
['Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).', 'It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.', 'We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.', 'Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).']
['system', 'ROUGE-S*', 'Average_R:', '0.00111', '(95%-conf.int.', '0.00111', '-', '0.00111)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00218', '(95%-conf.int.', '0.00218', '-', '0.00218)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2701', 'P:45', 'F:3']
['Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.']
['This paper proposes a framework for representing the meaning of phrases and sentences in vector space.', 'In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.', 'For example  assuming that individual words are represented by vectors  we can compute the meaning of a sentence by taking their mean (Foltz et al.  1998; Landauer and Dumais  1997).', 'For the hierarchical structure of natural language this binding problem becomes particularly acute.', 'Any adequate model of composition must be able to represent argument-verb meaning.']
['system', 'ROUGE-S*', 'Average_R:', '0.01152', '(95%-conf.int.', '0.01152', '-', '0.01152)']
['system', 'ROUGE-S*', 'Average_P:', '0.09559', '(95%-conf.int.', '0.09559', '-', '0.09559)']
['system', 'ROUGE-S*', 'Average_F:', '0.02057', '(95%-conf.int.', '0.02057', '-', '0.02057)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:136', 'F:13']
0.146686665037 0.0039622221782 0.00749666658337





input/ref/Task1/D09-1092_vardha.csv
input/res/Task1/D09-1092.csv
parsing: input/ref/Task1/D09-1092_vardha.csv
<S sid="17" ssid="13">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
 <S sid="20" ssid="16">We also explore how the characteristics of different languages affect topic model performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
    <S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
   <S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'"]
'55'
['55']
parsed_discourse_facet ['method_citation']
 sid="9" ssid="5">In this paper, we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
Error in Reference Offset
<S sid="118" ssid="67">We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
 <S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'"]
'55'
['55']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="51">In contrast, PLTM assigns a significant number of tokens to almost all 800 topics, in very similar proportions in both languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'"]
'102'
['102']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="4">This is unlike LDA, in which each document is assumed to have its own document-specific distribution over topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'"]
'38'
['38']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="105">We use both Jensen-Shannon divergence and cosine distance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'"]
'156'
['156']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="2">Each tuple is a set of documents that are loosely equivalent to each other, but written in different languages, e.g., corresponding Wikipedia articles in French, English and German.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
  <S sid="170" ssid="4">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
    <S sid="29" ssid="5">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
  <S sid="170" ssid="4">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="168" ssid="2">However, the growth of the web, and in particular Wikipedia, has made comparable text corpora &#8211; documents that are topically similar but are not direct translations of one another &#8211; considerably more abundant than true parallel corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'168'"]
'168'
['168']
parsed_discourse_facet ['method_citation']
    <S sid="29" ssid="5">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
 <S sid="110" ssid="59">Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'"]
'110'
['110']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'4'", "'134'", "'54'", "'35'"]
'131'
'4'
'134'
'54'
'35'
['131', '4', '134', '54', '35']
parsed_discourse_facet ['results_citation']
<S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="6" ssid = "2">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'148'", "'146'", "'4'", "'131'", "'6'"]
'148'
'146'
'4'
'131'
'6'
['148', '146', '4', '131', '6']
parsed_discourse_facet ['method_citation']
<S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of “objective.” The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S>
original cit marker offset is 0
new cit marker offset is 0



["'146'", "'131'", "'148'", "'25'", "'69'"]
'146'
'131'
'148'
'25'
'69'
['146', '131', '148', '25', '69']
parsed_discourse_facet ['method_citation']
<S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="195" ssid = "4">Additionally  PLTM can support the creation of bilingual lexica for low resource language pairs  providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'146'", "'195'", "'4'", "'19'"]
'25'
'146'
'195'
'4'
'19'
['25', '146', '195', '4', '19']
parsed_discourse_facet ['method_citation']
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="47" ssid = "13">We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="86" ssid = "35">Given a set of training document tuples  PLTM can be used to obtain posterior estimates of Φ'  ...   ΦL and αm.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'47'", "'148'", "'86'", "'35'"]
'131'
'47'
'148'
'86'
'35'
['131', '47', '148', '86', '35']
parsed_discourse_facet ['method_citation']
<S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S><S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="182" ssid = "16">As with EuroParl  we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.</S><S sid ="37" ssid = "3">PLTM assumes that the documents in a tuple share the same tuple-specific distribution over topics.</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'161'", "'59'", "'182'", "'37'", "'91'"]
'161'
'59'
'182'
'37'
'91'
['161', '59', '182', '37', '91']
parsed_discourse_facet ['method_citation']
<S sid ="158" ssid = "107">Results averaged over all query/target language pairs are shown in figure 7 for Jensen-Shannon divergence.</S><S sid ="187" ssid = "21">To demonstrate the wide variation in topics  we calculated the proportion of tokens in each language assigned to each topic.</S><S sid ="95" ssid = "44">Additionally  the predictive ability of PLTM is consistently slightly worse than that of (monolingual) LDA.</S><S sid ="72" ssid = "21">To answer this question  we compute the posterior probability of each topic in each tuple under the trained model.</S><S sid ="154" ssid = "103">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al.  2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'", "'187'", "'95'", "'72'", "'154'"]
'158'
'187'
'95'
'72'
'154'
['158', '187', '95', '72', '154']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'131'", "'35'", "'54'", "'91'"]
'134'
'131'
'35'
'54'
'91'
['134', '131', '35', '54', '91']
parsed_discourse_facet ['method_citation']
<S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="86" ssid = "35">Given a set of training document tuples  PLTM can be used to obtain posterior estimates of Φ'  ...   ΦL and αm.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S><S sid ="46" ssid = "12">  ΦL and αm from P(Φ1  ...   ΦL  αm  |W'  β) or by evaluating a point estimate.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'", "'86'", "'131'", "'91'", "'46'"]
'35'
'86'
'131'
'91'
'46'
['35', '86', '131', '91', '46']
parsed_discourse_facet ['method_citation']
<S sid ="67" ssid = "16">(Interestingly  all languages except Greek and Finnish use closely related words for “youth” or “young” in a separate topic.)</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S><S sid ="36" ssid = "2">Each tuple is a set of documents that are loosely equivalent to each other  but written in different languages  e.g.  corresponding Wikipedia articles in French  English and German.</S><S sid ="65" ssid = "14">This topic provides an illustration of the variation in technical terminology captured by PLTM  including the wide array of acronyms used by different languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'54'", "'38'", "'36'", "'65'"]
'67'
'54'
'38'
'36'
'65'
['67', '54', '38', '36', '65']
parsed_discourse_facet ['method_citation']
<S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="143" ssid = "92">We also do not count morphological variants: the model finds EN “rules” and DE “vorschriften ” but the lexicon contains only “rule” and “vorschrift.” Results remain strong as we increase K. With K = 3  T = 800  1349 of the 7200 candidate pairs for Spanish appeared in the lexicon. topic in different languages translations of each other?</S><S sid ="39" ssid = "5">Additionally  PLTM assumes that each “topic” consists of a set of discrete distributions over words—one for each language l = 1  ...   L. In other words  rather than using a single set of topics Φ = {φ1  ...   φT}  as in LDA  there are L sets of language-specific topics  Φ1  ...   ΦL  each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter βl.</S><S sid ="78" ssid = "27">Although the model does not distinguish between topic assignment variables within a given document tuple (so it is technically incorrect to speak of different posterior distributions over topics for different documents in a given tuple)  we can nevertheless divide topic assignment variables between languages and use them to estimate a Dirichlet-multinomial posterior distribution for each language in each tuple.</S><S sid ="40" ssid = "6">Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter α and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl ∼ P(wl  |zl Φl) = 11n φlwl |zl .</S>
original cit marker offset is 0
new cit marker offset is 0



["'148'", "'143'", "'39'", "'78'", "'40'"]
'148'
'143'
'39'
'78'
'40'
['148', '143', '39', '78', '40']
parsed_discourse_facet ['method_citation']
<S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="68" ssid = "17">The third topic demonstrates differences in inflectional variation.</S><S sid ="193" ssid = "2">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="129" ssid = "78">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'", "'68'", "'193'", "'25'", "'129'"]
'52'
'68'
'193'
'25'
'129'
['52', '68', '193', '25', '129']
parsed_discourse_facet ['method_citation']
<S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="92" ssid = "41">We perform five estimation runs for each document and then calculate standard errors using a bootstrap method.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'43'", "'52'", "'25'", "'92'"]
'147'
'43'
'52'
'25'
'92'
['147', '43', '52', '25', '92']
parsed_discourse_facet ['results_citation']
<S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="47" ssid = "13">We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'43'", "'148'", "'47'", "'131'"]
'25'
'43'
'148'
'47'
'131'
['25', '43', '148', '47', '131']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="55" ssid = "4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish  German  Greek  English  Spanish  Finnish  French  Italian  Dutch  Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'35'", "'134'", "'131'", "'55'"]
'4'
'35'
'134'
'131'
'55'
['4', '35', '134', '131', '55']
parsed_discourse_facet ['method_citation']
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'134'", "'4'", "'35'", "'25'"]
'131'
'134'
'4'
'35'
'25'
['131', '134', '4', '35', '25']
parsed_discourse_facet ['method_citation']
<S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S><S sid ="164" ssid = "113">Results vary by language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'161'", "'134'", "'91'", "'164'"]
'59'
'161'
'134'
'91'
'164'
['59', '161', '134', '91', '164']
parsed_discourse_facet ['method_citation']
<S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="195" ssid = "4">Additionally  PLTM can support the creation of bilingual lexica for low resource language pairs  providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S><S sid ="24" ssid = "20">By linking topics across languages  polylingual topic models can increase cross-cultural understanding by providing readers with the ability to characterize the contents of collections in unfamiliar languages and identify trends in topic prevalence.</S><S sid ="83" ssid = "32">The vast majority of divergences are relatively low (1.0 indicates no overlap in topics between languages in a given document tuple) indicating that  for each tuple  the model is not simply assigning all tokens in a particular language to a single topic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'146'", "'195'", "'24'", "'83'"]
'25'
'146'
'195'
'24'
'83'
['25', '146', '195', '24', '83']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid ="47" ssid = "13">We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="120" ssid = "69">From the results in figure 4  we know that leaving all document tuples intact should result in a mean JS divergence of less than 0.1.</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'47'", "'25'", "'120'", "'148'"]
'43'
'47'
'25'
'120'
'148'
['43', '47', '25', '120', '148']
parsed_discourse_facet ['aim_citation', 'results_citation']
['We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).']
['In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.', 'Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).', 'Additionally  PLTM can support the creation of bilingual lexica for low resource language pairs  providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.', u'We explore the model\xe2\u20ac\u2122s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.', 'We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.']
['system', 'ROUGE-S*', 'Average_R:', '0.00301', '(95%-conf.int.', '0.00301', '-', '0.00301)']
['system', 'ROUGE-S*', 'Average_P:', '0.09167', '(95%-conf.int.', '0.09167', '-', '0.09167)']
['system', 'ROUGE-S*', 'Average_F:', '0.00583', '(95%-conf.int.', '0.00583', '-', '0.00583)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3655', 'P:120', 'F:11']
['We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.']
['In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.', 'Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', 'To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.', u'English and the Romance languages use only singular and plural versions of \u201cobjective.\u201d The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.']
['system', 'ROUGE-S*', 'Average_R:', '0.00147', '(95%-conf.int.', '0.00147', '-', '0.00147)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.00288', '(95%-conf.int.', '0.00288', '-', '0.00288)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4095', 'P:78', 'F:6']
['Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.']
['From the results in figure 4  we know that leaving all document tuples intact should result in a mean JS divergence of less than 0.1.', u'We take the latter approach  and use the MAP estimate for \u03b1m and the predictive distributions over words for \u03a61  .', u'These tasks can either be accomplished by averaging over samples of \u03a61  .', 'To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.', 'Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).']
['system', 'ROUGE-S*', 'Average_R:', '0.00055', '(95%-conf.int.', '0.00055', '-', '0.00055)']
['system', 'ROUGE-S*', 'Average_P:', '0.00654', '(95%-conf.int.', '0.00654', '-', '0.00654)']
['system', 'ROUGE-S*', 'Average_F:', '0.00101', '(95%-conf.int.', '0.00101', '-', '0.00101)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:153', 'F:1']
['We also explore how the characteristics of different languages affect topic model performance.']
['In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', u'We explore the model\u2019s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.', 'To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.', 'Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).']
['system', 'ROUGE-S*', 'Average_R:', '0.00173', '(95%-conf.int.', '0.00173', '-', '0.00173)']
['system', 'ROUGE-S*', 'Average_P:', '0.47619', '(95%-conf.int.', '0.47619', '-', '0.47619)']
['system', 'ROUGE-S*', 'Average_F:', '0.00345', '(95%-conf.int.', '0.00345', '-', '0.00345)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5778', 'P:21', 'F:10']
['We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.']
['We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al.  2009).', 'Results averaged over all query/target language pairs are shown in figure 7 for Jensen-Shannon divergence.', 'Additionally  the predictive ability of PLTM is consistently slightly worse than that of (monolingual) LDA.', 'To demonstrate the wide variation in topics  we calculated the proportion of tokens in each language assigned to each topic.', 'To answer this question  we compute the posterior probability of each topic in each tuple under the trained model.']
['system', 'ROUGE-S*', 'Average_R:', '0.01504', '(95%-conf.int.', '0.01504', '-', '0.01504)']
['system', 'ROUGE-S*', 'Average_P:', '0.26374', '(95%-conf.int.', '0.26374', '-', '0.26374)']
['system', 'ROUGE-S*', 'Average_F:', '0.02845', '(95%-conf.int.', '0.02845', '-', '0.02845)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:91', 'F:24']
['The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.']
[u'We take the latter approach  and use the MAP estimate for \u03b1m and the predictive distributions over words for \u03a61  .', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', u"Given a set of training document tuples  PLTM can be used to obtain posterior estimates of \u03a6'  ...   \u03a6L and \u03b1m.", 'The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.', 'To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.']
['system', 'ROUGE-S*', 'Average_R:', '0.00043', '(95%-conf.int.', '0.00043', '-', '0.00043)']
['system', 'ROUGE-S*', 'Average_P:', '0.00526', '(95%-conf.int.', '0.00526', '-', '0.00526)']
['system', 'ROUGE-S*', 'Average_F:', '0.00079', '(95%-conf.int.', '0.00079', '-', '0.00079)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:190', 'F:1']
['First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.']
['Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', u'These tasks can either be accomplished by averaging over samples of \u03a61  .', 'To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.', u'We take the latter approach  and use the MAP estimate for \u03b1m and the predictive distributions over words for \u03a61  .']
['system', 'ROUGE-S*', 'Average_R:', '0.00793', '(95%-conf.int.', '0.00793', '-', '0.00793)']
['system', 'ROUGE-S*', 'Average_P:', '0.18681', '(95%-conf.int.', '0.18681', '-', '0.18681)']
['system', 'ROUGE-S*', 'Average_F:', '0.01521', '(95%-conf.int.', '0.01521', '-', '0.01521)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:91', 'F:17']
['A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.']
['The EuroParl corpus consists of parallel texts in eleven western European languages: Danish  German  Greek  English  Spanish  Finnish  French  Italian  Dutch  Portuguese and Swedish.', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', u'We explore the model\u2019s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.', 'The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.', 'Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.']
['system', 'ROUGE-S*', 'Average_R:', '0.00260', '(95%-conf.int.', '0.00260', '-', '0.00260)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00500', '(95%-conf.int.', '0.00500', '-', '0.00500)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:120', 'F:8']
['We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.']
['Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', u'We explore the model\u2019s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.', 'The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.', 'Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.']
['system', 'ROUGE-S*', 'Average_R:', '0.00430', '(95%-conf.int.', '0.00430', '-', '0.00430)']
['system', 'ROUGE-S*', 'Average_P:', '0.20000', '(95%-conf.int.', '0.20000', '-', '0.20000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00843', '(95%-conf.int.', '0.00843', '-', '0.00843)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:55', 'F:11']
['However, the growth of the web, and in particular Wikipedia, has made comparable text corpora &#8211; documents that are topically similar but are not direct translations of one another &#8211; considerably more abundant than true parallel corpora.']
[u'We use the \u201cleft-to-right\u201d method of (Wallach et al.  2009).', 'Results vary by language.', 'The remaining collection consists of over 121 million words.', 'As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.', 'Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.']
['system', 'ROUGE-S*', 'Average_R:', '0.00202', '(95%-conf.int.', '0.00202', '-', '0.00202)']
['system', 'ROUGE-S*', 'Average_P:', '0.00585', '(95%-conf.int.', '0.00585', '-', '0.00585)']
['system', 'ROUGE-S*', 'Average_F:', '0.00300', '(95%-conf.int.', '0.00300', '-', '0.00300)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:171', 'F:1']
0.13796499862 0.00390799996092 0.00740499992595





input/ref/Task1/P08-1102_sweta.csv
input/res/Task1/P08-1102.csv
parsing: input/ref/Task1/P08-1102_sweta.csv
<S sid="21" ssid="17">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="24">A subsequence of boundary-POS labelling result indicates a word with POS t only if the boundary tag sequence composed of its boundary part conforms to s or bm*e style, and all POS tags in its POS part equal to t. For example, a tag sequence b NN m NN e NN represents a threecharacter word with POS tag NN.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="43" ssid="15">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["43'"]
43'
['43']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["92'"]
92'
['92']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="5">To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&amp;T).</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="5">In following subsections, we describe the feature templates and the perceptron training algorithm.</S>
    <S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["33'", "'34'"]
33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="4">By maintaining a stack of size N at each position i of the sequence, we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S>
original cit marker offset is 0
new cit marker offset is 0



["79'"]
79'
['79']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="7">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["96'"]
96'
['96']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="16" ssid="12">Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="7">To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="22" ssid = "18">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here  Ci (i = L.n) denotes Chinese character  ti (i = L.m) denotes POS tag  and Cl:r (l < r) denotes character sequence ranges from Cl to Cr.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="104" ssid = "15">According to the usual practice in syntactic analysis  we choose chapters 1 − 260 (18074 sentences) as training set  chapter 271 − 300 (348 sentences) as test set and chapter 301 − 325 (350 sentences) as development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'", "'22'", "'16'", "'94'", "'104'"]
'87'
'22'
'16'
'94'
'104'
['87', '22', '16', '94', '104']
parsed_discourse_facet ['results_citation']
<S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'47'", "'48'", "'32'", "'133'"]
'69'
'47'
'48'
'32'
'133'
['69', '47', '48', '32', '133']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="99" ssid = "10">Then we trained LEX on each of the four corpora for 7 iterations.</S><S sid ="30" ssid = "2">It has comparable performance to CRFs  while with much faster training.</S><S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="122" ssid = "33">Another important feature is the labelling model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'99'", "'30'", "'101'", "'122'"]
'68'
'99'
'30'
'101'
'122'
['68', '99', '30', '101', '122']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="30" ssid = "2">It has comparable performance to CRFs  while with much faster training.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'87'", "'31'", "'94'", "'30'"]
'101'
'87'
'31'
'94'
'30'
['101', '87', '31', '94', '30']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="0" ssid = "0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'0'", "'87'", "'69'", "'48'"]
'47'
'0'
'87'
'69'
'48'
['47', '0', '87', '69', '48']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'48'", "'49'", "'133'", "'32'"]
'47'
'48'
'49'
'133'
'32'
['47', '48', '49', '133', '32']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'68'", "'94'", "'87'", "'7'"]
'16'
'68'
'94'
'87'
'7'
['16', '68', '94', '87', '7']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "19">We find that Joint S&T can also improve the segmentation accuracy.</S><S sid ="138" ssid = "1">This work was done while L. H. was visiting CAS/ICT.</S><S sid ="61" ssid = "12">In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.</S><S sid ="90" ssid = "1">We reported results from two set of experiments.</S><S sid ="107" ssid = "18">The evaluation results are shown in Table 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'138'", "'61'", "'90'", "'107'"]
'108'
'138'
'61'
'90'
'107'
['108', '138', '61', '90', '107']
parsed_discourse_facet ['method_citation']
<S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S><S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'133'", "'32'", "'69'", "'49'"]
'48'
'133'
'32'
'69'
'49'
['48', '133', '32', '69', '49']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'7'", "'49'", "'69'", "'16'"]
'47'
'7'
'49'
'69'
'16'
['47', '7', '49', '69', '16']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="54" ssid = "5">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S><S sid ="64" ssid = "15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid ="135" ssid = "6">In addition  all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus  whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'54'", "'64'", "'58'", "'135'"]
'2'
'54'
'64'
'58'
'135'
['2', '54', '64', '58', '135']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "11">Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS  CityU and MSR).</S><S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S><S sid ="121" ssid = "32">Among other features  the 4-gram POS LM plays the most important role  removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'47'", "'134'", "'32'", "'121'"]
'100'
'47'
'134'
'32'
'121'
['100', '47', '134', '32', '121']
parsed_discourse_facet ['method_citation']
<S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="52" ssid = "3">However  such features are generated dynamically during the decoding procedure so that the feature space enlarges much more rapidly.</S><S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="131" ssid = "2">Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.</S><S sid ="135" ssid = "6">In addition  all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus  whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'52'", "'2'", "'131'", "'135'"]
'133'
'52'
'2'
'131'
'135'
['133', '52', '2', '131', '135']
parsed_discourse_facet ['results_citation']
<S sid ="79" ssid = "4">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid ="73" ssid = "24">For instance  if the word w appears N times in training corpus and is labelled as POS t for n times  the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.</S><S sid ="98" ssid = "9">We found that LEX outperforms NON-LEX with a margin of about 0.002 at each iteration  and its learning curve reaches a tableland at iteration 7.</S><S sid ="37" ssid = "9">C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).</S><S sid ="70" ssid = "21">Given a training corpus with POS tags  we can train a word-POS co-occurrence model to approximate the probability that the word sequence of the labelled result co-exists with its corresponding POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'", "'73'", "'98'", "'37'", "'70'"]
'79'
'73'
'98'
'37'
'70'
['79', '73', '98', '37', '70']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="103" ssid = "14">We turned to experiments on CTB 5.0 to test the performance of the cascaded model.</S><S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'87'", "'103'", "'68'", "'101'"]
'16'
'87'
'103'
'68'
'101'
['16', '87', '103', '68', '101']
parsed_discourse_facet ['method_citation']
<S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="22" ssid = "18">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here  Ci (i = L.n) denotes Chinese character  ti (i = L.m) denotes POS tag  and Cl:r (l < r) denotes character sequence ranges from Cl to Cr.</S><S sid ="91" ssid = "2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2  including the Academia Sinica Corpus (AS)  the Hong Kong City University Corpus (CityU)  the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'", "'22'", "'91'", "'45'", "'16'"]
'31'
'22'
'91'
'45'
'16'
['31', '22', '91', '45', '16']
parsed_discourse_facet ['method_citation']
Length 0 input/ref/Task1/P08-1102_sweta.csv
0.0 0.0 0.0





input/ref/Task1/A97-1014_vardha.csv
input/res/Task1/A97-1014.csv
parsing: input/ref/Task1/A97-1014_vardha.csv
<S sid="72" ssid="17">In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
 <S sid="144" ssid="25">We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="4">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
  <S sid="36" ssid="26">As for free word order languages, the following features may cause problems: sition between the two poles.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
 <S sid="24" ssid="14">The typical treebank architecture is as follows: Structures: A context-free backbone is augmented with trace-filler representations of non-local dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
 <S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'"]
'151'
['151']
parsed_discourse_facet ['method_citation']
 <S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
 <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["'167'"]
'167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'"]
'39'
['39']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'"]
'151'
['151']
parsed_discourse_facet ['method_citation']
<S sid="71" ssid="16">However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'71'"]
'71'
['71']
parsed_discourse_facet ['method_citation']
 <S sid="144" ssid="25">We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A97-1014.csv
<S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="81" ssid = "26">During the second annotation stage  the annotation is enriched with information about thematic roles  quantifier scope and anaphoric reference.</S><S sid ="79" ssid = "24">PM stands for morphological particle  a label for German infinitival Z7t and superlative am.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'81'", "'79'", "'137'", "'162'"]
'4'
'81'
'79'
'137'
'162'
['4', '81', '79', '137', '162']
parsed_discourse_facet ['results_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="79" ssid = "24">PM stands for morphological particle  a label for German infinitival Z7t and superlative am.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="140" ssid = "21">For the implementation  we used Tcl/Tk Version 4.1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'4'", "'79'", "'58'", "'140'"]
'80'
'4'
'79'
'58'
'140'
['80', '4', '79', '58', '140']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'4'", "'137'", "'56'", "'149'"]
'80'
'4'
'137'
'56'
'149'
['80', '4', '137', '56', '149']
parsed_discourse_facet ['method_citation']
<S sid ="129" ssid = "10">In the second phase  secondary links and additional structural functions are supported.</S><S sid ="140" ssid = "21">For the implementation  we used Tcl/Tk Version 4.1.</S><S sid ="141" ssid = "22">The corpus is stored in a SQL database.</S><S sid ="116" ssid = "29">This extra marking makes it easy to distinguish between 'normal' and coordinated categories.</S><S sid ="105" ssid = "18">In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'129'", "'140'", "'141'", "'116'", "'105'"]
'129'
'140'
'141'
'116'
'105'
['129', '140', '141', '116', '105']
parsed_discourse_facet ['method_citation']
<S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="128" ssid = "9">In the first phase  the main functionality for building and displaying unordered trees is supplied.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="105" ssid = "18">In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'137'", "'128'", "'56'", "'105'"]
'74'
'137'
'128'
'56'
'105'
['74', '137', '128', '56', '105']
parsed_discourse_facet ['method_citation']
<S sid ="55" ssid = "45">A uniform representation of local and non-local dependencies makes the structure more transparent'.</S><S sid ="53" ssid = "43">A tree meeting these requirements is given below: Adv V NP NP V CPL NP V damn wird ihn Anna erkennen  dais er 'vein!</S><S sid ="120" ssid = "1">The development of linguistically interpreted corpora presents a laborious and time-consuming task.</S><S sid ="65" ssid = "10">The tree resembles traditional constituent structures.</S><S sid ="14" ssid = "4">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'53'", "'120'", "'65'", "'14'"]
'55'
'53'
'120'
'65'
'14'
['55', '53', '120', '65', '14']
parsed_discourse_facet ['method_citation']
<S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'58'", "'80'", "'149'", "'56'"]
'74'
'58'
'80'
'149'
'56'
['74', '58', '80', '149', '56']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'74'", "'56'", "'58'", "'149'"]
'80'
'74'
'56'
'58'
'149'
['80', '74', '56', '58', '149']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'56'", "'58'", "'137'", "'149'"]
'80'
'56'
'58'
'137'
'149'
['80', '56', '58', '137', '149']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.</S><S sid ="88" ssid = "1">As theory-independence is one of our objectives  the annotation scheme incorporates a number of widely accepted linguistic analyses  especially in the area of verbal  adverbial and adjectival syntax.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="164" ssid = "6">As modern linguistics is also becoming more aware of the importance of larger sets of naturally occurring data.  interpreted corpora. are a. valuable resource for theoretical and descriptive linguistic research.</S><S sid ="168" ssid = "10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'88'", "'5'", "'164'", "'168'"]
'3'
'88'
'5'
'164'
'168'
['3', '88', '5', '164', '168']
parsed_discourse_facet ['method_citation']
<S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'137'", "'74'", "'56'", "'149'"]
'58'
'137'
'74'
'56'
'149'
['58', '137', '74', '56', '149']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="101" ssid = "14">All components of this kernel are assigned the label NK and treated as sibling nodes.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'56'", "'101'", "'4'", "'149'"]
'80'
'56'
'101'
'4'
'149'
['80', '56', '101', '4', '149']
parsed_discourse_facet ['method_citation']
<S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'74'", "'58'", "'149'", "'80'"]
'56'
'74'
'58'
'149'
'80'
['56', '74', '58', '149', '80']
parsed_discourse_facet ['results_citation']
<S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="105" ssid = "18">In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'149'", "'56'", "'105'", "'137'", "'74'"]
'149'
'56'
'105'
'137'
'74'
['149', '56', '105', '137', '74']
parsed_discourse_facet ['method_citation']
['These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.']
["A uniform representation of local and non-local dependencies makes the structure more transparent'.", 'The development of linguistically interpreted corpora presents a laborious and time-consuming task.', 'The tree resembles traditional constituent structures.', "A tree meeting these requirements is given below: Adv V NP NP V CPL NP V damn wird ihn Anna erkennen  dais er 'vein!", 'Corpora annotated with syntactic structures are commonly referred to as trctbank.5.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:21', 'F:0']
['For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).']
["We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'Separable verb prefixes are labeled SVP.', 'During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.', 'Grammatical functions  encoded in edge labels  e.g.', 'During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:55', 'F:0']
['These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.']
['Separable verb prefixes are labeled SVP.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.', 'Grammatical functions  encoded in edge labels  e.g.', 'During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:21', 'F:0']
['For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).']
['Separable verb prefixes are labeled SVP.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.", 'All components of this kernel are assigned the label NK and treated as sibling nodes.', 'During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:55', 'F:0']
['However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.']
['Separable verb prefixes are labeled SVP.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'Grammatical functions  encoded in edge labels  e.g.', 'During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.', 'During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).']
['system', 'ROUGE-S*', 'Average_R:', '0.00158', '(95%-conf.int.', '0.00158', '-', '0.00158)']
['system', 'ROUGE-S*', 'Average_P:', '0.17857', '(95%-conf.int.', '0.17857', '-', '0.17857)']
['system', 'ROUGE-S*', 'Average_F:', '0.00314', '(95%-conf.int.', '0.00314', '-', '0.00314)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:28', 'F:5']
['These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.']
['During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.', 'In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).', 'During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5050', 'P:21', 'F:0']
['Corpora annotated with syntactic structures are commonly referred to as trctbank.5.']
['Separable verb prefixes are labeled SVP.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.', "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.", 'During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).']
['system', 'ROUGE-S*', 'Average_R:', '0.00219', '(95%-conf.int.', '0.00219', '-', '0.00219)']
['system', 'ROUGE-S*', 'Average_P:', '0.17857', '(95%-conf.int.', '0.17857', '-', '0.17857)']
['system', 'ROUGE-S*', 'Average_F:', '0.00434', '(95%-conf.int.', '0.00434', '-', '0.00434)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:28', 'F:5']
["Consider the German sentence (1) daran wird ihn Anna erkennen, &di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes."]
["We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.', 'The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.', 'Grammatical functions  encoded in edge labels  e.g.', 'During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).']
['system', 'ROUGE-S*', 'Average_R:', '0.00140', '(95%-conf.int.', '0.00140', '-', '0.00140)']
['system', 'ROUGE-S*', 'Average_P:', '0.01232', '(95%-conf.int.', '0.01232', '-', '0.01232)']
['system', 'ROUGE-S*', 'Average_F:', '0.00252', '(95%-conf.int.', '0.00252', '-', '0.00252)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3570', 'P:406', 'F:5']
['In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.']
['As modern linguistics is also becoming more aware of the importance of larger sets of naturally occurring data.  interpreted corpora. are a. valuable resource for theoretical and descriptive linguistic research.', 'We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.', u'The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- \xe2\u20ac\xa2lar representational strata.', 'In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.', 'As theory-independence is one of our objectives  the annotation scheme incorporates a number of widely accepted linguistic analyses  especially in the area of verbal  adverbial and adjectival syntax.']
['system', 'ROUGE-S*', 'Average_R:', '0.00050', '(95%-conf.int.', '0.00050', '-', '0.00050)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00095', '(95%-conf.int.', '0.00095', '-', '0.00095)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:91', 'F:1']
['We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.']
['For the implementation  we used Tcl/Tk Version 4.1.', 'Separable verb prefixes are labeled SVP.', "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.", 'PM stands for morphological particle  a label for German infinitival Z7t and superlative am.', 'Grammatical functions  encoded in edge labels  e.g.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:21', 'F:0']
['Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.']
['In the second phase  secondary links and additional structural functions are supported.', "This extra marking makes it easy to distinguish between 'normal' and coordinated categories.", 'In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).', 'For the implementation  we used Tcl/Tk Version 4.1.', 'The corpus is stored in a SQL database.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:105', 'F:0']
0.0345863633219 0.000515454540769 0.000995454536405





input/ref/Task1/P08-1102_aakansha.csv
input/res/Task1/P08-1102.csv
parsing: input/ref/Task1/P08-1102_aakansha.csv
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="14">As predications generated from such templates depend on the current character, we name these templates lexical-target.</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="121" ssid="32">Among other features, the 4-gram POS LM plays the most important role, removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'121'"]
'121'
['121']
parsed_discourse_facet ['method_citation']
<S sid="37" ssid="9">C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'"]
'37'
['37']
parsed_discourse_facet ['method_citation']
<S sid="73" ssid="24">For instance, if the word w appears N times in training corpus and is labelled as POS t for n times, the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'73'"]
'73'
['73']
parsed_discourse_facet ['method_citation']
<S sid="46" ssid="18">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) &#8712; X &#215; Y to a feature vector 4)(x, y) &#8712; Rd, and a parameter vector &#945;&#65533; &#8712; Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'"]
'46'
['46']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="22" ssid = "18">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here  Ci (i = L.n) denotes Chinese character  ti (i = L.m) denotes POS tag  and Cl:r (l < r) denotes character sequence ranges from Cl to Cr.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="104" ssid = "15">According to the usual practice in syntactic analysis  we choose chapters 1 − 260 (18074 sentences) as training set  chapter 271 − 300 (348 sentences) as test set and chapter 301 − 325 (350 sentences) as development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'", "'22'", "'16'", "'94'", "'104'"]
'87'
'22'
'16'
'94'
'104'
['87', '22', '16', '94', '104']
parsed_discourse_facet ['results_citation']
<S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'47'", "'48'", "'32'", "'133'"]
'69'
'47'
'48'
'32'
'133'
['69', '47', '48', '32', '133']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="99" ssid = "10">Then we trained LEX on each of the four corpora for 7 iterations.</S><S sid ="30" ssid = "2">It has comparable performance to CRFs  while with much faster training.</S><S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="122" ssid = "33">Another important feature is the labelling model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'99'", "'30'", "'101'", "'122'"]
'68'
'99'
'30'
'101'
'122'
['68', '99', '30', '101', '122']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="30" ssid = "2">It has comparable performance to CRFs  while with much faster training.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'87'", "'31'", "'94'", "'30'"]
'101'
'87'
'31'
'94'
'30'
['101', '87', '31', '94', '30']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="0" ssid = "0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'0'", "'87'", "'69'", "'48'"]
'47'
'0'
'87'
'69'
'48'
['47', '0', '87', '69', '48']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'48'", "'49'", "'133'", "'32'"]
'47'
'48'
'49'
'133'
'32'
['47', '48', '49', '133', '32']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'68'", "'94'", "'87'", "'7'"]
'16'
'68'
'94'
'87'
'7'
['16', '68', '94', '87', '7']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "19">We find that Joint S&T can also improve the segmentation accuracy.</S><S sid ="138" ssid = "1">This work was done while L. H. was visiting CAS/ICT.</S><S sid ="61" ssid = "12">In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.</S><S sid ="90" ssid = "1">We reported results from two set of experiments.</S><S sid ="107" ssid = "18">The evaluation results are shown in Table 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'138'", "'61'", "'90'", "'107'"]
'108'
'138'
'61'
'90'
'107'
['108', '138', '61', '90', '107']
parsed_discourse_facet ['method_citation']
<S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S><S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'133'", "'32'", "'69'", "'49'"]
'48'
'133'
'32'
'69'
'49'
['48', '133', '32', '69', '49']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'7'", "'49'", "'69'", "'16'"]
'47'
'7'
'49'
'69'
'16'
['47', '7', '49', '69', '16']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="54" ssid = "5">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S><S sid ="64" ssid = "15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid ="135" ssid = "6">In addition  all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus  whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'54'", "'64'", "'58'", "'135'"]
'2'
'54'
'64'
'58'
'135'
['2', '54', '64', '58', '135']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "11">Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS  CityU and MSR).</S><S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S><S sid ="121" ssid = "32">Among other features  the 4-gram POS LM plays the most important role  removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'47'", "'134'", "'32'", "'121'"]
'100'
'47'
'134'
'32'
'121'
['100', '47', '134', '32', '121']
parsed_discourse_facet ['method_citation']
<S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="52" ssid = "3">However  such features are generated dynamically during the decoding procedure so that the feature space enlarges much more rapidly.</S><S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="131" ssid = "2">Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.</S><S sid ="135" ssid = "6">In addition  all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus  whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'52'", "'2'", "'131'", "'135'"]
'133'
'52'
'2'
'131'
'135'
['133', '52', '2', '131', '135']
parsed_discourse_facet ['results_citation']
<S sid ="79" ssid = "4">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid ="73" ssid = "24">For instance  if the word w appears N times in training corpus and is labelled as POS t for n times  the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.</S><S sid ="98" ssid = "9">We found that LEX outperforms NON-LEX with a margin of about 0.002 at each iteration  and its learning curve reaches a tableland at iteration 7.</S><S sid ="37" ssid = "9">C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).</S><S sid ="70" ssid = "21">Given a training corpus with POS tags  we can train a word-POS co-occurrence model to approximate the probability that the word sequence of the labelled result co-exists with its corresponding POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'", "'73'", "'98'", "'37'", "'70'"]
'79'
'73'
'98'
'37'
'70'
['79', '73', '98', '37', '70']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="103" ssid = "14">We turned to experiments on CTB 5.0 to test the performance of the cascaded model.</S><S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'87'", "'103'", "'68'", "'101'"]
'16'
'87'
'103'
'68'
'101'
['16', '87', '103', '68', '101']
parsed_discourse_facet ['method_citation']
<S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="22" ssid = "18">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here  Ci (i = L.n) denotes Chinese character  ti (i = L.m) denotes POS tag  and Cl:r (l < r) denotes character sequence ranges from Cl to Cr.</S><S sid ="91" ssid = "2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2  including the Academia Sinica Corpus (AS)  the Hong Kong City University Corpus (CityU)  the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'", "'22'", "'91'", "'45'", "'16'"]
'31'
'22'
'91'
'45'
'16'
['31', '22', '91', '45', '16']
parsed_discourse_facet ['method_citation']
Length 0 input/ref/Task1/P08-1102_aakansha.csv
0.0 0.0 0.0





input/ref/Task1/A97-1014_sweta.csv
input/res/Task1/A97-1014.csv
parsing: input/ref/Task1/A97-1014_sweta.csv
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["168'"]
168'
['168']
parsed_discourse_facet ['method_citation']
<S sid="165" ssid="7">In addition the approach provides empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linea.rizations, and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.</S>
original cit marker offset is 0
new cit marker offset is 0



["165'"]
165'
['165']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="26">This amount of data suffices as training material to reliably assign the grammatical functions if the user determines the elements of a phrase and its type (step 1 of the list above).</S>
original cit marker offset is 0
new cit marker offset is 0



["145'"]
145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.<
original cit marker offset is 0
new cit marker offset is 0



["15'"]
15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="31">Apart from this rather technical problem, two further arguments speak against phrase structure as the structural pivot of the annotation scheme: Finally, the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies, which has proved tedious.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
<S sid="47" ssid="37">Argument structure can be represented in terms of unordered trees (with crossing branches).</S>
original cit marker offset is 0
new cit marker offset is 0



["47'"]
47'
['47']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
 <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["160'"]
160'
['160']
parsed_discourse_facet ['method_citation']
<S sid="166" ssid="8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
<S sid="166" ssid="8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="24">Sentences annotated in previous steps are used as training material for further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
  <S sid="71" ssid="16">However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["71'"]
71'
['71']
parsed_discourse_facet ['method_citation']
  <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A97-1014.csv
<S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="81" ssid = "26">During the second annotation stage  the annotation is enriched with information about thematic roles  quantifier scope and anaphoric reference.</S><S sid ="79" ssid = "24">PM stands for morphological particle  a label for German infinitival Z7t and superlative am.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="162" ssid = "4">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'81'", "'79'", "'137'", "'162'"]
'4'
'81'
'79'
'137'
'162'
['4', '81', '79', '137', '162']
parsed_discourse_facet ['results_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="79" ssid = "24">PM stands for morphological particle  a label for German infinitival Z7t and superlative am.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="140" ssid = "21">For the implementation  we used Tcl/Tk Version 4.1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'4'", "'79'", "'58'", "'140'"]
'80'
'4'
'79'
'58'
'140'
['80', '4', '79', '58', '140']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'4'", "'137'", "'56'", "'149'"]
'80'
'4'
'137'
'56'
'149'
['80', '4', '137', '56', '149']
parsed_discourse_facet ['method_citation']
<S sid ="129" ssid = "10">In the second phase  secondary links and additional structural functions are supported.</S><S sid ="140" ssid = "21">For the implementation  we used Tcl/Tk Version 4.1.</S><S sid ="141" ssid = "22">The corpus is stored in a SQL database.</S><S sid ="116" ssid = "29">This extra marking makes it easy to distinguish between 'normal' and coordinated categories.</S><S sid ="105" ssid = "18">In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'129'", "'140'", "'141'", "'116'", "'105'"]
'129'
'140'
'141'
'116'
'105'
['129', '140', '141', '116', '105']
parsed_discourse_facet ['method_citation']
<S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="128" ssid = "9">In the first phase  the main functionality for building and displaying unordered trees is supplied.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="105" ssid = "18">In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'137'", "'128'", "'56'", "'105'"]
'74'
'137'
'128'
'56'
'105'
['74', '137', '128', '56', '105']
parsed_discourse_facet ['method_citation']
<S sid ="55" ssid = "45">A uniform representation of local and non-local dependencies makes the structure more transparent'.</S><S sid ="53" ssid = "43">A tree meeting these requirements is given below: Adv V NP NP V CPL NP V damn wird ihn Anna erkennen  dais er 'vein!</S><S sid ="120" ssid = "1">The development of linguistically interpreted corpora presents a laborious and time-consuming task.</S><S sid ="65" ssid = "10">The tree resembles traditional constituent structures.</S><S sid ="14" ssid = "4">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'53'", "'120'", "'65'", "'14'"]
'55'
'53'
'120'
'65'
'14'
['55', '53', '120', '65', '14']
parsed_discourse_facet ['method_citation']
<S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'58'", "'80'", "'149'", "'56'"]
'74'
'58'
'80'
'149'
'56'
['74', '58', '80', '149', '56']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'74'", "'56'", "'58'", "'149'"]
'80'
'74'
'56'
'58'
'149'
['80', '74', '56', '58', '149']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'56'", "'58'", "'137'", "'149'"]
'80'
'56'
'58'
'137'
'149'
['80', '56', '58', '137', '149']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- •lar representational strata.</S><S sid ="88" ssid = "1">As theory-independence is one of our objectives  the annotation scheme incorporates a number of widely accepted linguistic analyses  especially in the area of verbal  adverbial and adjectival syntax.</S><S sid ="5" ssid = "2">In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.</S><S sid ="164" ssid = "6">As modern linguistics is also becoming more aware of the importance of larger sets of naturally occurring data.  interpreted corpora. are a. valuable resource for theoretical and descriptive linguistic research.</S><S sid ="168" ssid = "10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'88'", "'5'", "'164'", "'168'"]
'3'
'88'
'5'
'164'
'168'
['3', '88', '5', '164', '168']
parsed_discourse_facet ['method_citation']
<S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'137'", "'74'", "'56'", "'149'"]
'58'
'137'
'74'
'56'
'149'
['58', '137', '74', '56', '149']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="101" ssid = "14">All components of this kernel are assigned the label NK and treated as sibling nodes.</S><S sid ="4" ssid = "1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'56'", "'101'", "'4'", "'149'"]
'80'
'56'
'101'
'4'
'149'
['80', '56', '101', '4', '149']
parsed_discourse_facet ['method_citation']
<S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S><S sid ="58" ssid = "3">Grammatical functions  encoded in edge labels  e.g.</S><S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S><S sid ="80" ssid = "25">Separable verb prefixes are labeled SVP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'56'", "'74'", "'58'", "'149'", "'80'"]
'56'
'74'
'58'
'149'
'80'
['56', '74', '58', '149', '80']
parsed_discourse_facet ['results_citation']
<S sid ="149" ssid = "30">During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).</S><S sid ="56" ssid = "1">We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.</S><S sid ="105" ssid = "18">In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).</S><S sid ="137" ssid = "18">The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.</S><S sid ="74" ssid = "19">During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'149'", "'56'", "'105'", "'137'", "'74'"]
'149'
'56'
'105'
'137'
'74'
['149', '56', '105', '137', '74']
parsed_discourse_facet ['method_citation']
['In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.']
["A uniform representation of local and non-local dependencies makes the structure more transparent'.", 'The development of linguistically interpreted corpora presents a laborious and time-consuming task.', 'The tree resembles traditional constituent structures.', "A tree meeting these requirements is given below: Adv V NP NP V CPL NP V damn wird ihn Anna erkennen  dais er 'vein!", 'Corpora annotated with syntactic structures are commonly referred to as trctbank.5.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:91', 'F:0']
['In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.']
["We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'Separable verb prefixes are labeled SVP.', 'During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.', 'Grammatical functions  encoded in edge labels  e.g.', 'During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:91', 'F:0']
['These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.']
['Separable verb prefixes are labeled SVP.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.', 'Grammatical functions  encoded in edge labels  e.g.', 'During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:21', 'F:0']
['Sentences annotated in previous steps are used as training material for further processing.']
['Separable verb prefixes are labeled SVP.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.", 'All components of this kernel are assigned the label NK and treated as sibling nodes.', 'During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:21', 'F:0']
['However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.']
['Separable verb prefixes are labeled SVP.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'Grammatical functions  encoded in edge labels  e.g.', 'During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.', 'During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).']
['system', 'ROUGE-S*', 'Average_R:', '0.00158', '(95%-conf.int.', '0.00158', '-', '0.00158)']
['system', 'ROUGE-S*', 'Average_P:', '0.17857', '(95%-conf.int.', '0.17857', '-', '0.17857)']
['system', 'ROUGE-S*', 'Average_F:', '0.00314', '(95%-conf.int.', '0.00314', '-', '0.00314)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:28', 'F:5']
['For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).']
['During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.', 'In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).', 'During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).']
['system', 'ROUGE-S*', 'Average_R:', '0.00020', '(95%-conf.int.', '0.00020', '-', '0.00020)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00039', '(95%-conf.int.', '0.00039', '-', '0.00039)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5050', 'P:55', 'F:1']
['This amount of data suffices as training material to reliably assign the grammatical functions if the user determines the elements of a phrase and its type (step 1 of the list above).']
['Separable verb prefixes are labeled SVP.', "We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.', "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.", 'During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).']
['system', 'ROUGE-S*', 'Average_R:', '0.00219', '(95%-conf.int.', '0.00219', '-', '0.00219)']
['system', 'ROUGE-S*', 'Average_P:', '0.03676', '(95%-conf.int.', '0.03676', '-', '0.03676)']
['system', 'ROUGE-S*', 'Average_F:', '0.00414', '(95%-conf.int.', '0.00414', '-', '0.00414)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:136', 'F:5']
['Syntactically annotated corpora of German have been missing until now.']
["We distinguish the following levels of representation: 'A context-free constituent backbone can still be recovered from the surface string and argument structure by reattaching 'extracted' structures to a higher node.", 'During the first phase  the focus is on annotating correct structures and a coarse-grained classification of grammatical functions  which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC)  accusative objects (OA)  datives (DA)  etc.', 'The following commands are available: The three tagsets used by the annotation tool (for words  phrases  and edges) are variable and are stored together with the corpus.', 'Grammatical functions  encoded in edge labels  e.g.', 'During annotation  the highest rated grammatical function labels Gi are calculated using the Viterbi algorithm and assigned to the structure  i.e.  we calculate argma.x11 PQ (Ti 1Z-1  Ti.-2) PQ (Gi ITi).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3570', 'P:10', 'F:0']
['Syntactically annotated corpora of German have been missing until now.']
['As modern linguistics is also becoming more aware of the importance of larger sets of naturally occurring data.  interpreted corpora. are a. valuable resource for theoretical and descriptive linguistic research.', 'We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.', u'The resulting scheme reflects a stratificational notion of language  and makes only minimal assumpabout the interrelation of the particu- \xe2\u20ac\xa2lar representational strata.', 'In particular  we focus on several methodological issues concerning the annotation of non-configurational languages.', 'As theory-independence is one of our objectives  the annotation scheme incorporates a number of widely accepted linguistic analyses  especially in the area of verbal  adverbial and adjectival syntax.']
['system', 'ROUGE-S*', 'Average_R:', '0.00099', '(95%-conf.int.', '0.00099', '-', '0.00099)']
['system', 'ROUGE-S*', 'Average_P:', '0.20000', '(95%-conf.int.', '0.20000', '-', '0.20000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00197', '(95%-conf.int.', '0.00197', '-', '0.00197)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:10', 'F:2']
['In addition the approach provides empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linea.rizations, and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.']
['For the implementation  we used Tcl/Tk Version 4.1.', 'Separable verb prefixes are labeled SVP.', "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.", 'PM stands for morphological particle  a label for German infinitival Z7t and superlative am.', 'Grammatical functions  encoded in edge labels  e.g.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:325', 'F:0']
['Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.']
['In the second phase  secondary links and additional structural functions are supported.', "This extra marking makes it easy to distinguish between 'normal' and coordinated categories.", 'In addition  a. number of clear-cut NP cornponents can be defined outside that  juxtapositional kernel: pre- and postnominal genitives (GL  GR)  relative clauses (RC)  clausal and sentential complements (OC).', 'For the implementation  we used Tcl/Tk Version 4.1.', 'The corpus is stored in a SQL database.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:105', 'F:0']
0.0394099996417 0.00045090908681 0.000876363628397





input/ref/Task1/A00-2030_vardha.csv
input/res/Task1/A00-2030.csv
parsing: input/ref/Task1/A00-2030_vardha.csv
 <S sid="11" ssid="1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="10">Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'"]
'50'
['50']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
 <S sid="106" ssid="3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
 <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
 <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="2">The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
 <S sid="3" ssid="1">Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'"]
'3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="15">Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'"]
'32'
['32']
parsed_discourse_facet ['method_citation']
 <S sid="10" ssid="8">Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="13">Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
    <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
 <S sid="26" ssid="9">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'"]
'26'
['26']
parsed_discourse_facet ['method_citation']
    <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="2">Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="49" ssid = "9">By necessity  we adopted the strategy of hand marking only the semantics.</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="70" ssid = "11">Post-modifier constituents for the PER/NP.</S><S sid ="73" ssid = "14">We now briefly summarize the probability structure of the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'", "'49'", "'68'", "'70'", "'73'"]
'28'
'49'
'68'
'70'
'73'
['28', '49', '68', '70', '73']
parsed_discourse_facet ['results_citation']
<S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="76" ssid = "17">Finally  word features  fm  for modifiers are predicted based on the modifier  cm  the partof-speech tag of the modifier word   t„„ the part-of-speech tag of the head word th  the head word itself  wh  and whether or not the modifier head word  w„„ is known or unknown.</S><S sid ="39" ssid = "7">For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.</S><S sid ="30" ssid = "13">Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'101'", "'76'", "'39'", "'30'"]
'61'
'101'
'76'
'39'
'30'
['61', '101', '76', '39', '30']
parsed_discourse_facet ['method_citation']
<S sid ="33" ssid = "1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S sid ="41" ssid = "1">To train our integrated model  we required a large corpus of augmented parse trees.</S><S sid ="64" ssid = "5">Word features are introduced primarily to help with unknown words  as in (Weischedel et al. 1993).</S><S sid ="2" ssid = "2">In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'41'", "'64'", "'2'", "'23'"]
'33'
'41'
'64'
'2'
'23'
['33', '41', '64', '2', '23']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'101'", "'81'", "'74'", "'0'"]
'61'
'101'
'81'
'74'
'0'
['61', '101', '81', '74', '0']
parsed_discourse_facet ['method_citation']
<S sid ="60" ssid = "1">In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).</S><S sid ="72" ssid = "13">This generation process is continued until the entire tree has been produced.</S><S sid ="77" ssid = "18">The probability of a complete tree is the product of the probabilities of generating each element in the tree.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'72'", "'77'", "'88'", "'28'"]
'60'
'72'
'77'
'88'
'28'
['60', '72', '77', '88', '28']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="54" ssid = "3">These steps are given below:</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'46'", "'57'", "'54'", "'106'"]
'0'
'46'
'57'
'54'
'106'
['0', '46', '57', '54', '106']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="94" ssid = "13">Given a new sentence  the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S><S sid ="30" ssid = "13">Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.</S><S sid ="60" ssid = "1">In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).</S><S sid ="39" ssid = "7">For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'94'", "'30'", "'60'", "'39'"]
'61'
'94'
'30'
'60'
'39'
['61', '94', '30', '60', '39']
parsed_discourse_facet ['method_citation']
<S sid ="110" ssid = "2">Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.</S><S sid ="41" ssid = "1">To train our integrated model  we required a large corpus of augmented parse trees.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'", "'41'", "'57'", "'51'", "'46'"]
'110'
'41'
'57'
'51'
'46'
['110', '41', '57', '51', '46']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'81'", "'0'", "'61'", "'74'"]
'51'
'81'
'0'
'61'
'74'
['51', '81', '0', '61', '74']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "15">Because generative statistical models had already proven successful for each of the first three stages  we were optimistic that some of their properties — especially their ability to learn from large amounts of data  and their robustness when presented with unexpected inputs — would also benefit semantic analysis.</S><S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="19" ssid = "2">Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'61'", "'19'", "'101'", "'51'"]
'32'
'61'
'19'
'101'
'51'
['32', '61', '19', '101', '51']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="54" ssid = "3">These steps are given below:</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'54'", "'81'", "'57'", "'46'"]
'0'
'54'
'81'
'57'
'46'
['0', '54', '81', '57', '46']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "13">Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'46'", "'105'", "'106'", "'11'"]
'30'
'46'
'105'
'106'
'11'
['30', '46', '105', '106', '11']
parsed_discourse_facet ['method_citation']
<S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="54" ssid = "3">These steps are given below:</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'", "'0'", "'46'", "'57'", "'54'"]
'105'
'0'
'46'
'57'
'54'
['105', '0', '46', '57', '54']
parsed_discourse_facet ['results_citation']
<S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'0'", "'57'", "'51'", "'105'"]
'46'
'0'
'57'
'51'
'105'
['46', '0', '57', '51', '105']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'57'", "'46'", "'51'", "'81'"]
'0'
'57'
'46'
'51'
'81'
['0', '57', '46', '51', '81']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="32" ssid = "15">Because generative statistical models had already proven successful for each of the first three stages  we were optimistic that some of their properties — especially their ability to learn from large amounts of data  and their robustness when presented with unexpected inputs — would also benefit semantic analysis.</S><S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S><S sid ="26" ssid = "9">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993)  and more recently  had begun using a generative statistical model for name finding (Bikel et al.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'32'", "'61'", "'11'", "'26'"]
'51'
'32'
'61'
'11'
'26'
['51', '32', '61', '11', '26']
parsed_discourse_facet ['method_citation']
<S sid ="110" ssid = "2">Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'", "'106'", "'46'", "'57'", "'105'"]
'110'
'106'
'46'
'57'
'105'
['110', '106', '46', '57', '105']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "3">These steps are given below:</S><S sid ="55" ssid = "1">syntactic modifier of the other  the inserted node serves to indicate the relation as well as the argument.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="7" ssid = "5">The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'55'", "'0'", "'62'", "'7'"]
'54'
'55'
'0'
'62'
'7'
['54', '55', '0', '62', '7']
parsed_discourse_facet ['method_citation']
<S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="110" ssid = "2">Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'110'", "'46'", "'105'", "'57'"]
'106'
'110'
'46'
'105'
'57'
['106', '110', '46', '105', '57']
parsed_discourse_facet ['aim_citation', 'results_citation']
['In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.', 'A Novel Use of Statistical Parsing to Extract Information from Text', 'David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.', 'For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:', 'To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:91', 'F:0']
['We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al.']
['Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.', 'David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.', 'We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.', 'Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.', 'A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.']
['system', 'ROUGE-S*', 'Average_R:', '0.00051', '(95%-conf.int.', '0.00051', '-', '0.00051)']
['system', 'ROUGE-S*', 'Average_P:', '0.00654', '(95%-conf.int.', '0.00654', '-', '0.00654)']
['system', 'ROUGE-S*', 'Average_F:', '0.00095', '(95%-conf.int.', '0.00095', '-', '0.00095)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:153', 'F:1']
['In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['syntactic modifier of the other  the inserted node serves to indicate the relation as well as the argument.', 'These steps are given below:', 'For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.', 'The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.', 'A Novel Use of Statistical Parsing to Extract Information from Text']
['system', 'ROUGE-S*', 'Average_R:', '0.00460', '(95%-conf.int.', '0.00460', '-', '0.00460)']
['system', 'ROUGE-S*', 'Average_P:', '0.02198', '(95%-conf.int.', '0.02198', '-', '0.02198)']
['system', 'ROUGE-S*', 'Average_F:', '0.00760', '(95%-conf.int.', '0.00760', '-', '0.00760)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:91', 'F:2']
['We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.']
['In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).', 'The probability of a complete tree is the product of the probabilities of generating each element in the tree.', 'For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).', 'Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.', 'This generation process is continued until the entire tree has been produced.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:36', 'F:0']
['In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.', 'A Novel Use of Statistical Parsing to Extract Information from Text', 'David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.', 'We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.', 'These steps are given below:']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:91', 'F:0']
['In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).', 'The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.', 'For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.', 'Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.', 'Given a new sentence  the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:91', 'F:0']
['The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).']
['A Novel Use of Statistical Parsing to Extract Information from Text', 'For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:', 'To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.', u'The categories for head constituents  cl\u201e are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c\u201e _1  and the head word of their parent  wp.', 'The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4465', 'P:78', 'F:0']
['Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.']
['Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.', u'Because generative statistical models had already proven successful for each of the first three stages  we were optimistic that some of their properties \u2014 especially their ability to learn from large amounts of data  and their robustness when presented with unexpected inputs \u2014 would also benefit semantic analysis.', 'We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.', 'To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.', 'The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.']
['system', 'ROUGE-S*', 'Average_R:', '0.00323', '(95%-conf.int.', '0.00323', '-', '0.00323)']
['system', 'ROUGE-S*', 'Average_P:', '0.07510', '(95%-conf.int.', '0.07510', '-', '0.07510)']
['system', 'ROUGE-S*', 'Average_F:', '0.00619', '(95%-conf.int.', '0.00619', '-', '0.00619)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5886', 'P:253', 'F:19']
['Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.']
['Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.', 'David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.', 'We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.', 'Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.', 'A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.']
['system', 'ROUGE-S*', 'Average_R:', '0.00154', '(95%-conf.int.', '0.00154', '-', '0.00154)']
['system', 'ROUGE-S*', 'Average_P:', '0.01087', '(95%-conf.int.', '0.01087', '-', '0.01087)']
['system', 'ROUGE-S*', 'Average_F:', '0.00269', '(95%-conf.int.', '0.00269', '-', '0.00269)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:276', 'F:3']
['Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.']
['Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.', 'We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).', 'Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.', 'We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.', 'A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.']
['system', 'ROUGE-S*', 'Average_R:', '0.00202', '(95%-conf.int.', '0.00202', '-', '0.00202)']
['system', 'ROUGE-S*', 'Average_P:', '0.01186', '(95%-conf.int.', '0.01186', '-', '0.01186)']
['system', 'ROUGE-S*', 'Average_F:', '0.00345', '(95%-conf.int.', '0.00345', '-', '0.00345)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:253', 'F:3']
['We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.']
['We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).', u'Because generative statistical models had already proven successful for each of the first three stages  we were optimistic that some of their properties \u2014 especially their ability to learn from large amounts of data  and their robustness when presented with unexpected inputs \u2014 would also benefit semantic analysis.', 'We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993)  and more recently  had begun using a generative statistical model for name finding (Bikel et al.', 'To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.', 'The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.']
['system', 'ROUGE-S*', 'Average_R:', '0.00022', '(95%-conf.int.', '0.00022', '-', '0.00022)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00043', '(95%-conf.int.', '0.00043', '-', '0.00043)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4560', 'P:78', 'F:1']
['Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.']
['Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.', 'A Novel Use of Statistical Parsing to Extract Information from Text', 'David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.', 'To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.', 'A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.']
['system', 'ROUGE-S*', 'Average_R:', '0.00420', '(95%-conf.int.', '0.00420', '-', '0.00420)']
['system', 'ROUGE-S*', 'Average_P:', '0.16364', '(95%-conf.int.', '0.16364', '-', '0.16364)']
['system', 'ROUGE-S*', 'Average_F:', '0.00818', '(95%-conf.int.', '0.00818', '-', '0.00818)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:55', 'F:9']
['Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.']
['Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.', 'A Novel Use of Statistical Parsing to Extract Information from Text', 'David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.', 'These steps are given below:', 'A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.']
['system', 'ROUGE-S*', 'Average_R:', '0.00581', '(95%-conf.int.', '0.00581', '-', '0.00581)']
['system', 'ROUGE-S*', 'Average_P:', '0.03676', '(95%-conf.int.', '0.03676', '-', '0.03676)']
['system', 'ROUGE-S*', 'Average_F:', '0.01003', '(95%-conf.int.', '0.01003', '-', '0.01003)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:136', 'F:5']
0.0261207690298 0.00170230767921 0.00303999997662





input/ref/Task1/P11-1061_swastika.csv
input/res/Task1/P11-1061.csv
parsing: input/ref/Task1/P11-1061_swastika.csv
    <S sid="144" ssid="7">For comparison, the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%, and goes up to 88.7% with a treebank dictionary.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
    <S sid="44" ssid="10">Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.</S
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
parsed_discourse_facet ['method_citation']
    <S sid="16" ssid="12">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (&#167;3), and then use graph label propagation to project syntactic information from English to the foreign language (&#167;4).</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="44" ssid="10">Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.</S
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
parsed_discourse_facet ['method_citation']
<S sid="110" ssid="10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S>
original cit marker offset is 0
new cit marker offset is 0



['110']
110
['110']
parsed_discourse_facet ['aim_citation']
    <S sid="115" ssid="15">The taggers were trained on datasets labeled with the universal tags.</S>
original cit marker offset is 0
new cit marker offset is 0



['115']
115
['115']
parsed_discourse_facet ['method_citation']
    <S sid="115" ssid="15">The taggers were trained on datasets labeled with the universal tags.</S>
original cit marker offset is 0
new cit marker offset is 0



['115']
115
['115']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['158']
158
['158']
parsed_discourse_facet ['result_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



['23']
23
['23']
parsed_discourse_facet ['result_citation']
<S sid="24" ssid="1">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['24']
24
['24']
parsed_discourse_facet ['aim_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['???']
???
['???']
parsed_discourse_facet ['method_citation']
    <S sid="16" ssid="12">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (&#167;3), and then use graph label propagation to project syntactic information from English to the foreign language (&#167;4).</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



['23']
23
['23']
parsed_discourse_facet ['result_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['158']
158
['158']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
<S sid="56" ssid="22">To define a similarity function between the English and the foreign vertices, we rely on high-confidence word alignments.</S>
original cit marker offset is 0
new cit marker offset is 0



['56']
56
['56']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



['161']
161
['161']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P11-1061.csv
<S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="62" ssid = "28">Note that since all English vertices were extracted from the parallel text  we will have an initial label distribution for all vertices in Ve.</S><S sid ="68" ssid = "34">In the label propagation stage  we propagate the automatic English tags to the aligned Italian trigram types  followed by further propagation solely among the Italian vertices. the Italian vertices are connected to an automatically labeled English vertex.</S><S sid ="42" ssid = "8">The foreign language vertices (denoted by Vf) correspond to foreign trigram types  exactly as in Subramanya et al. (2010).</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'62'", "'68'", "'42'", "'160'"]
'19'
'62'
'68'
'42'
'160'
['19', '62', '68', '42', '160']
parsed_discourse_facet ['results_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid ="30" ssid = "7">To initialize the graph we tag the English side of the parallel text using a supervised model.</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'113'", "'30'", "'102'", "'25'"]
'26'
'113'
'30'
'102'
'25'
['26', '113', '30', '102', '25']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'113'", "'25'", "'3'", "'19'"]
'16'
'113'
'25'
'3'
'19'
['16', '113', '25', '3', '19']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="119" ssid = "19">To provide a thorough analysis  we evaluated three baselines and two oracles in addition to two variants of our graph-based approach.</S><S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="82" ssid = "13">We formulate the update as follows: where ∀ui ∈ Vf \ Vfl  γi(y) and κi are defined as: We ran this procedure for 10 iterations.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'119'", "'54'", "'82'", "'28'"]
'134'
'119'
'54'
'82'
'28'
['134', '119', '54', '82', '28']
parsed_discourse_facet ['method_citation']
<S sid ="71" ssid = "2">We use label propagation in two stages to generate soft labels on all the vertices in the graph.</S><S sid ="61" ssid = "27">These tag distributions are used to initialize the label distributions over the English vertices in the graph.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="86" ssid = "17">For a sentence x and a state sequence z  a first order Markov model defines a distribution: (9) where Val(X) corresponds to the entire vocabulary.</S><S sid ="124" ssid = "24">We tried two versions of our graph-based approach: feature after the first stage of label propagation (Eq.</S>
original cit marker offset is 0
new cit marker offset is 0



["'71'", "'61'", "'3'", "'86'", "'124'"]
'71'
'61'
'3'
'86'
'124'
['71', '61', '3', '86', '124']
parsed_discourse_facet ['method_citation']
<S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="57" ssid = "23">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'26'", "'57'", "'97'", "'28'"]
'89'
'26'
'57'
'97'
'28'
['89', '26', '57', '97', '28']
parsed_discourse_facet ['method_citation']
<S sid ="93" ssid = "24">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'93'", "'26'", "'28'", "'97'", "'89'"]
'93'
'26'
'28'
'97'
'89'
['93', '26', '28', '97', '89']
parsed_discourse_facet ['method_citation']
<S sid ="92" ssid = "23">To optimize this function  we used L-BFGS  a quasi-Newton method (Liu and Nocedal  1989).</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="49" ssid = "15">We define a symmetric similarity function K(uZ7 uj) over two foreign language vertices uZ7 uj E Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1.</S><S sid ="142" ssid = "5">The “No LP” model does not outperform direct projection for German and Greek  but performs better for six out of eight languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'102'", "'19'", "'49'", "'142'"]
'92'
'102'
'19'
'49'
'142'
['92', '102', '19', '49', '142']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="159" ssid = "2">Because we are interested in applying our techniques to languages for which no labeled resources are available  we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'159'", "'97'", "'89'", "'28'"]
'26'
'159'
'97'
'89'
'28'
['26', '159', '97', '89', '28']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'97'", "'102'", "'28'", "'83'"]
'134'
'97'
'102'
'28'
'83'
['134', '97', '102', '28', '83']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="72" ssid = "3">In the first stage  we run a single step of label propagation  which transfers the label distributions from the English vertices to the connected foreign language vertices (say  Vf�) at the periphery of the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'97'", "'28'", "'102'", "'72'"]
'26'
'97'
'28'
'102'
'72'
['26', '97', '28', '102', '72']
parsed_discourse_facet ['method_citation']
<S sid ="95" ssid = "26">This feature ft incorporates information from the smoothed graph and prunes hidden states that are inconsistent with the thresholded vector tx.</S><S sid ="15" ssid = "11">First  we use a novel graph-based framework for projecting syntactic information across language boundaries.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid ="41" ssid = "7">Because the information flow in our graph is asymmetric (from English to the foreign language)  we use different types of vertices for each language.</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'15'", "'25'", "'41'", "'113'"]
'95'
'15'
'25'
'41'
'113'
['95', '15', '25', '41', '113']
parsed_discourse_facet ['method_citation']
<S sid ="21" ssid = "17">These universal POS categories not only facilitate the transfer of POS information from one language to another  but also relieve us from using controversial evaluation metrics 2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S><S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="10" ssid = "6">To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S><S sid ="36" ssid = "2">Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand  using individual words as the vertices throws away the context necessary for disambiguation; on the other hand  it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'16'", "'10'", "'29'", "'36'"]
'21'
'16'
'10'
'29'
'36'
['21', '16', '10', '29', '36']
parsed_discourse_facet ['results_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S><S sid ="35" ssid = "1">In graph-based learning approaches one constructs a graph whose vertices are labeled and unlabeled examples  and whose weighted edges encode the degree to which the examples they link have the same label (Zhu et al.  2003).</S><S sid ="159" ssid = "2">Because we are interested in applying our techniques to languages for which no labeled resources are available  we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'54'", "'89'", "'35'", "'159'"]
'26'
'54'
'89'
'35'
'159'
['26', '54', '89', '35', '159']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'28'", "'134'", "'97'", "'89'"]
'26'
'28'
'134'
'97'
'89'
['26', '28', '134', '97', '89']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="21" ssid = "17">These universal POS categories not only facilitate the transfer of POS information from one language to another  but also relieve us from using controversial evaluation metrics 2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'21'", "'160'", "'113'", "'29'"]
'16'
'21'
'160'
'113'
'29'
['16', '21', '160', '113', '29']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'28'", "'97'", "'26'", "'89'"]
'134'
'28'
'97'
'26'
'89'
['134', '28', '97', '26', '89']
parsed_discourse_facet ['method_citation']
<S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="49" ssid = "15">We define a symmetric similarity function K(uZ7 uj) over two foreign language vertices uZ7 uj E Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1.</S><S sid ="136" ssid = "36">For graph propagation  the hyperparameter v was set to 2 x 10−6 and was not tuned.</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'102'", "'49'", "'136'", "'83'"]
'97'
'102'
'49'
'136'
'83'
['97', '102', '49', '136', '83']
parsed_discourse_facet ['method_citation']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.']
[u'As discussed in more detail in \xa73  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', 'The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.', u'The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (\xa73.2).', 'This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.', u'Because we don\u2019t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.']
['system', 'ROUGE-S*', 'Average_R:', '0.00072', '(95%-conf.int.', '0.00072', '-', '0.00072)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00141', '(95%-conf.int.', '0.00141', '-', '0.00141)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4186', 'P:55', 'F:3']
['For comparison, the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%, and goes up to 88.7% with a treebank dictionary.']
['Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.', 'In the label propagation stage  we propagate the automatic English tags to the aligned Italian trigram types  followed by further propagation solely among the Italian vertices. the Italian vertices are connected to an automatically labeled English vertex.', 'Note that since all English vertices were extracted from the parallel text  we will have an initial label distribution for all vertices in Ve.', 'The foreign language vertices (denoted by Vf) correspond to foreign trigram types  exactly as in Subramanya et al. (2010).', 'Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.']
['system', 'ROUGE-S*', 'Average_R:', '0.00140', '(95%-conf.int.', '0.00140', '-', '0.00140)']
['system', 'ROUGE-S*', 'Average_P:', '0.02941', '(95%-conf.int.', '0.02941', '-', '0.02941)']
['system', 'ROUGE-S*', 'Average_F:', '0.00268', '(95%-conf.int.', '0.00268', '-', '0.00268)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:136', 'F:4']
['To this end, we construct a bilingual graph over word types to establish a connection between the two languages (&#167;3), and then use graph label propagation to project syntactic information from English to the foreign language (&#167;4).']
['To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.', 'Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand  using individual words as the vertices throws away the context necessary for disambiguation; on the other hand  it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences.', 'These universal POS categories not only facilitate the transfer of POS information from one language to another  but also relieve us from using controversial evaluation metrics 2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.', u'To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (\u0e22\u0e073.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.', u'To this end  we construct a bilingual graph over word types to establish a connection between the two languages (\u0e22\u0e073)  and then use graph label propagation to project syntactic information from English to the foreign language (\u0e22\u0e074).']
['system', 'ROUGE-S*', 'Average_R:', '0.02375', '(95%-conf.int.', '0.02375', '-', '0.02375)']
['system', 'ROUGE-S*', 'Average_P:', '0.82251', '(95%-conf.int.', '0.82251', '-', '0.82251)']
['system', 'ROUGE-S*', 'Average_F:', '0.04616', '(95%-conf.int.', '0.04616', '-', '0.04616)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:8001', 'P:231', 'F:190']
['Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).']
['Because we are interested in applying our techniques to languages for which no labeled resources are available  we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.', u'As discussed in more detail in \u0e22\u0e073  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', 'The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.', u'The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (\u0e22\u0e073.2).', 'This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.']
['system', 'ROUGE-S*', 'Average_R:', '0.00049', '(95%-conf.int.', '0.00049', '-', '0.00049)']
['system', 'ROUGE-S*', 'Average_P:', '0.00403', '(95%-conf.int.', '0.00403', '-', '0.00403)']
['system', 'ROUGE-S*', 'Average_F:', '0.00087', '(95%-conf.int.', '0.00087', '-', '0.00087)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4095', 'P:496', 'F:2']
['To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.']
['First  we use a novel graph-based framework for projecting syntactic information across language boundaries.', 'Because the information flow in our graph is asymmetric (from English to the foreign language)  we use different types of vertices for each language.', 'This feature ft incorporates information from the smoothed graph and prunes hidden states that are inconsistent with the thresholded vector tx.', 'For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.01639', '(95%-conf.int.', '0.01639', '-', '0.01639)']
['system', 'ROUGE-S*', 'Average_P:', '0.05871', '(95%-conf.int.', '0.05871', '-', '0.05871)']
['system', 'ROUGE-S*', 'Average_F:', '0.02563', '(95%-conf.int.', '0.02563', '-', '0.02563)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:528', 'F:31']
['The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.']
['We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.', u'We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value \u03c4: We describe how we choose \u03c4 in \xa76.4.', u'The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (\xa73.2).', 'This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.', u'Because we don\u2019t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.']
['system', 'ROUGE-S*', 'Average_R:', '0.00180', '(95%-conf.int.', '0.00180', '-', '0.00180)']
['system', 'ROUGE-S*', 'Average_P:', '0.05495', '(95%-conf.int.', '0.05495', '-', '0.05495)']
['system', 'ROUGE-S*', 'Average_F:', '0.00349', '(95%-conf.int.', '0.00349', '-', '0.00349)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:91', 'F:5']
['We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.']
['We tried two versions of our graph-based approach: feature after the first stage of label propagation (Eq.', 'These tag distributions are used to initialize the label distributions over the English vertices in the graph.', 'We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).', 'For a sentence x and a state sequence z  a first order Markov model defines a distribution: (9) where Val(X) corresponds to the entire vocabulary.', 'We use label propagation in two stages to generate soft labels on all the vertices in the graph.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:21', 'F:0']
['Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.']
['We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.', u'As discussed in more detail in \u0e22\u0e073  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', 'To initialize the graph we tag the English side of the parallel text using a supervised model.', 'For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.']
['system', 'ROUGE-S*', 'Average_R:', '0.00121', '(95%-conf.int.', '0.00121', '-', '0.00121)']
['system', 'ROUGE-S*', 'Average_P:', '0.20000', '(95%-conf.int.', '0.20000', '-', '0.20000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00240', '(95%-conf.int.', '0.00240', '-', '0.00240)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:15', 'F:3']
['Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.']
['We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.', u'We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value \u03c4: We describe how we choose \u03c4 in \xa76.4.', u'For graph propagation  the hyperparameter v was set to 2 x 10\u22126 and was not tuned.', 'This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.', 'We define a symmetric similarity function K(uZ7 uj) over two foreign language vertices uZ7 uj E Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:136', 'F:0']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.']
['We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.', 'Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.', 'To optimize this function  we used L-BFGS  a quasi-Newton method (Liu and Nocedal  1989).', u'The \u201cNo LP\u201d model does not outperform direct projection for German and Greek  but performs better for six out of eight languages.', 'We define a symmetric similarity function K(uZ7 uj) over two foreign language vertices uZ7 uj E Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00044', '(95%-conf.int.', '0.00044', '-', '0.00044)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00086', '(95%-conf.int.', '0.00086', '-', '0.00086)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:55', 'F:1']
['The taggers were trained on datasets labeled with the universal tags.']
['For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.', 'The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.', u'The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (\u0e22\u0e073.2).', u'As discussed in more detail in \u0e22\u0e073  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', 'This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.']
['system', 'ROUGE-S*', 'Average_R:', '0.00017', '(95%-conf.int.', '0.00017', '-', '0.00017)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00034', '(95%-conf.int.', '0.00034', '-', '0.00034)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5886', 'P:15', 'F:1']
0.119000908009 0.00421545450713 0.00762181811253





input/ref/Task1/W99-0623_vardha.csv
input/res/Task1/W99-0623.csv
parsing: input/ref/Task1/W99-0623_vardha.csv
    <S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="117" ssid="46">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'117'"]
'117'
['117']
parsed_discourse_facet ['method_citation']
<S sid="72" ssid="1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
    <S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'"]
'120'
['120']
parsed_discourse_facet ['method_citation']
  <S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'139'"]
'139'
['139']
parsed_discourse_facet ['method_citation']
 <S sid="84" ssid="13">The first shows how constituent features and context do not help in deciding which parser to trust.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'"]
'84'
['84']
parsed_discourse_facet ['method_citation']
    <S sid="76" ssid="5">The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'"]
'76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'"]
'38'
['38']
parsed_discourse_facet ['method_citation']
  <S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
 <S sid="72" ssid="1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
    <S sid="87" ssid="16">It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'"]
'87'
['87']
parsed_discourse_facet ['method_citation']
    <S sid="51" ssid="37">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
    <S sid="79" ssid="8">Precision is the portion of hypothesized constituents that are correct and recall is the portion of the Treebank constituents that are hypothesized.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
  <S sid="27" ssid="13">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'"]
'27'
['27']
parsed_discourse_facet ['method_citation']
 <S sid="77" ssid="6">Each parse is converted into a set of constituents represented as a tuples: (label, start, end).</S>
original cit marker offset is 0
new cit marker offset is 0



["'77'"]
'77'
['77']
parsed_discourse_facet ['method_citation']
  <S sid="11" ssid="7">Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
  <S sid="116" ssid="45">The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'116'"]
'116'
['116']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0623.csv
<S sid ="142" ssid = "4">Both of the switching techniques  as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.</S><S sid ="141" ssid = "3">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'141'", "'18'", "'81'", "'57'"]
'142'
'141'
'18'
'81'
'57'
['142', '141', '18', '81', '57']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'61'", "'81'", "'78'", "'139'"]
'112'
'61'
'81'
'78'
'139'
['112', '61', '81', '78', '139']
Error in Discourse Facet
<S sid ="62" ssid = "48">This is the parse that is closest to the centroid of the observed parses under the similarity metric.</S><S sid ="88" ssid = "17">For example  one parser could be more accurate at predicting noun phrases than the other parsers.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid ="115" ssid = "44">It is the performance we could achieve if an omniscient observer told us which parser to pick for each of the sentences.</S><S sid ="22" ssid = "8">If enough parsers suggest that a particular constituent belongs in the parse  we include it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'", "'88'", "'27'", "'115'", "'22'"]
'62'
'88'
'27'
'115'
'22'
['62', '88', '27', '115', '22']
Error in Discourse Facet
<S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="125" ssid = "54">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S><S sid ="41" ssid = "27">IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.</S><S sid ="44" ssid = "30">Each of the constituents must have received at least 1 votes from the k parsers  so a > I1 and 2 — 2k±-1 b > ri-5-111.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'21'", "'125'", "'41'", "'44'"]
'38'
'21'
'125'
'41'
'44'
['38', '21', '125', '41', '44']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'81'", "'15'", "'78'", "'139'"]
'112'
'81'
'15'
'78'
'139'
['112', '81', '15', '78', '139']
Error in Discourse Facet
<S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="136" ssid = "65">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S><S sid ="131" ssid = "60">It was then tested on section 22 of the Treebank in conjunction with the other parsers.</S><S sid ="103" ssid = "32">The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.</S><S sid ="132" ssid = "61">The results of this experiment can be seen in Table 5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'136'", "'131'", "'103'", "'132'"]
'21'
'136'
'131'
'103'
'132'
['21', '136', '131', '103', '132']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'78'", "'81'", "'61'", "'130'"]
'112'
'78'
'81'
'61'
'130'
['112', '78', '81', '61', '130']
Error in Discourse Facet
<S sid ="125" ssid = "54">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="1" ssid = "1">Three state-of-the-art statistical parsers are combined to produce more accurate parses  as well as new bounds on achievable Treebank parsing accuracy.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'27'", "'21'", "'1'", "'38'"]
'125'
'27'
'21'
'1'
'38'
['125', '27', '21', '1', '38']
Error in Discourse Facet
<S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="51" ssid = "37">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes  a completely flat structure.</S><S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'12'", "'51'", "'18'", "'57'"]
'55'
'12'
'51'
'18'
'57'
['55', '12', '51', '18', '57']
Error in Discourse Facet
<S sid ="142" ssid = "4">Both of the switching techniques  as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.</S><S sid ="127" ssid = "56">Parser 3  the most accurate parser  was chosen 71% of the time  and Parser 1  the least accurate parser was chosen 16% of the time.</S><S sid ="141" ssid = "3">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="140" ssid = "2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'127'", "'141'", "'12'", "'140'"]
'142'
'127'
'141'
'12'
'140'
['142', '127', '141', '12', '140']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'78'", "'61'", "'130'", "'15'"]
'112'
'78'
'61'
'130'
'15'
['112', '78', '61', '130', '15']
Error in Discourse Facet
<S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="51" ssid = "37">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes  a completely flat structure.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S><S sid ="26" ssid = "12">This technique has the advantage of requiring no training  but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'51'", "'52'", "'50'", "'26'"]
'55'
'51'
'52'
'50'
'26'
['55', '51', '52', '50', '26']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="114" ssid = "43">The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'81'", "'78'", "'61'", "'114'"]
'112'
'81'
'78'
'61'
'114'
['112', '81', '78', '61', '114']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'81'", "'12'", "'15'", "'130'"]
'112'
'81'
'12'
'15'
'130'
['112', '81', '12', '15', '130']
Error in Discourse Facet
<S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="114" ssid = "43">The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.</S><S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S>
original cit marker offset is 0
new cit marker offset is 0



["'78'", "'61'", "'114'", "'112'", "'81'"]
'78'
'61'
'114'
'112'
'81'
['78', '61', '114', '112', '81']
Error in Discourse Facet
<S sid ="129" ssid = "58">In the interest of testing the robustness of these combining techniques  we added a fourth  simple nonlexicalized PCFG parser.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'129'", "'11'", "'130'", "'15'", "'9'"]
'129'
'11'
'130'
'15'
'9'
['129', '11', '130', '15', '9']
Error in Discourse Facet
<S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S><S sid ="114" ssid = "43">The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.</S><S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["'81'", "'57'", "'114'", "'112'", "'78'"]
'81'
'57'
'114'
'112'
'78'
['81', '57', '114', '112', '78']
Error in Discourse Facet
['The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.']
['The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.', 'The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.', 'F-measure is the harmonic mean of precision and recall  2PR/(P + R).', 'The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.', "The second row is the accuracy of the best of the three parsers.'"]
['system', 'ROUGE-S*', 'Average_R:', '0.01484', '(95%-conf.int.', '0.01484', '-', '0.01484)']
['system', 'ROUGE-S*', 'Average_P:', '0.16667', '(95%-conf.int.', '0.16667', '-', '0.16667)']
['system', 'ROUGE-S*', 'Average_F:', '0.02726', '(95%-conf.int.', '0.02726', '-', '0.02726)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:66', 'F:11']
['Each parse is converted into a set of constituents represented as a tuples: (label, start, end).']
['The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.', 'We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.', "The second row is the accuracy of the best of the three parsers.'", 'F-measure is the harmonic mean of precision and recall  2PR/(P + R).', 'The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.']
['system', 'ROUGE-S*', 'Average_R:', '0.00142', '(95%-conf.int.', '0.00142', '-', '0.00142)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00271', '(95%-conf.int.', '0.00271', '-', '0.00271)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:36', 'F:1']
['One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure.']
['We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.', 'This technique has the advantage of requiring no training  but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.', 'One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes  a completely flat structure.', 'There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.', 'This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.']
['system', 'ROUGE-S*', 'Average_R:', '0.08521', '(95%-conf.int.', '0.08521', '-', '0.08521)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.15704', '(95%-conf.int.', '0.15704', '-', '0.15704)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:136', 'F:136']
['Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.']
['We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.', 'We have presented two general approaches to studying parser combination: parser switching and parse hybridization.', 'F-measure is the harmonic mean of precision and recall  2PR/(P + R).', 'The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.', "The second row is the accuracy of the best of the three parsers.'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00150', '(95%-conf.int.', '0.00150', '-', '0.00150)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00285', '(95%-conf.int.', '0.00285', '-', '0.00285)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:36', 'F:1']
['The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.']
['The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.', 'We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.', 'The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.', 'F-measure is the harmonic mean of precision and recall  2PR/(P + R).', "The second row is the accuracy of the best of the three parsers.'"]
['system', 'ROUGE-S*', 'Average_R:', '0.02273', '(95%-conf.int.', '0.02273', '-', '0.02273)']
['system', 'ROUGE-S*', 'Average_P:', '0.21818', '(95%-conf.int.', '0.21818', '-', '0.21818)']
['system', 'ROUGE-S*', 'Average_F:', '0.04117', '(95%-conf.int.', '0.04117', '-', '0.04117)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:528', 'P:55', 'F:12']
['Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.']
['Three state-of-the-art statistical parsers are combined to produce more accurate parses  as well as new bounds on achievable Treebank parsing accuracy.', u'Under certain conditions the constituent voting and na\xefve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.', u'Another technique for parse hybridization is to use a na\xefve Bayes classifier to determine which constituents to include in the parse.', u'The constituent voting and na\xefve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.', "One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set."]
['system', 'ROUGE-S*', 'Average_R:', '0.05738', '(95%-conf.int.', '0.05738', '-', '0.05738)']
['system', 'ROUGE-S*', 'Average_P:', '0.87500', '(95%-conf.int.', '0.87500', '-', '0.87500)']
['system', 'ROUGE-S*', 'Average_F:', '0.10769', '(95%-conf.int.', '0.10769', '-', '0.10769)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:120', 'F:105']
['The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.']
['It is the performance we could achieve if an omniscient observer told us which parser to pick for each of the sentences.', 'This is the parse that is closest to the centroid of the observed parses under the similarity metric.', u'Another technique for parse hybridization is to use a na\xc3\xafve Bayes classifier to determine which constituents to include in the parse.', 'For example  one parser could be more accurate at predicting noun phrases than the other parsers.', 'If enough parsers suggest that a particular constituent belongs in the parse  we include it.']
['system', 'ROUGE-S*', 'Average_R:', '0.00142', '(95%-conf.int.', '0.00142', '-', '0.00142)']
['system', 'ROUGE-S*', 'Average_P:', '0.00735', '(95%-conf.int.', '0.00735', '-', '0.00735)']
['system', 'ROUGE-S*', 'Average_F:', '0.00238', '(95%-conf.int.', '0.00238', '-', '0.00238)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:136', 'F:1']
['It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.']
['The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.', 'We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.', 'The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.', 'We are interested in combining the substructures of the input parses to produce a better parse.', "The second row is the accuracy of the best of the three parsers.'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00168', '(95%-conf.int.', '0.00168', '-', '0.00168)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00312', '(95%-conf.int.', '0.00313', '-', '0.00313)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:45', 'F:1']
0.293122496336 0.0232724997091 0.0430274994622





input/ref/Task1/D09-1092_swastika.csv
input/res/Task1/D09-1092.csv
parsing: input/ref/Task1/D09-1092_swastika.csv
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="114" ssid="63">Ideally, the &#8220;glue&#8221; documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.</S>
original cit marker offset is 0
new cit marker offset is 0



['114']
114
['114']
parsed_discourse_facet ['aim_citation']
<S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



['138']
138
['138']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



['55']
55
['55']
parsed_discourse_facet ['method_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['192']
192
['192']
parsed_discourse_facet ['result_citation']
<S sid="119" ssid="68">The lower the divergence, the more similar the distributions are to each other.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['result_citation']
<S sid="148" ssid="97">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="163" ssid="112">Performance continues to improve with longer documents, most likely due to better topic inference.</S>
original cit marker offset is 0
new cit marker offset is 0



['163']
163
['163']
parsed_discourse_facet ['result_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="148" ssid="97">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['192']
192
['192']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['method_citation']
    <S sid="10" ssid="6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'4'", "'134'", "'54'", "'35'"]
'131'
'4'
'134'
'54'
'35'
['131', '4', '134', '54', '35']
parsed_discourse_facet ['results_citation']
<S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="6" ssid = "2">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'148'", "'146'", "'4'", "'131'", "'6'"]
'148'
'146'
'4'
'131'
'6'
['148', '146', '4', '131', '6']
parsed_discourse_facet ['method_citation']
<S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of “objective.” The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S>
original cit marker offset is 0
new cit marker offset is 0



["'146'", "'131'", "'148'", "'25'", "'69'"]
'146'
'131'
'148'
'25'
'69'
['146', '131', '148', '25', '69']
parsed_discourse_facet ['method_citation']
<S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="195" ssid = "4">Additionally  PLTM can support the creation of bilingual lexica for low resource language pairs  providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'146'", "'195'", "'4'", "'19'"]
'25'
'146'
'195'
'4'
'19'
['25', '146', '195', '4', '19']
parsed_discourse_facet ['method_citation']
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="47" ssid = "13">We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="86" ssid = "35">Given a set of training document tuples  PLTM can be used to obtain posterior estimates of Φ'  ...   ΦL and αm.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'47'", "'148'", "'86'", "'35'"]
'131'
'47'
'148'
'86'
'35'
['131', '47', '148', '86', '35']
parsed_discourse_facet ['method_citation']
<S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S><S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="182" ssid = "16">As with EuroParl  we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.</S><S sid ="37" ssid = "3">PLTM assumes that the documents in a tuple share the same tuple-specific distribution over topics.</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'161'", "'59'", "'182'", "'37'", "'91'"]
'161'
'59'
'182'
'37'
'91'
['161', '59', '182', '37', '91']
parsed_discourse_facet ['method_citation']
<S sid ="158" ssid = "107">Results averaged over all query/target language pairs are shown in figure 7 for Jensen-Shannon divergence.</S><S sid ="187" ssid = "21">To demonstrate the wide variation in topics  we calculated the proportion of tokens in each language assigned to each topic.</S><S sid ="95" ssid = "44">Additionally  the predictive ability of PLTM is consistently slightly worse than that of (monolingual) LDA.</S><S sid ="72" ssid = "21">To answer this question  we compute the posterior probability of each topic in each tuple under the trained model.</S><S sid ="154" ssid = "103">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al.  2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'", "'187'", "'95'", "'72'", "'154'"]
'158'
'187'
'95'
'72'
'154'
['158', '187', '95', '72', '154']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'131'", "'35'", "'54'", "'91'"]
'134'
'131'
'35'
'54'
'91'
['134', '131', '35', '54', '91']
parsed_discourse_facet ['method_citation']
<S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="86" ssid = "35">Given a set of training document tuples  PLTM can be used to obtain posterior estimates of Φ'  ...   ΦL and αm.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S><S sid ="46" ssid = "12">  ΦL and αm from P(Φ1  ...   ΦL  αm  |W'  β) or by evaluating a point estimate.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'", "'86'", "'131'", "'91'", "'46'"]
'35'
'86'
'131'
'91'
'46'
['35', '86', '131', '91', '46']
parsed_discourse_facet ['method_citation']
<S sid ="67" ssid = "16">(Interestingly  all languages except Greek and Finnish use closely related words for “youth” or “young” in a separate topic.)</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S><S sid ="36" ssid = "2">Each tuple is a set of documents that are loosely equivalent to each other  but written in different languages  e.g.  corresponding Wikipedia articles in French  English and German.</S><S sid ="65" ssid = "14">This topic provides an illustration of the variation in technical terminology captured by PLTM  including the wide array of acronyms used by different languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'54'", "'38'", "'36'", "'65'"]
'67'
'54'
'38'
'36'
'65'
['67', '54', '38', '36', '65']
parsed_discourse_facet ['method_citation']
<S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="143" ssid = "92">We also do not count morphological variants: the model finds EN “rules” and DE “vorschriften ” but the lexicon contains only “rule” and “vorschrift.” Results remain strong as we increase K. With K = 3  T = 800  1349 of the 7200 candidate pairs for Spanish appeared in the lexicon. topic in different languages translations of each other?</S><S sid ="39" ssid = "5">Additionally  PLTM assumes that each “topic” consists of a set of discrete distributions over words—one for each language l = 1  ...   L. In other words  rather than using a single set of topics Φ = {φ1  ...   φT}  as in LDA  there are L sets of language-specific topics  Φ1  ...   ΦL  each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter βl.</S><S sid ="78" ssid = "27">Although the model does not distinguish between topic assignment variables within a given document tuple (so it is technically incorrect to speak of different posterior distributions over topics for different documents in a given tuple)  we can nevertheless divide topic assignment variables between languages and use them to estimate a Dirichlet-multinomial posterior distribution for each language in each tuple.</S><S sid ="40" ssid = "6">Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter α and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl ∼ P(wl  |zl Φl) = 11n φlwl |zl .</S>
original cit marker offset is 0
new cit marker offset is 0



["'148'", "'143'", "'39'", "'78'", "'40'"]
'148'
'143'
'39'
'78'
'40'
['148', '143', '39', '78', '40']
parsed_discourse_facet ['method_citation']
<S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="68" ssid = "17">The third topic demonstrates differences in inflectional variation.</S><S sid ="193" ssid = "2">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="129" ssid = "78">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'", "'68'", "'193'", "'25'", "'129'"]
'52'
'68'
'193'
'25'
'129'
['52', '68', '193', '25', '129']
parsed_discourse_facet ['method_citation']
<S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="92" ssid = "41">We perform five estimation runs for each document and then calculate standard errors using a bootstrap method.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'43'", "'52'", "'25'", "'92'"]
'147'
'43'
'52'
'25'
'92'
['147', '43', '52', '25', '92']
parsed_discourse_facet ['results_citation']
<S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="47" ssid = "13">We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'43'", "'148'", "'47'", "'131'"]
'25'
'43'
'148'
'47'
'131'
['25', '43', '148', '47', '131']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="55" ssid = "4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish  German  Greek  English  Spanish  Finnish  French  Italian  Dutch  Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'35'", "'134'", "'131'", "'55'"]
'4'
'35'
'134'
'131'
'55'
['4', '35', '134', '131', '55']
parsed_discourse_facet ['method_citation']
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'134'", "'4'", "'35'", "'25'"]
'131'
'134'
'4'
'35'
'25'
['131', '134', '4', '35', '25']
parsed_discourse_facet ['method_citation']
<S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S><S sid ="164" ssid = "113">Results vary by language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'161'", "'134'", "'91'", "'164'"]
'59'
'161'
'134'
'91'
'164'
['59', '161', '134', '91', '164']
parsed_discourse_facet ['method_citation']
<S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="195" ssid = "4">Additionally  PLTM can support the creation of bilingual lexica for low resource language pairs  providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S><S sid ="24" ssid = "20">By linking topics across languages  polylingual topic models can increase cross-cultural understanding by providing readers with the ability to characterize the contents of collections in unfamiliar languages and identify trends in topic prevalence.</S><S sid ="83" ssid = "32">The vast majority of divergences are relatively low (1.0 indicates no overlap in topics between languages in a given document tuple) indicating that  for each tuple  the model is not simply assigning all tokens in a particular language to a single topic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'146'", "'195'", "'24'", "'83'"]
'25'
'146'
'195'
'24'
'83'
['25', '146', '195', '24', '83']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid ="47" ssid = "13">We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="120" ssid = "69">From the results in figure 4  we know that leaving all document tuples intact should result in a mean JS divergence of less than 0.1.</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'47'", "'25'", "'120'", "'148'"]
'43'
'47'
'25'
'120'
'148'
['43', '47', '25', '120', '148']
parsed_discourse_facet ['aim_citation', 'results_citation']
['In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models.']
['In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.', 'Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).', 'Additionally  PLTM can support the creation of bilingual lexica for low resource language pairs  providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.', u'We explore the model\xe2\u20ac\u2122s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.', 'We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.']
['system', 'ROUGE-S*', 'Average_R:', '0.00055', '(95%-conf.int.', '0.00055', '-', '0.00055)']
['system', 'ROUGE-S*', 'Average_P:', '0.04444', '(95%-conf.int.', '0.04444', '-', '0.04444)']
['system', 'ROUGE-S*', 'Average_F:', '0.00108', '(95%-conf.int.', '0.00108', '-', '0.00108)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3655', 'P:45', 'F:2']
['We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.']
['In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.', 'Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', 'To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.', u'English and the Romance languages use only singular and plural versions of \u201cobjective.\u201d The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.']
['system', 'ROUGE-S*', 'Average_R:', '0.00147', '(95%-conf.int.', '0.00147', '-', '0.00147)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.00288', '(95%-conf.int.', '0.00288', '-', '0.00288)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4095', 'P:78', 'F:6']
['We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).']
['From the results in figure 4  we know that leaving all document tuples intact should result in a mean JS divergence of less than 0.1.', u'We take the latter approach  and use the MAP estimate for \u03b1m and the predictive distributions over words for \u03a61  .', u'These tasks can either be accomplished by averaging over samples of \u03a61  .', 'To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.', 'Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).']
['system', 'ROUGE-S*', 'Average_R:', '0.00055', '(95%-conf.int.', '0.00055', '-', '0.00055)']
['system', 'ROUGE-S*', 'Average_P:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00103', '(95%-conf.int.', '0.00103', '-', '0.00103)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:120', 'F:1']
['Ideally, the &#8220;glue&#8221; documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.']
['In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', u'We explore the model\u2019s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.', 'To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.', 'Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).']
['system', 'ROUGE-S*', 'Average_R:', '0.00571', '(95%-conf.int.', '0.00571', '-', '0.00571)']
['system', 'ROUGE-S*', 'Average_P:', '0.27500', '(95%-conf.int.', '0.27500', '-', '0.27500)']
['system', 'ROUGE-S*', 'Average_F:', '0.01119', '(95%-conf.int.', '0.01119', '-', '0.01119)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5778', 'P:120', 'F:33']
['The lower the divergence, the more similar the distributions are to each other.']
['We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al.  2009).', 'Results averaged over all query/target language pairs are shown in figure 7 for Jensen-Shannon divergence.', 'Additionally  the predictive ability of PLTM is consistently slightly worse than that of (monolingual) LDA.', 'To demonstrate the wide variation in topics  we calculated the proportion of tokens in each language assigned to each topic.', 'To answer this question  we compute the posterior probability of each topic in each tuple under the trained model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:6', 'F:0']
['The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.']
[u'We take the latter approach  and use the MAP estimate for \u03b1m and the predictive distributions over words for \u03a61  .', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', u"Given a set of training document tuples  PLTM can be used to obtain posterior estimates of \u03a6'  ...   \u03a6L and \u03b1m.", 'The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.', 'To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.']
['system', 'ROUGE-S*', 'Average_R:', '0.00043', '(95%-conf.int.', '0.00043', '-', '0.00043)']
['system', 'ROUGE-S*', 'Average_P:', '0.00526', '(95%-conf.int.', '0.00526', '-', '0.00526)']
['system', 'ROUGE-S*', 'Average_F:', '0.00079', '(95%-conf.int.', '0.00079', '-', '0.00079)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:190', 'F:1']
['To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.']
['Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', u'These tasks can either be accomplished by averaging over samples of \u03a61  .', 'To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.', u'We take the latter approach  and use the MAP estimate for \u03b1m and the predictive distributions over words for \u03a61  .']
['system', 'ROUGE-S*', 'Average_R:', '0.09790', '(95%-conf.int.', '0.09790', '-', '0.09790)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.17834', '(95%-conf.int.', '0.17834', '-', '0.17834)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:210', 'F:210']
['Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.']
['The EuroParl corpus consists of parallel texts in eleven western European languages: Danish  German  Greek  English  Spanish  Finnish  French  Italian  Dutch  Portuguese and Swedish.', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', u'We explore the model\u2019s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.', 'The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.', 'Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.']
['system', 'ROUGE-S*', 'Average_R:', '0.00325', '(95%-conf.int.', '0.00325', '-', '0.00325)']
['system', 'ROUGE-S*', 'Average_P:', '0.04329', '(95%-conf.int.', '0.04329', '-', '0.04329)']
['system', 'ROUGE-S*', 'Average_F:', '0.00604', '(95%-conf.int.', '0.00604', '-', '0.00604)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:231', 'F:10']
['We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.']
['Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', u'We explore the model\u2019s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.', 'The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.', 'Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.']
['system', 'ROUGE-S*', 'Average_R:', '0.00235', '(95%-conf.int.', '0.00235', '-', '0.00235)']
['system', 'ROUGE-S*', 'Average_P:', '0.13333', '(95%-conf.int.', '0.13333', '-', '0.13333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00461', '(95%-conf.int.', '0.00461', '-', '0.00461)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:45', 'F:6']
['We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
[u'We use the \u201cleft-to-right\u201d method of (Wallach et al.  2009).', 'Results vary by language.', 'The remaining collection consists of over 121 million words.', 'As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.', 'Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:45', 'F:0']
0.158656998413 0.0112209998878 0.020595999794





input/ref/Task1/D09-1092_sweta.csv
input/res/Task1/D09-1092.csv
parsing: input/ref/Task1/D09-1092_sweta.csv
 <S sid="32" ssid="8">Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="5">Additionally, PLTM assumes that each &#8220;topic&#8221; consists of a set of discrete distributions over words&#8212;one for each language l = 1, ... , L. In other words, rather than using a single set of topics &#934; = {&#966;1, ... , &#966;T}, as in LDA, there are L sets of language-specific topics, &#934;1, ... , &#934;L, each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter &#946;l.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



["138'"]
138'
['138']
parsed_discourse_facet ['method_citation']
 <S sid="196" ssid="5">When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>
original cit marker offset is 0
new cit marker offset is 0



["196'"]
196'
['196']
parsed_discourse_facet ['method_citation']
<S sid="111" ssid="60">In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts &#8211; i.e., we put each of these documents in a single-document tuple.</S>
original cit marker offset is 0
new cit marker offset is 0



["111'"]
111'
['111']
parsed_discourse_facet ['method_citation']
<S sid="128" ssid="77">Although the PLTM is clearly not a substitute for a machine translation system&#8212;it has no way to represent syntax or even multi-word phrases&#8212;it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.</S>
original cit marker offset is 0
new cit marker offset is 0



["128'"]
128'
['128']
parsed_discourse_facet ['method_citation']
<S sid="122" ssid="71">Divergence drops significantly when the proportion of &#8220;glue&#8221; tuples increases from 0.01 to 0.25.</S>
original cit marker offset is 0
new cit marker offset is 0



["122'"]
122'
['122']
parsed_discourse_facet ['method_citation']
 <S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
 <S sid="110" ssid="59">Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S>
original cit marker offset is 0
new cit marker offset is 0



["110'"]
110'
['110']
parsed_discourse_facet ['method_citation']
<S sid="30" ssid="6">However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["30'"]
30'
['30']
parsed_discourse_facet ['method_citation']
    <S sid="146" ssid="95">In addition to enhancing lexicons by aligning topic-specific vocabulary, PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["146'"]
146'
['146']
parsed_discourse_facet ['method_citation']
<S sid="77" ssid="26">Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.</
original cit marker offset is 0
new cit marker offset is 0



["77'"]
77'
['77']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="105">We use both Jensen-Shannon divergence and cosine distance.</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="7">They also provide little analysis of the differences between polylingual and single-language topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>
original cit marker offset is 0
new cit marker offset is 0



["19'"]
19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="131" ssid="80">We evaluate sets of high-probability words in each topic and multilingual &#8220;synsets&#8221; by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["131'"]
131'
['131']
parsed_discourse_facet ['method_citation']
<S sid="196" ssid="5">When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>
original cit marker offset is 0
new cit marker offset is 0



["196'"]
196'
['196']
parsed_discourse_facet ['method_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["192'"]
192'
['192']
parsed_discourse_facet ['method_citation']
<S sid="195" ssid="4">Additionally, PLTM can support the creation of bilingual lexica for low resource language pairs, providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["195'"]
195'
['195']
parsed_discourse_facet ['method_citation']
<S sid="6" ssid="2">Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["6'"]
6'
['6']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'4'", "'134'", "'54'", "'35'"]
'131'
'4'
'134'
'54'
'35'
['131', '4', '134', '54', '35']
parsed_discourse_facet ['results_citation']
<S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="6" ssid = "2">Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'148'", "'146'", "'4'", "'131'", "'6'"]
'148'
'146'
'4'
'131'
'6'
['148', '146', '4', '131', '6']
parsed_discourse_facet ['method_citation']
<S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="69" ssid = "18">English and the Romance languages use only singular and plural versions of “objective.” The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.</S>
original cit marker offset is 0
new cit marker offset is 0



["'146'", "'131'", "'148'", "'25'", "'69'"]
'146'
'131'
'148'
'25'
'69'
['146', '131', '148', '25', '69']
parsed_discourse_facet ['method_citation']
<S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="195" ssid = "4">Additionally  PLTM can support the creation of bilingual lexica for low resource language pairs  providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="19" ssid = "15">We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'146'", "'195'", "'4'", "'19'"]
'25'
'146'
'195'
'4'
'19'
['25', '146', '195', '4', '19']
parsed_discourse_facet ['method_citation']
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="47" ssid = "13">We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="86" ssid = "35">Given a set of training document tuples  PLTM can be used to obtain posterior estimates of Φ'  ...   ΦL and αm.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'47'", "'148'", "'86'", "'35'"]
'131'
'47'
'148'
'86'
'35'
['131', '47', '148', '86', '35']
parsed_discourse_facet ['method_citation']
<S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S><S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="182" ssid = "16">As with EuroParl  we can calculate the JensenShannon divergence between pairs of documents within a comparable document tuple.</S><S sid ="37" ssid = "3">PLTM assumes that the documents in a tuple share the same tuple-specific distribution over topics.</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'161'", "'59'", "'182'", "'37'", "'91'"]
'161'
'59'
'182'
'37'
'91'
['161', '59', '182', '37', '91']
parsed_discourse_facet ['method_citation']
<S sid ="158" ssid = "107">Results averaged over all query/target language pairs are shown in figure 7 for Jensen-Shannon divergence.</S><S sid ="187" ssid = "21">To demonstrate the wide variation in topics  we calculated the proportion of tokens in each language assigned to each topic.</S><S sid ="95" ssid = "44">Additionally  the predictive ability of PLTM is consistently slightly worse than that of (monolingual) LDA.</S><S sid ="72" ssid = "21">To answer this question  we compute the posterior probability of each topic in each tuple under the trained model.</S><S sid ="154" ssid = "103">We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al.  2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'", "'187'", "'95'", "'72'", "'154'"]
'158'
'187'
'95'
'72'
'154'
['158', '187', '95', '72', '154']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'131'", "'35'", "'54'", "'91'"]
'134'
'131'
'35'
'54'
'91'
['134', '131', '35', '54', '91']
parsed_discourse_facet ['method_citation']
<S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="86" ssid = "35">Given a set of training document tuples  PLTM can be used to obtain posterior estimates of Φ'  ...   ΦL and αm.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S><S sid ="46" ssid = "12">  ΦL and αm from P(Φ1  ...   ΦL  αm  |W'  β) or by evaluating a point estimate.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'", "'86'", "'131'", "'91'", "'46'"]
'35'
'86'
'131'
'91'
'46'
['35', '86', '131', '91', '46']
parsed_discourse_facet ['method_citation']
<S sid ="67" ssid = "16">(Interestingly  all languages except Greek and Finnish use closely related words for “youth” or “young” in a separate topic.)</S><S sid ="54" ssid = "3">Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.</S><S sid ="38" ssid = "4">This is unlike LDA  in which each document is assumed to have its own document-specific distribution over topics.</S><S sid ="36" ssid = "2">Each tuple is a set of documents that are loosely equivalent to each other  but written in different languages  e.g.  corresponding Wikipedia articles in French  English and German.</S><S sid ="65" ssid = "14">This topic provides an illustration of the variation in technical terminology captured by PLTM  including the wide array of acronyms used by different languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'67'", "'54'", "'38'", "'36'", "'65'"]
'67'
'54'
'38'
'36'
'65'
['67', '54', '38', '36', '65']
parsed_discourse_facet ['method_citation']
<S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="143" ssid = "92">We also do not count morphological variants: the model finds EN “rules” and DE “vorschriften ” but the lexicon contains only “rule” and “vorschrift.” Results remain strong as we increase K. With K = 3  T = 800  1349 of the 7200 candidate pairs for Spanish appeared in the lexicon. topic in different languages translations of each other?</S><S sid ="39" ssid = "5">Additionally  PLTM assumes that each “topic” consists of a set of discrete distributions over words—one for each language l = 1  ...   L. In other words  rather than using a single set of topics Φ = {φ1  ...   φT}  as in LDA  there are L sets of language-specific topics  Φ1  ...   ΦL  each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter βl.</S><S sid ="78" ssid = "27">Although the model does not distinguish between topic assignment variables within a given document tuple (so it is technically incorrect to speak of different posterior distributions over topics for different documents in a given tuple)  we can nevertheless divide topic assignment variables between languages and use them to estimate a Dirichlet-multinomial posterior distribution for each language in each tuple.</S><S sid ="40" ssid = "6">Anew document tuple w = (w1  ...   wL) is generated by first drawing a tuple-specific topic distribution from an asymmetric Dirichlet prior with concentration parameter α and base measure m: Then  for each language l  a latent topic assignment is drawn for each token in that language: Finally  the observed tokens are themselves drawn using the language-specific topic parameters: wl ∼ P(wl  |zl Φl) = 11n φlwl |zl .</S>
original cit marker offset is 0
new cit marker offset is 0



["'148'", "'143'", "'39'", "'78'", "'40'"]
'148'
'143'
'39'
'78'
'40'
['148', '143', '39', '78', '40']
parsed_discourse_facet ['method_citation']
<S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="68" ssid = "17">The third topic demonstrates differences in inflectional variation.</S><S sid ="193" ssid = "2">We analyzed the characteristics of PLTM in comparison to monolingual LDA  and demonstrated that it is possible to discover aligned topics.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="129" ssid = "78">We therefore evaluate the ability of the PLTM to generate bilingual lexica  similar to other work in unsupervised translation modeling (Haghighi et al.  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'", "'68'", "'193'", "'25'", "'129'"]
'52'
'68'
'193'
'25'
'129'
['52', '68', '193', '25', '129']
parsed_discourse_facet ['method_citation']
<S sid ="147" ssid = "96">These aligned document pairs could then be fed into standard machine translation systems as training data.</S><S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid ="52" ssid = "1">Our first set of experiments focuses on document tuples that are known to consist of direct translations.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="92" ssid = "41">We perform five estimation runs for each document and then calculate standard errors using a bootstrap method.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'43'", "'52'", "'25'", "'92'"]
'147'
'43'
'52'
'25'
'92'
['147', '43', '52', '25', '92']
parsed_discourse_facet ['results_citation']
<S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S><S sid ="47" ssid = "13">We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'43'", "'148'", "'47'", "'131'"]
'25'
'43'
'148'
'47'
'131'
['25', '43', '148', '47', '131']
parsed_discourse_facet ['method_citation']
<S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="55" ssid = "4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish  German  Greek  English  Spanish  Finnish  French  Italian  Dutch  Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'35'", "'134'", "'131'", "'55'"]
'4'
'35'
'134'
'131'
'55'
['4', '35', '134', '131', '55']
parsed_discourse_facet ['method_citation']
<S sid ="131" ssid = "80">We evaluate sets of high-probability words in each topic and multilingual “synsets” by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="4" ssid = "4">We explore the model’s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.</S><S sid ="35" ssid = "1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'", "'134'", "'4'", "'35'", "'25'"]
'131'
'134'
'4'
'35'
'25'
['131', '134', '4', '35', '25']
parsed_discourse_facet ['method_citation']
<S sid ="59" ssid = "8">The remaining collection consists of over 121 million words.</S><S sid ="161" ssid = "110">As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.</S><S sid ="134" ssid = "83">Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.</S><S sid ="91" ssid = "40">We use the “left-to-right” method of (Wallach et al.  2009).</S><S sid ="164" ssid = "113">Results vary by language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'161'", "'134'", "'91'", "'164'"]
'59'
'161'
'134'
'91'
'164'
['59', '161', '134', '91', '164']
parsed_discourse_facet ['method_citation']
<S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="146" ssid = "95">In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S sid ="195" ssid = "4">Additionally  PLTM can support the creation of bilingual lexica for low resource language pairs  providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S><S sid ="24" ssid = "20">By linking topics across languages  polylingual topic models can increase cross-cultural understanding by providing readers with the ability to characterize the contents of collections in unfamiliar languages and identify trends in topic prevalence.</S><S sid ="83" ssid = "32">The vast majority of divergences are relatively low (1.0 indicates no overlap in topics between languages in a given document tuple) indicating that  for each tuple  the model is not simply assigning all tokens in a particular language to a single topic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'146'", "'195'", "'24'", "'83'"]
'25'
'146'
'195'
'24'
'83'
['25', '146', '195', '24', '83']
parsed_discourse_facet ['method_citation']
<S sid ="43" ssid = "9">These tasks can either be accomplished by averaging over samples of Φ1  .</S><S sid ="47" ssid = "13">We take the latter approach  and use the MAP estimate for αm and the predictive distributions over words for Φ1  .</S><S sid ="25" ssid = "1">Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).</S><S sid ="120" ssid = "69">From the results in figure 4  we know that leaving all document tuples intact should result in a mean JS divergence of less than 0.1.</S><S sid ="148" ssid = "97">To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'", "'47'", "'25'", "'120'", "'148'"]
'43'
'47'
'25'
'120'
'148'
['43', '47', '25', '120', '148']
parsed_discourse_facet ['aim_citation', 'results_citation']
['When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.']
['In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.', 'Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).', 'Additionally  PLTM can support the creation of bilingual lexica for low resource language pairs  providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.', u'We explore the model\xe2\u20ac\u2122s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.', 'We employ a set of direct translations  the EuroParl corpus  to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.']
['system', 'ROUGE-S*', 'Average_R:', '0.00164', '(95%-conf.int.', '0.00164', '-', '0.00164)']
['system', 'ROUGE-S*', 'Average_P:', '0.05000', '(95%-conf.int.', '0.05000', '-', '0.05000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00318', '(95%-conf.int.', '0.00318', '-', '0.00318)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3655', 'P:120', 'F:6']
['We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.']
['In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.', 'Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', 'To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.', u'English and the Romance languages use only singular and plural versions of \u201cobjective.\u201d The other Germanic languages include compound words  while Greek and Finnish are dominated by inflected variants of the same lexical item.']
['system', 'ROUGE-S*', 'Average_R:', '0.00147', '(95%-conf.int.', '0.00147', '-', '0.00147)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.00288', '(95%-conf.int.', '0.00288', '-', '0.00288)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4095', 'P:78', 'F:6']
['Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).']
['From the results in figure 4  we know that leaving all document tuples intact should result in a mean JS divergence of less than 0.1.', u'We take the latter approach  and use the MAP estimate for \u03b1m and the predictive distributions over words for \u03a61  .', u'These tasks can either be accomplished by averaging over samples of \u03a61  .', 'To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.', 'Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).']
['system', 'ROUGE-S*', 'Average_R:', '0.00437', '(95%-conf.int.', '0.00437', '-', '0.00437)']
['system', 'ROUGE-S*', 'Average_P:', '0.01270', '(95%-conf.int.', '0.01270', '-', '0.01270)']
['system', 'ROUGE-S*', 'Average_F:', '0.00650', '(95%-conf.int.', '0.00650', '-', '0.00650)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:630', 'F:8']
['Additionally, PLTM assumes that each &#8220;topic&#8221; consists of a set of discrete distributions over words&#8212;one for each language l = 1, ... , L. In other words, rather than using a single set of topics &#934; = {&#966;1, ... , &#966;T}, as in LDA, there are L sets of language-specific topics, &#934;1, ... , &#934;L, each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter &#946;l.']
['In addition to enhancing lexicons by aligning topic-specific vocabulary  PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', u'We explore the model\u2019s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.', 'To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.', 'Topic models have been used for analyzing topic trends in research literature (Mann et al.  2006; Hall et al.  2008)  inferring captions for images (Blei and Jordan  2003)  social network analysis in email (McCallum et al.  2005)  and expanding queries with topically related words in information retrieval (Wei and Croft  2006).']
['system', 'ROUGE-S*', 'Average_R:', '0.01540', '(95%-conf.int.', '0.01540', '-', '0.01540)']
['system', 'ROUGE-S*', 'Average_P:', '0.12660', '(95%-conf.int.', '0.12660', '-', '0.12660)']
['system', 'ROUGE-S*', 'Average_F:', '0.02746', '(95%-conf.int.', '0.02746', '-', '0.02746)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5778', 'P:703', 'F:89']
['Divergence drops significantly when the proportion of &#8220;glue&#8221; tuples increases from 0.01 to 0.25.']
['We then use this model to infer topic distributions for each of the 11 documents in each of the held-out document tuples using a method similar to that used to calculate held-out probabilities (Wallach et al.  2009).', 'Results averaged over all query/target language pairs are shown in figure 7 for Jensen-Shannon divergence.', 'Additionally  the predictive ability of PLTM is consistently slightly worse than that of (monolingual) LDA.', 'To demonstrate the wide variation in topics  we calculated the proportion of tokens in each language assigned to each topic.', 'To answer this question  we compute the posterior probability of each topic in each tuple under the trained model.']
['system', 'ROUGE-S*', 'Average_R:', '0.00188', '(95%-conf.int.', '0.00188', '-', '0.00188)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00358', '(95%-conf.int.', '0.00358', '-', '0.00358)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:78', 'F:3']
['In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts &#8211; i.e., we put each of these documents in a single-document tuple.']
[u'We take the latter approach  and use the MAP estimate for \u03b1m and the predictive distributions over words for \u03a61  .', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', u"Given a set of training document tuples  PLTM can be used to obtain posterior estimates of \u03a6'  ...   \u03a6L and \u03b1m.", 'The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.', 'To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.']
['system', 'ROUGE-S*', 'Average_R:', '0.00980', '(95%-conf.int.', '0.00980', '-', '0.00980)']
['system', 'ROUGE-S*', 'Average_P:', '0.13450', '(95%-conf.int.', '0.13450', '-', '0.13450)']
['system', 'ROUGE-S*', 'Average_F:', '0.01828', '(95%-conf.int.', '0.01828', '-', '0.01828)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:171', 'F:23']
['We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.']
['Bilingual topic models for parallel texts with word-to-word alignments have been studied previously using the HM-bitam model (Zhao and Xing  2007).', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', u'These tasks can either be accomplished by averaging over samples of \u03a61  .', 'To evaluate this scenario  we train PLTM on a set of document tuples from EuroParl  infer topic distributions for a set of held-out documents  and then measure our ability to align documents in one language with their translations in another language.', u'We take the latter approach  and use the MAP estimate for \u03b1m and the predictive distributions over words for \u03a61  .']
['system', 'ROUGE-S*', 'Average_R:', '0.00932', '(95%-conf.int.', '0.00932', '-', '0.00932)']
['system', 'ROUGE-S*', 'Average_P:', '0.21978', '(95%-conf.int.', '0.21978', '-', '0.21978)']
['system', 'ROUGE-S*', 'Average_F:', '0.01789', '(95%-conf.int.', '0.01789', '-', '0.01789)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:91', 'F:20']
['We evaluate sets of high-probability words in each topic and multilingual &#8220;synsets&#8221; by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).']
['The EuroParl corpus consists of parallel texts in eleven western European languages: Danish  German  Greek  English  Spanish  Finnish  French  Italian  Dutch  Portuguese and Swedish.', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', u'We explore the model\u2019s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.', 'The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.', 'Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.']
['system', 'ROUGE-S*', 'Average_R:', '0.04414', '(95%-conf.int.', '0.04414', '-', '0.04414)']
['system', 'ROUGE-S*', 'Average_P:', '0.79532', '(95%-conf.int.', '0.79532', '-', '0.79532)']
['system', 'ROUGE-S*', 'Average_F:', '0.08364', '(95%-conf.int.', '0.08364', '-', '0.08364)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3081', 'P:171', 'F:136']
['Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.']
['Although direct translations in multiple languages are relatively rare (in contrast with comparable documents)  we use direct translations to explore the characteristics of the model.', u'We evaluate sets of high-probability words in each topic and multilingual \u201csynsets\u201d by comparing them to entries in human-constructed bilingual dictionaries  as done by Haghighi et al. (2008).', u'We explore the model\u2019s characteristics using two large corpora  each with over ten different languages  and demonstrate its usefulness in supporting machine translation and tracking topic trends across languages.', 'The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al.  2003) for modeling polylingual document tuples.', 'Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.']
['system', 'ROUGE-S*', 'Average_R:', '0.00548', '(95%-conf.int.', '0.00548', '-', '0.00548)']
['system', 'ROUGE-S*', 'Average_P:', '0.11667', '(95%-conf.int.', '0.11667', '-', '0.11667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01046', '(95%-conf.int.', '0.01046', '-', '0.01046)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2556', 'P:120', 'F:14']
['We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
[u'We use the \u201cleft-to-right\u201d method of (Wallach et al.  2009).', 'Results vary by language.', 'The remaining collection consists of over 121 million words.', 'As noted before  many of the documents in the EuroParl collection consist of short  formulaic sentences.', 'Each lexicon is a set of pairs consisting of an English word and a translated word  1we  wt}.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:45', 'F:0']
0.157094998429 0.0093499999065 0.0173869998261





input/ref/Task1/W99-0613_vardha.csv
input/res/Task1/W99-0613.csv
parsing: input/ref/Task1/W99-0613_vardha.csv
    <S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="35" ssid="29">AdaBoost finds a weighted combination of simple (weak) classifiers, where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
    <S sid="134" ssid="1">This section describes an algorithm based on boosting algorithms, which were previously developed for supervised machine learning problems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'"]
'134'
['134']
parsed_discourse_facet ['method_citation']
    <S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'"]
'236'
['236']
parsed_discourse_facet ['method_citation']
    <S sid="8" ssid="2">Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
    <S sid="42" ssid="36">(Riloff and Shepherd 97) describe a bootstrapping approach for acquiring nouns in particular categories (such as &amp;quot;vehicle&amp;quot; or &amp;quot;weapon&amp;quot; categories).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
    <S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'"]
'236'
['236']
parsed_discourse_facet ['method_citation']
    <S sid="222" ssid="1">The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'222'"]
'222'
['222']
parsed_discourse_facet ['method_citation']
    <S sid="30" ssid="24">(Blum and Mitchell 98) offer a promising formulation of redundancy, also prove some results about how the use of can help classification, and suggest an objective function when training with unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'"]
'30'
['30']
parsed_discourse_facet ['method_citation']
 <S sid="26" ssid="20">We present two algorithms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'"]
'26'
['26']
parsed_discourse_facet ['method_citation']
    <S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="26">The algorithm can be viewed as heuristically optimizing an objective function suggested by (Blum and Mitchell 98); empirically it is shown to be quite successful in optimizing this criterion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'"]
'32'
['32']
parsed_discourse_facet ['method_citation']
    <S sid="47" ssid="1">971,746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'"]
'47'
['47']
parsed_discourse_facet ['method_citation']
    <S sid="127" ssid="60">The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'"]
'127'
['127']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'228'", "'0'", "'75'", "'77'"]
'142'
'228'
'0'
'75'
'77'
['142', '228', '0', '75', '77']
parsed_discourse_facet ['results_citation']
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="102" ssid = "35">(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'77'", "'102'", "'52'", "'75'"]
'142'
'77'
'102'
'52'
'75'
['142', '77', '102', '52', '75']
parsed_discourse_facet ['method_citation']
<S sid ="225" ssid = "4">We again assume a training set of n examples {x1 . xri} where the first m examples have labels {y1 ... yin}  and the last (n — m) examples are unlabeled.</S><S sid ="203" ssid = "70">Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).</S><S sid ="140" ssid = "7">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S><S sid ="79" ssid = "12">2 We now introduce a new algorithm for learning from unlabeled examples  which we will call DLCoTrain (DL stands for decision list  the term Cotrain is taken from (Blum and Mitchell 98)).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'225'", "'203'", "'140'", "'79'", "'31'"]
'225'
'203'
'140'
'79'
'31'
['225', '203', '140', '79', '31']
parsed_discourse_facet ['method_citation']
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="102" ssid = "35">(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'102'", "'77'", "'52'", "'228'"]
'142'
'102'
'77'
'52'
'228'
['142', '102', '77', '52', '228']
parsed_discourse_facet ['method_citation']
<S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="207" ssid = "74">Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S><S sid ="166" ssid = "33">The first m pairs have labels yi  whereas for i = m + 1    n the pairs are unlabeled.</S><S sid ="84" ssid = "17">For each label (Per s on  organization and Location)  take the n contextual rules with the highest value of Count' (x) whose unsmoothed3 strength is above some threshold pmin.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'", "'207'", "'248'", "'166'", "'84'"]
'236'
'207'
'248'
'166'
'84'
['236', '207', '248', '166', '84']
parsed_discourse_facet ['method_citation']
<S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'228'", "'142'", "'75'", "'77'", "'0'"]
'228'
'142'
'75'
'77'
'0'
['228', '142', '75', '77', '0']
parsed_discourse_facet ['method_citation']
<S sid ="207" ssid = "74">Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S><S sid ="86" ssid = "19">Thus at each iteration the method induces at most n x k rules  where k is the number of possible labels (k = 3 in the experiments in this paper). step 3.</S><S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="10" ssid = "4">The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'248'", "'86'", "'204'", "'10'"]
'207'
'248'
'86'
'204'
'10'
['207', '248', '86', '204', '10']
parsed_discourse_facet ['method_citation']
<S sid ="202" ssid = "69">The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels  and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.</S><S sid ="124" ssid = "57">In particular  it may not be possible to learn functions fi (x f2(x2 t) for i = m + 1...n: either because there is some noise in the data  or because it is just not realistic to expect to learn perfect classifiers given the features used for representation.</S><S sid ="121" ssid = "54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).</S><S sid ="50" ssid = "4">It is a sequence of proper nouns within an NP; its last word Cooper is the head of the NP; and the NP has an appositive modifier (a vice president at S.&P.) whose head is a singular noun (president).</S><S sid ="25" ssid = "19">The unlabeled data gives many such &quot;hints&quot; that two features should predict the same label  and these hints turn out to be surprisingly useful when building a classifier.</S>
original cit marker offset is 0
new cit marker offset is 0



["'202'", "'124'", "'121'", "'50'", "'25'"]
'202'
'124'
'121'
'50'
'25'
['202', '124', '121', '50', '25']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="12" ssid = "6">The approach uses both spelling and contextual rules.</S><S sid ="150" ssid = "17">Pseudo-code describing the generalized boosting algorithm of Schapire and Singer is given in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'52'", "'75'", "'12'", "'150'"]
'157'
'52'
'75'
'12'
'150'
['157', '52', '75', '12', '150']
parsed_discourse_facet ['method_citation']
<S sid ="63" ssid = "17">In this case nonalpha is the string formed by removing all upper/lower case letters from the spelling (e.g.  for Thomas E. Petry nonalpha= .</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="47" ssid = "1">971 746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'52'", "'157'", "'77'", "'47'"]
'63'
'52'
'157'
'77'
'47'
['63', '52', '157', '77', '47']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="239" ssid = "6">Of these cases  38 were temporal expressions (either a day of the week or month of the year).</S><S sid ="93" ssid = "26">To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.</S><S sid ="249" ssid = "16">The test accuracy more or less asymptotes.</S><S sid ="197" ssid = "64">Note that in our formalism a weakhypothesis can abstain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'239'", "'93'", "'249'", "'197'"]
'204'
'239'
'93'
'249'
'197'
['204', '239', '93', '249', '197']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S><S sid ="93" ssid = "26">To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'75'", "'228'", "'0'", "'93'"]
'204'
'75'
'228'
'0'
'93'
['204', '75', '228', '0', '93']
parsed_discourse_facet ['method_citation']
<S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="193" ssid = "60">In Input: {(x1 i  Initialize: Vi  j : e(xi) = 0.</S><S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'", "'157'", "'77'", "'193'", "'142'"]
'52'
'157'
'77'
'193'
'142'
['52', '157', '77', '193', '142']
parsed_discourse_facet ['results_citation']
['This section describes an algorithm based on boosting algorithms, which were previously developed for supervised machine learning problems.']
['2 We now introduce a new algorithm for learning from unlabeled examples  which we will call DLCoTrain (DL stands for decision list  the term Cotrain is taken from (Blum and Mitchell 98)).', "Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).", u'We again assume a training set of n examples {x1 . xri} where the first m examples have labels {y1 ... yin}  and the last (n \xe2\u20ac\u201d m) examples are unlabeled.', 'Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).', 'AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.']
['system', 'ROUGE-S*', 'Average_R:', '0.00240', '(95%-conf.int.', '0.00240', '-', '0.00240)']
['system', 'ROUGE-S*', 'Average_P:', '0.07576', '(95%-conf.int.', '0.07576', '-', '0.07576)']
['system', 'ROUGE-S*', 'Average_F:', '0.00466', '(95%-conf.int.', '0.00466', '-', '0.00466)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:66', 'F:5']
['We present two algorithms.']
['In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.', '971 746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).', 'In this case nonalpha is the string formed by removing all upper/lower case letters from the spelling (e.g.  for Thomas E. Petry nonalpha= .', 'Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.', 'The NP is a complement to a preposition  which is the head of a PP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:1', 'F:0']
['We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.']
["For each label (Per s on  organization and Location)  take the n contextual rules with the highest value of Count' (x) whose unsmoothed3 strength is above some threshold pmin.", 'We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', 'The first m pairs have labels yi  whereas for i = m + 1    n the pairs are unlabeled.', 'With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.', 'Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.']
['system', 'ROUGE-S*', 'Average_R:', '0.03529', '(95%-conf.int.', '0.03529', '-', '0.03529)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.06818', '(95%-conf.int.', '0.06818', '-', '0.06818)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:45', 'F:45']
['The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.']
['In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.', u'The input to AdaBoost is a set of training examples ((xi   yi)    (x\u201e.\u201e yrn)).', 'In Input: {(x1 i  Initialize: Vi  j : e(xi) = 0.', 'Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.', 'The NP is a complement to a preposition  which is the head of a PP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:36', 'F:0']
['We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.']
['The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.', 'Thus at each iteration the method induces at most n x k rules  where k is the number of possible labels (k = 3 in the experiments in this paper). step 3.', 'With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.', 'Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.', 'In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.']
['system', 'ROUGE-S*', 'Average_R:', '0.00063', '(95%-conf.int.', '0.00063', '-', '0.00063)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00122', '(95%-conf.int.', '0.00122', '-', '0.00122)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:45', 'F:1']
['The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.']
['They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).', 'The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels  and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.', 'In particular  it may not be possible to learn functions fi (x f2(x2 t) for i = m + 1...n: either because there is some noise in the data  or because it is just not realistic to expect to learn perfect classifiers given the features used for representation.', 'It is a sequence of proper nouns within an NP; its last word Cooper is the head of the NP; and the NP has an appositive modifier (a vice president at S.&P.) whose head is a singular noun (president).', 'The unlabeled data gives many such &quot;hints&quot; that two features should predict the same label  and these hints turn out to be surprisingly useful when building a classifier.']
['system', 'ROUGE-S*', 'Average_R:', '0.00304', '(95%-conf.int.', '0.00304', '-', '0.00304)']
['system', 'ROUGE-S*', 'Average_P:', '0.05229', '(95%-conf.int.', '0.05229', '-', '0.05229)']
['system', 'ROUGE-S*', 'Average_F:', '0.00575', '(95%-conf.int.', '0.00575', '-', '0.00575)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:153', 'F:8']
['AdaBoost finds a weighted combination of simple (weak) classifiers, where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.']
['In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.', '(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.', 'Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).', u'The input to AdaBoost is a set of training examples ((xi   yi)    (x\u201e.\u201e yrn)).', 'The NP is a complement to a preposition  which is the head of a PP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00580', '(95%-conf.int.', '0.00580', '-', '0.00580)']
['system', 'ROUGE-S*', 'Average_P:', '0.04412', '(95%-conf.int.', '0.04412', '-', '0.04412)']
['system', 'ROUGE-S*', 'Average_F:', '0.01025', '(95%-conf.int.', '0.01025', '-', '0.01025)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:136', 'F:6']
['The algorithm can be viewed as heuristically optimizing an objective function suggested by (Blum and Mitchell 98); empirically it is shown to be quite successful in optimizing this criterion.']
['Unsupervised Models for Named Entity Classification Collins', 'Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).', 'To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.', 'In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.', 'Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).']
['system', 'ROUGE-S*', 'Average_R:', '0.00128', '(95%-conf.int.', '0.00128', '-', '0.00128)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00226', '(95%-conf.int.', '0.00226', '-', '0.00226)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:105', 'F:1']
['Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.']
['In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.', 'Unsupervised Models for Named Entity Classification Collins', 'Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).', u'The input to AdaBoost is a set of training examples ((xi   yi)    (x\u201e.\u201e yrn)).', 'Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:105', 'F:0']
0.133767776291 0.00538222216242 0.0102577776638





input/ref/Task1/W99-0613_sweta.csv
input/res/Task1/W99-0613.csv
parsing: input/ref/Task1/W99-0613_sweta.csv
<S sid="121" ssid="54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page, and other pages pointing to the page).</S>
original cit marker offset is 0
new cit marker offset is 0



["121'"]
121'
['121']
parsed_discourse_facet ['method_citation']
<S sid="252" ssid="3">The method uses a &amp;quot;soft&amp;quot; measure of the agreement between two classifiers as an objective function; we described an algorithm which directly optimizes this function.</S>
original cit marker offset is 0
new cit marker offset is 0



["252'"]
252'
['252']
parsed_discourse_facet ['method_citation']
 <S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



["213'"]
213'
['213']
parsed_discourse_facet ['method_citation']
 <S sid="250" ssid="1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S>
original cit marker offset is 0
new cit marker offset is 0



["250'"]
250'
['250']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="33">(Brin 98) ,describes a system for extracting (author, book-title) pairs from the World Wide Web using an approach that bootstraps from an initial seed set of examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="202" ssid="69">The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels, and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.</S>
original cit marker offset is 0
new cit marker offset is 0



["202'"]
202'
['202']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="15">The following features were used: full-string=x The full string (e.g., for Maury Cooper, full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word, this feature applies for any words that the string contains (e.g., Maury Cooper contributes two such features, contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g., IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods, and contains at least one period.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="176" ssid="43">(7) is at 0 when: 1) Vi : sign(gi (xi)) = sign(g2 (xi)); 2) Ig3(xi)l oo; and 3) sign(gi (xi)) = yi for i = 1, , m. In fact, Zco provides a bound on the sum of the classification error of the labeled examples and the number of disagreements between the two classifiers on the unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["176'"]
176'
['176']
parsed_discourse_facet ['method_citation']
 <S sid="108" ssid="41">In the cotraining case, (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples, and (2) must agree with each other on the unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S>
    <S sid="28" ssid="22">(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'", "'28'"]
27'
'28'
['27', '28']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["7'"]
7'
['7']
parsed_discourse_facet ['method_citation']
 <S sid="172" ssid="39">To see this, note thai the first two terms in the above equation correspond to the function that AdaBoost attempts to minimize in the standard supervised setting (Equ.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["85'"]
85'
['85']
parsed_discourse_facet ['method_citation']
 <S sid="214" ssid="81">This modification brings the method closer to the DL-CoTrain algorithm described earlier, and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples, preventing one label from dominating &#8212; this deserves more theoretical investigation.</S>
original cit marker offset is 0
new cit marker offset is 0



["214'"]
214'
['214']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'228'", "'0'", "'75'", "'77'"]
'142'
'228'
'0'
'75'
'77'
['142', '228', '0', '75', '77']
parsed_discourse_facet ['results_citation']
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="102" ssid = "35">(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'77'", "'102'", "'52'", "'75'"]
'142'
'77'
'102'
'52'
'75'
['142', '77', '102', '52', '75']
parsed_discourse_facet ['method_citation']
<S sid ="225" ssid = "4">We again assume a training set of n examples {x1 . xri} where the first m examples have labels {y1 ... yin}  and the last (n — m) examples are unlabeled.</S><S sid ="203" ssid = "70">Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).</S><S sid ="140" ssid = "7">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S><S sid ="79" ssid = "12">2 We now introduce a new algorithm for learning from unlabeled examples  which we will call DLCoTrain (DL stands for decision list  the term Cotrain is taken from (Blum and Mitchell 98)).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'225'", "'203'", "'140'", "'79'", "'31'"]
'225'
'203'
'140'
'79'
'31'
['225', '203', '140', '79', '31']
parsed_discourse_facet ['method_citation']
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="102" ssid = "35">(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'102'", "'77'", "'52'", "'228'"]
'142'
'102'
'77'
'52'
'228'
['142', '102', '77', '52', '228']
parsed_discourse_facet ['method_citation']
<S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="207" ssid = "74">Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S><S sid ="166" ssid = "33">The first m pairs have labels yi  whereas for i = m + 1    n the pairs are unlabeled.</S><S sid ="84" ssid = "17">For each label (Per s on  organization and Location)  take the n contextual rules with the highest value of Count' (x) whose unsmoothed3 strength is above some threshold pmin.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'", "'207'", "'248'", "'166'", "'84'"]
'236'
'207'
'248'
'166'
'84'
['236', '207', '248', '166', '84']
parsed_discourse_facet ['method_citation']
<S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'228'", "'142'", "'75'", "'77'", "'0'"]
'228'
'142'
'75'
'77'
'0'
['228', '142', '75', '77', '0']
parsed_discourse_facet ['method_citation']
<S sid ="207" ssid = "74">Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S><S sid ="86" ssid = "19">Thus at each iteration the method induces at most n x k rules  where k is the number of possible labels (k = 3 in the experiments in this paper). step 3.</S><S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="10" ssid = "4">The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'248'", "'86'", "'204'", "'10'"]
'207'
'248'
'86'
'204'
'10'
['207', '248', '86', '204', '10']
parsed_discourse_facet ['method_citation']
<S sid ="202" ssid = "69">The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels  and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.</S><S sid ="124" ssid = "57">In particular  it may not be possible to learn functions fi (x f2(x2 t) for i = m + 1...n: either because there is some noise in the data  or because it is just not realistic to expect to learn perfect classifiers given the features used for representation.</S><S sid ="121" ssid = "54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).</S><S sid ="50" ssid = "4">It is a sequence of proper nouns within an NP; its last word Cooper is the head of the NP; and the NP has an appositive modifier (a vice president at S.&P.) whose head is a singular noun (president).</S><S sid ="25" ssid = "19">The unlabeled data gives many such &quot;hints&quot; that two features should predict the same label  and these hints turn out to be surprisingly useful when building a classifier.</S>
original cit marker offset is 0
new cit marker offset is 0



["'202'", "'124'", "'121'", "'50'", "'25'"]
'202'
'124'
'121'
'50'
'25'
['202', '124', '121', '50', '25']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="12" ssid = "6">The approach uses both spelling and contextual rules.</S><S sid ="150" ssid = "17">Pseudo-code describing the generalized boosting algorithm of Schapire and Singer is given in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'52'", "'75'", "'12'", "'150'"]
'157'
'52'
'75'
'12'
'150'
['157', '52', '75', '12', '150']
parsed_discourse_facet ['method_citation']
<S sid ="63" ssid = "17">In this case nonalpha is the string formed by removing all upper/lower case letters from the spelling (e.g.  for Thomas E. Petry nonalpha= .</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="47" ssid = "1">971 746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'52'", "'157'", "'77'", "'47'"]
'63'
'52'
'157'
'77'
'47'
['63', '52', '157', '77', '47']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="239" ssid = "6">Of these cases  38 were temporal expressions (either a day of the week or month of the year).</S><S sid ="93" ssid = "26">To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.</S><S sid ="249" ssid = "16">The test accuracy more or less asymptotes.</S><S sid ="197" ssid = "64">Note that in our formalism a weakhypothesis can abstain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'239'", "'93'", "'249'", "'197'"]
'204'
'239'
'93'
'249'
'197'
['204', '239', '93', '249', '197']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S><S sid ="93" ssid = "26">To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'75'", "'228'", "'0'", "'93'"]
'204'
'75'
'228'
'0'
'93'
['204', '75', '228', '0', '93']
parsed_discourse_facet ['method_citation']
<S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="193" ssid = "60">In Input: {(x1 i  Initialize: Vi  j : e(xi) = 0.</S><S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'", "'157'", "'77'", "'193'", "'142'"]
'52'
'157'
'77'
'193'
'142'
['52', '157', '77', '193', '142']
parsed_discourse_facet ['results_citation']
['There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.']
['2 We now introduce a new algorithm for learning from unlabeled examples  which we will call DLCoTrain (DL stands for decision list  the term Cotrain is taken from (Blum and Mitchell 98)).', "Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).", u'We again assume a training set of n examples {x1 . xri} where the first m examples have labels {y1 ... yin}  and the last (n \xe2\u20ac\u201d m) examples are unlabeled.', 'Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).', 'AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.']
['system', 'ROUGE-S*', 'Average_R:', '0.00337', '(95%-conf.int.', '0.00337', '-', '0.00337)']
['system', 'ROUGE-S*', 'Average_P:', '0.08974', '(95%-conf.int.', '0.08974', '-', '0.08974)']
['system', 'ROUGE-S*', 'Average_F:', '0.00649', '(95%-conf.int.', '0.00649', '-', '0.00649)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:78', 'F:7']
['The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).', '(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance.']
['In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.', '971 746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).', 'In this case nonalpha is the string formed by removing all upper/lower case letters from the spelling (e.g.  for Thomas E. Petry nonalpha= .', 'Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.', 'The NP is a complement to a preposition  which is the head of a PP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:231', 'F:0']
['In the cotraining case, (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples, and (2) must agree with each other on the unlabeled examples.']
['Pseudo-code describing the generalized boosting algorithm of Schapire and Singer is given in Figure 1.', 'Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).', 'The approach uses both spelling and contextual rules.', 'Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.', 'The NP is a complement to a preposition  which is the head of a PP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:231', 'F:0']
['Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.']
["For each label (Per s on  organization and Location)  take the n contextual rules with the highest value of Count' (x) whose unsmoothed3 strength is above some threshold pmin.", 'We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', 'The first m pairs have labels yi  whereas for i = m + 1    n the pairs are unlabeled.', 'With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.', 'Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.']
['system', 'ROUGE-S*', 'Average_R:', '0.00549', '(95%-conf.int.', '0.00549', '-', '0.00549)']
['system', 'ROUGE-S*', 'Average_P:', '0.12727', '(95%-conf.int.', '0.12727', '-', '0.12727)']
['system', 'ROUGE-S*', 'Average_F:', '0.01053', '(95%-conf.int.', '0.01053', '-', '0.01053)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:55', 'F:7']
['This modification brings the method closer to the DL-CoTrain algorithm described earlier, and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples, preventing one label from dominating &#8212; this deserves more theoretical investigation.']
['In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.', u'The input to AdaBoost is a set of training examples ((xi   yi)    (x\u201e.\u201e yrn)).', 'In Input: {(x1 i  Initialize: Vi  j : e(xi) = 0.', 'Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.', 'The NP is a complement to a preposition  which is the head of a PP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00168', '(95%-conf.int.', '0.00168', '-', '0.00168)']
['system', 'ROUGE-S*', 'Average_P:', '0.00433', '(95%-conf.int.', '0.00433', '-', '0.00433)']
['system', 'ROUGE-S*', 'Average_F:', '0.00242', '(95%-conf.int.', '0.00242', '-', '0.00242)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:231', 'F:1']
['The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels, and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.']
['The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.', 'Thus at each iteration the method induces at most n x k rules  where k is the number of possible labels (k = 3 in the experiments in this paper). step 3.', 'With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.', 'Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.', 'In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.']
['system', 'ROUGE-S*', 'Average_R:', '0.00752', '(95%-conf.int.', '0.00752', '-', '0.00752)']
['system', 'ROUGE-S*', 'Average_P:', '0.13187', '(95%-conf.int.', '0.13187', '-', '0.13187)']
['system', 'ROUGE-S*', 'Average_F:', '0.01423', '(95%-conf.int.', '0.01423', '-', '0.01423)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:91', 'F:12']
['The following features were used: full-string=x The full string (e.g., for Maury Cooper, full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word, this feature applies for any words that the string contains (e.g., Maury Cooper contributes two such features, contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g., IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods, and contains at least one period.']
['They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).', 'The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels  and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.', 'In particular  it may not be possible to learn functions fi (x f2(x2 t) for i = m + 1...n: either because there is some noise in the data  or because it is just not realistic to expect to learn perfect classifiers given the features used for representation.', 'It is a sequence of proper nouns within an NP; its last word Cooper is the head of the NP; and the NP has an appositive modifier (a vice president at S.&P.) whose head is a singular noun (president).', 'The unlabeled data gives many such &quot;hints&quot; that two features should predict the same label  and these hints turn out to be surprisingly useful when building a classifier.']
['system', 'ROUGE-S*', 'Average_R:', '0.00571', '(95%-conf.int.', '0.00571', '-', '0.00571)']
['system', 'ROUGE-S*', 'Average_P:', '0.01661', '(95%-conf.int.', '0.01661', '-', '0.01661)']
['system', 'ROUGE-S*', 'Average_F:', '0.00850', '(95%-conf.int.', '0.00850', '-', '0.00850)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:903', 'F:15']
['The method uses a &quot;soft&quot; measure of the agreement between two classifiers as an objective function; we described an algorithm which directly optimizes this function.']
['In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.', '(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.', 'Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).', u'The input to AdaBoost is a set of training examples ((xi   yi)    (x\u201e.\u201e yrn)).', 'The NP is a complement to a preposition  which is the head of a PP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00097', '(95%-conf.int.', '0.00097', '-', '0.00097)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00180', '(95%-conf.int.', '0.00180', '-', '0.00180)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:78', 'F:1']
['(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.']
['Unsupervised Models for Named Entity Classification Collins', 'Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).', 'To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.', 'In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.', 'Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:630', 'F:0']
['Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.']
['In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.', 'Unsupervised Models for Named Entity Classification Collins', 'Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).', u'The input to AdaBoost is a set of training examples ((xi   yi)    (x\u201e.\u201e yrn)).', 'Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).']
['system', 'ROUGE-S*', 'Average_R:', '0.00810', '(95%-conf.int.', '0.00810', '-', '0.00810)']
['system', 'ROUGE-S*', 'Average_P:', '0.10909', '(95%-conf.int.', '0.10909', '-', '0.10909)']
['system', 'ROUGE-S*', 'Average_F:', '0.01508', '(95%-conf.int.', '0.01508', '-', '0.01508)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:55', 'F:6']
0.0491729995083 0.00328399996716 0.00590499994095





input/ref/Task1/D10-1044_aakansha.csv
input/res/Task1/D10-1044.csv
parsing: input/ref/Task1/D10-1044_aakansha.csv
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S>
    <S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'96'"]
'95'
'96'
['95', '96']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'"]
'75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="28">For comparison to information-retrieval inspired baselines, eg (L&#168;u et al., 2007), we select sentences from OUT using language model perplexities from IN.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'"]
'22'
['22']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="23">The 2nd block contains the IR system, which was tuned by selecting text in multiples of the size of the EMEA training corpus, according to dev set performance.</S>
    <S sid="120" ssid="24">This significantly underperforms log-linear combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'", "'120'"]
'119'
'120'
['119', '120']
parsed_discourse_facet ['result_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
    <S sid="24" ssid="21">Sentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="4">We focus here instead on adapting the two most important features: the language model (LM), which estimates the probability p(wIh) of a target word w following an ngram h; and the translation models (TM) p(slt) and p(t1s), which give the probability of source phrase s translating to target phrase t, and vice versa.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D10-1044.csv
<S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daum´e’s (2007) work that splits each feature into domain-specific and general copies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'128'", "'140'", "'10'", "'4'", "'136'"]
'128'
'140'
'10'
'4'
'136'
['128', '140', '10', '4', '136']
parsed_discourse_facet ['results_citation']
<S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="82" ssid = "19">This is not unreasonable given the application to phrase pairs from OUT  but it suggests that an interesting alternative might be to use a plain log-linear weighting function exp(Ei Aifi(s  t))  with outputs in [0  oo].</S><S sid ="142" ssid = "11">There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'38'", "'82'", "'142'", "'130'"]
'143'
'38'
'82'
'142'
'130'
['143', '38', '82', '142', '130']
parsed_discourse_facet ['method_citation']
<S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S><S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="31" ssid = "28">For comparison to information-retrieval inspired baselines  eg (L¨u et al.  2007)  we select sentences from OUT using language model perplexities from IN.</S><S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'7'", "'31'", "'128'", "'78'"]
'140'
'7'
'31'
'128'
'78'
['140', '7', '31', '128', '78']
parsed_discourse_facet ['method_citation']
<S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="102" ssid = "6">The dev corpus was taken from the NIST05 evaluation set  augmented with some randomly-selected material reserved from the training set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'127'", "'143'", "'65'", "'102'"]
'130'
'127'
'143'
'65'
'102'
['130', '127', '143', '65', '102']
parsed_discourse_facet ['method_citation']
<S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="55" ssid = "19">This suggests a direct parallel to (1): where ˜p(s  t) is a joint empirical distribution extracted from the IN dev set using the standard procedure.2 An alternative form of linear combination is a maximum a posteriori (MAP) combination (Bacchiani et al.  2004).</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'21'", "'49'", "'55'", "'127'"]
'75'
'21'
'49'
'55'
'127'
['75', '21', '49', '55', '127']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'130'", "'49'", "'38'", "'143'"]
'127'
'130'
'49'
'38'
'143'
['127', '130', '49', '38', '143']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'20'", "'38'", "'130'", "'49'"]
'127'
'20'
'38'
'130'
'49'
['127', '20', '38', '130', '49']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S><S sid ="37" ssid = "1">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features  interpreted as log probabilities  many of which have their own internal parameters and objectives.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'49'", "'111'", "'37'", "'21'"]
'127'
'49'
'111'
'37'
'21'
['127', '49', '111', '37', '21']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'81'", "'151'", "'75'", "'143'"]
'7'
'81'
'151'
'75'
'143'
['7', '81', '151', '75', '143']
parsed_discourse_facet ['method_citation']
<S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'75'", "'127'", "'151'", "'143'"]
'21'
'75'
'127'
'151'
'143'
['21', '75', '127', '151', '143']
parsed_discourse_facet ['method_citation']
<S sid ="97" ssid = "1">We carried out translation experiments in two different settings.</S><S sid ="53" ssid = "17">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).</S><S sid ="106" ssid = "10">The corpora for both settings are summarized in table 1.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="26" ssid = "23">Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009)  who weight sentences according to sub-corpus and genre membership.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'53'", "'106'", "'81'", "'26'"]
'97'
'53'
'106'
'81'
'26'
['97', '53', '106', '81', '26']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="31" ssid = "28">For comparison to information-retrieval inspired baselines  eg (L¨u et al.  2007)  we select sentences from OUT using language model perplexities from IN.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'81'", "'31'", "'78'", "'140'"]
'7'
'81'
'31'
'78'
'140'
['7', '81', '31', '78', '140']
parsed_discourse_facet ['method_citation']
<S sid ="60" ssid = "24">The matching sentence pairs are then added to the IN corpus  and the system is re-trained.</S><S sid ="30" ssid = "27">A similar maximumlikelihood approach was used by Foster and Kuhn (2007)  but for language models only.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="145" ssid = "2">Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'30'", "'24'", "'65'", "'145'"]
'60'
'30'
'24'
'65'
'145'
['60', '30', '24', '65', '145']
parsed_discourse_facet ['results_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="22" ssid = "19">Within this framework  we use features intended to capture degree of generality  including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'20'", "'130'", "'49'", "'22'"]
'127'
'20'
'130'
'49'
'22'
['127', '20', '130', '49', '22']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="119" ssid = "23">The 2nd block contains the IR system  which was tuned by selecting text in multiples of the size of the EMEA training corpus  according to dev set performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'130'", "'20'", "'49'", "'119'"]
'127'
'130'
'20'
'49'
'119'
['127', '130', '20', '49', '119']
parsed_discourse_facet ['method_citation']
<S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S><S sid ="66" ssid = "3">The weight on each sentence is a value in [0  1] computed by a perceptron with Boolean features that indicate collection and genre membership.</S><S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="113" ssid = "17">The corpus was wordaligned using both HMM and IBM2 models  and the phrase table was the union of phrases extracted from these separate alignments  with a length limit of 7.</S><S sid ="102" ssid = "6">The dev corpus was taken from the NIST05 evaluation set  augmented with some randomly-selected material reserved from the training set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'66'", "'75'", "'113'", "'102'"]
'143'
'66'
'75'
'113'
'102'
['143', '66', '75', '113', '102']
parsed_discourse_facet ['method_citation']
<S sid ="44" ssid = "8">When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.</S><S sid ="79" ssid = "16">This combination generalizes (2) and (3): we use either at = a to obtain a fixed-weight linear combination  or at = cI(t)/(cI(t) + 0) to obtain a MAP combination.</S><S sid ="62" ssid = "26">To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S><S sid ="115" ssid = "19">Table 2 shows results for both settings and all methods described in sections 2 and 3.</S><S sid ="13" ssid = "10">The techniques we develop can be extended in a relatively straightforward manner to the more general case when OUT consists of multiple sub-domains.</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'79'", "'62'", "'115'", "'13'"]
'44'
'79'
'62'
'115'
'13'
['44', '79', '62', '115', '13']
parsed_discourse_facet ['method_citation']
<S sid ="106" ssid = "10">The corpora for both settings are summarized in table 1.</S><S sid ="60" ssid = "24">The matching sentence pairs are then added to the IN corpus  and the system is re-trained.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="53" ssid = "17">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'60'", "'81'", "'38'", "'53'"]
'106'
'60'
'81'
'38'
'53'
['106', '60', '81', '38', '53']
parsed_discourse_facet ['method_citation']
<S sid ="95" ssid = "32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S><S sid ="50" ssid = "14">Linear weights are difficult to incorporate into the standard MERT procedure because they are “hidden” within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="59" ssid = "23">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid ="51" ssid = "15">For the LM  adaptive weights are set as follows: where α is a weight vector containing an element αi for each domain (just IN and OUT in our case)  pi are the corresponding domain-specific models  and ˜p(w  h) is an empirical distribution from a targetlanguage training corpus—we used the IN dev set for this.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'50'", "'21'", "'59'", "'51'"]
'95'
'50'
'21'
'59'
'51'
['95', '50', '21', '59', '51']
parsed_discourse_facet ['aim_citation', 'results_citation']
['Our second contribution is to apply instance weighting at the level of phrase pairs.']
['The 2nd block contains the IR system  which was tuned by selecting text in multiples of the size of the EMEA training corpus  according to dev set performance.', 'The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.', u'Daum\xb4e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.', 'This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.', u'The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the \u201cflattened\u201d variant described in section 3.2.']
['system', 'ROUGE-S*', 'Average_R:', '0.00044', '(95%-conf.int.', '0.00044', '-', '0.00044)']
['system', 'ROUGE-S*', 'Average_P:', '0.04762', '(95%-conf.int.', '0.04762', '-', '0.04762)']
['system', 'ROUGE-S*', 'Average_F:', '0.00087', '(95%-conf.int.', '0.00087', '-', '0.00087)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:21', 'F:1']
['To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.']
['The dev corpus was taken from the NIST05 evaluation set  augmented with some randomly-selected material reserved from the training set.', 'The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.', 'Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.', 'Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).', u'The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the \u201cflattened\u201d variant described in section 3.2.']
['system', 'ROUGE-S*', 'Average_R:', '0.00433', '(95%-conf.int.', '0.00433', '-', '0.00433)']
['system', 'ROUGE-S*', 'Average_P:', '0.10833', '(95%-conf.int.', '0.10833', '-', '0.10833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:120', 'F:13']
['Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.', 'We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.']
['This is not unreasonable given the application to phrase pairs from OUT  but it suggests that an interesting alternative might be to use a plain log-linear weighting function exp(Ei Aifi(s  t))  with outputs in [0  oo].', 'The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.', 'There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).', 'Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).', 'The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.']
['system', 'ROUGE-S*', 'Average_R:', '0.02795', '(95%-conf.int.', '0.02795', '-', '0.02795)']
['system', 'ROUGE-S*', 'Average_P:', '0.15220', '(95%-conf.int.', '0.15220', '-', '0.15220)']
['system', 'ROUGE-S*', 'Average_F:', '0.04723', '(95%-conf.int.', '0.04723', '-', '0.04723)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3828', 'P:703', 'F:107']
['In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.']
['Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.', 'This is a standard adaptation problem for SMT.', 'Domain adaptation is a common concern when optimizing empirical NLP applications.', u'Recent work by Finkel and Manning (2009) which re-casts Daum\xb4e\u2019s approach in a hierarchical MAP framework may be applicable to this problem.', u'It is also worth pointing out a connection with Daum\xb4e\u2019s (2007) work that splits each feature into domain-specific and general copies.']
['system', 'ROUGE-S*', 'Average_R:', '0.00340', '(95%-conf.int.', '0.00340', '-', '0.00340)']
['system', 'ROUGE-S*', 'Average_P:', '0.05128', '(95%-conf.int.', '0.05128', '-', '0.05128)']
['system', 'ROUGE-S*', 'Average_F:', '0.00638', '(95%-conf.int.', '0.00638', '-', '0.00638)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:78', 'F:4']
['We focus here instead on adapting the two most important features: the language model (LM), which estimates the probability p(wIh) of a target word w following an ngram h; and the translation models (TM) p(slt) and p(t1s), which give the probability of source phrase s translating to target phrase t, and vice versa.']
[u'To set \u03b2  we used the same criterion as for \u03b1  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose \u201crelevant\u201d sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L\xa8u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).', u'For the LM  adaptive weights are set as follows: where \u03b1 is a weight vector containing an element \u03b1i for each domain (just IN and OUT in our case)  pi are the corresponding domain-specific models  and \u02dcp(w  h) is an empirical distribution from a targetlanguage training corpus\u2014we used the IN dev set for this.', 'Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.', u'Linear weights are difficult to incorporate into the standard MERT procedure because they are \u201chidden\u201d within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.', 'This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.']
['system', 'ROUGE-S*', 'Average_R:', '0.00416', '(95%-conf.int.', '0.00416', '-', '0.00416)']
['system', 'ROUGE-S*', 'Average_P:', '0.13960', '(95%-conf.int.', '0.13960', '-', '0.13960)']
['system', 'ROUGE-S*', 'Average_F:', '0.00808', '(95%-conf.int.', '0.00808', '-', '0.00808)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:11781', 'P:351', 'F:49']
['In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.']
['The logistic function  whose outputs are in [0  1]  forces pp(s  t) ', 'For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:325', 'P:210', 'F:0']
['Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.']
['The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.', u'Daum\xb4e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.', 'This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.', 'Within this framework  we use features intended to capture degree of generality  including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.', u'The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the \u201cflattened\u201d variant described in section 3.2.']
['system', 'ROUGE-S*', 'Average_R:', '0.03636', '(95%-conf.int.', '0.03636', '-', '0.03636)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.07018', '(95%-conf.int.', '0.07018', '-', '0.07018)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:78', 'F:78']
['Sentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language.', 'Our second contribution is to apply instance weighting at the level of phrase pairs.']
['The matching sentence pairs are then added to the IN corpus  and the system is re-trained.', 'The logistic function  whose outputs are in [0  1]  forces pp(s  t) ', 'The corpora for both settings are summarized in table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00585', '(95%-conf.int.', '0.00585', '-', '0.00585)']
['system', 'ROUGE-S*', 'Average_P:', '0.00654', '(95%-conf.int.', '0.00654', '-', '0.00654)']
['system', 'ROUGE-S*', 'Average_F:', '0.00617', '(95%-conf.int.', '0.00617', '-', '0.00617)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:171', 'P:153', 'F:1']
['In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.']
['Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.', u'For comparison to information-retrieval inspired baselines  eg (L\xa8u et al.  2007)  we select sentences from OUT using language model perplexities from IN.', 'For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.', u'Recent work by Finkel and Manning (2009) which re-casts Daum\xb4e\u2019s approach in a hierarchical MAP framework may be applicable to this problem.', 'This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00043', '(95%-conf.int.', '0.00043', '-', '0.00043)']
['system', 'ROUGE-S*', 'Average_P:', '0.00476', '(95%-conf.int.', '0.00476', '-', '0.00476)']
['system', 'ROUGE-S*', 'Average_F:', '0.00078', '(95%-conf.int.', '0.00078', '-', '0.00078)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:210', 'F:1']
0.16781444258 0.00921333323096 0.0164466664839





input/ref/Task1/P11-1061_aakansha.csv
input/res/Task1/P11-1061.csv
parsing: input/ref/Task1/P11-1061_aakansha.csv
<S sid="40" ssid="6">We extend Subramanya et al.&#8217;s intuitions to our bilingual setup.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="11">First, we use a novel graph-based framework for projecting syntactic information across language boundaries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="29" ssid="6">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'"]
'158'
['158']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'"]
'158'
['158']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="1">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="17" ssid="13">Second, we treat the projected labels as features in an unsupervised model (&#167;5), rather than using them directly for supervised training.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="111" ssid="11">We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).</S>
original cit marker offset is 0
new cit marker offset is 0



["'111'"]
'111'
['111']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="29" ssid="6">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'161'"]
'161'
['161']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1061.csv
<S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="62" ssid = "28">Note that since all English vertices were extracted from the parallel text  we will have an initial label distribution for all vertices in Ve.</S><S sid ="68" ssid = "34">In the label propagation stage  we propagate the automatic English tags to the aligned Italian trigram types  followed by further propagation solely among the Italian vertices. the Italian vertices are connected to an automatically labeled English vertex.</S><S sid ="42" ssid = "8">The foreign language vertices (denoted by Vf) correspond to foreign trigram types  exactly as in Subramanya et al. (2010).</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'62'", "'68'", "'42'", "'160'"]
'19'
'62'
'68'
'42'
'160'
['19', '62', '68', '42', '160']
parsed_discourse_facet ['results_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid ="30" ssid = "7">To initialize the graph we tag the English side of the parallel text using a supervised model.</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'113'", "'30'", "'102'", "'25'"]
'26'
'113'
'30'
'102'
'25'
['26', '113', '30', '102', '25']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'113'", "'25'", "'3'", "'19'"]
'16'
'113'
'25'
'3'
'19'
['16', '113', '25', '3', '19']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="119" ssid = "19">To provide a thorough analysis  we evaluated three baselines and two oracles in addition to two variants of our graph-based approach.</S><S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="82" ssid = "13">We formulate the update as follows: where ∀ui ∈ Vf \ Vfl  γi(y) and κi are defined as: We ran this procedure for 10 iterations.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'119'", "'54'", "'82'", "'28'"]
'134'
'119'
'54'
'82'
'28'
['134', '119', '54', '82', '28']
parsed_discourse_facet ['method_citation']
<S sid ="71" ssid = "2">We use label propagation in two stages to generate soft labels on all the vertices in the graph.</S><S sid ="61" ssid = "27">These tag distributions are used to initialize the label distributions over the English vertices in the graph.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="86" ssid = "17">For a sentence x and a state sequence z  a first order Markov model defines a distribution: (9) where Val(X) corresponds to the entire vocabulary.</S><S sid ="124" ssid = "24">We tried two versions of our graph-based approach: feature after the first stage of label propagation (Eq.</S>
original cit marker offset is 0
new cit marker offset is 0



["'71'", "'61'", "'3'", "'86'", "'124'"]
'71'
'61'
'3'
'86'
'124'
['71', '61', '3', '86', '124']
parsed_discourse_facet ['method_citation']
<S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="57" ssid = "23">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'26'", "'57'", "'97'", "'28'"]
'89'
'26'
'57'
'97'
'28'
['89', '26', '57', '97', '28']
parsed_discourse_facet ['method_citation']
<S sid ="93" ssid = "24">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'93'", "'26'", "'28'", "'97'", "'89'"]
'93'
'26'
'28'
'97'
'89'
['93', '26', '28', '97', '89']
parsed_discourse_facet ['method_citation']
<S sid ="92" ssid = "23">To optimize this function  we used L-BFGS  a quasi-Newton method (Liu and Nocedal  1989).</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="49" ssid = "15">We define a symmetric similarity function K(uZ7 uj) over two foreign language vertices uZ7 uj E Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1.</S><S sid ="142" ssid = "5">The “No LP” model does not outperform direct projection for German and Greek  but performs better for six out of eight languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'102'", "'19'", "'49'", "'142'"]
'92'
'102'
'19'
'49'
'142'
['92', '102', '19', '49', '142']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="159" ssid = "2">Because we are interested in applying our techniques to languages for which no labeled resources are available  we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'159'", "'97'", "'89'", "'28'"]
'26'
'159'
'97'
'89'
'28'
['26', '159', '97', '89', '28']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'97'", "'102'", "'28'", "'83'"]
'134'
'97'
'102'
'28'
'83'
['134', '97', '102', '28', '83']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="72" ssid = "3">In the first stage  we run a single step of label propagation  which transfers the label distributions from the English vertices to the connected foreign language vertices (say  Vf�) at the periphery of the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'97'", "'28'", "'102'", "'72'"]
'26'
'97'
'28'
'102'
'72'
['26', '97', '28', '102', '72']
parsed_discourse_facet ['method_citation']
<S sid ="95" ssid = "26">This feature ft incorporates information from the smoothed graph and prunes hidden states that are inconsistent with the thresholded vector tx.</S><S sid ="15" ssid = "11">First  we use a novel graph-based framework for projecting syntactic information across language boundaries.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid ="41" ssid = "7">Because the information flow in our graph is asymmetric (from English to the foreign language)  we use different types of vertices for each language.</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'15'", "'25'", "'41'", "'113'"]
'95'
'15'
'25'
'41'
'113'
['95', '15', '25', '41', '113']
parsed_discourse_facet ['method_citation']
<S sid ="21" ssid = "17">These universal POS categories not only facilitate the transfer of POS information from one language to another  but also relieve us from using controversial evaluation metrics 2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S><S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="10" ssid = "6">To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S><S sid ="36" ssid = "2">Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand  using individual words as the vertices throws away the context necessary for disambiguation; on the other hand  it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'16'", "'10'", "'29'", "'36'"]
'21'
'16'
'10'
'29'
'36'
['21', '16', '10', '29', '36']
parsed_discourse_facet ['results_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S><S sid ="35" ssid = "1">In graph-based learning approaches one constructs a graph whose vertices are labeled and unlabeled examples  and whose weighted edges encode the degree to which the examples they link have the same label (Zhu et al.  2003).</S><S sid ="159" ssid = "2">Because we are interested in applying our techniques to languages for which no labeled resources are available  we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'54'", "'89'", "'35'", "'159'"]
'26'
'54'
'89'
'35'
'159'
['26', '54', '89', '35', '159']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'28'", "'134'", "'97'", "'89'"]
'26'
'28'
'134'
'97'
'89'
['26', '28', '134', '97', '89']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="21" ssid = "17">These universal POS categories not only facilitate the transfer of POS information from one language to another  but also relieve us from using controversial evaluation metrics 2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'21'", "'160'", "'113'", "'29'"]
'16'
'21'
'160'
'113'
'29'
['16', '21', '160', '113', '29']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'28'", "'97'", "'26'", "'89'"]
'134'
'28'
'97'
'26'
'89'
['134', '28', '97', '26', '89']
parsed_discourse_facet ['method_citation']
<S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="49" ssid = "15">We define a symmetric similarity function K(uZ7 uj) over two foreign language vertices uZ7 uj E Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1.</S><S sid ="136" ssid = "36">For graph propagation  the hyperparameter v was set to 2 x 10−6 and was not tuned.</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'102'", "'49'", "'136'", "'83'"]
'97'
'102'
'49'
'136'
'83'
['97', '102', '49', '136', '83']
parsed_discourse_facet ['method_citation']
['We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).']
[u'As discussed in more detail in \xa73  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', 'The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.', u'The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (\xa73.2).', 'This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.', u'Because we don\u2019t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.']
['system', 'ROUGE-S*', 'Average_R:', '0.00048', '(95%-conf.int.', '0.00048', '-', '0.00048)']
['system', 'ROUGE-S*', 'Average_P:', '0.00221', '(95%-conf.int.', '0.00221', '-', '0.00221)']
['system', 'ROUGE-S*', 'Average_F:', '0.00079', '(95%-conf.int.', '0.00079', '-', '0.00079)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4186', 'P:903', 'F:2']
['We extend Subramanya et al.&#8217;s intuitions to our bilingual setup.']
['Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.', 'In the label propagation stage  we propagate the automatic English tags to the aligned Italian trigram types  followed by further propagation solely among the Italian vertices. the Italian vertices are connected to an automatically labeled English vertex.', 'Note that since all English vertices were extracted from the parallel text  we will have an initial label distribution for all vertices in Ve.', 'The foreign language vertices (denoted by Vf) correspond to foreign trigram types  exactly as in Subramanya et al. (2010).', 'Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.']
['system', 'ROUGE-S*', 'Average_R:', '0.00035', '(95%-conf.int.', '0.00035', '-', '0.00035)']
['system', 'ROUGE-S*', 'Average_P:', '0.04762', '(95%-conf.int.', '0.04762', '-', '0.04762)']
['system', 'ROUGE-S*', 'Average_F:', '0.00070', '(95%-conf.int.', '0.00070', '-', '0.00070)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:21', 'F:1']
['The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.']
['To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.', 'Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand  using individual words as the vertices throws away the context necessary for disambiguation; on the other hand  it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences.', 'These universal POS categories not only facilitate the transfer of POS information from one language to another  but also relieve us from using controversial evaluation metrics 2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.', u'To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (\u0e22\u0e073.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.', u'To this end  we construct a bilingual graph over word types to establish a connection between the two languages (\u0e22\u0e073)  and then use graph label propagation to project syntactic information from English to the foreign language (\u0e22\u0e074).']
['system', 'ROUGE-S*', 'Average_R:', '0.00412', '(95%-conf.int.', '0.00412', '-', '0.00412)']
['system', 'ROUGE-S*', 'Average_P:', '0.36264', '(95%-conf.int.', '0.36264', '-', '0.36264)']
['system', 'ROUGE-S*', 'Average_F:', '0.00816', '(95%-conf.int.', '0.00816', '-', '0.00816)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:8001', 'P:91', 'F:33']
['These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.']
['Because we are interested in applying our techniques to languages for which no labeled resources are available  we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.', u'As discussed in more detail in \u0e22\u0e073  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', 'The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.', u'The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (\u0e22\u0e073.2).', 'This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.']
['system', 'ROUGE-S*', 'Average_R:', '0.00293', '(95%-conf.int.', '0.00293', '-', '0.00293)']
['system', 'ROUGE-S*', 'Average_P:', '0.04348', '(95%-conf.int.', '0.04348', '-', '0.04348)']
['system', 'ROUGE-S*', 'Average_F:', '0.00549', '(95%-conf.int.', '0.00549', '-', '0.00549)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4095', 'P:276', 'F:12']
['To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.']
['First  we use a novel graph-based framework for projecting syntactic information across language boundaries.', 'Because the information flow in our graph is asymmetric (from English to the foreign language)  we use different types of vertices for each language.', 'This feature ft incorporates information from the smoothed graph and prunes hidden states that are inconsistent with the thresholded vector tx.', 'For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.01639', '(95%-conf.int.', '0.01639', '-', '0.01639)']
['system', 'ROUGE-S*', 'Average_P:', '0.05871', '(95%-conf.int.', '0.05871', '-', '0.05871)']
['system', 'ROUGE-S*', 'Average_F:', '0.02563', '(95%-conf.int.', '0.02563', '-', '0.02563)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:528', 'F:31']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.']
['We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.', u'We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value \u03c4: We describe how we choose \u03c4 in \xa76.4.', u'The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (\xa73.2).', 'This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.', u'Because we don\u2019t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.']
['system', 'ROUGE-S*', 'Average_R:', '0.00036', '(95%-conf.int.', '0.00036', '-', '0.00036)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00071', '(95%-conf.int.', '0.00071', '-', '0.00071)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:55', 'F:1']
['To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.']
['We tried two versions of our graph-based approach: feature after the first stage of label propagation (Eq.', 'These tag distributions are used to initialize the label distributions over the English vertices in the graph.', 'We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).', 'For a sentence x and a state sequence z  a first order Markov model defines a distribution: (9) where Val(X) corresponds to the entire vocabulary.', 'We use label propagation in two stages to generate soft labels on all the vertices in the graph.']
['system', 'ROUGE-S*', 'Average_R:', '0.00313', '(95%-conf.int.', '0.00313', '-', '0.00313)']
['system', 'ROUGE-S*', 'Average_P:', '0.01425', '(95%-conf.int.', '0.01425', '-', '0.01425)']
['system', 'ROUGE-S*', 'Average_F:', '0.00514', '(95%-conf.int.', '0.00514', '-', '0.00514)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:351', 'F:5']
['To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.']
['We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.', u'As discussed in more detail in \u0e22\u0e073  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', 'To initialize the graph we tag the English side of the parallel text using a supervised model.', 'For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.']
['system', 'ROUGE-S*', 'Average_R:', '0.01006', '(95%-conf.int.', '0.01006', '-', '0.01006)']
['system', 'ROUGE-S*', 'Average_P:', '0.04735', '(95%-conf.int.', '0.04735', '-', '0.04735)']
['system', 'ROUGE-S*', 'Average_F:', '0.01659', '(95%-conf.int.', '0.01659', '-', '0.01659)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:528', 'F:25']
['Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.']
['We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.', u'We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value \u03c4: We describe how we choose \u03c4 in \xa76.4.', u'For graph propagation  the hyperparameter v was set to 2 x 10\u22126 and was not tuned.', 'This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.', 'We define a symmetric similarity function K(uZ7 uj) over two foreign language vertices uZ7 uj E Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:136', 'F:0']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.']
['We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.', 'Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.', 'To optimize this function  we used L-BFGS  a quasi-Newton method (Liu and Nocedal  1989).', u'The \u201cNo LP\u201d model does not outperform direct projection for German and Greek  but performs better for six out of eight languages.', 'We define a symmetric similarity function K(uZ7 uj) over two foreign language vertices uZ7 uj E Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00044', '(95%-conf.int.', '0.00044', '-', '0.00044)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00086', '(95%-conf.int.', '0.00086', '-', '0.00086)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:55', 'F:1']
['To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).']
['For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.', 'The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.', u'The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (\u0e22\u0e073.2).', u'As discussed in more detail in \u0e22\u0e073  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', 'This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.']
['system', 'ROUGE-S*', 'Average_R:', '0.00017', '(95%-conf.int.', '0.00017', '-', '0.00017)']
['system', 'ROUGE-S*', 'Average_P:', '0.01515', '(95%-conf.int.', '0.01515', '-', '0.01515)']
['system', 'ROUGE-S*', 'Average_F:', '0.00034', '(95%-conf.int.', '0.00034', '-', '0.00034)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5886', 'P:66', 'F:1']
0.0570699994812 0.00349363633188 0.00585545449222





input/ref/Task1/P05-1013_aakansha.csv
input/res/Task1/P05-1013.csv
parsing: input/ref/Task1/P05-1013_aakansha.csv
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="17">However, the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech, with a best performance of 73% UAS (Holan, 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="86" ssid="13">As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'"]
'86'
['86']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="7">As shown in Table 3, the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.</S>
    <S sid="81"  ssid="8">However, the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'81'"]
'80'
'81'
['80', '81']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'"]
'49'
['49']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="10">While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P05-1013.csv
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'48'", "'21'", "'36'"]
'50'
'75'
'48'
'21'
'36'
['50', '75', '48', '21', '36']
parsed_discourse_facet ['results_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="62" ssid = "1">In the experiments below  we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs 3 previously tested on Swedish (Nivre et al.  2004) and English (Nivre and Scholz  2004).</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="91" ssid = "2">Table 5 shows the overall parsing accuracy attained with the three different encoding schemes  compared to the baseline (no special arc labels) and to training directly on non-projective dependency graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'75'", "'62'", "'21'", "'91'"]
'48'
'75'
'62'
'21'
'91'
['48', '75', '62', '21', '91']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="77" ssid = "4">The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'77'", "'48'", "'21'"]
'50'
'75'
'77'
'48'
'21'
['50', '75', '77', '48', '21']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'71'", "'21'", "'45'"]
'50'
'75'
'71'
'21'
'45'
['50', '75', '71', '21', '45']
parsed_discourse_facet ['method_citation']
<S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'50'", "'48'", "'45'", "'21'"]
'75'
'50'
'48'
'45'
'21'
['75', '50', '48', '45', '21']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p↓.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'51'", "'36'", "'21'", "'48'"]
'50'
'51'
'36'
'21'
'48'
['50', '51', '36', '21', '48']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'48'", "'21'", "'36'", "'75'"]
'50'
'48'
'21'
'36'
'75'
['50', '48', '21', '36', '75']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p↓.</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'21'", "'51'", "'48'", "'75'"]
'50'
'21'
'51'
'48'
'75'
['50', '21', '51', '48', '75']
parsed_discourse_facet ['method_citation']
<S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p↓.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="23" ssid = "19">By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'", "'51'", "'21'", "'48'", "'23'"]
'36'
'51'
'21'
'48'
'23'
['36', '51', '21', '48', '23']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'48'", "'21'", "'45'"]
'50'
'75'
'48'
'21'
'45'
['50', '75', '48', '21', '45']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="83" ssid = "10">It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="6" ssid = "2">However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.</S><S sid ="35" ssid = "6">The dependency graph in Figure 1 satisfies all the defining conditions above  but it fails to satisfy the condition ofprojectivity (Kahane et al.  1998): The arc connecting the head jedna (one) to the dependent Z (out-of) spans the token je (is)  which is not dominated by jedna.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'83'", "'7'", "'6'", "'35'"]
'2'
'83'
'7'
'6'
'35'
['2', '83', '7', '6', '35']
parsed_discourse_facet ['method_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'75'", "'45'", "'21'", "'20'"]
'48'
'75'
'45'
'21'
'20'
['48', '75', '45', '21', '20']
parsed_discourse_facet ['method_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S><S sid ="73" ssid = "12">More details on the memory-based prediction can be found in Nivre et al. (2004) and Nivre and Scholz (2004).</S><S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'45'", "'73'", "'20'", "'106'"]
'48'
'45'
'73'
'20'
'106'
['48', '45', '73', '20', '106']
parsed_discourse_facet ['results_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="52" ssid = "23">Thus  the arc from je to jedna will be labeled 5b↓ (to indicate that there is a syntactic head below it).</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'48'", "'75'", "'36'", "'52'"]
'50'
'48'
'75'
'36'
'52'
['50', '48', '75', '36', '52']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="84" ssid = "11">In the second part of the experiment  we applied the inverse transformation based on breadth-first search under the three different encoding schemes.</S><S sid ="79" ssid = "6">In the first part of the experiment  dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'48'", "'21'", "'84'", "'79'"]
'50'
'48'
'21'
'84'
'79'
['50', '48', '21', '84', '79']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "8">For each token  three types of features may be taken into account: the word form; the part-of-speech assigned by an automatic tagger; and labels on previously assigned dependency arcs involving the token – the arc from its head and the arcs to its leftmost and rightmost dependent  respectively.</S><S sid ="23" ssid = "19">By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S><S sid ="64" ssid = "3">At each point during the derivation  the parser has a choice between pushing the next input token onto the stack – with or without adding an arc from the token on top of the stack to the token pushed – and popping a token from the stack – with or without adding an arc from the next input token to the token popped.</S><S sid ="59" ssid = "30">The details of the transformation procedure are slightly different depending on the encoding schemes: d↑h let the linear head be the syntactic head). target arc must have the form wl −→ wm; if no target arc is found  Head is used as backoff. must have the form wl −→ wm and no outgoing arcs of the form wm p'↓ −→ wo; no backoff.</S><S sid ="46" ssid = "17">In principle  it would be possible to encode the exact position of the syntactic head in the label of the arc from the linear head  but this would give a potentially infinite set of arc labels and would make the training of the parser very hard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'23'", "'64'", "'59'", "'46'"]
'69'
'23'
'64'
'59'
'46'
['69', '23', '64', '59', '46']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'71'", "'21'", "'45'"]
'50'
'75'
'71'
'21'
'45'
['50', '75', '71', '21', '45']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "18">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 15–25%  the total proportion of non-projective arcs is normally only 1–2%.</S><S sid ="101" ssid = "12">However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.</S><S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S><S sid ="87" ssid = "14">However  it can be noted that the results for the least informative encoding  Path  are almost comparable  while the third encoding  Head  gives substantially worse results for both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'14'", "'101'", "'89'", "'87'"]
'107'
'14'
'101'
'89'
'87'
['107', '14', '101', '89', '87']
parsed_discourse_facet ['method_citation']
<S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 15–25%  the total proportion of non-projective arcs is normally only 1–2%.</S><S sid ="88" ssid = "15">We also see that the increase in the size of the label sets for Head and Head+Path is far below the theoretical upper bounds given in Table 1.</S><S sid ="28" ssid = "24">First  in section 4  we evaluate the graph transformation techniques in themselves  with data from the Prague Dependency Treebank and the Danish Dependency Treebank.</S><S sid ="103" ssid = "14">On the other hand  given that all schemes have similar parsing accuracy overall  this means that the Path scheme is the least likely to introduce errors on projective arcs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'14'", "'88'", "'28'", "'103'"]
'89'
'14'
'88'
'28'
'103'
['89', '14', '88', '28', '103']
parsed_discourse_facet ['aim_citation', 'results_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'50'", "'75'", "'21'", "'36'"]
'48'
'50'
'75'
'21'
'36'
['48', '50', '75', '21', '36']
parsed_discourse_facet ['method_citation']
['We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\xcb\u2021c  1998).', 'In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.', 'In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00621', '(95%-conf.int.', '0.00621', '-', '0.00621)']
['system', 'ROUGE-S*', 'Average_P:', '0.27273', '(95%-conf.int.', '0.27273', '-', '0.27273)']
['system', 'ROUGE-S*', 'Average_F:', '0.01215', '(95%-conf.int.', '0.01215', '-', '0.01215)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:55', 'F:15']
['By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.']
['By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.', u'In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p\u2193.', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.10020', '(95%-conf.int.', '0.10020', '-', '0.10020)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.18215', '(95%-conf.int.', '0.18215', '-', '0.18215)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4950', 'P:496', 'F:496']
['As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00300', '(95%-conf.int.', '0.00300', '-', '0.00300)']
['system', 'ROUGE-S*', 'Average_P:', '0.09890', '(95%-conf.int.', '0.09890', '-', '0.09890)']
['system', 'ROUGE-S*', 'Average_F:', '0.00582', '(95%-conf.int.', '0.00582', '-', '0.00582)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:91', 'F:9']
['As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', u'In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p\u2193.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00889', '(95%-conf.int.', '0.00889', '-', '0.00889)']
['system', 'ROUGE-S*', 'Average_P:', '0.06349', '(95%-conf.int.', '0.06349', '-', '0.06349)']
['system', 'ROUGE-S*', 'Average_F:', '0.01559', '(95%-conf.int.', '0.01559', '-', '0.01559)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2701', 'P:378', 'F:24']
['While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.']
['However  it can be noted that the results for the least informative encoding  Path  are almost comparable  while the third encoding  Head  gives substantially worse results for both data sets.', 'However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.', 'The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.', 'Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.', u'While the proportion of sentences containing non-projective dependencies is often 15\u201325%  the total proportion of non-projective arcs is normally only 1\u20132%.']
['system', 'ROUGE-S*', 'Average_R:', '0.02444', '(95%-conf.int.', '0.02444', '-', '0.02444)']
['system', 'ROUGE-S*', 'Average_P:', '0.72527', '(95%-conf.int.', '0.72527', '-', '0.72527)']
['system', 'ROUGE-S*', 'Average_F:', '0.04728', '(95%-conf.int.', '0.04728', '-', '0.04728)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2701', 'P:91', 'F:66']
['We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00248', '(95%-conf.int.', '0.00248', '-', '0.00248)']
['system', 'ROUGE-S*', 'Average_P:', '0.10909', '(95%-conf.int.', '0.10909', '-', '0.10909)']
['system', 'ROUGE-S*', 'Average_F:', '0.00486', '(95%-conf.int.', '0.00486', '-', '0.00486)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:55', 'F:6']
['In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00287', '(95%-conf.int.', '0.00287', '-', '0.00287)']
['system', 'ROUGE-S*', 'Average_P:', '0.10989', '(95%-conf.int.', '0.10989', '-', '0.10989)']
['system', 'ROUGE-S*', 'Average_F:', '0.00559', '(95%-conf.int.', '0.00559', '-', '0.00559)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:91', 'F:10']
['In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00041', '(95%-conf.int.', '0.00041', '-', '0.00041)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00080', '(95%-conf.int.', '0.00080', '-', '0.00080)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:91', 'F:1']
['However, the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech, with a best performance of 73% UAS (Holan, 2004).']
['The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).', u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:105', 'F:0']
['We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).']
['Table 5 shows the overall parsing accuracy attained with the three different encoding schemes  compared to the baseline (no special arc labels) and to training directly on non-projective dependency graphs.', u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\xcb\u2021c  1998).', 'In the experiments below  we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs 3 previously tested on Swedish (Nivre et al.  2004) and English (Nivre and Scholz  2004).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00383', '(95%-conf.int.', '0.00383', '-', '0.00383)']
['system', 'ROUGE-S*', 'Average_P:', '0.25455', '(95%-conf.int.', '0.25455', '-', '0.25455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00755', '(95%-conf.int.', '0.00755', '-', '0.00755)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3655', 'P:55', 'F:14']
['The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.']
['In principle  it would be possible to encode the exact position of the syntactic head in the label of the arc from the linear head  but this would give a potentially infinite set of arc labels and would make the training of the parser very hard.', u'For each token  three types of features may be taken into account: the word form; the part-of-speech assigned by an automatic tagger; and labels on previously assigned dependency arcs involving the token \u2013 the arc from its head and the arcs to its leftmost and rightmost dependent  respectively.', u"The details of the transformation procedure are slightly different depending on the encoding schemes: d\u2191h let the linear head be the syntactic head). target arc must have the form wl \u2212\u2192 wm; if no target arc is found  Head is used as backoff. must have the form wl \u2212\u2192 wm and no outgoing arcs of the form wm p'\u2193 \u2212\u2192 wo; no backoff.", u'At each point during the derivation  the parser has a choice between pushing the next input token onto the stack \u2013 with or without adding an arc from the token on top of the stack to the token pushed \u2013 and popping a token from the stack \u2013 with or without adding an arc from the next input token to the token popped.', 'By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.']
['system', 'ROUGE-S*', 'Average_R:', '0.02087', '(95%-conf.int.', '0.02087', '-', '0.02087)']
['system', 'ROUGE-S*', 'Average_P:', '0.23617', '(95%-conf.int.', '0.23617', '-', '0.23617)']
['system', 'ROUGE-S*', 'Average_F:', '0.03835', '(95%-conf.int.', '0.03835', '-', '0.03835)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:8385', 'P:741', 'F:175']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00344', '(95%-conf.int.', '0.00344', '-', '0.00344)']
['system', 'ROUGE-S*', 'Average_P:', '0.11429', '(95%-conf.int.', '0.11429', '-', '0.11429)']
['system', 'ROUGE-S*', 'Average_F:', '0.00668', '(95%-conf.int.', '0.00668', '-', '0.00668)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:105', 'F:12']
['In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00287', '(95%-conf.int.', '0.00287', '-', '0.00287)']
['system', 'ROUGE-S*', 'Average_P:', '0.10989', '(95%-conf.int.', '0.10989', '-', '0.10989)']
['system', 'ROUGE-S*', 'Average_F:', '0.00559', '(95%-conf.int.', '0.00559', '-', '0.00559)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:91', 'F:10']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00067', '(95%-conf.int.', '0.00067', '-', '0.00067)']
['system', 'ROUGE-S*', 'Average_P:', '0.01905', '(95%-conf.int.', '0.01905', '-', '0.01905)']
['system', 'ROUGE-S*', 'Average_F:', '0.00129', '(95%-conf.int.', '0.00129', '-', '0.00129)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:105', 'F:2']
['As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.']
['To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', u'In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p\u2193.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.']
['system', 'ROUGE-S*', 'Average_R:', '0.09033', '(95%-conf.int.', '0.09033', '-', '0.09033)']
['system', 'ROUGE-S*', 'Average_P:', '0.79365', '(95%-conf.int.', '0.79365', '-', '0.79365)']
['system', 'ROUGE-S*', 'Average_F:', '0.16221', '(95%-conf.int.', '0.16221', '-', '0.16221)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3321', 'P:378', 'F:300']
['As shown in Table 3, the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.', 'However, the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.']
['It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.', 'We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.', 'The dependency graph in Figure 1 satisfies all the defining conditions above  but it fails to satisfy the condition ofprojectivity (Kahane et al.  1998): The arc connecting the head jedna (one) to the dependent Z (out-of) spans the token je (is)  which is not dominated by jedna.', 'From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.', 'However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.']
['system', 'ROUGE-S*', 'Average_R:', '0.00562', '(95%-conf.int.', '0.00562', '-', '0.00562)']
['system', 'ROUGE-S*', 'Average_P:', '0.12865', '(95%-conf.int.', '0.12865', '-', '0.12865)']
['system', 'ROUGE-S*', 'Average_F:', '0.01077', '(95%-conf.int.', '0.01077', '-', '0.01077)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3916', 'P:171', 'F:22']
0.252913123419 0.0172581248921 0.0316674998021





input/ref/Task1/P08-1043_aakansha.csv
input/res/Task1/P08-1043.csv
parsing: input/ref/Task1/P08-1043_aakansha.csv
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="69" ssid="1">We represent all morphological analyses of a given utterance using a lattice structure.</S>
    <S sid="70" ssid="2">Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'70'"]
'69'
'70'
['69', '70']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="17">The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty, 2006) and (Cohen and Smith, 2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="11">Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
    <S sid="134" ssid="12">Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training 	similar to (Cohen and Smith, 2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'134'"]
'133'
'134'
['133', '134']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="17">The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="155" ssid="33">Evaluation We use 8 different measures to evaluate the performance of our system on the joint disambiguation task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="31" ssid = "10">Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'92'", "'86'", "'192'", "'31'"]
'183'
'92'
'86'
'192'
'31'
['183', '92', '86', '192', '31']
parsed_discourse_facet ['results_citation']
<S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="173" ssid = "11">The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter (α) which is tuned separately for each of the tasks.</S><S sid ="77" ssid = "9">Segments with the same surface form but different PoS tags are treated as different lexemes  and are represented as separate arcs (e.g. the two arcs labeled neim from node 6 to 7).</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'192'", "'173'", "'77'", "'133'"]
'183'
'192'
'173'
'77'
'133'
['183', '192', '173', '77', '133']
parsed_discourse_facet ['method_citation']
<S sid ="141" ssid = "19">This analyzer setting is similar to that of (Cohen and Smith  2007)  and models using it are denoted nohsp  Parser and Grammar We used BitPar (Schmid  2004)  an efficient general purpose parser 10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.</S><S sid ="89" ssid = "21">Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree π s.t.</S><S sid ="51" ssid = "9">Tsarfaty (2006) used a morphological analyzer (Segal  2000)  a PoS tagger (Bar-Haim et al.  2005)  and a general purpose parser (Schmid  2000) in an integrated framework in which morphological and syntactic components interact to share information  leading to improved performance on the joint task.</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="82" ssid = "14">In sequential tagging models such as (Adler and Elhadad  2006; Bar-Haim et al.  2007; Smith et al.  2005) weights are assigned according to a language model The input for the joint task is a sequence W = w1  ...   wn of space-delimited tokens.</S>
original cit marker offset is 0
new cit marker offset is 0



["'141'", "'89'", "'51'", "'156'", "'82'"]
'141'
'89'
'51'
'156'
'82'
['141', '89', '51', '156', '82']
parsed_discourse_facet ['method_citation']
<S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'183'", "'86'", "'0'", "'92'"]
'192'
'183'
'86'
'0'
'92'
['192', '183', '86', '0', '92']
parsed_discourse_facet ['method_citation']
<S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'183'", "'0'", "'86'", "'92'"]
'192'
'183'
'0'
'86'
'92'
['192', '183', '0', '86', '92']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="89" ssid = "21">Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree π s.t.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'86'", "'192'", "'89'", "'92'"]
'183'
'86'
'192'
'89'
'92'
['183', '86', '192', '89', '92']
parsed_discourse_facet ['method_citation']
<S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'86'", "'57'", "'192'", "'183'"]
'92'
'86'
'57'
'192'
'183'
['92', '86', '57', '192', '183']
parsed_discourse_facet ['method_citation']
<S sid ="31" ssid = "10">Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="186" ssid = "24">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'", "'92'", "'133'", "'0'", "'186'"]
'31'
'92'
'133'
'0'
'186'
['31', '92', '133', '0', '186']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'92'", "'192'", "'183'", "'133'"]
'86'
'92'
'192'
'183'
'133'
['86', '92', '192', '183', '133']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'192'", "'183'", "'0'", "'92'"]
'86'
'192'
'183'
'0'
'92'
['86', '192', '183', '0', '92']
parsed_discourse_facet ['method_citation']
<S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="123" ssid = "1">Previous work on morphological and syntactic disambiguation in Hebrew used different sets of data  different splits  differing annotation schemes  and different evaluation measures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'57'", "'192'", "'86'", "'123'"]
'92'
'57'
'192'
'86'
'123'
['92', '57', '192', '86', '123']
parsed_discourse_facet ['method_citation']
<S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="69" ssid = "1">We represent all morphological analyses of a given utterance using a lattice structure.</S><S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="113" ssid = "45">The remaining arcs are marked OOV.</S><S sid ="50" ssid = "8">The joint morphological and syntactic hypothesis was first discussed in (Tsarfaty  2006; Tsarfaty and Sima’an  2004) and empirically explored in (Tsarfaty  2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'", "'69'", "'14'", "'113'", "'50'"]
'57'
'69'
'14'
'113'
'50'
['57', '69', '14', '113', '50']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'92'", "'192'", "'183'", "'57'"]
'86'
'92'
'192'
'183'
'57'
['86', '92', '192', '183', '57']
parsed_discourse_facet ['results_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'192'", "'0'", "'92'", "'183'"]
'86'
'192'
'0'
'92'
'183'
['86', '192', '0', '92', '183']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'183'", "'192'", "'92'", "'0'"]
'86'
'183'
'192'
'92'
'0'
['86', '183', '192', '92', '0']
parsed_discourse_facet ['method_citation']
<S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S><S sid ="18" ssid = "14">Cohen and Smith (2007) followed up on these results and proposed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'92'", "'183'", "'133'", "'18'"]
'192'
'92'
'183'
'133'
'18'
['192', '92', '183', '133', '18']
parsed_discourse_facet ['method_citation']
['The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.']
['Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', 'This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.']
['system', 'ROUGE-S*', 'Average_R:', '0.00151', '(95%-conf.int.', '0.00151', '-', '0.00151)']
['system', 'ROUGE-S*', 'Average_P:', '0.02564', '(95%-conf.int.', '0.02564', '-', '0.02564)']
['system', 'ROUGE-S*', 'Average_F:', '0.00285', '(95%-conf.int.', '0.00285', '-', '0.00285)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:78', 'F:2']
['Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.']
[u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.', 'Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', u'A morphological analyzer M : W\u2014* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:153', 'F:0']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
[u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', u'A morphological analyzer M : W\u2014* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.01234', '(95%-conf.int.', '0.01234', '-', '0.01234)']
['system', 'ROUGE-S*', 'Average_P:', '0.14167', '(95%-conf.int.', '0.14167', '-', '0.14167)']
['system', 'ROUGE-S*', 'Average_F:', '0.02270', '(95%-conf.int.', '0.02270', '-', '0.02270)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:120', 'F:17']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
[u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', u'A morphological analyzer M : W\u2014* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.01234', '(95%-conf.int.', '0.01234', '-', '0.01234)']
['system', 'ROUGE-S*', 'Average_P:', '0.14167', '(95%-conf.int.', '0.14167', '-', '0.14167)']
['system', 'ROUGE-S*', 'Average_F:', '0.02270', '(95%-conf.int.', '0.02270', '-', '0.02270)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:120', 'F:17']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['In sequential tagging models such as (Adler and Elhadad  2006; Bar-Haim et al.  2007; Smith et al.  2005) weights are assigned according to a language model The input for the joint task is a sequence W = w1  ...   wn of space-delimited tokens.', u'Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree \xcf\u20ac s.t.', 'Tsarfaty (2006) used a morphological analyzer (Segal  2000)  a PoS tagger (Bar-Haim et al.  2005)  and a general purpose parser (Schmid  2000) in an integrated framework in which morphological and syntactic components interact to share information  leading to improved performance on the joint task.', 'To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).', 'This analyzer setting is similar to that of (Cohen and Smith  2007)  and models using it are denoted nohsp  Parser and Grammar We used BitPar (Schmid  2004)  an efficient general purpose parser 10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.']
['system', 'ROUGE-S*', 'Average_R:', '0.00253', '(95%-conf.int.', '0.00253', '-', '0.00253)']
['system', 'ROUGE-S*', 'Average_P:', '0.20833', '(95%-conf.int.', '0.20833', '-', '0.20833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00501', '(95%-conf.int.', '0.00501', '-', '0.00501)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:9870', 'P:120', 'F:25']
['Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['Segments with the same surface form but different PoS tags are treated as different lexemes  and are represented as separate arcs (e.g. the two arcs labeled neim from node 6 to 7).', u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.', u'The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter (\u03b1) which is tuned separately for each of the tasks.', 'Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00161', '(95%-conf.int.', '0.00161', '-', '0.00161)']
['system', 'ROUGE-S*', 'Average_P:', '0.01058', '(95%-conf.int.', '0.01058', '-', '0.01058)']
['system', 'ROUGE-S*', 'Average_F:', '0.00279', '(95%-conf.int.', '0.00279', '-', '0.00279)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:378', 'F:4']
['We represent all morphological analyses of a given utterance using a lattice structure.', 'Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.']
[u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.', u'For brevity we omit the segments from the analysis  and so analysis of the form \u201cfmnh\u201d as f/REL mnh/VB is represented simply as REL VB.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', u'A morphological analyzer M : W\u2014* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00994', '(95%-conf.int.', '0.00994', '-', '0.00994)']
['system', 'ROUGE-S*', 'Average_P:', '0.09942', '(95%-conf.int.', '0.09942', '-', '0.09942)']
['system', 'ROUGE-S*', 'Average_F:', '0.01807', '(95%-conf.int.', '0.01807', '-', '0.01807)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:171', 'F:17']
['Evaluation We use 8 different measures to evaluate the performance of our system on the joint disambiguation task.']
['Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.', u'Cohen and Smith approach this by introducing the \xce\xb1 hyperparameter  which performs best when optimized independently for each sentence (cf.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', 'Cohen and Smith (2007) followed up on these results and proposed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own.', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00154', '(95%-conf.int.', '0.00154', '-', '0.00154)']
['system', 'ROUGE-S*', 'Average_P:', '0.08333', '(95%-conf.int.', '0.08333', '-', '0.08333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00302', '(95%-conf.int.', '0.00302', '-', '0.00302)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:36', 'F:3']
['Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
[u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', u'A morphological analyzer M : W\u2014* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00363', '(95%-conf.int.', '0.00363', '-', '0.00363)']
['system', 'ROUGE-S*', 'Average_P:', '0.01323', '(95%-conf.int.', '0.01323', '-', '0.01323)']
['system', 'ROUGE-S*', 'Average_F:', '0.00569', '(95%-conf.int.', '0.00569', '-', '0.00569)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:378', 'F:5']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.', u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', u'A morphological analyzer M : W\u2014* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00067', '(95%-conf.int.', '0.00067', '-', '0.00067)']
['system', 'ROUGE-S*', 'Average_P:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00125', '(95%-conf.int.', '0.00125', '-', '0.00125)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:120', 'F:1']
0.0732199992678 0.00461099995389 0.00840799991592





input/ref/Task1/J01-2004_aakansha.csv
input/res/Task1/J01-2004.csv
parsing: input/ref/Task1/J01-2004_aakansha.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'"]
'372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="17" ssid="5">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



["'302'"]
'302'
['302']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="37">This underspecification of the nonterminal predictions (e.g., VP-VBD in the example in Figure 2, as opposed to NP), allows lexical items to become part of the left context, and so be used to condition production probabilities, even the production probabilities of constituents that dominate them in the unfactored tree.</S>
    <S sid="80" ssid="38">It also brings words further downstream into the look-ahead at the point of specification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'", "'80'"]
'79'
'80'
['79', '80']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="42">Our approach is found to yield very accurate parses efficiently, and, in addition, to lend itself straightforwardly to estimating word probabilities on-line, that is, in a single pass from left to right.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="291" ssid="47">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'291'"]
'291'
['291']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['method_citation']
<S sid="209" ssid="113">This parser is essentially a stochastic version of the top-down parser described in Aho, Sethi, and Ullman (1986).</S>
    <S sid="210" ssid="114">It uses a PCFG with a conditional probability model of the sort defined in the previous section.</S>
original cit marker offset is 0
new cit marker offset is 0



["'209'", "'210'"]
'209'
'210'
['209', '210']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'"]
'372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="8">Two features of our top-down parsing approach will emerge as key to its success.</S>
    <S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S><S sid="32" ssid="20">A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'21'", "'32'"]
'20'
'21'
'32'
['20', '21', '32']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="21">Because the rooted partial derivation is fully connected, all of the conditioning information that might be extracted from the top-down left context has already been specified, and a conditional probability model built on this information will not impose any additional burden on the search.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'"]
'33'
['33']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="168" ssid = "72">For example  in Figure 2  constituent (NP-DT-NN) is simply NP.</S><S sid ="356" ssid = "112">The DARPA '93 HUB1 test setup consists of 213 utterances read from the Wall Street Journal  a total of 3 446 words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'224'", "'322'", "'358'", "'168'", "'356'"]
'224'
'322'
'358'
'168'
'356'
['224', '322', '358', '168', '356']
parsed_discourse_facet ['results_citation']
<S sid ="17" ssid = "5">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S sid ="102" ssid = "6">One approach to syntactic language modeling is to use this distribution directly as a language model.</S><S sid ="340" ssid = "96">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid ="113" ssid = "17">In empirical trials  Goddeau used the top two stack entries to condition the word probability.</S><S sid ="12" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'102'", "'340'", "'113'", "'12'"]
'17'
'102'
'340'
'113'
'12'
['17', '102', '340', '113', '12']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="95" ssid = "53">For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'95'", "'302'", "'224'", "'104'"]
'134'
'95'
'302'
'224'
'104'
['134', '95', '302', '224', '104']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="167" ssid = "71">In order to avoid any confusions in identifying the nonterminal label of a particular rule production in either its factored or rionfactored version  we introduce the function constituent (A) for every nonterminal in the factored grammar Gf  which is simply the label of the constituent whose factorization results in A.</S><S sid ="112" ssid = "16">Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state  and conditioning on those entries in a way similar to an n-gram.</S><S sid ="85" ssid = "43">The simple definition of c-command that we will be using in this paper is the following: a node A c-commands a node B if and only if (i) A does not dominate B; and (ii) the lowest branching node (i.e.  non-unary node) that dominates A also dominates B.'</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'167'", "'112'", "'85'", "'390'"]
'134'
'167'
'112'
'85'
'390'
['134', '167', '112', '85', '390']
parsed_discourse_facet ['method_citation']
<S sid ="321" ssid = "77">In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.</S><S sid ="95" ssid = "53">For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].</S><S sid ="199" ssid = "103">Table 1 gives a breakdown of the different levels of conditioning information used in the empirical trials  with a mnemonic label that will be used when presenting results.</S><S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="11" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'321'", "'95'", "'199'", "'5'", "'11'"]
'321'
'95'
'199'
'5'
'11'
['321', '95', '199', '5', '11']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="87" ssid = "45">Notice that the subject NP c-commands the object NP  but not vice versa  since the lowest branching node that dominates the object NP is the VP  which does not dominate the subject NP.</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S><S sid ="85" ssid = "43">The simple definition of c-command that we will be using in this paper is the following: a node A c-commands a node B if and only if (i) A does not dominate B; and (ii) the lowest branching node (i.e.  non-unary node) that dominates A also dominates B.'</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'87'", "'302'", "'104'", "'85'"]
'134'
'87'
'302'
'104'
'85'
['134', '87', '302', '104', '85']
parsed_discourse_facet ['method_citation']
<S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="52" ssid = "10">Consider  for example  the parse tree shown in (a) in Figure 1: the start symbol is St  which expands into an S. The S node expands into an NP followed by a VP.</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S>
original cit marker offset is 0
new cit marker offset is 0



["'224'", "'134'", "'52'", "'322'", "'306'"]
'224'
'134'
'52'
'322'
'306'
['224', '134', '52', '322', '306']
parsed_discourse_facet ['method_citation']
<S sid ="112" ssid = "16">Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state  and conditioning on those entries in a way similar to an n-gram.</S><S sid ="129" ssid = "33">The shift-reduce parser will have  perhaps  built the structure shown  and the stack state will have an NP entry with the head &quot;cat&quot; at the top of the stack  and a VBD entry with the head &quot;chased&quot; second on the stack.</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="87" ssid = "45">Notice that the subject NP c-commands the object NP  but not vice versa  since the lowest branching node that dominates the object NP is the VP  which does not dominate the subject NP.</S><S sid ="109" ssid = "13">A shift-reduce parser operates from left to right using a stack and a pointer to the next word in the input string.9 Each stack entry consists minimally of a nonterminal label.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'129'", "'302'", "'87'", "'109'"]
'112'
'129'
'302'
'87'
'109'
['112', '129', '302', '87', '109']
parsed_discourse_facet ['method_citation']
<S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="213" ssid = "117">The parser takes an input string 4  a PCFG G  and a priority queue of candidate analyses.</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'358'", "'306'", "'224'", "'213'", "'322'"]
'358'
'306'
'224'
'213'
'322'
['358', '306', '224', '213', '322']
parsed_discourse_facet ['method_citation']
<S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="59" ssid = "17">A PCFG is a CFG with a probability assigned to each rule; specifically  each righthand side has a probability given the left-hand side of the rule.</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="52" ssid = "10">Consider  for example  the parse tree shown in (a) in Figure 1: the start symbol is St  which expands into an S. The S node expands into an NP followed by a VP.</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'302'", "'59'", "'134'", "'52'", "'104'"]
'302'
'59'
'134'
'52'
'104'
['302', '59', '134', '52', '104']
parsed_discourse_facet ['method_citation']
<S sid ="210" ssid = "114">It uses a PCFG with a conditional probability model of the sort defined in the previous section.</S><S sid ="142" ssid = "46">A simple PCFG conditions rule probabilities on the left-hand side of the rule.</S><S sid ="323" ssid = "79">They trained and tested the SLM on a modified  more &quot;speech-like&quot; version of the Penn Treebank.</S><S sid ="0" ssid = "0">Probabilistic Top-Down Parsing and Language Modeling</S><S sid ="370" ssid = "126">We followed Chelba (2000) in using an LM weight of 16 for the lattice trigram.</S>
original cit marker offset is 0
new cit marker offset is 0



["'210'", "'142'", "'323'", "'0'", "'370'"]
'210'
'142'
'323'
'0'
'370'
['210', '142', '323', '0', '370']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S><S sid ="154" ssid = "58">We use the relative frequency estimator  and smooth our production probabilities by interpolating the relative frequency estimates with those obtained by &quot;annotating&quot; less contextual information.</S><S sid ="0" ssid = "0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'224'", "'104'", "'154'", "'0'"]
'134'
'224'
'104'
'154'
'0'
['134', '224', '104', '154', '0']
parsed_discourse_facet ['method_citation']
<S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="95" ssid = "53">For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].</S><S sid ="124" ssid = "28">Each distinct derivation path within the beam has a probability and a stack state associated with it.</S><S sid ="147" ssid = "51">One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).</S><S sid ="168" ssid = "72">For example  in Figure 2  constituent (NP-DT-NN) is simply NP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'302'", "'95'", "'124'", "'147'", "'168'"]
'302'
'95'
'124'
'147'
'168'
['302', '95', '124', '147', '168']
parsed_discourse_facet ['results_citation']
<S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S>
original cit marker offset is 0
new cit marker offset is 0



["'358'", "'306'", "'322'", "'224'", "'134'"]
'358'
'306'
'322'
'224'
'134'
['358', '306', '322', '224', '134']
parsed_discourse_facet ['method_citation']
<S sid ="289" ssid = "45">These results  achieved using very straightforward conditioning events and considering only the left context  are within one to four points of the best published Observed running time on Section 23 of the Penn Treebank  with the full conditional probability model and beam of 10-11  using one 300 MHz UltraSPARC processor and 256MB of RAM of a Sun Enterprise 450. accuracies cited above.'</S><S sid ="25" ssid = "13">A parser that is not left to right  but which has rooted derivations  e.g.  a headfirst parser  will be able to calculate generative joint probabilities for entire strings; however  it will not be able to calculate probabilities for each word conditioned on previously generated words  unless each derivation generates the words in the string in exactly the same order.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="141" ssid = "45">We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.</S><S sid ="22" ssid = "10">A left-toright parser whose derivations are not rooted  i.e.  with derivations that can consist of disconnected tree fragments  such as an LR or shift-reduce parser  cannot incrementally calculate the probability of each prefix string being generated by the probabilistic grammar  because their derivations include probability mass from unrooted structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'289'", "'25'", "'390'", "'141'", "'22'"]
'289'
'25'
'390'
'141'
'22'
['289', '25', '390', '141', '22']
parsed_discourse_facet ['method_citation']
<S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'224'", "'306'", "'358'", "'134'", "'322'"]
'224'
'306'
'358'
'134'
'322'
['224', '306', '358', '134', '322']
parsed_discourse_facet ['method_citation']
<S sid ="147" ssid = "51">One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="278" ssid = "34">Section 22 (41 817 words  1 700 sentences) served as the development corpus  on which the parser was tested until stable versions were ready to run on the test data  to avoid developing the parser to fit the specific test data.</S><S sid ="141" ssid = "45">We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'302'", "'278'", "'141'", "'390'"]
'147'
'302'
'278'
'141'
'390'
['147', '302', '278', '141', '390']
parsed_discourse_facet ['method_citation']
['Because the rooted partial derivation is fully connected, all of the conditioning information that might be extracted from the top-down left context has already been specified, and a conditional probability model built on this information will not impose any additional burden on the search.']
['Section 22 (41 817 words  1 700 sentences) served as the development corpus  on which the parser was tested until stable versions were ready to run on the test data  to avoid developing the parser to fit the specific test data.', 'One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).', "In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.", 'With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.', 'We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.']
['system', 'ROUGE-S*', 'Average_R:', '0.00162', '(95%-conf.int.', '0.00162', '-', '0.00162)']
['system', 'ROUGE-S*', 'Average_P:', '0.04211', '(95%-conf.int.', '0.04211', '-', '0.04211)']
['system', 'ROUGE-S*', 'Average_F:', '0.00311', '(95%-conf.int.', '0.00311', '-', '0.00311)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4950', 'P:190', 'F:8']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['For example  in Figure 2  constituent (NP-DT-NN) is simply NP.', 'Each distinct derivation path within the beam has a probability and a stack state associated with it.', "In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.", 'For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].', 'One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:91', 'F:0']
['It also brings words further downstream into the look-ahead at the point of specification.', 'This underspecification of the nonterminal predictions (e.g., VP-VBD in the example in Figure 2, as opposed to NP), allows lexical items to become part of the left context, and so be used to condition production probabilities, even the production probabilities of constituents that dominate them in the unfactored tree.']
['Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.', 'Consider  for example  the parse tree shown in (a) in Figure 1: the start symbol is St  which expands into an S. The S node expands into an NP followed by a VP.', 'For each word position i  we have a separate priority queue H  of analyses with look-ahead w .', 'Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).', "The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00466', '(95%-conf.int.', '0.00466', '-', '0.00466)']
['system', 'ROUGE-S*', 'Average_P:', '0.03448', '(95%-conf.int.', '0.03448', '-', '0.03448)']
['system', 'ROUGE-S*', 'Average_F:', '0.00821', '(95%-conf.int.', '0.00821', '-', '0.00821)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:406', 'F:14']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.', "The DARPA '93 HUB1 test setup consists of 213 utterances read from the Wall Street Journal  a total of 3 446 words.", 'For each word position i  we have a separate priority queue H  of analyses with look-ahead w .', 'For example  in Figure 2  constituent (NP-DT-NN) is simply NP.', "We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:91', 'F:0']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['They trained and tested the SLM on a modified  more &quot;speech-like&quot; version of the Penn Treebank.', 'We followed Chelba (2000) in using an LM weight of 16 for the lattice trigram.', 'A simple PCFG conditions rule probabilities on the left-hand side of the rule.', 'It uses a PCFG with a conditional probability model of the sort defined in the previous section.', 'Probabilistic Top-Down Parsing and Language Modeling']
['system', 'ROUGE-S*', 'Average_R:', '0.00405', '(95%-conf.int.', '0.00405', '-', '0.00405)']
['system', 'ROUGE-S*', 'Average_P:', '0.03297', '(95%-conf.int.', '0.03297', '-', '0.03297)']
['system', 'ROUGE-S*', 'Average_F:', '0.00721', '(95%-conf.int.', '0.00721', '-', '0.00721)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:91', 'F:3']
['Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.']
["In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.", 'Consider  for example  the parse tree shown in (a) in Figure 1: the start symbol is St  which expands into an S. The S node expands into an NP followed by a VP.', 'A PCFG is a CFG with a probability assigned to each rule; specifically  each righthand side has a probability given the left-hand side of the rule.', 'The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.', "The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00456', '(95%-conf.int.', '0.00456', '-', '0.00456)']
['system', 'ROUGE-S*', 'Average_P:', '0.02989', '(95%-conf.int.', '0.02989', '-', '0.02989)']
['system', 'ROUGE-S*', 'Average_F:', '0.00791', '(95%-conf.int.', '0.00791', '-', '0.00791)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:435', 'F:13']
['This parser is essentially a stochastic version of the top-down parser described in Aho, Sethi, and Ullman (1986).', 'It uses a PCFG with a conditional probability model of the sort defined in the previous section.']
['Probabilistic Top-Down Parsing and Language Modeling', 'For each word position i  we have a separate priority queue H  of analyses with look-ahead w .', 'We use the relative frequency estimator  and smooth our production probabilities by interpolating the relative frequency estimates with those obtained by &quot;annotating&quot; less contextual information.', 'The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.', "The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00144', '(95%-conf.int.', '0.00144', '-', '0.00144)']
['system', 'ROUGE-S*', 'Average_P:', '0.01961', '(95%-conf.int.', '0.01961', '-', '0.01961)']
['system', 'ROUGE-S*', 'Average_F:', '0.00269', '(95%-conf.int.', '0.00269', '-', '0.00269)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:153', 'F:3']
['This paper will examine language modeling for speech recognition from a natural language processing point of view.']
['One approach to syntactic language modeling is to use this distribution directly as a language model.', 'A small recognition experiment also demonstrates the utility of the model.', 'The trigram model was also trained on Sections 00-20 of the C&J corpus.', 'In empirical trials  Goddeau used the top two stack entries to condition the word probability.', 'This paper will examine language modeling for speech recognition from a natural language processing point of view.']
['system', 'ROUGE-S*', 'Average_R:', '0.06707', '(95%-conf.int.', '0.06707', '-', '0.06707)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.12571', '(95%-conf.int.', '0.12571', '-', '0.12571)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:55', 'F:55']
0.144882498189 0.0104249998697 0.0193549997581





input/ref/Task1/P08-1102_swastika.csv
input/res/Task1/P08-1102.csv
parsing: input/ref/Task1/P08-1102_swastika.csv
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="10">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="8">All feature templates and their instances are shown in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



['36']
36
['36']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
<S sid="97" ssid="8">Figure 3 shows their learning curves depicting the F-measure on the development set after 1 to 10 training iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



['97']
97
['97']
parsed_discourse_facet ['result_citation']
<S sid="91" ssid="2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['aim_citation']
<S sid="16" ssid="12">Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



['34']
34
['34']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="22" ssid = "18">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here  Ci (i = L.n) denotes Chinese character  ti (i = L.m) denotes POS tag  and Cl:r (l < r) denotes character sequence ranges from Cl to Cr.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="104" ssid = "15">According to the usual practice in syntactic analysis  we choose chapters 1 − 260 (18074 sentences) as training set  chapter 271 − 300 (348 sentences) as test set and chapter 301 − 325 (350 sentences) as development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'", "'22'", "'16'", "'94'", "'104'"]
'87'
'22'
'16'
'94'
'104'
['87', '22', '16', '94', '104']
parsed_discourse_facet ['results_citation']
<S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'47'", "'48'", "'32'", "'133'"]
'69'
'47'
'48'
'32'
'133'
['69', '47', '48', '32', '133']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="99" ssid = "10">Then we trained LEX on each of the four corpora for 7 iterations.</S><S sid ="30" ssid = "2">It has comparable performance to CRFs  while with much faster training.</S><S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="122" ssid = "33">Another important feature is the labelling model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'99'", "'30'", "'101'", "'122'"]
'68'
'99'
'30'
'101'
'122'
['68', '99', '30', '101', '122']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="30" ssid = "2">It has comparable performance to CRFs  while with much faster training.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'87'", "'31'", "'94'", "'30'"]
'101'
'87'
'31'
'94'
'30'
['101', '87', '31', '94', '30']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="0" ssid = "0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'0'", "'87'", "'69'", "'48'"]
'47'
'0'
'87'
'69'
'48'
['47', '0', '87', '69', '48']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'48'", "'49'", "'133'", "'32'"]
'47'
'48'
'49'
'133'
'32'
['47', '48', '49', '133', '32']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="94" ssid = "5">With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'68'", "'94'", "'87'", "'7'"]
'16'
'68'
'94'
'87'
'7'
['16', '68', '94', '87', '7']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "19">We find that Joint S&T can also improve the segmentation accuracy.</S><S sid ="138" ssid = "1">This work was done while L. H. was visiting CAS/ICT.</S><S sid ="61" ssid = "12">In this layer  each knowledge source is treated as a feature with a corresponding weight denoting its relative importance.</S><S sid ="90" ssid = "1">We reported results from two set of experiments.</S><S sid ="107" ssid = "18">The evaluation results are shown in Table 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'138'", "'61'", "'90'", "'107'"]
'108'
'138'
'61'
'90'
'107'
['108', '138', '61', '90', '107']
parsed_discourse_facet ['method_citation']
<S sid ="48" ssid = "20">We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.</S><S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S><S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'133'", "'32'", "'69'", "'49'"]
'48'
'133'
'32'
'69'
'49'
['48', '133', '32', '69', '49']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="7" ssid = "3">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.</S><S sid ="49" ssid = "21">To alleviate overfitting on the training examples  we use the refinement strategy called “averaged parameters” (Collins  2002) to the algorithm in Algorithm 1.</S><S sid ="69" ssid = "20">Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'7'", "'49'", "'69'", "'16'"]
'47'
'7'
'49'
'69'
'16'
['47', '7', '49', '69', '16']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="54" ssid = "5">We noticed that the templates related to word unigrams and bigrams bring to the feature space an enlargement much rapider than the character-base ones  not to mention the higher-order grams such as trigrams or 4-grams.</S><S sid ="64" ssid = "15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result  a 4-gram POS language model functioning as the product of statetransition probabilities in HMM  and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S><S sid ="58" ssid = "9">Instead of incorporating all features into the perceptron directly  we first trained the perceptron using character-based features  and several other sub-models using additional ones such as word or POS n-grams  then trained the outside-layer linear model using the outputs of these sub-models  including the perceptron.</S><S sid ="135" ssid = "6">In addition  all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus  whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'54'", "'64'", "'58'", "'135'"]
'2'
'54'
'64'
'58'
'135'
['2', '54', '64', '58', '135']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "11">Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS  CityU and MSR).</S><S sid ="47" ssid = "19">For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.</S><S sid ="134" ssid = "5">If this cascaded linear model were chosen  could more accurate generative models (LMs  word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely  or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S><S sid ="32" ssid = "4">We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.</S><S sid ="121" ssid = "32">Among other features  the 4-gram POS LM plays the most important role  removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'47'", "'134'", "'32'", "'121'"]
'100'
'47'
'134'
'32'
'121'
['100', '47', '134', '32', '121']
parsed_discourse_facet ['method_citation']
<S sid ="133" ssid = "4">However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?</S><S sid ="52" ssid = "3">However  such features are generated dynamically during the decoding procedure so that the feature space enlarges much more rapidly.</S><S sid ="2" ssid = "2">With a character-based perceptron as the core  combined with realvalued features such as language models  the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S><S sid ="131" ssid = "2">Under this model  many knowledge sources that may be intractable to be incorporated into the perceptron directly  can be utilized effectively in the outside-layer linear model.</S><S sid ="135" ssid = "6">In addition  all knowledge sources we used in the core perceptron and the outside-layer linear model come from the training corpus  whereas many open knowledge sources (lexicon etc.) can be used to improve performance (Ng and Low  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'52'", "'2'", "'131'", "'135'"]
'133'
'52'
'2'
'131'
'135'
['133', '52', '2', '131', '135']
parsed_discourse_facet ['results_citation']
<S sid ="79" ssid = "4">By maintaining a stack of size N at each position i of the sequence  we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S><S sid ="73" ssid = "24">For instance  if the word w appears N times in training corpus and is labelled as POS t for n times  the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.</S><S sid ="98" ssid = "9">We found that LEX outperforms NON-LEX with a margin of about 0.002 at each iteration  and its learning curve reaches a tableland at iteration 7.</S><S sid ="37" ssid = "9">C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).</S><S sid ="70" ssid = "21">Given a training corpus with POS tags  we can train a word-POS co-occurrence model to approximate the probability that the word sequence of the labelled result co-exists with its corresponding POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'", "'73'", "'98'", "'37'", "'70'"]
'79'
'73'
'98'
'37'
'70'
['79', '73', '98', '37', '70']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S><S sid ="87" ssid = "12">Line 6 enumerates all POS’s for the word w spanning length l and ending at position i.</S><S sid ="103" ssid = "14">We turned to experiments on CTB 5.0 to test the performance of the cascaded model.</S><S sid ="68" ssid = "19">It is an important measure of fluency of the translation in SMT.</S><S sid ="101" ssid = "12">On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'87'", "'103'", "'68'", "'101'"]
'16'
'87'
'103'
'68'
'101'
['16', '87', '103', '68', '101']
parsed_discourse_facet ['method_citation']
<S sid ="31" ssid = "3">The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.</S><S sid ="22" ssid = "18">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here  Ci (i = L.n) denotes Chinese character  ti (i = L.m) denotes POS tag  and Cl:r (l < r) denotes character sequence ranges from Cl to Cr.</S><S sid ="91" ssid = "2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2  including the Academia Sinica Corpus (AS)  the Hong Kong City University Corpus (CityU)  the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S><S sid ="45" ssid = "17">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x ∈ X to outputs y ∈ Y   where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S><S sid ="16" ssid = "12">Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'", "'22'", "'91'", "'45'", "'16'"]
'31'
'22'
'91'
'45'
'16'
['31', '22', '91', '45', '16']
parsed_discourse_facet ['method_citation']
['Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.']
['It is an important measure of fluency of the translation in SMT.', 'Then we trained LEX on each of the four corpora for 7 iterations.', 'Another important feature is the labelling model.', 'It has comparable performance to CRFs  while with much faster training.', 'On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:66', 'F:0']
['All feature templates and their instances are shown in Table 1.']
['However  can the perceptron incorporate all the knowledge used in the outside-layer linear model?', 'For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.', 'We trained a character-based perceptron for Chinese Joint S&T  and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.', 'We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.', u'To alleviate overfitting on the training examples  we use the refinement strategy called \u201caveraged parameters\u201d (Collins  2002) to the algorithm in Algorithm 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:15', 'F:0']
['The feature templates we adopted are selected from those of Ng and Low (2004).']
['It is an important measure of fluency of the translation in SMT.', 'We turned to experiments on CTB 5.0 to test the performance of the cascaded model.', 'On the three corpora  it also outperformed the word-based perceptron model of Zhang and Clark (2007).', u'Line 6 enumerates all POS\xe2\u20ac\u2122s for the word w spanning length l and ending at position i.', 'Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:21', 'F:0']
['The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.']
['For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.', 'Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.', u'Line 6 enumerates all POS\xe2\u20ac\u2122s for the word w spanning length l and ending at position i.', 'We used the algorithm depicted in Algorithm 1 to tune the parameter vector a.', 'A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging']
['system', 'ROUGE-S*', 'Average_R:', '0.00376', '(95%-conf.int.', '0.00376', '-', '0.00376)']
['system', 'ROUGE-S*', 'Average_P:', '0.06593', '(95%-conf.int.', '0.06593', '-', '0.06593)']
['system', 'ROUGE-S*', 'Average_F:', '0.00711', '(95%-conf.int.', '0.00711', '-', '0.00711)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:91', 'F:6']
['The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.']
['For an input character sequence x  we aim to find an output F(x) satisfying: vector 4)(x  y) and the parameter vector a.', 'Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.', 'Formally  an n-gram word LM approximates the probability of a word sequence W = w1:m with the following product: Notice that a bi-gram POS LM functions as the product of transition probabilities in HMM.', 'CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.', u'To alleviate overfitting on the training examples  we use the refinement strategy called \u201caveraged parameters\u201d (Collins  2002) to the algorithm in Algorithm 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00040', '(95%-conf.int.', '0.00040', '-', '0.00040)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00078', '(95%-conf.int.', '0.00078', '-', '0.00078)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:91', 'F:1']
['We trained a character-based perceptron for Chinese Joint S&T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.']
['It is an important measure of fluency of the translation in SMT.', 'CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM  and usually behaves the best in the two tasks.', 'With precision P and recall R  the balance F-measure is defined as: F = 2PR/(P + R).', u'Line 6 enumerates all POS\xe2\u20ac\u2122s for the word w spanning length l and ending at position i.', 'Shown in Figure 1  the cascaded model has a two-layer architecture  with a characterbased perceptron as the core combined with other real-valued features such as language models.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:91', 'F:0']
['The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.']
['The perceptron has been used in many NLP tasks  such as POS tagging (Collins  2002)  Chinese word segmentation (Ng and Low  2004; Zhang and Clark  2007) and so on.', '2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here  Ci (i = L.n) denotes Chinese character  ti (i = L.m) denotes POS tag  and Cl:r (l ']
['system', 'ROUGE-S*', 'Average_R:', '0.00150', '(95%-conf.int.', '0.00150', '-', '0.00150)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00264', '(95%-conf.int.', '0.00264', '-', '0.00264)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:91', 'F:1']
0.0125585712492 0.00080857141702 0.0015042856928





input/ref/Task1/W11-2123_aakansha.csv
input/res/Task1/W11-2123.csv
parsing: input/ref/Task1/W11-2123_aakansha.csv
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="136" ssid="8">We offer a state function s(wn1) = wn&#65533; where substring wn&#65533; is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'"]
'136'
['136']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="3">Queries take the form p(wn|wn&#8722;1 1 ) where wn1 is an n-gram.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="205" ssid="24">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al., 2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'205'"]
'205'
['205']
parsed_discourse_facet ['method_citation']
<S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
<S sid="204" ssid="23">For RandLM, we used the settings in the documentation: 8 bits per value and false positive probability 1 256.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'"]
'204'
['204']
parsed_discourse_facet ['method_citation']
<S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="229" ssid="48">Then we ran binary search to determine the least amount of memory with which it would run.</S>
original cit marker offset is 0
new cit marker offset is 0



["'229'"]
'229'
['229']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="93" ssid="71">The cost of storing these averages, in bits, is Because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.</S>
original cit marker offset is 0
new cit marker offset is 0



["'93'"]
'93'
['93']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W11-2123.csv
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="27" ssid = "5">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'183'", "'69'", "'27'", "'231'"]
'0'
'183'
'69'
'27'
'231'
['0', '183', '69', '27', '231']
parsed_discourse_facet ['results_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="149" ssid = "21">RandLM and SRILM also remove context that will not extend  but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.</S><S sid ="193" ssid = "12">It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'183'", "'69'", "'149'", "'193'"]
'0'
'183'
'69'
'149'
'193'
['0', '183', '69', '149', '193']
parsed_discourse_facet ['method_citation']
<S sid ="62" ssid = "40">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S><S sid ="142" ssid = "14">Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn−1 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.</S><S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="216" ssid = "35">Though we are not able to calculate their memory usage on our model  results reported in their paper suggest lower memory consumption than TRIE on large-scale models  at the expense of CPU time.</S><S sid ="50" ssid = "28">Given counts cn1 where e.g. c1 is the vocabulary size  total memory consumption  in bits  is Our PROBING data structure places all n-grams of the same order into a single giant hash table.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'", "'142'", "'3'", "'216'", "'50'"]
'62'
'142'
'3'
'216'
'50'
['62', '142', '3', '216', '50']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="137" ssid = "9">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="157" ssid = "29">Each visited entry wni stores backoff b(wni ).</S><S sid ="27" ssid = "5">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'137'", "'183'", "'157'", "'27'"]
'0'
'137'
'183'
'157'
'27'
['0', '137', '183', '157', '27']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="233" ssid = "52">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid ="193" ssid = "12">It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'3'", "'233'", "'193'", "'69'"]
'183'
'3'
'233'
'193'
'69'
['183', '3', '233', '193', '69']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="38" ssid = "16">The ratio of buckets to entries is controlled by space multiplier m > 1.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="71" ssid = "49">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'0'", "'38'", "'88'", "'71'"]
'183'
'0'
'38'
'88'
'71'
['183', '0', '38', '88', '71']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="71" ssid = "49">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'0'", "'14'", "'71'", "'88'"]
'69'
'0'
'14'
'71'
'88'
['69', '0', '14', '71', '88']
parsed_discourse_facet ['method_citation']
<S sid ="124" ssid = "28">Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="227" ssid = "46">The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.</S><S sid ="21" ssid = "16">Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'69'", "'227'", "'21'", "'13'"]
'124'
'69'
'227'
'21'
'13'
['124', '69', '227', '21', '13']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="137" ssid = "9">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid ="116" ssid = "20">Both implementations employ a state object  opaque to the application  that carries information from one query to the next; we discuss both further in Section 4.2.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'0'", "'137'", "'116'", "'231'"]
'183'
'0'
'137'
'116'
'231'
['183', '0', '137', '116', '231']
parsed_discourse_facet ['method_citation']
<S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="118" ssid = "22">The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'183'", "'69'", "'118'", "'14'"]
'88'
'183'
'69'
'118'
'14'
['88', '183', '69', '118', '14']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="227" ssid = "46">The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.</S><S sid ="124" ssid = "28">Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'231'", "'183'", "'227'", "'124'"]
'69'
'231'
'183'
'227'
'124'
['69', '231', '183', '227', '124']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "29">Each visited entry wni stores backoff b(wni ).</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="53" ssid = "31">Sorted arrays store key-value pairs in an array sorted by key  incurring no space overhead.</S><S sid ="27" ssid = "5">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S><S sid ="118" ssid = "22">The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'0'", "'53'", "'27'", "'118'"]
'157'
'0'
'53'
'27'
'118'
['157', '0', '53', '27', '118']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="195" ssid = "14">Interpolation search has a more expensive pivot but performs less pivoting and reads  so it is slow on small data and faster on large data.</S><S sid ="116" ssid = "20">Both implementations employ a state object  opaque to the application  that carries information from one query to the next; we discuss both further in Section 4.2.</S><S sid ="137" ssid = "9">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid ="62" ssid = "40">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'195'", "'116'", "'137'", "'62'"]
'183'
'195'
'116'
'137'
'62'
['183', '195', '116', '137', '62']
parsed_discourse_facet ['results_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S><S sid ="233" ssid = "52">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid ="193" ssid = "12">It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'69'", "'231'", "'233'", "'193'"]
'183'
'69'
'231'
'233'
'193'
['183', '69', '231', '233', '193']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="201" ssid = "20">Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'69'", "'88'", "'183'", "'201'"]
'0'
'69'
'88'
'183'
'201'
['0', '69', '88', '183', '201']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="62" ssid = "40">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S><S sid ="102" ssid = "6">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie)  allocating memory all at once (eliminating the need for full pointers)  and being easy to compile.</S><S sid ="179" ssid = "51">We do not experiment with models larger than physical memory in this paper because TPT is unreleased  factors such as disk speed are hard to replicate  and in such situations we recommend switching to a more compact representation  such as RandLM.</S><S sid ="142" ssid = "14">Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn−1 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'62'", "'102'", "'179'", "'142'"]
'3'
'62'
'102'
'179'
'142'
['3', '62', '102', '179', '142']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="2" ssid = "2">The structure uses linear probing hash tables and is designed for speed.</S><S sid ="129" ssid = "1">In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.</S><S sid ="71" ssid = "49">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S><S sid ="201" ssid = "20">Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'2'", "'129'", "'71'", "'201'"]
'69'
'2'
'129'
'71'
'201'
['69', '2', '129', '71', '201']
parsed_discourse_facet ['method_citation']
<S sid ="199" ssid = "18">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid ="205" ssid = "24">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al.  2009).</S><S sid ="182" ssid = "1">This section measures performance on shared tasks in order of increasing complexity: sparse lookups  evaluating perplexity of a large file  and translation with Moses.</S><S sid ="67" ssid = "45">While sorted arrays could be used to implement the same data structure as PROBING  effectively making m = 1  we abandoned this implementation because it is slower and larger than a trie implementation.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'", "'205'", "'182'", "'67'", "'45'"]
'199'
'205'
'182'
'67'
'45'
['199', '205', '182', '67', '45']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="149" ssid = "21">RandLM and SRILM also remove context that will not extend  but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.</S><S sid ="201" ssid = "20">Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'183'", "'69'", "'149'", "'201'"]
'0'
'183'
'69'
'149'
'201'
['0', '183', '69', '149', '201']
parsed_discourse_facet ['aim_citation', 'results_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="118" ssid = "22">The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'0'", "'69'", "'88'", "'118'"]
'183'
'0'
'69'
'88'
'118'
['183', '0', '69', '88', '118']
parsed_discourse_facet ['method_citation']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['Interpolation search has a more expensive pivot but performs less pivoting and reads  so it is slow on small data and faster on large data.', 'Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.', u'If the key distribution\u2019s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |\u2212 1] to estimate pivots; this optimization alone led to a 24% speed improvement.', 'Both implementations employ a state object  opaque to the application  that carries information from one query to the next; we discuss both further in Section 4.2.', u'The state function is integrated into the query process so that  in lieu of the query p(wnjwn\u22121 1 )  the application issues query p(wnjs(wn\u22121 1 )) which also returns s(wn1 ).']
['system', 'ROUGE-S*', 'Average_R:', '0.00051', '(95%-conf.int.', '0.00051', '-', '0.00051)']
['system', 'ROUGE-S*', 'Average_P:', '0.02564', '(95%-conf.int.', '0.02564', '-', '0.02564)']
['system', 'ROUGE-S*', 'Average_F:', '0.00100', '(95%-conf.int.', '0.00100', '-', '0.00100)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3916', 'P:78', 'F:2']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['KenLM: Faster and Smaller Language Model Queries', u'Hash tables are a common sparse mapping technique used by SRILM\u2019s default and BerkeleyLM\u2019s hashed variant.', 'Sorted arrays store key-value pairs in an array sorted by key  incurring no space overhead.', 'Each visited entry wni stores backoff b(wni ).', u'The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM\u2019s default.']
['system', 'ROUGE-S*', 'Average_R:', '0.00707', '(95%-conf.int.', '0.00707', '-', '0.00707)']
['system', 'ROUGE-S*', 'Average_P:', '0.08974', '(95%-conf.int.', '0.08974', '-', '0.08974)']
['system', 'ROUGE-S*', 'Average_F:', '0.01311', '(95%-conf.int.', '0.01311', '-', '0.01311)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:78', 'F:7']
['The cost of storing these averages, in bits, is Because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.']
['This section measures performance on shared tasks in order of increasing complexity: sparse lookups  evaluating perplexity of a large file  and translation with Moses.', 'We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al.  2009).', 'The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.', 'For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.', 'While sorted arrays could be used to implement the same data structure as PROBING  effectively making m = 1  we abandoned this implementation because it is slower and larger than a trie implementation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00028', '(95%-conf.int.', '0.00028', '-', '0.00028)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00055', '(95%-conf.int.', '0.00055', '-', '0.00055)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3570', 'P:91', 'F:1']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.', 'Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\xe2\u20ac\u2122s inverted variant  and BerkeleyLM except for the scrolling variant.', 'It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.', 'The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.']
['system', 'ROUGE-S*', 'Average_R:', '0.00016', '(95%-conf.int.', '0.00016', '-', '0.00016)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00031', '(95%-conf.int.', '0.00031', '-', '0.00031)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6328', 'P:55', 'F:1']
['We offer a state function s(wn1) = wn&#65533; where substring wn&#65533; is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.']
['Though we are not able to calculate their memory usage on our model  results reported in their paper suggest lower memory consumption than TRIE on large-scale models  at the expense of CPU time.', u'If the key distribution\u2019s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |\u2212 1] to estimate pivots; this optimization alone led to a 24% speed improvement.', u'Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn\u22121 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.', 'Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', 'Given counts cn1 where e.g. c1 is the vocabulary size  total memory consumption  in bits  is Our PROBING data structure places all n-grams of the same order into a single giant hash table.']
['system', 'ROUGE-S*', 'Average_R:', '0.00032', '(95%-conf.int.', '0.00032', '-', '0.00032)']
['system', 'ROUGE-S*', 'Average_P:', '0.01905', '(95%-conf.int.', '0.01905', '-', '0.01905)']
['system', 'ROUGE-S*', 'Average_F:', '0.00063', '(95%-conf.int.', '0.00063', '-', '0.00063)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6216', 'P:105', 'F:2']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['KenLM: Faster and Smaller Language Model Queries', u'Hash tables are a common sparse mapping technique used by SRILM\u2019s default and BerkeleyLM\u2019s hashed variant.', u'The state function is integrated into the query process so that  in lieu of the query p(wnjwn\u22121 1 )  the application issues query p(wnjs(wn\u22121 1 )) which also returns s(wn1 ).', 'Each visited entry wni stores backoff b(wni ).', 'Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.']
['system', 'ROUGE-S*', 'Average_R:', '0.00298', '(95%-conf.int.', '0.00298', '-', '0.00298)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.00573', '(95%-conf.int.', '0.00573', '-', '0.00573)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:78', 'F:6']
['We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.']
['The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\u2019s inverted variant  and BerkeleyLM except for the scrolling variant.', 'Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.', u'The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI\u2019s speed reported in Guthrie and Hepple (2010) under different conditions.', 'Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.']
['system', 'ROUGE-S*', 'Average_R:', '0.00027', '(95%-conf.int.', '0.00027', '-', '0.00027)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00054', '(95%-conf.int.', '0.00054', '-', '0.00054)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3655', 'P:45', 'F:1']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\xe2\u20ac\u2122s inverted variant  and BerkeleyLM except for the scrolling variant.', 'The structure uses linear probing hash tables and is designed for speed.', 'Nodes in the trie are based on arrays sorted by vocabulary identifier.', 'In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.']
['system', 'ROUGE-S*', 'Average_R:', '0.00419', '(95%-conf.int.', '0.00419', '-', '0.00419)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.00795', '(95%-conf.int.', '0.00795', '-', '0.00795)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:78', 'F:6']
['The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.']
['KenLM: Faster and Smaller Language Model Queries', 'RandLM and SRILM also remove context that will not extend  but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\xe2\u20ac\u2122s inverted variant  and BerkeleyLM except for the scrolling variant.', 'It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.', 'Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.']
['system', 'ROUGE-S*', 'Average_R:', '0.00032', '(95%-conf.int.', '0.00032', '-', '0.00032)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00062', '(95%-conf.int.', '0.00062', '-', '0.00062)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:55', 'F:1']
['Then we ran binary search to determine the least amount of memory with which it would run.']
['The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie)  allocating memory all at once (eliminating the need for full pointers)  and being easy to compile.', 'Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', u'Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn\u22121 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.', u'If the key distribution\u2019s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |\u2212 1] to estimate pivots; this optimization alone led to a 24% speed improvement.', 'We do not experiment with models larger than physical memory in this paper because TPT is unreleased  factors such as disk speed are hard to replicate  and in such situations we recommend switching to a more compact representation  such as RandLM.']
['system', 'ROUGE-S*', 'Average_R:', '0.00016', '(95%-conf.int.', '0.00016', '-', '0.00016)']
['system', 'ROUGE-S*', 'Average_P:', '0.04762', '(95%-conf.int.', '0.04762', '-', '0.04762)']
['system', 'ROUGE-S*', 'Average_F:', '0.00032', '(95%-conf.int.', '0.00032', '-', '0.00032)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6216', 'P:21', 'F:1']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['KenLM: Faster and Smaller Language Model Queries', u'Hash tables are a common sparse mapping technique used by SRILM\u2019s default and BerkeleyLM\u2019s hashed variant.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\u2019s inverted variant  and BerkeleyLM except for the scrolling variant.', u'The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI\u2019s speed reported in Guthrie and Hepple (2010) under different conditions.', 'Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.']
['system', 'ROUGE-S*', 'Average_R:', '0.00076', '(95%-conf.int.', '0.00076', '-', '0.00076)']
['system', 'ROUGE-S*', 'Average_P:', '0.03636', '(95%-conf.int.', '0.03636', '-', '0.03636)']
['system', 'ROUGE-S*', 'Average_F:', '0.00149', '(95%-conf.int.', '0.00149', '-', '0.00149)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:55', 'F:2']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
[u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\u2019s inverted variant  and BerkeleyLM except for the scrolling variant.', u'By contrast  BerkeleyLM\u2019s hash and compressed variants will return incorrect results based on an n \u22121-gram.', 'MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.', 'Nodes in the trie are based on arrays sorted by vocabulary identifier.', 'KenLM: Faster and Smaller Language Model Queries']
['system', 'ROUGE-S*', 'Average_R:', '0.00653', '(95%-conf.int.', '0.00653', '-', '0.00653)']
['system', 'ROUGE-S*', 'Average_P:', '0.11538', '(95%-conf.int.', '0.11538', '-', '0.11538)']
['system', 'ROUGE-S*', 'Average_F:', '0.01236', '(95%-conf.int.', '0.01236', '-', '0.01236)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:78', 'F:9']
['Queries take the form p(wn|wn&#8722;1 1 ) where wn1 is an n-gram.']
['Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.', u'By contrast  BerkeleyLM\u2019s hash and compressed variants will return incorrect results based on an n \u22121-gram.', 'The ratio of buckets to entries is controlled by space multiplier m > 1.', 'Nodes in the trie are based on arrays sorted by vocabulary identifier.', 'KenLM: Faster and Smaller Language Model Queries']
['system', 'ROUGE-S*', 'Average_R:', '0.00145', '(95%-conf.int.', '0.00145', '-', '0.00145)']
['system', 'ROUGE-S*', 'Average_P:', '0.05556', '(95%-conf.int.', '0.05556', '-', '0.05556)']
['system', 'ROUGE-S*', 'Average_F:', '0.00283', '(95%-conf.int.', '0.00283', '-', '0.00283)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:36', 'F:2']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.', 'KenLM: Faster and Smaller Language Model Queries', u'By contrast  BerkeleyLM\u2019s hash and compressed variants will return incorrect results based on an n \u22121-gram.', 'Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\u2019s inverted variant  and BerkeleyLM except for the scrolling variant.']
['system', 'ROUGE-S*', 'Average_R:', '0.00307', '(95%-conf.int.', '0.00307', '-', '0.00307)']
['system', 'ROUGE-S*', 'Average_P:', '0.08974', '(95%-conf.int.', '0.08974', '-', '0.08974)']
['system', 'ROUGE-S*', 'Average_F:', '0.00594', '(95%-conf.int.', '0.00594', '-', '0.00594)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:78', 'F:7']
['For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.']
['Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.', u'By contrast  BerkeleyLM\u2019s hash and compressed variants will return incorrect results based on an n \u22121-gram.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\u2019s inverted variant  and BerkeleyLM except for the scrolling variant.', u'The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM\u2019s default.', 'KenLM: Faster and Smaller Language Model Queries']
['system', 'ROUGE-S*', 'Average_R:', '0.00226', '(95%-conf.int.', '0.00226', '-', '0.00226)']
['system', 'ROUGE-S*', 'Average_P:', '0.01538', '(95%-conf.int.', '0.01538', '-', '0.01538)']
['system', 'ROUGE-S*', 'Average_F:', '0.00394', '(95%-conf.int.', '0.00394', '-', '0.00394)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:325', 'F:5']
['We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al., 2009).']
['The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\xe2\u20ac\u2122s inverted variant  and BerkeleyLM except for the scrolling variant.', 'IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.', 'Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.', 'Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.']
['system', 'ROUGE-S*', 'Average_R:', '0.00190', '(95%-conf.int.', '0.00190', '-', '0.00190)']
['system', 'ROUGE-S*', 'Average_P:', '0.04412', '(95%-conf.int.', '0.04412', '-', '0.04412)']
['system', 'ROUGE-S*', 'Average_F:', '0.00364', '(95%-conf.int.', '0.00364', '-', '0.00364)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:136', 'F:6']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.', 'The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.', u'The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI\u2019s speed reported in Guthrie and Hepple (2010) under different conditions.', 'It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\u2019s inverted variant  and BerkeleyLM except for the scrolling variant.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4851', 'P:78', 'F:0']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.', 'KenLM: Faster and Smaller Language Model Queries', 'RandLM and SRILM also remove context that will not extend  but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\xe2\u20ac\u2122s inverted variant  and BerkeleyLM except for the scrolling variant.', 'Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.']
['system', 'ROUGE-S*', 'Average_R:', '0.00455', '(95%-conf.int.', '0.00455', '-', '0.00455)']
['system', 'ROUGE-S*', 'Average_P:', '0.14103', '(95%-conf.int.', '0.14103', '-', '0.14103)']
['system', 'ROUGE-S*', 'Average_F:', '0.00882', '(95%-conf.int.', '0.00882', '-', '0.00882)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:78', 'F:11']
0.0501683330546 0.00204333332198 0.00387666664513





input/ref/Task1/P87-1015_vardha.csv
input/res/Task1/P87-1015.csv
parsing: input/ref/Task1/P87-1015_vardha.csv
<S sid="165" ssid="50">This class of formalisms have the properties that their derivation trees are local sets, and manipulate objects, using a finite number of composition operations that use a finite number of symbols.</S>
original cit marker offset is 0
new cit marker offset is 0



["'165'"]
'165'
['165']
parsed_discourse_facet ['method_citation']
 <S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'"]
'119'
['119']
parsed_discourse_facet ['method_citation']
    <S sid="3" ssid="1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'"]
'3'
['3']
parsed_discourse_facet ['method_citation']
    <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
    <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
    <S sid="133" ssid="18">To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'"]
'133'
['133']
parsed_discourse_facet ['method_citation']
 <S sid="138" ssid="23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
    <S sid="156" ssid="41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'"]
'156'
['156']
parsed_discourse_facet ['method_citation']
 <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
 <S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'"]
'119'
['119']
parsed_discourse_facet ['method_citation']
  <S sid="133" ssid="18">To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'"]
'133'
['133']
parsed_discourse_facet ['method_citation']
 <S sid="164" ssid="49">Although embedding this version of LCFRS's in the framework of ILFP developed by Rounds (1985) is straightforward, our motivation was to capture properties shared by a family of grammatical systems and generalize them defining a class of related formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'164'"]
'164'
['164']
parsed_discourse_facet ['method_citation']
 <S sid="204" ssid="10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's, HG's, TAG's, and MCTAG's are all local sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'"]
'204'
['204']
parsed_discourse_facet ['method_citation']
    <S sid="9" ssid="7">We examine both the complexity of the paths of trees in the tree sets, and the kinds of dependencies that the formalisms can impose between paths.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="28" ssid="13">A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
 <S sid="138" ssid="23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P87-1015.csv
<S sid ="115" ssid = "21">From the point of view of recognition  independent paths in the derivation structures suggests that a top-down parser (for example) can work on each branch independently  which may lead to efficient parsing using an algorithm based on the Divide and Conquer technique.</S><S sid ="222" ssid = "28">However  in order to capture the properties of various grammatical systems under consideration  our notation is more restrictive that ILFP  which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="67" ssid = "52">On the one hand  the definition of composition in Steedman (1985)  which technically permits composition of functions with unbounded number of arguments  generates tree sets with dependent paths such as those shown in Figure 6.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'222'", "'195'", "'67'", "'153'"]
'115'
'222'
'195'
'67'
'153'
['115', '222', '195', '67', '153']
parsed_discourse_facet ['results_citation']
<S sid ="37" ssid = "22">Like CFG's  the choice is predetermined by a finite number of rules encapsulated in the grammar.</S><S sid ="231" ssid = "37">In this paper  our goal has been to use the notion of LCFRS's to classify grammatical systems on the basis of their strong generative capacity.</S><S sid ="197" ssid = "3">We address the question of whether or not a formalism can generate only structural descriptions with independent paths.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S><S sid ="92" ssid = "77">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'231'", "'197'", "'116'", "'92'"]
'37'
'231'
'197'
'116'
'92'
['37', '231', '197', '116', '92']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.</S><S sid ="131" ssid = "16">The composition operations in the case of CFG's are parameterized by the productions.</S><S sid ="65" ssid = "50">While the generative power of CG's is greater that of CFG's  it appears to be highly constrained.</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="179" ssid = "64">It can be seen that M performs a top-down recognition of the input al ... nin logspace.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'131'", "'65'", "'227'", "'179'"]
'204'
'131'
'65'
'227'
'179'
['204', '131', '65', '227', '179']
parsed_discourse_facet ['method_citation']
<S sid ="194" ssid = "79">Thus  M works in logspace and recognition can be done on a deterministic TM in polynomial tape.</S><S sid ="92" ssid = "77">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S><S sid ="65" ssid = "50">While the generative power of CG's is greater that of CFG's  it appears to be highly constrained.</S><S sid ="20" ssid = "5">As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.</S><S sid ="131" ssid = "16">The composition operations in the case of CFG's are parameterized by the productions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'92'", "'65'", "'20'", "'131'"]
'194'
'92'
'65'
'20'
'131'
['194', '92', '65', '20', '131']
parsed_discourse_facet ['method_citation']
<S sid ="155" ssid = "40">CFG's  TAG's  MCTAG's and HG's are all members of this class since they satisfy these restrictions.</S><S sid ="37" ssid = "22">Like CFG's  the choice is predetermined by a finite number of rules encapsulated in the grammar.</S><S sid ="117" ssid = "2">Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.</S><S sid ="92" ssid = "77">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S><S sid ="48" ssid = "33">There has been recent interest in the application of Indexed Grammars (IG's) to natural languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'", "'37'", "'117'", "'92'", "'48'"]
'155'
'37'
'117'
'92'
'48'
['155', '37', '117', '92', '48']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "5">As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.</S><S sid ="216" ssid = "22">Having defined LCFRS's  in Section 4.2 we established the semilinearity (and hence constant growth property) of the languages generated.</S><S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="179" ssid = "64">It can be seen that M performs a top-down recognition of the input al ... nin logspace.</S><S sid ="131" ssid = "16">The composition operations in the case of CFG's are parameterized by the productions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'216'", "'59'", "'179'", "'131'"]
'20'
'216'
'59'
'179'
'131'
['20', '216', '59', '179', '131']
parsed_discourse_facet ['method_citation']
<S sid ="162" ssid = "47">Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="219" ssid = "25">The composition operations are mapped onto operations that use concatenation to define the substrings spanned by the resulting structures.</S><S sid ="0" ssid = "0">CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*</S><S sid ="193" ssid = "78">Since the work tapes store integers (which can be written in binary) that never exceed the size of the input  no configuration has space exceeding 0(log n).</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'", "'195'", "'219'", "'0'", "'193'"]
'162'
'195'
'219'
'0'
'193'
['162', '195', '219', '0', '193']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "9">By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.</S><S sid ="115" ssid = "21">From the point of view of recognition  independent paths in the derivation structures suggests that a top-down parser (for example) can work on each branch independently  which may lead to efficient parsing using an algorithm based on the Divide and Conquer technique.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="1" ssid = "1">We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'115'", "'153'", "'1'", "'195'"]
'11'
'115'
'153'
'1'
'195'
['11', '115', '153', '1', '195']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*</S><S sid ="219" ssid = "25">The composition operations are mapped onto operations that use concatenation to define the substrings spanned by the resulting structures.</S><S sid ="125" ssid = "10">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S sid ="58" ssid = "43">Unification is used in LFG's to produce structures having two dependent spines of unbounded length as in Figure 5.</S><S sid ="192" ssid = "77">Thus  the ATM has no more than 6km&quot; + 1 work tapes  where km&quot; is the maximum number of substrings spanned by a derived structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'219'", "'125'", "'58'", "'192'"]
'0'
'219'
'125'
'58'
'192'
['0', '219', '125', '58', '192']
parsed_discourse_facet ['method_citation']
<S sid ="193" ssid = "78">Since the work tapes store integers (which can be written in binary) that never exceed the size of the input  no configuration has space exceeding 0(log n).</S><S sid ="129" ssid = "14">Frontier nodes are annotated by zero arty functions corresponding to elementary structures.</S><S sid ="127" ssid = "12">Nodes are annotated by the name of the composition operation used at that step in the derivation.</S><S sid ="172" ssid = "57">A step of an ATM consists of reading a symbol from each tape and optionally moving each head to the left or right one tape cell.</S><S sid ="173" ssid = "58">A configuration of M consists of a state of the finite control  the nonblank contents of the input tape and k work tapes  and the position of each head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'193'", "'129'", "'127'", "'172'", "'173'"]
'193'
'129'
'127'
'172'
'173'
['193', '129', '127', '172', '173']
parsed_discourse_facet ['method_citation']
<S sid ="189" ssid = "74">For rules p : A fpo such that fp is constant function  giving an elementary structure  fp is defined such that fp() = (Si ... xi() where each z is a constant string.</S><S sid ="183" ssid = "68">We assume that M is in an existential state qA  with integers i1 and i2 representing zi in the (2i — 1)th and 22th work tape  for 1 < i < k. For each rule p : A fp(B  C) such that fp is mapped onto the function fp defined by the following rule. jp((xi .. •  rnt)  (1ii  • • • • Yn3))= (Zi   • • •   Zk) M breaks xi   zk into substrings xi    xn  and yi ... y&quot; conforming to the definition of fp.</S><S sid ="160" ssid = "45">A derived structure will be mapped onto a sequence zi of substrings (not necessarily contiguous in the input)  and the composition operations will be mapped onto functions that can defined as follows3. f((zi • • •   zni)  (m. • • •  Yn3)) = (Z1  • • •   Zn3) where each z  is the concatenation of strings from z 's and yk's.</S><S sid ="148" ssid = "33">If 0(A) gives the number of occurrences of each terminal in the structure named by A  then  given the constraints imposed on the formalism  for each rule A --. fp(Ai    An) we have the equality where c„ is some constant.</S><S sid ="201" ssid = "7">It is interesting to note  however  that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG  FUG and IG's.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'183'", "'160'", "'148'", "'201'"]
'189'
'183'
'160'
'148'
'201'
['189', '183', '160', '148', '201']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*</S><S sid ="162" ssid = "47">Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="196" ssid = "2">We contrasted formalisms such as CFG's  HG's  TAG's and MCTAG's  with formalisms such as IG's and unificational systems such as LFG's and FUG's.</S><S sid ="204" ssid = "10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'162'", "'227'", "'196'", "'204'"]
'0'
'162'
'227'
'196'
'204'
['0', '162', '227', '196', '204']
parsed_discourse_facet ['method_citation']
<S sid ="58" ssid = "43">Unification is used in LFG's to produce structures having two dependent spines of unbounded length as in Figure 5.</S><S sid ="214" ssid = "20">LCFRS's share several properties possessed by the class of mildly context-sensitive formalisms discussed by Joshi (1983/85).</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="196" ssid = "2">We contrasted formalisms such as CFG's  HG's  TAG's and MCTAG's  with formalisms such as IG's and unificational systems such as LFG's and FUG's.</S><S sid ="163" ssid = "48">This representation of structures by substrings and the composition operation by its effect on substrings is related to the work of Rounds (1985).</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'214'", "'227'", "'196'", "'163'"]
'58'
'214'
'227'
'196'
'163'
['58', '214', '227', '196', '163']
parsed_discourse_facet ['results_citation']
<S sid ="50" ssid = "35">The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.</S><S sid ="202" ssid = "8">As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.</S><S sid ="7" ssid = "5">The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG's and IG's.</S><S sid ="230" ssid = "36">LCFRS's have only been loosely defined in this paper; we have yet to provide a complete set of formal properties associated with members of this class.</S><S sid ="110" ssid = "16">The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'202'", "'7'", "'230'", "'110'"]
'50'
'202'
'7'
'230'
'110'
['50', '202', '7', '230', '110']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "42">For example  in TAG's a derived auxiliary tree spans two substrings (to the left and right of the foot node)  and the adjunction operation inserts another substring (spanned by the subtree under the node where adjunction takes place) between them (see Figure 3).</S><S sid ="162" ssid = "47">Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .</S><S sid ="83" ssid = "68">The following context-free production captures the derivation step of the grammar shown in Figure 7  in which the trees in the auxiliary tree set are adjoined into themselves at the root node (address c).</S><S sid ="67" ssid = "52">On the one hand  the definition of composition in Steedman (1985)  which technically permits composition of functions with unbounded number of arguments  generates tree sets with dependent paths such as those shown in Figure 6.</S><S sid ="154" ssid = "39">Since each composition operation is linear and nonerasing  a bounded sequences of substrings associated with the resulting structure is obtained by combining the substrings in each of its arguments using only the concatenation operation  including each substring exactly once.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'162'", "'83'", "'67'", "'154'"]
'157'
'162'
'83'
'67'
'154'
['157', '162', '83', '67', '154']
parsed_discourse_facet ['method_citation']
<S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="156" ssid = "41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S><S sid ="204" ssid = "10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.</S><S sid ="213" ssid = "19">By sharing stacks (in IG's) or by using nonlinear equations over f-structures (in FUG's and LFG's)  structures with unbounded dependencies between paths can be generated.</S><S sid ="125" ssid = "10">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'227'", "'156'", "'204'", "'213'", "'125'"]
'227'
'156'
'204'
'213'
'125'
['227', '156', '204', '213', '125']
parsed_discourse_facet ['method_citation']
["We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments."]
['We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.', 'CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*', 'Since the work tapes store integers (which can be written in binary) that never exceed the size of the input  no configuration has space exceeding 0(log n).', 'Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .', 'The composition operations are mapped onto operations that use concatenation to define the substrings spanned by the resulting structures.']
['system', 'ROUGE-S*', 'Average_R:', '0.00242', '(95%-conf.int.', '0.00242', '-', '0.00242)']
['system', 'ROUGE-S*', 'Average_P:', '0.06061', '(95%-conf.int.', '0.06061', '-', '0.06061)']
['system', 'ROUGE-S*', 'Average_F:', '0.00465', '(95%-conf.int.', '0.00465', '-', '0.00465)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:66', 'F:4']
['This class of formalisms have the properties that their derivation trees are local sets, and manipulate objects, using a finite number of composition operations that use a finite number of symbols.']
['From the point of view of recognition  independent paths in the derivation structures suggests that a top-down parser (for example) can work on each branch independently  which may lead to efficient parsing using an algorithm based on the Divide and Conquer technique.', 'We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.', 'In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.', 'However  in order to capture the properties of various grammatical systems under consideration  our notation is more restrictive that ILFP  which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.', 'On the one hand  the definition of composition in Steedman (1985)  which technically permits composition of functions with unbounded number of arguments  generates tree sets with dependent paths such as those shown in Figure 6.']
['system', 'ROUGE-S*', 'Average_R:', '0.00560', '(95%-conf.int.', '0.00560', '-', '0.00560)']
['system', 'ROUGE-S*', 'Average_P:', '0.20833', '(95%-conf.int.', '0.20833', '-', '0.20833)']
['system', 'ROUGE-S*', 'Average_F:', '0.01091', '(95%-conf.int.', '0.01091', '-', '0.01091)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4465', 'P:120', 'F:25']
["In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs."]
['We address the question of whether or not a formalism can generate only structural descriptions with independent paths.', 'From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).', "Like CFG's  the choice is predetermined by a finite number of rules encapsulated in the grammar.", "In this paper  our goal has been to use the notion of LCFRS's to classify grammatical systems on the basis of their strong generative capacity.", 'We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.']
['system', 'ROUGE-S*', 'Average_R:', '0.00850', '(95%-conf.int.', '0.00850', '-', '0.00850)']
['system', 'ROUGE-S*', 'Average_P:', '0.15152', '(95%-conf.int.', '0.15152', '-', '0.15152)']
['system', 'ROUGE-S*', 'Average_F:', '0.01610', '(95%-conf.int.', '0.01610', '-', '0.01610)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:66', 'F:10']
["We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments."]
['Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.', "By sharing stacks (in IG's) or by using nonlinear equations over f-structures (in FUG's and LFG's)  structures with unbounded dependencies between paths can be generated.", "The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.", 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', "Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings."]
['system', 'ROUGE-S*', 'Average_R:', '0.00256', '(95%-conf.int.', '0.00256', '-', '0.00256)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.00498', '(95%-conf.int.', '0.00498', '-', '0.00498)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:66', 'F:6']
['A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.']
['Since each composition operation is linear and nonerasing  a bounded sequences of substrings associated with the resulting structure is obtained by combining the substrings in each of its arguments using only the concatenation operation  including each substring exactly once.', 'The following context-free production captures the derivation step of the grammar shown in Figure 7  in which the trees in the auxiliary tree set are adjoined into themselves at the root node (address c).', 'Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .', "For example  in TAG's a derived auxiliary tree spans two substrings (to the left and right of the foot node)  and the adjunction operation inserts another substring (spanned by the subtree under the node where adjunction takes place) between them (see Figure 3).", 'On the one hand  the definition of composition in Steedman (1985)  which technically permits composition of functions with unbounded number of arguments  generates tree sets with dependent paths such as those shown in Figure 6.']
['system', 'ROUGE-S*', 'Average_R:', '0.00444', '(95%-conf.int.', '0.00444', '-', '0.00444)']
['system', 'ROUGE-S*', 'Average_P:', '0.37778', '(95%-conf.int.', '0.37778', '-', '0.37778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00878', '(95%-conf.int.', '0.00878', '-', '0.00878)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3828', 'P:45', 'F:17']
['We examine both the complexity of the paths of trees in the tree sets, and the kinds of dependencies that the formalisms can impose between paths.']
["As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.", "LCFRS's have only been loosely defined in this paper; we have yet to provide a complete set of formal properties associated with members of this class.", "The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.", "The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG's and IG's.", 'The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .']
['system', 'ROUGE-S*', 'Average_R:', '0.00810', '(95%-conf.int.', '0.00810', '-', '0.00810)']
['system', 'ROUGE-S*', 'Average_P:', '0.34545', '(95%-conf.int.', '0.34545', '-', '0.34545)']
['system', 'ROUGE-S*', 'Average_F:', '0.01583', '(95%-conf.int.', '0.01583', '-', '0.01583)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:55', 'F:19']
['To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.']
["Having defined LCFRS's  in Section 4.2 we established the semilinearity (and hence constant growth property) of the languages generated.", 'Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.', "The composition operations in the case of CFG's are parameterized by the productions.", "As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.", 'It can be seen that M performs a top-down recognition of the input al ... nin logspace.']
['system', 'ROUGE-S*', 'Average_R:', '0.00327', '(95%-conf.int.', '0.00327', '-', '0.00327)']
['system', 'ROUGE-S*', 'Average_P:', '0.02941', '(95%-conf.int.', '0.02941', '-', '0.02941)']
['system', 'ROUGE-S*', 'Average_F:', '0.00588', '(95%-conf.int.', '0.00588', '-', '0.00588)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:136', 'F:4']
['To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.']
[u'We assume that M is in an existential state qA  with integers i1 and i2 representing zi in the (2i \u2014 1)th and 22th work tape  for 1 ', 'For rules p : A fpo such that fp is constant function  giving an elementary structure  fp is defined such that fp() = (Si ... xi() where each z is a constant string.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:136', 'F:0']
['Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.']
['Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.', "The composition operations in the case of CFG's are parameterized by the productions.", "The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.", 'It can be seen that M performs a top-down recognition of the input al ... nin logspace.', "While the generative power of CG's is greater that of CFG's  it appears to be highly constrained."]
['system', 'ROUGE-S*', 'Average_R:', '0.00290', '(95%-conf.int.', '0.00290', '-', '0.00290)']
['system', 'ROUGE-S*', 'Average_P:', '0.07273', '(95%-conf.int.', '0.07273', '-', '0.07273)']
['system', 'ROUGE-S*', 'Average_F:', '0.00558', '(95%-conf.int.', '0.00558', '-', '0.00558)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:55', 'F:4']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
['Thus  M works in logspace and recognition can be done on a deterministic TM in polynomial tape.', "The composition operations in the case of CFG's are parameterized by the productions.", 'We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.', "As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.", "While the generative power of CG's is greater that of CFG's  it appears to be highly constrained."]
['system', 'ROUGE-S*', 'Average_R:', '0.01667', '(95%-conf.int.', '0.01667', '-', '0.01667)']
['system', 'ROUGE-S*', 'Average_P:', '0.10833', '(95%-conf.int.', '0.10833', '-', '0.10833)']
['system', 'ROUGE-S*', 'Average_F:', '0.02889', '(95%-conf.int.', '0.02889', '-', '0.02889)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:120', 'F:13']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
["Unification is used in LFG's to produce structures having two dependent spines of unbounded length as in Figure 5.", 'CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*', 'Thus  the ATM has no more than 6km&quot; + 1 work tapes  where km&quot; is the maximum number of substrings spanned by a derived structure.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'The composition operations are mapped onto operations that use concatenation to define the substrings spanned by the resulting structures.']
['system', 'ROUGE-S*', 'Average_R:', '0.00266', '(95%-conf.int.', '0.00266', '-', '0.00266)']
['system', 'ROUGE-S*', 'Average_P:', '0.02500', '(95%-conf.int.', '0.02500', '-', '0.02500)']
['system', 'ROUGE-S*', 'Average_F:', '0.00481', '(95%-conf.int.', '0.00481', '-', '0.00481)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:120', 'F:3']
0.133642726058 0.00519272722552 0.00967363627569





input/ref/Task1/A00-2018_akanksha.csv
input/res/Task1/A00-2018.csv
parsing: input/ref/Task1/A00-2018_akanksha.csv
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'"]
'90'
'91'
['90', '91']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="17">Maximum-entropy models have two benefits for a parser builder.</S>
    <S sid="49" ssid="18">First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &amp;quot;features&amp;quot; suggests that the probability model should be easily changeable &#8212; just change the set of features used.</S>
    <S sid="51" ssid="20">Second, and this is a point we have not yet mentioned, the features used in these models need have no particular independence of one another.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'49'", "'51'"]
'48'
'49'
'51'
['48', '49', '51']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
    <S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
    <S sid="93" ssid="4">This allows the second pass to see expansions not present in the training corpus.</S>
    <S sid="94" ssid="5">We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'", "'92'", "'93'", "'94'"]
'90'
'91'
'92'
'93'
'94'
['90', '91', '92', '93', '94']
parsed_discourse_facet ['method_citation']
NA
original cit marker offset is 0
new cit marker offset is 0



['0']
0
['0']
Error in Reference Offset
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
    <S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
    <S sid="93" ssid="4">This allows the second pass to see expansions not present in the training corpus.</S>
    <S sid="94" ssid="5">We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'", "'92'", "'93'", "'94'"]
'90'
'91'
'92'
'93'
'94'
['90', '91', '92', '93', '94']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="7">To compute a probability in a log-linear model one first defines a set of &amp;quot;features&amp;quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.</S>
    <S sid="39" ssid="8">In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.</S>
    <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'39'", "'40'"]
'38'
'39'
'40'
['38', '39', '40']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['result_citation']
<S sid="85" ssid="54">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models, this simplifies the model significantly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="32">As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &amp;quot;before&amp;quot;), and the label of the grandparent of c (la).</S><S sid="143" ssid="34">The first is simply that if we first guess the pre-terminal, when we go to guess the head the first thing we can condition upon is the pre-terminal, i.e., we compute p(h I t).</S><S sid="146" ssid="37">The second major reason why first guessing the pre-terminal makes so much difference is that it can be used when backing off the lexical head in computing the probability of the rule expansion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'143'", "'146'"]
'63'
'143'
'146'
['63', '143', '146']
parsed_discourse_facet ['method_citation']
???<S sid="78" ssid="47">With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.</S>                                        <S sid="79" ssid="48">We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.</S>
original cit marker offset is 0
new cit marker offset is 0



["'78'", "'79'"]
'78'
'79'
['78', '79']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/A00-2018.csv
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'23'", "'62'", "'33'", "'27'"]
'30'
'23'
'62'
'33'
'27'
['30', '23', '62', '33', '27']
parsed_discourse_facet ['results_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'27'", "'62'", "'33'", "'23'"]
'30'
'27'
'62'
'33'
'23'
['30', '27', '62', '33', '23']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'62'", "'33'", "'89'", "'23'"]
'30'
'62'
'33'
'89'
'23'
['30', '62', '33', '89', '23']
parsed_discourse_facet ['method_citation']
<S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="20" ssid = "9">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="29" ssid = "18">So  for example  in a second-order Markov PCFG  L2 would be conditioned on L1 and M. In our complete model  of course  the probability of each label in the expansions is also conditioned on other material as specified in Equation 1  e.g.  p(e t  h  H).</S><S sid ="91" ssid = "2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'", "'20'", "'174'", "'29'", "'91'"]
'27'
'20'
'174'
'29'
'91'
['27', '20', '174', '29', '91']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'23'", "'62'", "'27'", "'33'"]
'30'
'23'
'62'
'27'
'33'
['30', '23', '62', '27', '33']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="63" ssid = "32">As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'1'", "'5'", "'27'", "'63'"]
'101'
'1'
'5'
'27'
'63'
['101', '1', '5', '27', '63']
parsed_discourse_facet ['method_citation']
<S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'", "'30'", "'62'", "'89'", "'23'"]
'27'
'30'
'62'
'89'
'23'
['27', '30', '62', '89', '23']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'62'", "'23'", "'89'", "'33'"]
'30'
'62'
'23'
'89'
'33'
['30', '62', '23', '89', '33']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "7">To compute a probability in a log-linear model one first defines a set of &quot;features&quot;  functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.</S><S sid ="91" ssid = "2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="20" ssid = "9">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid ="26" ssid = "15">Thus an expansion e(c) looks like: The expansion is generated by guessing first M  then in order L1 through L „.+1 (= A)  and similarly for RI through In a pure Markov PCFG we are given the left-hand side label 1 and then probabilistically generate the right-hand side conditioning on no information other than 1 and (possibly) previously generated pieces of the right-hand side itself.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'91'", "'174'", "'20'", "'26'"]
'38'
'91'
'174'
'20'
'26'
['38', '91', '174', '20', '26']
parsed_discourse_facet ['method_citation']
<S sid ="29" ssid = "18">So  for example  in a second-order Markov PCFG  L2 would be conditioned on L1 and M. In our complete model  of course  the probability of each label in the expansions is also conditioned on other material as specified in Equation 1  e.g.  p(e t  h  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="91" ssid = "2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'", "'27'", "'174'", "'91'", "'33'"]
'29'
'27'
'174'
'91'
'33'
['29', '27', '174', '91', '33']
parsed_discourse_facet ['method_citation']
<S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="101" ssid = "12">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S><S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'", "'27'", "'89'", "'101'", "'30'"]
'62'
'27'
'89'
'101'
'30'
['62', '27', '89', '101', '30']
parsed_discourse_facet ['method_citation']
<S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="115" ssid = "6">As noted in [5]  that system is based upon a &quot;tree-bank grammar&quot; - a grammar read directly off the training corpus.</S><S sid ="141" ssid = "32">(For example  part-ofspeech tagging using the most probable preterminal for each word is 90% accurate [8].)</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="184" ssid = "11">The first is the slight  but important  improvement achieved by using this model over conventional deleted interpolation  as indicated in Figure 2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'115'", "'141'", "'62'", "'184'"]
'23'
'115'
'141'
'62'
'184'
['23', '115', '141', '62', '184']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "9">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid ="76" ssid = "45">This requires finding the appropriate Ais for Equation 3  which is accomplished using an algorithm such as iterative scaling [II] in which values for the Ai are initially &quot;guessed&quot; and then modified until they converge on stable values.</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="110" ssid = "1">In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach  the aspect of the parser that is most novel.</S><S sid ="101" ssid = "12">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'76'", "'27'", "'110'", "'101'"]
'20'
'76'
'27'
'110'
'101'
['20', '76', '27', '110', '101']
parsed_discourse_facet ['results_citation']
<S sid ="74" ssid = "43">Suppose we were  in fact  going to compute a true maximum entropy model based upon the features used in Equation 7  Ii (t 1)  f2(t 1 1p)  f3(t 1 lp) .</S><S sid ="126" ssid = "17">This is indicated in Figure 2  where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%  0.4% lower than the results on the official test corpus.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'126'", "'125'", "'174'", "'27'"]
'74'
'126'
'125'
'174'
'27'
['74', '126', '125', '174', '27']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="42" ssid = "11">This feature is obviously composed of two sub-features  one recognizing t  the other 1.</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'89'", "'23'", "'42'", "'62'"]
'30'
'89'
'23'
'42'
'62'
['30', '89', '23', '42', '62']
parsed_discourse_facet ['method_citation']
<S sid ="176" ssid = "3">That the previous three best parsers on this test [5 9 17] all perform within a percentage point of each other  despite quite different basic mechanisms  led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training  and to conjecture that perhaps we were at it.</S><S sid ="58" ssid = "27">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid ="180" ssid = "7">From our perspective  perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.</S><S sid ="44" ssid = "13">Now consider computing a conditional probability p(a H) with a set of features h that connect a to the history H. In a log-linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability  the higher the absolute value of the associated A.</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'", "'58'", "'180'", "'44'", "'13'"]
'176'
'58'
'180'
'44'
'13'
['176', '58', '180', '44', '13']
parsed_discourse_facet ['method_citation']
['As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.', 'We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.']
['For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).', u'In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / \xe2\u20ac\u201d that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).', 'In Equation 1 we wrote this as p(t I 1  H).', 'Thus we would use p(L2 I L1  M  1  t  h  H).', 'For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).']
['system', 'ROUGE-S*', 'Average_R:', '0.00101', '(95%-conf.int.', '0.00101', '-', '0.00101)']
['system', 'ROUGE-S*', 'Average_P:', '0.00178', '(95%-conf.int.', '0.00178', '-', '0.00178)']
['system', 'ROUGE-S*', 'Average_F:', '0.00129', '(95%-conf.int.', '0.00129', '-', '0.00129)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:561', 'F:1']
['In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.', 'To compute a probability in a log-linear model one first defines a set of &quot;features&quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.', 'In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.']
['As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.', 'We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ', 'To compute a probability in a log-linear model one first defines a set of &quot;features&quot;  functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.']
['system', 'ROUGE-S*', 'Average_R:', '0.13287', '(95%-conf.int.', '0.13287', '-', '0.13287)']
['system', 'ROUGE-S*', 'Average_P:', '0.30127', '(95%-conf.int.', '0.30127', '-', '0.30127)']
['system', 'ROUGE-S*', 'Average_F:', '0.18441', '(95%-conf.int.', '0.18441', '-', '0.18441)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:946', 'F:285']
['Second, and this is a point we have not yet mentioned, the features used in these models need have no particular independence of one another.', 'Maximum-entropy models have two benefits for a parser builder.', 'First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &quot;features&quot; suggests that the probability model should be easily changeable &#8212; just change the set of features used.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ', u'In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / \xe2\u20ac\u201d that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).', 'In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.']
['system', 'ROUGE-S*', 'Average_R:', '0.00169', '(95%-conf.int.', '0.00169', '-', '0.00169)']
['system', 'ROUGE-S*', 'Average_P:', '0.00645', '(95%-conf.int.', '0.00645', '-', '0.00645)']
['system', 'ROUGE-S*', 'Average_F:', '0.00268', '(95%-conf.int.', '0.00268', '-', '0.00268)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:465', 'F:3']
['We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.']
['For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).', 'In Equation 1 we wrote this as p(t I 1  H).', u'In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / \xe2\u20ac\u201d that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).', '(Actually  we use a minor variant described in [4].)', 'Thus we would use p(L2 I L1  M  1  t  h  H).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:66', 'F:0']
['We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.']
['This feature is obviously composed of two sub-features  one recognizing t  the other 1.', 'In Equation 1 we wrote this as p(t I 1  H).', 'Thus we would use p(L2 I L1  M  1  t  h  H).', '(Actually  we use a minor variant described in [4].)', 'For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:66', 'F:0']
no Reference Text in gold A00-2018
['We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.']
['For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).', 'In Equation 1 we wrote this as p(t I 1  H).', 'Thus we would use p(L2 I L1  M  1  t  h  H).', '(Actually  we use a minor variant described in [4].)', 'For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:66', 'F:0']
['As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.', 'We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.', 'This allows the second pass to see expansions not present in the training corpus.', 'For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.', 'We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.']
['For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).', u'In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / \xe2\u20ac\u201d that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).', 'In Equation 1 we wrote this as p(t I 1  H).', 'Thus we would use p(L2 I L1  M  1  t  h  H).', 'For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).']
['system', 'ROUGE-S*', 'Average_R:', '0.01010', '(95%-conf.int.', '0.01010', '-', '0.01010)']
['system', 'ROUGE-S*', 'Average_P:', '0.00452', '(95%-conf.int.', '0.00452', '-', '0.00452)']
['system', 'ROUGE-S*', 'Average_F:', '0.00625', '(95%-conf.int.', '0.00625', '-', '0.00625)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:2211', 'F:10']
['We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.']
['For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).', u'In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / \xe2\u20ac\u201d that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).', 'In Equation 1 we wrote this as p(t I 1  H).', 'Thus we would use p(L2 I L1  M  1  t  h  H).', 'For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).']
['system', 'ROUGE-S*', 'Average_R:', '0.00303', '(95%-conf.int.', '0.00303', '-', '0.00303)']
['system', 'ROUGE-S*', 'Average_P:', '0.00317', '(95%-conf.int.', '0.00317', '-', '0.00317)']
['system', 'ROUGE-S*', 'Average_F:', '0.00310', '(95%-conf.int.', '0.00310', '-', '0.00310)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:946', 'F:3']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.']
['That the previous three best parsers on this test [5 9 17] all perform within a percentage point of each other  despite quite different basic mechanisms  led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training  and to conjecture that perhaps we were at it.', "From our perspective  perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.", u'Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c \xe2\u20ac\u201d information outside c that our probability model deems important in determining the probability in question.', 'Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.', 'Now consider computing a conditional probability p(a H) with a set of features h that connect a to the history H. In a log-linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability  the higher the absolute value of the associated A.']
['system', 'ROUGE-S*', 'Average_R:', '0.00275', '(95%-conf.int.', '0.00275', '-', '0.00275)']
['system', 'ROUGE-S*', 'Average_P:', '0.04926', '(95%-conf.int.', '0.04926', '-', '0.04926)']
['system', 'ROUGE-S*', 'Average_F:', '0.00522', '(95%-conf.int.', '0.00522', '-', '0.00522)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:7260', 'P:406', 'F:20']
['As partition-function calculation is typically the major on-line computational problem for maximum-entropy models, this simplifies the model significantly.']
['As noted in [5]  that system is based upon a &quot;tree-bank grammar&quot; - a grammar read directly off the training corpus.', 'In Equation 1 we wrote this as p(t I 1  H).', '(For example  part-ofspeech tagging using the most probable preterminal for each word is 90% accurate [8].)', 'The first is the slight  but important  improvement achieved by using this model over conventional deleted interpolation  as indicated in Figure 2.', 'For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:91', 'F:0']
['We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.', 'With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ', 'Suppose we were  in fact  going to compute a true maximum entropy model based upon the features used in Equation 7  Ii (t 1)  f2(t 1 1p)  f3(t 1 lp) .', 'For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.', 'This is indicated in Figure 2  where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%  0.4% lower than the results on the official test corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00348', '(95%-conf.int.', '0.00348', '-', '0.00348)']
['system', 'ROUGE-S*', 'Average_P:', '0.02366', '(95%-conf.int.', '0.02366', '-', '0.02366)']
['system', 'ROUGE-S*', 'Average_F:', '0.00607', '(95%-conf.int.', '0.00607', '-', '0.00607)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:465', 'F:11']
0.0354645451321 0.0140845453265 0.0190018180091





input/ref/Task1/A00-2030_aakansha.csv
input/res/Task1/A00-2030.csv
parsing: input/ref/Task1/A00-2030_aakansha.csv
<S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="9">By necessity, we adopted the strategy of hand marking only the semantics.</S>
    <S sid="50" ssid="10">Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'50'"]
'49'
'50'
['49', '50']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="8">Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'"]
'16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S><S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'", "'33'", "'34'"]
'23'
'24'
'33'
'34'
['23', '24', '33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="60" ssid="1">In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).</S>
    <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'61'"]
'60'
'61'
['60', '61']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'"]
'33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).</S>
    <S sid="12" ssid="2">The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).</S><S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'12'", "'16'"]
'11'
'12'
'16'
['11', '12', '16']
parsed_discourse_facet ['method_citation']
<S sid="105" ssid="2">A single model proved capable of performing all necessary sentential processing, both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'"]
'105'
['105']
parsed_discourse_facet ['result_citation']
<S sid="60" ssid="1">In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).</S>
    <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'61'"]
'60'
'61'
['60', '61']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'"]
'16'
['16']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S><S sid ="49" ssid = "9">By necessity  we adopted the strategy of hand marking only the semantics.</S><S sid ="68" ssid = "9">The next steps are to generate in order: In this case  there are none.</S><S sid ="70" ssid = "11">Post-modifier constituents for the PER/NP.</S><S sid ="73" ssid = "14">We now briefly summarize the probability structure of the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'", "'49'", "'68'", "'70'", "'73'"]
'28'
'49'
'68'
'70'
'73'
['28', '49', '68', '70', '73']
parsed_discourse_facet ['results_citation']
<S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="76" ssid = "17">Finally  word features  fm  for modifiers are predicted based on the modifier  cm  the partof-speech tag of the modifier word   t„„ the part-of-speech tag of the head word th  the head word itself  wh  and whether or not the modifier head word  w„„ is known or unknown.</S><S sid ="39" ssid = "7">For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.</S><S sid ="30" ssid = "13">Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'101'", "'76'", "'39'", "'30'"]
'61'
'101'
'76'
'39'
'30'
['61', '101', '76', '39', '30']
parsed_discourse_facet ['method_citation']
<S sid ="33" ssid = "1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S sid ="41" ssid = "1">To train our integrated model  we required a large corpus of augmented parse trees.</S><S sid ="64" ssid = "5">Word features are introduced primarily to help with unknown words  as in (Weischedel et al. 1993).</S><S sid ="2" ssid = "2">In this paper we report adapting a lexic al ized  probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S sid ="23" ssid = "6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'41'", "'64'", "'2'", "'23'"]
'33'
'41'
'64'
'2'
'23'
['33', '41', '64', '2', '23']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'101'", "'81'", "'74'", "'0'"]
'61'
'101'
'81'
'74'
'0'
['61', '101', '81', '74', '0']
parsed_discourse_facet ['method_citation']
<S sid ="60" ssid = "1">In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).</S><S sid ="72" ssid = "13">This generation process is continued until the entire tree has been produced.</S><S sid ="77" ssid = "18">The probability of a complete tree is the product of the probabilities of generating each element in the tree.</S><S sid ="88" ssid = "7">For purposes of pruning  and only for purposes of pruning  the prior probability of each constituent category is multiplied by the generative probability of that constituent (Goodman  1997).</S><S sid ="28" ssid = "11">Finally  our newly constructed parser  like that of (Collins 1997)  was based on a generative statistical model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'72'", "'77'", "'88'", "'28'"]
'60'
'72'
'77'
'88'
'28'
['60', '72', '77', '88', '28']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="54" ssid = "3">These steps are given below:</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'46'", "'57'", "'54'", "'106'"]
'0'
'46'
'57'
'54'
'106'
['0', '46', '57', '54', '106']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="94" ssid = "13">Given a new sentence  the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S><S sid ="30" ssid = "13">Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.</S><S sid ="60" ssid = "1">In our statistical model  trees are generated according to a process similar to that described in (Collins 1996  1997).</S><S sid ="39" ssid = "7">For example  the coreference relation between &quot;Nance&quot; and &quot;a paid consultant to ABC News&quot; is indicated by &quot;per-desc-of.&quot; In this case  because the argument does not connect directly to the relation  the intervening nodes are labeled with semantics &quot;-ptr&quot; to indicate the connection.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'94'", "'30'", "'60'", "'39'"]
'61'
'94'
'30'
'60'
'39'
['61', '94', '30', '60', '39']
parsed_discourse_facet ['method_citation']
<S sid ="110" ssid = "2">Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.</S><S sid ="41" ssid = "1">To train our integrated model  we required a large corpus of augmented parse trees.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'", "'41'", "'57'", "'51'", "'46'"]
'110'
'41'
'57'
'51'
'46'
['110', '41', '57', '51', '46']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="74" ssid = "15">The categories for head constituents  cl„ are predicted based solely on the category of the parent node  cp: Modifier constituent categories  cm  are predicted based on their parent node  cp  the head constituent of their parent node  chp  the previously generated modifier  c„ _1  and the head word of their parent  wp.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'81'", "'0'", "'61'", "'74'"]
'51'
'81'
'0'
'61'
'74'
['51', '81', '0', '61', '74']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "15">Because generative statistical models had already proven successful for each of the first three stages  we were optimistic that some of their properties — especially their ability to learn from large amounts of data  and their robustness when presented with unexpected inputs — would also benefit semantic analysis.</S><S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="19" ssid = "2">Currently  the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing  replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S><S sid ="101" ssid = "6">We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97)  and evaluated name finding accuracy on the MUC7 named entity test.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'61'", "'19'", "'101'", "'51'"]
'32'
'61'
'19'
'101'
'51'
['32', '61', '19', '101', '51']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="54" ssid = "3">These steps are given below:</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'54'", "'81'", "'57'", "'46'"]
'0'
'54'
'81'
'57'
'46'
['0', '54', '81', '57', '46']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "13">Although each model differed in its detailed probability structure  we believed that the essential elements of all three models could be generalized in a single probability model.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'46'", "'105'", "'106'", "'11'"]
'30'
'46'
'105'
'106'
'11'
['30', '46', '105', '106', '11']
parsed_discourse_facet ['method_citation']
<S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="54" ssid = "3">These steps are given below:</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'", "'0'", "'46'", "'57'", "'54'"]
'105'
'0'
'46'
'57'
'54'
['105', '0', '46', '57', '54']
parsed_discourse_facet ['results_citation']
<S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'", "'0'", "'57'", "'51'", "'105'"]
'46'
'0'
'57'
'51'
'105'
['46', '0', '57', '51', '105']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="81" ssid = "3">For modifier constituents  the mixture components are: For part-of-speech tags  the mixture components are: Finally  for word features  the mixture components are:</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'57'", "'46'", "'51'", "'81'"]
'0'
'57'
'46'
'51'
'81'
['0', '57', '46', '51', '81']
parsed_discourse_facet ['method_citation']
<S sid ="51" ssid = "11">To produce a corpus of augmented parse trees  we used the following multi-step training procedure which exploited the Penn TREEBANK Applying this procedure yielded a new version of the semantically annotated corpus  now annotated with complete augmented trees like that in Figure 3.</S><S sid ="32" ssid = "15">Because generative statistical models had already proven successful for each of the first three stages  we were optimistic that some of their properties — especially their ability to learn from large amounts of data  and their robustness when presented with unexpected inputs — would also benefit semantic analysis.</S><S sid ="61" ssid = "2">The detailed probability structure differs  however  in that it was designed to jointly perform part-of-speech tagging  name finding  syntactic parsing  and relation finding in a single process.</S><S sid ="11" ssid = "1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh  1998).</S><S sid ="26" ssid = "9">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993)  and more recently  had begun using a generative statistical model for name finding (Bikel et al.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'32'", "'61'", "'11'", "'26'"]
'51'
'32'
'61'
'11'
'26'
['51', '32', '61', '11', '26']
parsed_discourse_facet ['method_citation']
<S sid ="110" ssid = "2">Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.</S><S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'", "'106'", "'46'", "'57'", "'105'"]
'110'
'106'
'46'
'57'
'105'
['110', '106', '46', '57', '105']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "3">These steps are given below:</S><S sid ="55" ssid = "1">syntactic modifier of the other  the inserted node serves to indicate the relation as well as the argument.</S><S sid ="0" ssid = "0">A Novel Use of Statistical Parsing to Extract Information from Text</S><S sid ="62" ssid = "3">For each constituent  the head is generated first  followed by the modifiers  which are generated from the head outward.</S><S sid ="7" ssid = "5">The technique was benchmarked in the Seventh Message Understanding Conference (MUC-7) in 1998.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'55'", "'0'", "'62'", "'7'"]
'54'
'55'
'0'
'62'
'7'
['54', '55', '0', '62', '7']
parsed_discourse_facet ['method_citation']
<S sid ="106" ssid = "3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S><S sid ="110" ssid = "2">Technical agents for part of this work were Fort Huachucha and AFRL under contract numbers DABT63-94-C-0062  F30602-97-C-0096  and 4132-BBN-001.</S><S sid ="46" ssid = "6">Initially  we tried to annotate the training corpus by hand marking  for each sentence  the entire augmented tree.</S><S sid ="105" ssid = "2">A single model proved capable of performing all necessary sentential processing  both syntactic and semantic.</S><S sid ="57" ssid = "3">David Edwin Lewis &quot; a node is inserted to indicate that &quot;Lt. Cmdr.&quot; is a descriptor for &quot;David Edwin Lewis.&quot; 5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'110'", "'46'", "'105'", "'57'"]
'106'
'110'
'46'
'105'
'57'
['106', '110', '46', '105', '57']
parsed_discourse_facet ['aim_citation', 'results_citation']
Length 0 input/ref/Task1/A00-2030_aakansha.csv
0.0 0.0 0.0





input/ref/Task1/W06-3114_sweta.csv
input/res/Task1/W06-3114.csv
parsing: input/ref/Task1/W06-3114_sweta.csv
 <S sid="108" ssid="1">The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="27">For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="44">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages, as demonstrated by the results for English-German and English-French.</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
 <S sid="172" ssid="3">Due to many similarly performing systems, we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
 <S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["36'"]
36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="103" ssid="19">Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["103'"]
103'
['103']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="60">One annotator suggested that this was the case for as much as 10% of our test sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["102'"]
102'
['102']
parsed_discourse_facet ['method_citation']
<S sid="123" ssid="16">For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.</S>
original cit marker offset is 0
new cit marker offset is 0



["123'"]
123'
['123']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="27">For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="1">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="19">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["126'"]
126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="173" ssid="4">The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["173'"]
173'
['173']
parsed_discourse_facet ['method_citation']
 <S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["170'"]
170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="84" ssid="23">The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["84'"]
84'
['84']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="80" ssid = "19">We collected around 300–400 judgements per judgement type (adequacy or fluency)  per system  per language pair.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'126'", "'80'", "'136'", "'0'"]
'134'
'126'
'80'
'136'
'0'
['134', '126', '80', '136', '0']
parsed_discourse_facet ['results_citation']
<S sid ="159" ssid = "52">(b) does the translation have the same meaning  including connotations?</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'159'", "'0'", "'84'", "'17'", "'170'"]
'159'
'0'
'84'
'17'
'170'
['159', '0', '84', '17', '170']
parsed_discourse_facet ['method_citation']
<S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'", "'18'", "'16'", "'126'", "'84'"]
'170'
'18'
'16'
'126'
'84'
['170', '18', '16', '126', '84']
parsed_discourse_facet ['method_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'17'", "'170'", "'84'", "'38'"]
'126'
'17'
'170'
'84'
'38'
['126', '17', '170', '84', '38']
parsed_discourse_facet ['method_citation']
<S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'", "'38'", "'126'", "'17'", "'84'"]
'170'
'38'
'126'
'17'
'84'
['170', '38', '126', '17', '84']
parsed_discourse_facet ['method_citation']
<S sid ="52" ssid = "18">Pairwise comparison: We can use the same method to assess the statistical significance of one system outperforming another.</S><S sid ="21" ssid = "14">The out-of-domain test set differs from the Europarl data in various ways.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="129" ssid = "22">All systems (except for Systran  which was not tuned to Europarl) did considerably worse on outof-domain training data.</S><S sid ="35" ssid = "1">For the automatic evaluation  we used BLEU  since it is the most established metric in the field.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'", "'21'", "'28'", "'129'", "'35'"]
'52'
'21'
'28'
'129'
'35'
['52', '21', '28', '129', '35']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="113" ssid = "6">The confidence intervals are computed by bootstrap resampling for BLEU  and by standard significance testing for the manual scores  as described earlier in the paper.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'113'", "'126'", "'28'", "'0'"]
'16'
'113'
'126'
'28'
'0'
['16', '113', '126', '28', '0']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="109" ssid = "2">The scores and confidence intervals are detailed first in the Figures 7–10 in table form (including ranks)  and then in graphical form in Figures 11–16.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'16'", "'38'", "'84'", "'109'"]
'18'
'16'
'38'
'84'
'109'
['18', '16', '38', '84', '109']
parsed_discourse_facet ['method_citation']
<S sid ="9" ssid = "2">Training and testing is based on the Europarl corpus.</S><S sid ="44" ssid = "10">We computed BLEU scores for each submission with a single reference translation.</S><S sid ="143" ssid = "36">For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.</S><S sid ="25" ssid = "18">We received submissions from 14 groups from 11 institutions  as listed in Figure 2.</S><S sid ="35" ssid = "1">For the automatic evaluation  we used BLEU  since it is the most established metric in the field.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'44'", "'143'", "'25'", "'35'"]
'9'
'44'
'143'
'25'
'35'
['9', '44', '143', '25', '35']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="68" ssid = "7">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'68'", "'0'", "'28'", "'136'"]
'134'
'68'
'0'
'28'
'136'
['134', '68', '0', '28', '136']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="68" ssid = "7">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'0'", "'68'", "'126'", "'17'"]
'134'
'0'
'68'
'126'
'17'
['134', '0', '68', '126', '17']
parsed_discourse_facet ['method_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="159" ssid = "52">(b) does the translation have the same meaning  including connotations?</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'134'", "'17'", "'84'", "'159'"]
'126'
'134'
'17'
'84'
'159'
['126', '134', '17', '84', '159']
parsed_discourse_facet ['method_citation']
<S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'38'", "'155'", "'170'", "'6'"]
'84'
'38'
'155'
'170'
'6'
['84', '38', '155', '170', '6']
parsed_discourse_facet ['results_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'134'", "'0'", "'17'", "'84'"]
'126'
'134'
'0'
'17'
'84'
['126', '134', '0', '17', '84']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="68" ssid = "7">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'0'", "'126'", "'68'", "'17'"]
'134'
'0'
'126'
'68'
'17'
['134', '0', '126', '68', '17']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'170'", "'84'", "'126'", "'17'"]
'0'
'170'
'84'
'126'
'17'
['0', '170', '84', '126', '17']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "19">We collected around 300–400 judgements per judgement type (adequacy or fluency)  per system  per language pair.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="150" ssid = "43">For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French)  at least as measured by BLEU.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'84'", "'150'", "'16'", "'155'"]
'80'
'84'
'150'
'16'
'155'
['80', '84', '150', '16', '155']
parsed_discourse_facet ['method_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'84'", "'38'", "'17'", "'170'"]
'126'
'84'
'38'
'17'
'170'
['126', '84', '38', '17', '170']
parsed_discourse_facet ['method_citation']
['The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.']
['The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:45', 'F:0']
['The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.']
['Manual and Automatic Evaluation of Machine Translation between European Languages', u'We collected around 300\xe2\u20ac\u201c400 judgements per judgement type (adequacy or fluency)  per system  per language pair.', 'The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'Again  we can compute average scores for all systems for the different language pairs (Figure 6).']
['system', 'ROUGE-S*', 'Average_R:', '0.00943', '(95%-conf.int.', '0.00943', '-', '0.00943)']
['system', 'ROUGE-S*', 'Average_P:', '0.25455', '(95%-conf.int.', '0.25455', '-', '0.25455)']
['system', 'ROUGE-S*', 'Average_F:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:55', 'F:14']
['One annotator suggested that this was the case for as much as 10% of our test sentences.']
['Training and testing is based on the Europarl corpus.', 'We received submissions from 14 groups from 11 institutions  as listed in Figure 2.', 'We computed BLEU scores for each submission with a single reference translation.', 'For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.', 'For the automatic evaluation  we used BLEU  since it is the most established metric in the field.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:15', 'F:0']
['In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.']
['In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.', 'The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.05702', '(95%-conf.int.', '0.05702', '-', '0.05702)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.10788', '(95%-conf.int.', '0.10788', '-', '0.10788)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:91', 'F:91']
['The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages, as demonstrated by the results for English-German and English-French.']
['The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00351', '(95%-conf.int.', '0.00351', '-', '0.00351)']
['system', 'ROUGE-S*', 'Average_P:', '0.04412', '(95%-conf.int.', '0.04412', '-', '0.04412)']
['system', 'ROUGE-S*', 'Average_F:', '0.00650', '(95%-conf.int.', '0.00650', '-', '0.00650)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:136', 'F:6']
['While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.']
['English was again paired with German  French  and Spanish.', 'The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).', 'For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.01328', '(95%-conf.int.', '0.01328', '-', '0.01328)']
['system', 'ROUGE-S*', 'Average_P:', '0.08225', '(95%-conf.int.', '0.08225', '-', '0.08225)']
['system', 'ROUGE-S*', 'Average_F:', '0.02286', '(95%-conf.int.', '0.02286', '-', '0.02286)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:231', 'F:19']
['The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.']
['The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00526', '(95%-conf.int.', '0.00526', '-', '0.00526)']
['system', 'ROUGE-S*', 'Average_P:', '0.13636', '(95%-conf.int.', '0.13636', '-', '0.13636)']
['system', 'ROUGE-S*', 'Average_F:', '0.01013', '(95%-conf.int.', '0.01013', '-', '0.01013)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:66', 'F:9']
['For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.']
['(b) does the translation have the same meaning  including connotations?', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'Again  we can compute average scores for all systems for the different language pairs (Figure 6).']
['system', 'ROUGE-S*', 'Average_R:', '0.00332', '(95%-conf.int.', '0.00332', '-', '0.00332)']
['system', 'ROUGE-S*', 'Average_P:', '0.10714', '(95%-conf.int.', '0.10714', '-', '0.10714)']
['system', 'ROUGE-S*', 'Average_F:', '0.00644', '(95%-conf.int.', '0.00644', '-', '0.00644)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:28', 'F:3']
['The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data.']
[u'We asked participants to each judge 200\xe2\u20ac\u201c300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.', 'Manual and Automatic Evaluation of Machine Translation between European Languages', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'Again  we can compute average scores for all systems for the different language pairs (Figure 6).']
['system', 'ROUGE-S*', 'Average_R:', '0.00471', '(95%-conf.int.', '0.00471', '-', '0.00471)']
['system', 'ROUGE-S*', 'Average_P:', '0.13333', '(95%-conf.int.', '0.13333', '-', '0.13333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00909', '(95%-conf.int.', '0.00909', '-', '0.00909)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:45', 'F:6']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['Manual and Automatic Evaluation of Machine Translation between European Languages', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.04677', '(95%-conf.int.', '0.04677', '-', '0.04677)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.08936', '(95%-conf.int.', '0.08936', '-', '0.08936)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:55', 'F:55']
['For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.']
[u'We asked participants to each judge 200\xe2\u20ac\u201c300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.', 'Manual and Automatic Evaluation of Machine Translation between European Languages', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'Again  we can compute average scores for all systems for the different language pairs (Figure 6).']
['system', 'ROUGE-S*', 'Average_R:', '0.00471', '(95%-conf.int.', '0.00471', '-', '0.00471)']
['system', 'ROUGE-S*', 'Average_P:', '0.21429', '(95%-conf.int.', '0.21429', '-', '0.21429)']
['system', 'ROUGE-S*', 'Average_F:', '0.00921', '(95%-conf.int.', '0.00921', '-', '0.00921)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:28', 'F:6']
['For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.']
['(b) does the translation have the same meaning  including connotations?', 'Manual and Automatic Evaluation of Machine Translation between European Languages', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00122', '(95%-conf.int.', '0.00122', '-', '0.00122)']
['system', 'ROUGE-S*', 'Average_P:', '0.03571', '(95%-conf.int.', '0.03571', '-', '0.03571)']
['system', 'ROUGE-S*', 'Average_F:', '0.00236', '(95%-conf.int.', '0.00236', '-', '0.00236)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:28', 'F:1']
['The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:']
['For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French)  at least as measured by BLEU.', u'We collected around 300\xe2\u20ac\u201c400 judgements per judgement type (adequacy or fluency)  per system  per language pair.', 'For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.']
['system', 'ROUGE-S*', 'Average_R:', '0.01957', '(95%-conf.int.', '0.01957', '-', '0.01957)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.03838', '(95%-conf.int.', '0.03838', '-', '0.03838)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:28', 'F:28']
0.308288459167 0.0129846152847 0.0246453844258





input/ref/Task1/J01-2004_swastika.csv
input/res/Task1/J01-2004.csv
parsing: input/ref/Task1/J01-2004_swastika.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="3">In the past few years, however, some improvements have been made over these language models through the use of statistical methods of natural language processing, and the development of innovative, linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
<S sid="215" ssid="119">The first word in the string remaining to be parsed, w1, we will call the look-ahead word.</S>
original cit marker offset is 0
new cit marker offset is 0



['215']
215
['215']
parsed_discourse_facet ['method_citation']
    <S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



['302']
302
['302']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="100" ssid="4">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['method_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="168" ssid = "72">For example  in Figure 2  constituent (NP-DT-NN) is simply NP.</S><S sid ="356" ssid = "112">The DARPA '93 HUB1 test setup consists of 213 utterances read from the Wall Street Journal  a total of 3 446 words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'224'", "'322'", "'358'", "'168'", "'356'"]
'224'
'322'
'358'
'168'
'356'
['224', '322', '358', '168', '356']
parsed_discourse_facet ['results_citation']
<S sid ="17" ssid = "5">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S sid ="102" ssid = "6">One approach to syntactic language modeling is to use this distribution directly as a language model.</S><S sid ="340" ssid = "96">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid ="113" ssid = "17">In empirical trials  Goddeau used the top two stack entries to condition the word probability.</S><S sid ="12" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'102'", "'340'", "'113'", "'12'"]
'17'
'102'
'340'
'113'
'12'
['17', '102', '340', '113', '12']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="95" ssid = "53">For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'95'", "'302'", "'224'", "'104'"]
'134'
'95'
'302'
'224'
'104'
['134', '95', '302', '224', '104']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="167" ssid = "71">In order to avoid any confusions in identifying the nonterminal label of a particular rule production in either its factored or rionfactored version  we introduce the function constituent (A) for every nonterminal in the factored grammar Gf  which is simply the label of the constituent whose factorization results in A.</S><S sid ="112" ssid = "16">Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state  and conditioning on those entries in a way similar to an n-gram.</S><S sid ="85" ssid = "43">The simple definition of c-command that we will be using in this paper is the following: a node A c-commands a node B if and only if (i) A does not dominate B; and (ii) the lowest branching node (i.e.  non-unary node) that dominates A also dominates B.'</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'167'", "'112'", "'85'", "'390'"]
'134'
'167'
'112'
'85'
'390'
['134', '167', '112', '85', '390']
parsed_discourse_facet ['method_citation']
<S sid ="321" ssid = "77">In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.</S><S sid ="95" ssid = "53">For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].</S><S sid ="199" ssid = "103">Table 1 gives a breakdown of the different levels of conditioning information used in the empirical trials  with a mnemonic label that will be used when presenting results.</S><S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="11" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'321'", "'95'", "'199'", "'5'", "'11'"]
'321'
'95'
'199'
'5'
'11'
['321', '95', '199', '5', '11']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="87" ssid = "45">Notice that the subject NP c-commands the object NP  but not vice versa  since the lowest branching node that dominates the object NP is the VP  which does not dominate the subject NP.</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S><S sid ="85" ssid = "43">The simple definition of c-command that we will be using in this paper is the following: a node A c-commands a node B if and only if (i) A does not dominate B; and (ii) the lowest branching node (i.e.  non-unary node) that dominates A also dominates B.'</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'87'", "'302'", "'104'", "'85'"]
'134'
'87'
'302'
'104'
'85'
['134', '87', '302', '104', '85']
parsed_discourse_facet ['method_citation']
<S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="52" ssid = "10">Consider  for example  the parse tree shown in (a) in Figure 1: the start symbol is St  which expands into an S. The S node expands into an NP followed by a VP.</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S>
original cit marker offset is 0
new cit marker offset is 0



["'224'", "'134'", "'52'", "'322'", "'306'"]
'224'
'134'
'52'
'322'
'306'
['224', '134', '52', '322', '306']
parsed_discourse_facet ['method_citation']
<S sid ="112" ssid = "16">Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state  and conditioning on those entries in a way similar to an n-gram.</S><S sid ="129" ssid = "33">The shift-reduce parser will have  perhaps  built the structure shown  and the stack state will have an NP entry with the head &quot;cat&quot; at the top of the stack  and a VBD entry with the head &quot;chased&quot; second on the stack.</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="87" ssid = "45">Notice that the subject NP c-commands the object NP  but not vice versa  since the lowest branching node that dominates the object NP is the VP  which does not dominate the subject NP.</S><S sid ="109" ssid = "13">A shift-reduce parser operates from left to right using a stack and a pointer to the next word in the input string.9 Each stack entry consists minimally of a nonterminal label.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'129'", "'302'", "'87'", "'109'"]
'112'
'129'
'302'
'87'
'109'
['112', '129', '302', '87', '109']
parsed_discourse_facet ['method_citation']
<S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="213" ssid = "117">The parser takes an input string 4  a PCFG G  and a priority queue of candidate analyses.</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'358'", "'306'", "'224'", "'213'", "'322'"]
'358'
'306'
'224'
'213'
'322'
['358', '306', '224', '213', '322']
parsed_discourse_facet ['method_citation']
<S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="59" ssid = "17">A PCFG is a CFG with a probability assigned to each rule; specifically  each righthand side has a probability given the left-hand side of the rule.</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="52" ssid = "10">Consider  for example  the parse tree shown in (a) in Figure 1: the start symbol is St  which expands into an S. The S node expands into an NP followed by a VP.</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'302'", "'59'", "'134'", "'52'", "'104'"]
'302'
'59'
'134'
'52'
'104'
['302', '59', '134', '52', '104']
parsed_discourse_facet ['method_citation']
<S sid ="210" ssid = "114">It uses a PCFG with a conditional probability model of the sort defined in the previous section.</S><S sid ="142" ssid = "46">A simple PCFG conditions rule probabilities on the left-hand side of the rule.</S><S sid ="323" ssid = "79">They trained and tested the SLM on a modified  more &quot;speech-like&quot; version of the Penn Treebank.</S><S sid ="0" ssid = "0">Probabilistic Top-Down Parsing and Language Modeling</S><S sid ="370" ssid = "126">We followed Chelba (2000) in using an LM weight of 16 for the lattice trigram.</S>
original cit marker offset is 0
new cit marker offset is 0



["'210'", "'142'", "'323'", "'0'", "'370'"]
'210'
'142'
'323'
'0'
'370'
['210', '142', '323', '0', '370']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S><S sid ="154" ssid = "58">We use the relative frequency estimator  and smooth our production probabilities by interpolating the relative frequency estimates with those obtained by &quot;annotating&quot; less contextual information.</S><S sid ="0" ssid = "0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'224'", "'104'", "'154'", "'0'"]
'134'
'224'
'104'
'154'
'0'
['134', '224', '104', '154', '0']
parsed_discourse_facet ['method_citation']
<S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="95" ssid = "53">For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].</S><S sid ="124" ssid = "28">Each distinct derivation path within the beam has a probability and a stack state associated with it.</S><S sid ="147" ssid = "51">One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).</S><S sid ="168" ssid = "72">For example  in Figure 2  constituent (NP-DT-NN) is simply NP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'302'", "'95'", "'124'", "'147'", "'168'"]
'302'
'95'
'124'
'147'
'168'
['302', '95', '124', '147', '168']
parsed_discourse_facet ['results_citation']
<S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S>
original cit marker offset is 0
new cit marker offset is 0



["'358'", "'306'", "'322'", "'224'", "'134'"]
'358'
'306'
'322'
'224'
'134'
['358', '306', '322', '224', '134']
parsed_discourse_facet ['method_citation']
<S sid ="289" ssid = "45">These results  achieved using very straightforward conditioning events and considering only the left context  are within one to four points of the best published Observed running time on Section 23 of the Penn Treebank  with the full conditional probability model and beam of 10-11  using one 300 MHz UltraSPARC processor and 256MB of RAM of a Sun Enterprise 450. accuracies cited above.'</S><S sid ="25" ssid = "13">A parser that is not left to right  but which has rooted derivations  e.g.  a headfirst parser  will be able to calculate generative joint probabilities for entire strings; however  it will not be able to calculate probabilities for each word conditioned on previously generated words  unless each derivation generates the words in the string in exactly the same order.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="141" ssid = "45">We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.</S><S sid ="22" ssid = "10">A left-toright parser whose derivations are not rooted  i.e.  with derivations that can consist of disconnected tree fragments  such as an LR or shift-reduce parser  cannot incrementally calculate the probability of each prefix string being generated by the probabilistic grammar  because their derivations include probability mass from unrooted structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'289'", "'25'", "'390'", "'141'", "'22'"]
'289'
'25'
'390'
'141'
'22'
['289', '25', '390', '141', '22']
parsed_discourse_facet ['method_citation']
<S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'224'", "'306'", "'358'", "'134'", "'322'"]
'224'
'306'
'358'
'134'
'322'
['224', '306', '358', '134', '322']
parsed_discourse_facet ['method_citation']
<S sid ="147" ssid = "51">One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="278" ssid = "34">Section 22 (41 817 words  1 700 sentences) served as the development corpus  on which the parser was tested until stable versions were ready to run on the test data  to avoid developing the parser to fit the specific test data.</S><S sid ="141" ssid = "45">We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'302'", "'278'", "'141'", "'390'"]
'147'
'302'
'278'
'141'
'390'
['147', '302', '278', '141', '390']
parsed_discourse_facet ['method_citation']
['First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.']
['Section 22 (41 817 words  1 700 sentences) served as the development corpus  on which the parser was tested until stable versions were ready to run on the test data  to avoid developing the parser to fit the specific test data.', 'One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).', "In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.", 'With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.', 'We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.']
['system', 'ROUGE-S*', 'Average_R:', '0.00747', '(95%-conf.int.', '0.00747', '-', '0.00747)']
['system', 'ROUGE-S*', 'Average_P:', '0.11385', '(95%-conf.int.', '0.11385', '-', '0.11385)']
['system', 'ROUGE-S*', 'Average_F:', '0.01403', '(95%-conf.int.', '0.01403', '-', '0.01403)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4950', 'P:325', 'F:37']
['The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.']
['For example  in Figure 2  constituent (NP-DT-NN) is simply NP.', 'Each distinct derivation path within the beam has a probability and a stack state associated with it.', "In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.", 'For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].', 'One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).']
['system', 'ROUGE-S*', 'Average_R:', '0.02005', '(95%-conf.int.', '0.02005', '-', '0.02005)']
['system', 'ROUGE-S*', 'Average_P:', '0.09117', '(95%-conf.int.', '0.09117', '-', '0.09117)']
['system', 'ROUGE-S*', 'Average_F:', '0.03287', '(95%-conf.int.', '0.03287', '-', '0.03287)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:351', 'F:32']
['The first word in the string remaining to be parsed, w1, we will call the look-ahead word.']
['Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.', 'Consider  for example  the parse tree shown in (a) in Figure 1: the start symbol is St  which expands into an S. The S node expands into an NP followed by a VP.', 'For each word position i  we have a separate priority queue H  of analyses with look-ahead w .', 'Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).', "The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00233', '(95%-conf.int.', '0.00233', '-', '0.00233)']
['system', 'ROUGE-S*', 'Average_P:', '0.19444', '(95%-conf.int.', '0.19444', '-', '0.19444)']
['system', 'ROUGE-S*', 'Average_F:', '0.00461', '(95%-conf.int.', '0.00461', '-', '0.00461)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:36', 'F:7']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.', "The DARPA '93 HUB1 test setup consists of 213 utterances read from the Wall Street Journal  a total of 3 446 words.", 'For each word position i  we have a separate priority queue H  of analyses with look-ahead w .', 'For example  in Figure 2  constituent (NP-DT-NN) is simply NP.', "We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:91', 'F:0']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['They trained and tested the SLM on a modified  more &quot;speech-like&quot; version of the Penn Treebank.', 'We followed Chelba (2000) in using an LM weight of 16 for the lattice trigram.', 'A simple PCFG conditions rule probabilities on the left-hand side of the rule.', 'It uses a PCFG with a conditional probability model of the sort defined in the previous section.', 'Probabilistic Top-Down Parsing and Language Modeling']
['system', 'ROUGE-S*', 'Average_R:', '0.00405', '(95%-conf.int.', '0.00405', '-', '0.00405)']
['system', 'ROUGE-S*', 'Average_P:', '0.03297', '(95%-conf.int.', '0.03297', '-', '0.03297)']
['system', 'ROUGE-S*', 'Average_F:', '0.00721', '(95%-conf.int.', '0.00721', '-', '0.00721)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:91', 'F:3']
["In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds."]
["In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.", 'Consider  for example  the parse tree shown in (a) in Figure 1: the start symbol is St  which expands into an S. The S node expands into an NP followed by a VP.', 'A PCFG is a CFG with a probability assigned to each rule; specifically  each righthand side has a probability given the left-hand side of the rule.', 'The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.', "The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'"]
['system', 'ROUGE-S*', 'Average_R:', '0.02737', '(95%-conf.int.', '0.02737', '-', '0.02737)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.05328', '(95%-conf.int.', '0.05328', '-', '0.05328)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:78', 'F:78']
['Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).']
['Probabilistic Top-Down Parsing and Language Modeling', 'For each word position i  we have a separate priority queue H  of analyses with look-ahead w .', 'We use the relative frequency estimator  and smooth our production probabilities by interpolating the relative frequency estimates with those obtained by &quot;annotating&quot; less contextual information.', 'The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.', "The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00817', '(95%-conf.int.', '0.00817', '-', '0.00817)']
['system', 'ROUGE-S*', 'Average_P:', '0.05667', '(95%-conf.int.', '0.05667', '-', '0.05667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01429', '(95%-conf.int.', '0.01429', '-', '0.01429)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:300', 'F:17']
['In the past few years, however, some improvements have been made over these language models through the use of statistical methods of natural language processing, and the development of innovative, linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.']
['One approach to syntactic language modeling is to use this distribution directly as a language model.', 'A small recognition experiment also demonstrates the utility of the model.', 'The trigram model was also trained on Sections 00-20 of the C&J corpus.', 'In empirical trials  Goddeau used the top two stack entries to condition the word probability.', 'This paper will examine language modeling for speech recognition from a natural language processing point of view.']
['system', 'ROUGE-S*', 'Average_R:', '0.03415', '(95%-conf.int.', '0.03415', '-', '0.03415)']
['system', 'ROUGE-S*', 'Average_P:', '0.09333', '(95%-conf.int.', '0.09333', '-', '0.09333)']
['system', 'ROUGE-S*', 'Average_F:', '0.05000', '(95%-conf.int.', '0.05000', '-', '0.05000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:300', 'F:28']
0.197803747527 0.0129487498381 0.0220362497245





input/ref/Task1/D10-1044_swastika.csv
input/res/Task1/D10-1044.csv
parsing: input/ref/Task1/D10-1044_swastika.csv
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
<S sid="71" ssid="8">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c&#955;(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



['71']
71
['71']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
<S sid="71" ssid="8">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c&#955;(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



['71']
71
['71']
parsed_discourse_facet ['result_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="9">An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (Och, 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



['45']
45
['45']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



['75']
75
['75']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
  <S sid="42" ssid="6">The natural baseline approach is to concatenate data from IN and OUT.</S>
original cit marker offset is 0
new cit marker offset is 0



['42']
42
['42']
parsed_discourse_facet ['aim_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/D10-1044.csv
<S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daum´e’s (2007) work that splits each feature into domain-specific and general copies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'128'", "'140'", "'10'", "'4'", "'136'"]
'128'
'140'
'10'
'4'
'136'
['128', '140', '10', '4', '136']
parsed_discourse_facet ['results_citation']
<S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="82" ssid = "19">This is not unreasonable given the application to phrase pairs from OUT  but it suggests that an interesting alternative might be to use a plain log-linear weighting function exp(Ei Aifi(s  t))  with outputs in [0  oo].</S><S sid ="142" ssid = "11">There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'38'", "'82'", "'142'", "'130'"]
'143'
'38'
'82'
'142'
'130'
['143', '38', '82', '142', '130']
parsed_discourse_facet ['method_citation']
<S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S><S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="31" ssid = "28">For comparison to information-retrieval inspired baselines  eg (L¨u et al.  2007)  we select sentences from OUT using language model perplexities from IN.</S><S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'7'", "'31'", "'128'", "'78'"]
'140'
'7'
'31'
'128'
'78'
['140', '7', '31', '128', '78']
parsed_discourse_facet ['method_citation']
<S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="102" ssid = "6">The dev corpus was taken from the NIST05 evaluation set  augmented with some randomly-selected material reserved from the training set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'127'", "'143'", "'65'", "'102'"]
'130'
'127'
'143'
'65'
'102'
['130', '127', '143', '65', '102']
parsed_discourse_facet ['method_citation']
<S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="55" ssid = "19">This suggests a direct parallel to (1): where ˜p(s  t) is a joint empirical distribution extracted from the IN dev set using the standard procedure.2 An alternative form of linear combination is a maximum a posteriori (MAP) combination (Bacchiani et al.  2004).</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'21'", "'49'", "'55'", "'127'"]
'75'
'21'
'49'
'55'
'127'
['75', '21', '49', '55', '127']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'130'", "'49'", "'38'", "'143'"]
'127'
'130'
'49'
'38'
'143'
['127', '130', '49', '38', '143']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'20'", "'38'", "'130'", "'49'"]
'127'
'20'
'38'
'130'
'49'
['127', '20', '38', '130', '49']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S><S sid ="37" ssid = "1">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features  interpreted as log probabilities  many of which have their own internal parameters and objectives.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'49'", "'111'", "'37'", "'21'"]
'127'
'49'
'111'
'37'
'21'
['127', '49', '111', '37', '21']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'81'", "'151'", "'75'", "'143'"]
'7'
'81'
'151'
'75'
'143'
['7', '81', '151', '75', '143']
parsed_discourse_facet ['method_citation']
<S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'75'", "'127'", "'151'", "'143'"]
'21'
'75'
'127'
'151'
'143'
['21', '75', '127', '151', '143']
parsed_discourse_facet ['method_citation']
<S sid ="97" ssid = "1">We carried out translation experiments in two different settings.</S><S sid ="53" ssid = "17">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).</S><S sid ="106" ssid = "10">The corpora for both settings are summarized in table 1.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="26" ssid = "23">Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009)  who weight sentences according to sub-corpus and genre membership.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'53'", "'106'", "'81'", "'26'"]
'97'
'53'
'106'
'81'
'26'
['97', '53', '106', '81', '26']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="31" ssid = "28">For comparison to information-retrieval inspired baselines  eg (L¨u et al.  2007)  we select sentences from OUT using language model perplexities from IN.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'81'", "'31'", "'78'", "'140'"]
'7'
'81'
'31'
'78'
'140'
['7', '81', '31', '78', '140']
parsed_discourse_facet ['method_citation']
<S sid ="60" ssid = "24">The matching sentence pairs are then added to the IN corpus  and the system is re-trained.</S><S sid ="30" ssid = "27">A similar maximumlikelihood approach was used by Foster and Kuhn (2007)  but for language models only.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="145" ssid = "2">Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'30'", "'24'", "'65'", "'145'"]
'60'
'30'
'24'
'65'
'145'
['60', '30', '24', '65', '145']
parsed_discourse_facet ['results_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="22" ssid = "19">Within this framework  we use features intended to capture degree of generality  including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'20'", "'130'", "'49'", "'22'"]
'127'
'20'
'130'
'49'
'22'
['127', '20', '130', '49', '22']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="119" ssid = "23">The 2nd block contains the IR system  which was tuned by selecting text in multiples of the size of the EMEA training corpus  according to dev set performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'130'", "'20'", "'49'", "'119'"]
'127'
'130'
'20'
'49'
'119'
['127', '130', '20', '49', '119']
parsed_discourse_facet ['method_citation']
<S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S><S sid ="66" ssid = "3">The weight on each sentence is a value in [0  1] computed by a perceptron with Boolean features that indicate collection and genre membership.</S><S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="113" ssid = "17">The corpus was wordaligned using both HMM and IBM2 models  and the phrase table was the union of phrases extracted from these separate alignments  with a length limit of 7.</S><S sid ="102" ssid = "6">The dev corpus was taken from the NIST05 evaluation set  augmented with some randomly-selected material reserved from the training set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'66'", "'75'", "'113'", "'102'"]
'143'
'66'
'75'
'113'
'102'
['143', '66', '75', '113', '102']
parsed_discourse_facet ['method_citation']
<S sid ="44" ssid = "8">When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.</S><S sid ="79" ssid = "16">This combination generalizes (2) and (3): we use either at = a to obtain a fixed-weight linear combination  or at = cI(t)/(cI(t) + 0) to obtain a MAP combination.</S><S sid ="62" ssid = "26">To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S><S sid ="115" ssid = "19">Table 2 shows results for both settings and all methods described in sections 2 and 3.</S><S sid ="13" ssid = "10">The techniques we develop can be extended in a relatively straightforward manner to the more general case when OUT consists of multiple sub-domains.</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'79'", "'62'", "'115'", "'13'"]
'44'
'79'
'62'
'115'
'13'
['44', '79', '62', '115', '13']
parsed_discourse_facet ['method_citation']
<S sid ="106" ssid = "10">The corpora for both settings are summarized in table 1.</S><S sid ="60" ssid = "24">The matching sentence pairs are then added to the IN corpus  and the system is re-trained.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="53" ssid = "17">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'60'", "'81'", "'38'", "'53'"]
'106'
'60'
'81'
'38'
'53'
['106', '60', '81', '38', '53']
parsed_discourse_facet ['method_citation']
<S sid ="95" ssid = "32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S><S sid ="50" ssid = "14">Linear weights are difficult to incorporate into the standard MERT procedure because they are “hidden” within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="59" ssid = "23">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid ="51" ssid = "15">For the LM  adaptive weights are set as follows: where α is a weight vector containing an element αi for each domain (just IN and OUT in our case)  pi are the corresponding domain-specific models  and ˜p(w  h) is an empirical distribution from a targetlanguage training corpus—we used the IN dev set for this.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'50'", "'21'", "'59'", "'51'"]
'95'
'50'
'21'
'59'
'51'
['95', '50', '21', '59', '51']
parsed_discourse_facet ['aim_citation', 'results_citation']
['To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.']
['The dev corpus was taken from the NIST05 evaluation set  augmented with some randomly-selected material reserved from the training set.', 'The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.', 'Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.', 'Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).', u'The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the \u201cflattened\u201d variant described in section 3.2.']
['system', 'ROUGE-S*', 'Average_R:', '0.00433', '(95%-conf.int.', '0.00433', '-', '0.00433)']
['system', 'ROUGE-S*', 'Average_P:', '0.10833', '(95%-conf.int.', '0.10833', '-', '0.10833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:120', 'F:13']
['In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.']
['Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.', 'This is a standard adaptation problem for SMT.', 'Domain adaptation is a common concern when optimizing empirical NLP applications.', u'Recent work by Finkel and Manning (2009) which re-casts Daum\xb4e\u2019s approach in a hierarchical MAP framework may be applicable to this problem.', u'It is also worth pointing out a connection with Daum\xb4e\u2019s (2007) work that splits each feature into domain-specific and general copies.']
['system', 'ROUGE-S*', 'Average_R:', '0.00340', '(95%-conf.int.', '0.00340', '-', '0.00340)']
['system', 'ROUGE-S*', 'Average_P:', '0.01905', '(95%-conf.int.', '0.01905', '-', '0.01905)']
['system', 'ROUGE-S*', 'Average_F:', '0.00577', '(95%-conf.int.', '0.00577', '-', '0.00577)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:210', 'F:4']
['Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.']
['The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.', u'Daum\xb4e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.', 'This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.', 'Within this framework  we use features intended to capture degree of generality  including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.', u'The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the \u201cflattened\u201d variant described in section 3.2.']
['system', 'ROUGE-S*', 'Average_R:', '0.03636', '(95%-conf.int.', '0.03636', '-', '0.03636)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.07018', '(95%-conf.int.', '0.07018', '-', '0.07018)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:78', 'F:78']
['To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.']
['The logistic function  whose outputs are in [0  1]  forces pp(s  t) ', 'For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.']
['system', 'ROUGE-S*', 'Average_R:', '0.00308', '(95%-conf.int.', '0.00308', '-', '0.00308)']
['system', 'ROUGE-S*', 'Average_P:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00449', '(95%-conf.int.', '0.00449', '-', '0.00449)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:325', 'P:120', 'F:1']
['We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.']
['The 2nd block contains the IR system  which was tuned by selecting text in multiples of the size of the EMEA training corpus  according to dev set performance.', 'The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.', u'Daum\xb4e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.', 'This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.', u'The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the \u201cflattened\u201d variant described in section 3.2.']
['system', 'ROUGE-S*', 'Average_R:', '0.00176', '(95%-conf.int.', '0.00176', '-', '0.00176)']
['system', 'ROUGE-S*', 'Average_P:', '0.08889', '(95%-conf.int.', '0.08889', '-', '0.08889)']
['system', 'ROUGE-S*', 'Average_F:', '0.00344', '(95%-conf.int.', '0.00344', '-', '0.00344)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:45', 'F:4']
['We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.']
['The matching sentence pairs are then added to the IN corpus  and the system is re-trained.', 'The logistic function  whose outputs are in [0  1]  forces pp(s  t) ', 'The corpora for both settings are summarized in table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00585', '(95%-conf.int.', '0.00585', '-', '0.00585)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00926', '(95%-conf.int.', '0.00926', '-', '0.00926)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:171', 'P:45', 'F:1']
['In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.']
['Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.', u'For comparison to information-retrieval inspired baselines  eg (L\xa8u et al.  2007)  we select sentences from OUT using language model perplexities from IN.', 'For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.', u'Recent work by Finkel and Manning (2009) which re-casts Daum\xb4e\u2019s approach in a hierarchical MAP framework may be applicable to this problem.', 'This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00043', '(95%-conf.int.', '0.00043', '-', '0.00043)']
['system', 'ROUGE-S*', 'Average_P:', '0.00476', '(95%-conf.int.', '0.00476', '-', '0.00476)']
['system', 'ROUGE-S*', 'Average_F:', '0.00078', '(95%-conf.int.', '0.00078', '-', '0.00078)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:210', 'F:1']
0.178797140303 0.00788714274447 0.0146071426485





input/ref/Task1/P11-1060_sweta.csv
input/res/Task1/P11-1060.csv
parsing: input/ref/Task1/P11-1060_sweta.csv
<S sid="8" ssid="4">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
 <S sid="45" ssid="21">The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["45'"]
45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="27">It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent&#8217;s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
 <S sid="166" ssid="51">Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language&#8212;think grounded al., 2010). compositional semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
 <S sid="8" ssid="4">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
 <S sid="11" ssid="7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["11'"]
11'
['11']
parsed_discourse_facet ['method_citation']
  <S sid="115" ssid="91">After training, given a new utterance x, our system outputs the most likely y, summing out the latent logical form z: argmaxy p&#952;(T)(y  |x, z &#8712; &#732;ZL,&#952;(T)).</S>
original cit marker offset is 0
new cit marker offset is 0



["115'"]
115'
['115']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="70">We now turn to the task of mapping natural language For the example in Figure 4(b), the de- utterances to DCS trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["94'"]
94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="157" ssid="42">Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["157'"]
157'
['157']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["106'"]
106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="52">In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="23">Next, we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S>
original cit marker offset is 0
new cit marker offset is 0



["138'"]
138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="16">The CSP has two types of constraints: (i) x &#8712; w(p) for each node x labeled with predicate p &#8712; P; and (ii) xj = yj0 (the j-th component of x must equal the j'-th component of y) for each edge (x, y) labeled with j0j &#8712; R. A solution to the CSP is an assignment of nodes to values that satisfies all the constraints.</S>
original cit marker offset is 0
new cit marker offset is 0



["40'"]
40'
['40']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["171'"]
171'
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="158" ssid = "43">5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation  DCS  which offers a new perspective lexical semantics.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="23" ssid = "19">We trained our model using an EM-like algorithm (Section 3) on two benchmarks  GEO and JOBS (Section 4).</S><S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'158'", "'21'", "'23'", "'132'"]
'26'
'158'
'21'
'23'
'132'
['26', '158', '21', '23', '132']
parsed_discourse_facet ['results_citation']
<S sid ="39" ssid = "15">Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.</S><S sid ="56" ssid = "32">The basic version of DCS described thus far handles a core subset of language.</S><S sid ="4" ssid = "4">On two stansemantic parsing benchmarks our system obtains the highest published accuracies  despite requiring no annotated logical forms.</S><S sid ="136" ssid = "21">In fact  DCS performs comparably to even the version of SEMRESP trained using logical forms.</S><S sid ="83" ssid = "59">It suffices to define Xi(d) for a single column i.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'56'", "'4'", "'136'", "'83'"]
'39'
'56'
'4'
'136'
'83'
['39', '56', '4', '136', '83']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S><S sid ="148" ssid = "33">This bootstrapping behavior occurs naturally: The “easy” examples are processed first  where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'100'", "'115'", "'88'", "'148'"]
'26'
'100'
'115'
'88'
'148'
['26', '100', '115', '88', '148']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="138" ssid = "23">Next  we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="53" ssid = "29">Having instantiated s as a value  everything above this node is an ordinary CSP: s constrains the count node  which in turns constrains the root node to |s|.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'138'", "'26'", "'53'"]
'100'
'21'
'138'
'26'
'53'
['100', '21', '138', '26', '53']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S><S sid ="94" ssid = "70">We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'94'", "'134'", "'8'", "'58'"]
'18'
'94'
'134'
'8'
'58'
['18', '94', '134', '8', '58']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'21'", "'100'", "'115'", "'42'"]
'26'
'21'
'100'
'115'
'42'
['26', '21', '100', '115', '42']
parsed_discourse_facet ['method_citation']
<S sid ="87" ssid = "63">For example  in Figure 4(a)  before execution  the denotation of the DCS tree is hh{[(CA  OR)  (OR)] ... }; ø; (E  Qhstatei�w  ø)ii; after applying X1  we have hh{[(OR)]  ... }; øii.</S><S sid ="113" ssid = "89">Formally  let ˜O(θ  θ') be the objective function O(θ) with ZL(x) ˜ZL θI(x).</S><S sid ="20" ssid = "16">At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="74" ssid = "50">Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'", "'113'", "'20'", "'154'", "'74'"]
'87'
'113'
'20'
'154'
'74'
['87', '113', '20', '154', '74']
parsed_discourse_facet ['method_citation']
<S sid ="117" ssid = "2">In each dataset  each sentence x is annotated with a Prolog logical form  which we use only to evaluate and get an answer y.</S><S sid ="1" ssid = "1">Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'117'", "'1'", "'15'", "'88'", "'62'"]
'117'
'1'
'15'
'88'
'62'
['117', '1', '15', '88', '62']
parsed_discourse_facet ['method_citation']
<S sid ="83" ssid = "59">It suffices to define Xi(d) for a single column i.</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="101" ssid = "77">Formally  pθ(z  |x) ∝ eφ(x z)Tθ  where θ and φ(x  z) are parameter and feature vectors  respectively.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'169'", "'8'", "'101'", "'134'"]
'83'
'169'
'8'
'101'
'134'
['83', '169', '8', '101', '134']
parsed_discourse_facet ['method_citation']
<S sid ="1" ssid = "1">Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid ="99" ssid = "75">To further reduce the search space  F imposes a few additional constraints  e.g.  limiting the number of marked nodes to 2 and only allowing trace predicates between arity 1 predicates.</S><S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S><S sid ="129" ssid = "14">We also define an augmented lexicon L+ which includes a prototype word x for each predicate appearing in (iii) above (e.g.  (large  size))  which cancels the predicates triggered by x’s POS tag.</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'99'", "'132'", "'129'", "'26'"]
'1'
'99'
'132'
'129'
'26'
['1', '99', '132', '129', '26']
parsed_discourse_facet ['method_citation']
<S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="54" ssid = "30">A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S><S sid ="166" ssid = "51">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'54'", "'42'", "'166'", "'132'"]
'21'
'54'
'42'
'166'
'132'
['21', '54', '42', '166', '132']
parsed_discourse_facet ['method_citation']
<S sid ="59" ssid = "35">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S><S sid ="54" ssid = "30">A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="45" ssid = "21">The logical forms in DCS are called DCS trees  where nodes are labeled with predicates  and edges are labeled with relations.</S><S sid ="43" ssid = "19">Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter  2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'54'", "'154'", "'45'", "'43'"]
'59'
'54'
'154'
'45'
'43'
['59', '54', '154', '45', '43']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "30">A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="74" ssid = "50">Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S><S sid ="107" ssid = "83">Our model is arc-factored  so we can sum over all DCS trees in ZL(x) using dynamic programming.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'42'", "'21'", "'74'", "'107'"]
'54'
'42'
'21'
'74'
'107'
['54', '42', '21', '74', '107']
parsed_discourse_facet ['results_citation']
<S sid ="59" ssid = "35">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S><S sid ="148" ssid = "33">This bootstrapping behavior occurs naturally: The “easy” examples are processed first  where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S><S sid ="47" ssid = "23">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.</S><S sid ="103" ssid = "79">To define the features  we technically need to augment each tree z ∈ ZL(x) with alignment information—namely  for each predicate in z  the span in x (if any) that triggered it.</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'148'", "'47'", "'103'", "'62'"]
'59'
'148'
'47'
'103'
'62'
['59', '148', '47', '103', '62']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="23" ssid = "19">We trained our model using an EM-like algorithm (Section 3) on two benchmarks  GEO and JOBS (Section 4).</S><S sid ="86" ssid = "62">Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  −(i  ø)]{α1 = ø}.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'23'", "'86'", "'115'", "'100'"]
'26'
'23'
'86'
'115'
'100'
['26', '23', '86', '115', '100']
parsed_discourse_facet ['method_citation']
<S sid ="113" ssid = "89">Formally  let ˜O(θ  θ') be the objective function O(θ) with ZL(x) ˜ZL θI(x).</S><S sid ="78" ssid = "54">The stores are also concatenated (α + α').</S><S sid ="74" ssid = "50">Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="111" ssid = "87">Let ˜ZL θ(x) be this approximation of ZL(x).</S>
original cit marker offset is 0
new cit marker offset is 0



["'113'", "'78'", "'74'", "'154'", "'111'"]
'113'
'78'
'74'
'154'
'111'
['113', '78', '74', '154', '111']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="63" ssid = "39">Think of d as consisting of n columns  one for each active node according to a pre-order traversal of z.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="86" ssid = "62">Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  −(i  ø)]{α1 = ø}.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'100'", "'63'", "'115'", "'86'"]
'26'
'100'
'63'
'115'
'86'
['26', '100', '63', '115', '86']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'26'", "'115'", "'42'"]
'100'
'21'
'26'
'115'
'42'
['100', '21', '26', '115', '42']
parsed_discourse_facet ['method_citation']
<S sid ="151" ssid = "36">Inspecting the final parameters calculus formulae.</S><S sid ="56" ssid = "32">The basic version of DCS described thus far handles a core subset of language.</S><S sid ="16" ssid = "12">Which one should we use?</S><S sid ="118" ssid = "3">This evaluation is done with respect to a world w. Recall that a world w maps each predicate p ∈ P to a set of tuples w(p).</S><S sid ="33" ssid = "9">As another example  w(average) = {(S  ¯x) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'56'", "'16'", "'118'", "'33'"]
'151'
'56'
'16'
'118'
'33'
['151', '56', '16', '118', '33']
parsed_discourse_facet ['aim_citation', 'results_citation']
['Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem.']
['This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.', 'Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.', 'The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).', u'This bootstrapping behavior occurs naturally: The \u201ceasy\u201d examples are processed first  where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.', u'To define the features  we technically need to augment each tree z \u2208 ZL(x) with alignment information\u2014namely  for each predicate in z  the span in x (if any) that triggered it.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:120', 'F:0']
['It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent&#8217;s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.']
['We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.', 'CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).', 'On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.', 'We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.', 'In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00581', '(95%-conf.int.', '0.00581', '-', '0.00581)']
['system', 'ROUGE-S*', 'Average_P:', '0.01970', '(95%-conf.int.', '0.01970', '-', '0.01970)']
['system', 'ROUGE-S*', 'Average_F:', '0.00897', '(95%-conf.int.', '0.00897', '-', '0.00897)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:406', 'F:8']
['Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.']
['Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.', 'To further reduce the search space  F imposes a few additional constraints  e.g.  limiting the number of marked nodes to 2 and only allowing trace predicates between arity 1 predicates.', 'We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.', u'We also define an augmented lexicon L+ which includes a prototype word x for each predicate appearing in (iii) above (e.g.  (large  size))  which cancels the predicates triggered by x\xe2\u20ac\u2122s POS tag.', 'Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.03162', '(95%-conf.int.', '0.03162', '-', '0.03162)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.06130', '(95%-conf.int.', '0.06130', '-', '0.06130)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3321', 'P:105', 'F:105']
['Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.']
['Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.', 'In each dataset  each sentence x is annotated with a Prolog logical form  which we use only to evaluate and get an answer y.', 'Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.', 'Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.', 'Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:55', 'F:0']
['This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.']
['Inspecting the final parameters calculus formulae.', u'As another example  w(average) = {(S  \xafx) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.', 'The basic version of DCS described thus far handles a core subset of language.', u'This evaluation is done with respect to a world w. Recall that a world w maps each predicate p \u2208 P to a set of tuples w(p).', 'Which one should we use?']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:190', 'F:0']
["The CSP has two types of constraints: (i) x &#8712; w(p) for each node x labeled with predicate p &#8712; P; and (ii) xj = yj0 (the j-th component of x must equal the j'-th component of y) for each edge (x, y) labeled with j0j &#8712; R. A solution to the CSP is an assignment of nodes to values that satisfies all the constraints."]
[u'After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy p\u03b8(T)(y  |x  z \u2208 \u02dcZL \u03b8(T)).', u'Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.', 'We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.', 'The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).', 'The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:300', 'F:0']
['After training, given a new utterance x, our system outputs the most likely y, summing out the latent logical form z: argmaxy p&#952;(T)(y  |x, z &#8712; &#732;ZL,&#952;(T)).']
['In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.', 'It suffices to define Xi(d) for a single column i.', u'Formally  p\u03b8(z  |x) \u221d e\u03c6(x z)T\u03b8  where \u03b8 and \u03c6(x  z) are parameter and feature vectors  respectively.', 'The combination rules are encoded in the tems  despite using no annotated logical forms.', 'On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.']
['system', 'ROUGE-S*', 'Average_R:', '0.00135', '(95%-conf.int.', '0.00135', '-', '0.00135)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00240', '(95%-conf.int.', '0.00240', '-', '0.00240)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:91', 'F:1']
['On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.']
[u'Extending this notation to denotations  let (hA; \u03b1ii[i] = hh{ai : a \u2208 A}; \u03b1iii.', 'At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.', u'For example  in Figure 4(a)  before execution  the denotation of the DCS tree is hh{[(CA  OR)  (OR)] ... }; \xf8; (E  Qhstatei\ufffdw  \xf8)ii; after applying X1  we have hh{[(OR)]  ... }; \xf8ii.', u"Formally  let \u02dcO(\u03b8  \u03b8') be the objective function O(\u03b8) with ZL(x) \u02dcZL \u03b8I(x).", 'The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:105', 'F:0']
['In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries.']
[u'Extending this notation to denotations  let (hA; \u03b1ii[i] = hh{ai : a \u2208 A}; \u03b1iii.', u'Let \u02dcZL \u03b8(x) be this approximation of ZL(x).', 'The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).', u"Formally  let \u02dcO(\u03b8  \u03b8') be the objective function O(\u03b8) with ZL(x) \u02dcZL \u03b8I(x).", u"The stores are also concatenated (\u03b1 + \u03b1')."]
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:406', 'P:45', 'F:0']
['On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.']
['5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation  DCS  which offers a new perspective lexical semantics.', 'We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.', 'Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.', 'The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).', 'We trained our model using an EM-like algorithm (Section 3) on two benchmarks  GEO and JOBS (Section 4).']
['system', 'ROUGE-S*', 'Average_R:', '0.00340', '(95%-conf.int.', '0.00340', '-', '0.00340)']
['system', 'ROUGE-S*', 'Average_P:', '0.10476', '(95%-conf.int.', '0.10476', '-', '0.10476)']
['system', 'ROUGE-S*', 'Average_F:', '0.00658', '(95%-conf.int.', '0.00658', '-', '0.00658)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3240', 'P:105', 'F:11']
['Next, we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).']
[u'After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy p\u03b8(T)(y  |x  z \u2208 \u02dcZL \u03b8(T)).', 'We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.', 'Think of d as consisting of n columns  one for each active node according to a pre-order traversal of z.', u'Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  \u2212(i  \xf8)]{\u03b11 = \xf8}.', u'Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.']
['system', 'ROUGE-S*', 'Average_R:', '0.00157', '(95%-conf.int.', '0.00157', '-', '0.00157)']
['system', 'ROUGE-S*', 'Average_P:', '0.02198', '(95%-conf.int.', '0.02198', '-', '0.02198)']
['system', 'ROUGE-S*', 'Average_F:', '0.00293', '(95%-conf.int.', '0.00293', '-', '0.00293)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:91', 'F:2']
['Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language&#8212;think grounded al., 2010). compositional semantics.']
[u'Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.', u'After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy p\u03b8(T)(y  |x  z \u2208 \u02dcZL \u03b8(T)).', 'We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.', 'The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).', 'The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00328', '(95%-conf.int.', '0.00328', '-', '0.00328)']
['system', 'ROUGE-S*', 'Average_P:', '0.02372', '(95%-conf.int.', '0.02372', '-', '0.02372)']
['system', 'ROUGE-S*', 'Average_F:', '0.00576', '(95%-conf.int.', '0.00576', '-', '0.00576)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:253', 'F:6']
0.0984291658464 0.00391916663401 0.00732833327226





input/ref/Task1/W06-3114_aakansha.csv
input/res/Task1/W06-3114.csv
parsing: input/ref/Task1/W06-3114_aakansha.csv
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="2">Training and testing is based on the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="2">Training and testing is based on the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'"]
'102'
['102']
parsed_discourse_facet ['method_citation']
<S sid="84" ssid="23">The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'"]
'84'
['84']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="4">To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="19">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="6">Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="3">&#8226; We evaluated translation from English, in addition to into English.</S>
    <S sid="6" ssid="4">English was again paired with German, French, and Spanish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'6'"]
'5'
'6'
['5', '6']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="80" ssid = "19">We collected around 300–400 judgements per judgement type (adequacy or fluency)  per system  per language pair.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'126'", "'80'", "'136'", "'0'"]
'134'
'126'
'80'
'136'
'0'
['134', '126', '80', '136', '0']
parsed_discourse_facet ['results_citation']
<S sid ="159" ssid = "52">(b) does the translation have the same meaning  including connotations?</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'159'", "'0'", "'84'", "'17'", "'170'"]
'159'
'0'
'84'
'17'
'170'
['159', '0', '84', '17', '170']
parsed_discourse_facet ['method_citation']
<S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'", "'18'", "'16'", "'126'", "'84'"]
'170'
'18'
'16'
'126'
'84'
['170', '18', '16', '126', '84']
parsed_discourse_facet ['method_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'17'", "'170'", "'84'", "'38'"]
'126'
'17'
'170'
'84'
'38'
['126', '17', '170', '84', '38']
parsed_discourse_facet ['method_citation']
<S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'", "'38'", "'126'", "'17'", "'84'"]
'170'
'38'
'126'
'17'
'84'
['170', '38', '126', '17', '84']
parsed_discourse_facet ['method_citation']
<S sid ="52" ssid = "18">Pairwise comparison: We can use the same method to assess the statistical significance of one system outperforming another.</S><S sid ="21" ssid = "14">The out-of-domain test set differs from the Europarl data in various ways.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="129" ssid = "22">All systems (except for Systran  which was not tuned to Europarl) did considerably worse on outof-domain training data.</S><S sid ="35" ssid = "1">For the automatic evaluation  we used BLEU  since it is the most established metric in the field.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'", "'21'", "'28'", "'129'", "'35'"]
'52'
'21'
'28'
'129'
'35'
['52', '21', '28', '129', '35']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="113" ssid = "6">The confidence intervals are computed by bootstrap resampling for BLEU  and by standard significance testing for the manual scores  as described earlier in the paper.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'113'", "'126'", "'28'", "'0'"]
'16'
'113'
'126'
'28'
'0'
['16', '113', '126', '28', '0']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="109" ssid = "2">The scores and confidence intervals are detailed first in the Figures 7–10 in table form (including ranks)  and then in graphical form in Figures 11–16.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'16'", "'38'", "'84'", "'109'"]
'18'
'16'
'38'
'84'
'109'
['18', '16', '38', '84', '109']
parsed_discourse_facet ['method_citation']
<S sid ="9" ssid = "2">Training and testing is based on the Europarl corpus.</S><S sid ="44" ssid = "10">We computed BLEU scores for each submission with a single reference translation.</S><S sid ="143" ssid = "36">For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.</S><S sid ="25" ssid = "18">We received submissions from 14 groups from 11 institutions  as listed in Figure 2.</S><S sid ="35" ssid = "1">For the automatic evaluation  we used BLEU  since it is the most established metric in the field.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'44'", "'143'", "'25'", "'35'"]
'9'
'44'
'143'
'25'
'35'
['9', '44', '143', '25', '35']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="68" ssid = "7">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'68'", "'0'", "'28'", "'136'"]
'134'
'68'
'0'
'28'
'136'
['134', '68', '0', '28', '136']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="68" ssid = "7">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'0'", "'68'", "'126'", "'17'"]
'134'
'0'
'68'
'126'
'17'
['134', '0', '68', '126', '17']
parsed_discourse_facet ['method_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="159" ssid = "52">(b) does the translation have the same meaning  including connotations?</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'134'", "'17'", "'84'", "'159'"]
'126'
'134'
'17'
'84'
'159'
['126', '134', '17', '84', '159']
parsed_discourse_facet ['method_citation']
<S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'38'", "'155'", "'170'", "'6'"]
'84'
'38'
'155'
'170'
'6'
['84', '38', '155', '170', '6']
parsed_discourse_facet ['results_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'134'", "'0'", "'17'", "'84'"]
'126'
'134'
'0'
'17'
'84'
['126', '134', '0', '17', '84']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="68" ssid = "7">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'0'", "'126'", "'68'", "'17'"]
'134'
'0'
'126'
'68'
'17'
['134', '0', '126', '68', '17']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'170'", "'84'", "'126'", "'17'"]
'0'
'170'
'84'
'126'
'17'
['0', '170', '84', '126', '17']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "19">We collected around 300–400 judgements per judgement type (adequacy or fluency)  per system  per language pair.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="150" ssid = "43">For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French)  at least as measured by BLEU.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'84'", "'150'", "'16'", "'155'"]
'80'
'84'
'150'
'16'
'155'
['80', '84', '150', '16', '155']
parsed_discourse_facet ['method_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'84'", "'38'", "'17'", "'170'"]
'126'
'84'
'38'
'17'
'170'
['126', '84', '38', '17', '170']
parsed_discourse_facet ['method_citation']
['&#8226; We evaluated translation from English, in addition to into English.', 'English was again paired with German, French, and Spanish.']
['The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00292', '(95%-conf.int.', '0.00292', '-', '0.00292)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.00566', '(95%-conf.int.', '0.00566', '-', '0.00566)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:55', 'F:5']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['Manual and Automatic Evaluation of Machine Translation between European Languages', u'We collected around 300\xe2\u20ac\u201c400 judgements per judgement type (adequacy or fluency)  per system  per language pair.', 'The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'Again  we can compute average scores for all systems for the different language pairs (Figure 6).']
['system', 'ROUGE-S*', 'Average_R:', '0.01886', '(95%-conf.int.', '0.01886', '-', '0.01886)']
['system', 'ROUGE-S*', 'Average_P:', '0.50909', '(95%-conf.int.', '0.50909', '-', '0.50909)']
['system', 'ROUGE-S*', 'Average_F:', '0.03636', '(95%-conf.int.', '0.03636', '-', '0.03636)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:55', 'F:28']
['We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.']
['Training and testing is based on the Europarl corpus.', 'We received submissions from 14 groups from 11 institutions  as listed in Figure 2.', 'We computed BLEU scores for each submission with a single reference translation.', 'For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.', 'For the automatic evaluation  we used BLEU  since it is the most established metric in the field.']
['system', 'ROUGE-S*', 'Average_R:', '0.00504', '(95%-conf.int.', '0.00504', '-', '0.00504)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00892', '(95%-conf.int.', '0.00892', '-', '0.00892)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:78', 'F:3']
['Training and testing is based on the Europarl corpus.']
['In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.', 'The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00376', '(95%-conf.int.', '0.00376', '-', '0.00376)']
['system', 'ROUGE-S*', 'Average_P:', '0.60000', '(95%-conf.int.', '0.60000', '-', '0.60000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00747', '(95%-conf.int.', '0.00747', '-', '0.00747)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:10', 'F:6']
['The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.']
['The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00409', '(95%-conf.int.', '0.00409', '-', '0.00409)']
['system', 'ROUGE-S*', 'Average_P:', '0.08974', '(95%-conf.int.', '0.08974', '-', '0.08974)']
['system', 'ROUGE-S*', 'Average_F:', '0.00783', '(95%-conf.int.', '0.00783', '-', '0.00783)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:78', 'F:7']
['We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.']
['English was again paired with German  French  and Spanish.', 'The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).', 'For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:78', 'F:0']
['Training and testing is based on the Europarl corpus.']
['The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00175', '(95%-conf.int.', '0.00175', '-', '0.00175)']
['system', 'ROUGE-S*', 'Average_P:', '0.30000', '(95%-conf.int.', '0.30000', '-', '0.30000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00349', '(95%-conf.int.', '0.00349', '-', '0.00349)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:10', 'F:3']
['To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources.']
['(b) does the translation have the same meaning  including connotations?', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'Again  we can compute average scores for all systems for the different language pairs (Figure 6).']
['system', 'ROUGE-S*', 'Average_R:', '0.00332', '(95%-conf.int.', '0.00332', '-', '0.00332)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00626', '(95%-conf.int.', '0.00626', '-', '0.00626)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:55', 'F:3']
['Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary.']
[u'We asked participants to each judge 200\xe2\u20ac\u201c300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.', 'Manual and Automatic Evaluation of Machine Translation between European Languages', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'Again  we can compute average scores for all systems for the different language pairs (Figure 6).']
['system', 'ROUGE-S*', 'Average_R:', '0.00078', '(95%-conf.int.', '0.00078', '-', '0.00078)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00152', '(95%-conf.int.', '0.00152', '-', '0.00152)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:45', 'F:1']
['The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.']
['Manual and Automatic Evaluation of Machine Translation between European Languages', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:45', 'F:0']
['The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:']
[u'We asked participants to each judge 200\xe2\u20ac\u201c300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.', 'Manual and Automatic Evaluation of Machine Translation between European Languages', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'Again  we can compute average scores for all systems for the different language pairs (Figure 6).']
['system', 'ROUGE-S*', 'Average_R:', '0.00157', '(95%-conf.int.', '0.00157', '-', '0.00157)']
['system', 'ROUGE-S*', 'Average_P:', '0.07143', '(95%-conf.int.', '0.07143', '-', '0.07143)']
['system', 'ROUGE-S*', 'Average_F:', '0.00307', '(95%-conf.int.', '0.00307', '-', '0.00307)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:28', 'F:2']
['The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.']
['(b) does the translation have the same meaning  including connotations?', 'Manual and Automatic Evaluation of Machine Translation between European Languages', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:45', 'F:0']
['Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.']
['For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French)  at least as measured by BLEU.', u'We collected around 300\xe2\u20ac\u201c400 judgements per judgement type (adequacy or fluency)  per system  per language pair.', 'For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00280', '(95%-conf.int.', '0.00280', '-', '0.00280)']
['system', 'ROUGE-S*', 'Average_P:', '0.06061', '(95%-conf.int.', '0.06061', '-', '0.06061)']
['system', 'ROUGE-S*', 'Average_F:', '0.00534', '(95%-conf.int.', '0.00534', '-', '0.00534)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:66', 'F:4']
0.141308460451 0.00345307689651 0.00660923071839





input/ref/Task1/P11-1061_sweta.csv
input/res/Task1/P11-1061.csv
parsing: input/ref/Task1/P11-1061_sweta.csv
 <S sid="9" ssid="5">Unfortunately, the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al., 2010), making its practical usability questionable at best.</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="47" ssid="13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["47'"]
47'
['47']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["10'"]
10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="1">Given the bilingual graph described in the previous section, we can use label propagation to project the English POS labels to the foreign language.</S>
original cit marker offset is 0
new cit marker offset is 0



["70'"]
70'
['70']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="18">This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
 <S sid="83" ssid="14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value &#964;: We describe how we choose &#964; in &#167;6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["83'"]
83'
['83']
parsed_discourse_facet ['method_citation']
<S sid="113" ssid="13">For each language under consideration, Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["113'"]
113'
['113']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
 <S sid="13" ssid="9">(2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available.</S>
original cit marker offset is 0
new cit marker offset is 0



["13'"]
13'
['13']
parsed_discourse_facet ['method_citation']
 <S sid="3" ssid="3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="20">We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Syntactic universals are a well studied concept in linguistics (Carnie, 2002; Newmeyer, 2005), and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["19'"]
19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="16">Figure 2 shows an excerpt of a sentence from the Italian test set and the tags assigned by four different models, as well as the gold tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



["161'"]
161'
['161']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["23'"]
23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["23'"]
23'
['23']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1061.csv
<S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="62" ssid = "28">Note that since all English vertices were extracted from the parallel text  we will have an initial label distribution for all vertices in Ve.</S><S sid ="68" ssid = "34">In the label propagation stage  we propagate the automatic English tags to the aligned Italian trigram types  followed by further propagation solely among the Italian vertices. the Italian vertices are connected to an automatically labeled English vertex.</S><S sid ="42" ssid = "8">The foreign language vertices (denoted by Vf) correspond to foreign trigram types  exactly as in Subramanya et al. (2010).</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'62'", "'68'", "'42'", "'160'"]
'19'
'62'
'68'
'42'
'160'
['19', '62', '68', '42', '160']
parsed_discourse_facet ['results_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid ="30" ssid = "7">To initialize the graph we tag the English side of the parallel text using a supervised model.</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'113'", "'30'", "'102'", "'25'"]
'26'
'113'
'30'
'102'
'25'
['26', '113', '30', '102', '25']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'113'", "'25'", "'3'", "'19'"]
'16'
'113'
'25'
'3'
'19'
['16', '113', '25', '3', '19']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="119" ssid = "19">To provide a thorough analysis  we evaluated three baselines and two oracles in addition to two variants of our graph-based approach.</S><S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="82" ssid = "13">We formulate the update as follows: where ∀ui ∈ Vf \ Vfl  γi(y) and κi are defined as: We ran this procedure for 10 iterations.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'119'", "'54'", "'82'", "'28'"]
'134'
'119'
'54'
'82'
'28'
['134', '119', '54', '82', '28']
parsed_discourse_facet ['method_citation']
<S sid ="71" ssid = "2">We use label propagation in two stages to generate soft labels on all the vertices in the graph.</S><S sid ="61" ssid = "27">These tag distributions are used to initialize the label distributions over the English vertices in the graph.</S><S sid ="3" ssid = "3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S><S sid ="86" ssid = "17">For a sentence x and a state sequence z  a first order Markov model defines a distribution: (9) where Val(X) corresponds to the entire vocabulary.</S><S sid ="124" ssid = "24">We tried two versions of our graph-based approach: feature after the first stage of label propagation (Eq.</S>
original cit marker offset is 0
new cit marker offset is 0



["'71'", "'61'", "'3'", "'86'", "'124'"]
'71'
'61'
'3'
'86'
'124'
['71', '61', '3', '86', '124']
parsed_discourse_facet ['method_citation']
<S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="57" ssid = "23">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'26'", "'57'", "'97'", "'28'"]
'89'
'26'
'57'
'97'
'28'
['89', '26', '57', '97', '28']
parsed_discourse_facet ['method_citation']
<S sid ="93" ssid = "24">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'93'", "'26'", "'28'", "'97'", "'89'"]
'93'
'26'
'28'
'97'
'89'
['93', '26', '28', '97', '89']
parsed_discourse_facet ['method_citation']
<S sid ="92" ssid = "23">To optimize this function  we used L-BFGS  a quasi-Newton method (Liu and Nocedal  1989).</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="19" ssid = "15">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid ="49" ssid = "15">We define a symmetric similarity function K(uZ7 uj) over two foreign language vertices uZ7 uj E Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1.</S><S sid ="142" ssid = "5">The “No LP” model does not outperform direct projection for German and Greek  but performs better for six out of eight languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'102'", "'19'", "'49'", "'142'"]
'92'
'102'
'19'
'49'
'142'
['92', '102', '19', '49', '142']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="159" ssid = "2">Because we are interested in applying our techniques to languages for which no labeled resources are available  we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'159'", "'97'", "'89'", "'28'"]
'26'
'159'
'97'
'89'
'28'
['26', '159', '97', '89', '28']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'97'", "'102'", "'28'", "'83'"]
'134'
'97'
'102'
'28'
'83'
['134', '97', '102', '28', '83']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="72" ssid = "3">In the first stage  we run a single step of label propagation  which transfers the label distributions from the English vertices to the connected foreign language vertices (say  Vf�) at the periphery of the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'97'", "'28'", "'102'", "'72'"]
'26'
'97'
'28'
'102'
'72'
['26', '97', '28', '102', '72']
parsed_discourse_facet ['method_citation']
<S sid ="95" ssid = "26">This feature ft incorporates information from the smoothed graph and prunes hidden states that are inconsistent with the thresholded vector tx.</S><S sid ="15" ssid = "11">First  we use a novel graph-based framework for projecting syntactic information across language boundaries.</S><S sid ="25" ssid = "2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid ="41" ssid = "7">Because the information flow in our graph is asymmetric (from English to the foreign language)  we use different types of vertices for each language.</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'15'", "'25'", "'41'", "'113'"]
'95'
'15'
'25'
'41'
'113'
['95', '15', '25', '41', '113']
parsed_discourse_facet ['method_citation']
<S sid ="21" ssid = "17">These universal POS categories not only facilitate the transfer of POS information from one language to another  but also relieve us from using controversial evaluation metrics 2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S><S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="10" ssid = "6">To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S><S sid ="36" ssid = "2">Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand  using individual words as the vertices throws away the context necessary for disambiguation; on the other hand  it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'16'", "'10'", "'29'", "'36'"]
'21'
'16'
'10'
'29'
'36'
['21', '16', '10', '29', '36']
parsed_discourse_facet ['results_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="54" ssid = "20">Given this similarity function  we define a nearest neighbor graph  where the edge weight for the n most similar vertices is set to the value of the similarity function and to 0 for all other vertices.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S><S sid ="35" ssid = "1">In graph-based learning approaches one constructs a graph whose vertices are labeled and unlabeled examples  and whose weighted edges encode the degree to which the examples they link have the same label (Zhu et al.  2003).</S><S sid ="159" ssid = "2">Because we are interested in applying our techniques to languages for which no labeled resources are available  we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'54'", "'89'", "'35'", "'159'"]
'26'
'54'
'89'
'35'
'159'
['26', '54', '89', '35', '159']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'28'", "'134'", "'97'", "'89'"]
'26'
'28'
'134'
'97'
'89'
['26', '28', '134', '97', '89']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "12">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid ="21" ssid = "17">These universal POS categories not only facilitate the transfer of POS information from one language to another  but also relieve us from using controversial evaluation metrics 2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S><S sid ="160" ssid = "3">Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.</S><S sid ="113" ssid = "13">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid ="29" ssid = "6">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'21'", "'160'", "'113'", "'29'"]
'16'
'21'
'160'
'113'
'29'
['16', '21', '160', '113', '29']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "34">Because we don’t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.</S><S sid ="28" ssid = "5">The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (§3.2).</S><S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="26" ssid = "3">As discussed in more detail in §3  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.</S><S sid ="89" ssid = "20">The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'28'", "'97'", "'26'", "'89'"]
'134'
'28'
'97'
'26'
'89'
['134', '28', '97', '26', '89']
parsed_discourse_facet ['method_citation']
<S sid ="97" ssid = "28">This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.</S><S sid ="102" ssid = "2">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid ="49" ssid = "15">We define a symmetric similarity function K(uZ7 uj) over two foreign language vertices uZ7 uj E Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1.</S><S sid ="136" ssid = "36">For graph propagation  the hyperparameter v was set to 2 x 10−6 and was not tuned.</S><S sid ="83" ssid = "14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value τ: We describe how we choose τ in §6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'102'", "'49'", "'136'", "'83'"]
'97'
'102'
'49'
'136'
'83'
['97', '102', '49', '136', '83']
parsed_discourse_facet ['method_citation']
['To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).']
[u'As discussed in more detail in \xa73  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', 'The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.', u'The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (\xa73.2).', 'This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.', u'Because we don\u2019t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4186', 'P:66', 'F:0']
['Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).']
['Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.', 'In the label propagation stage  we propagate the automatic English tags to the aligned Italian trigram types  followed by further propagation solely among the Italian vertices. the Italian vertices are connected to an automatically labeled English vertex.', 'Note that since all English vertices were extracted from the parallel text  we will have an initial label distribution for all vertices in Ve.', 'The foreign language vertices (denoted by Vf) correspond to foreign trigram types  exactly as in Subramanya et al. (2010).', 'Our results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data  but have translations into a resource-rich language.']
['system', 'ROUGE-S*', 'Average_R:', '0.00982', '(95%-conf.int.', '0.00982', '-', '0.00982)']
['system', 'ROUGE-S*', 'Average_P:', '0.50909', '(95%-conf.int.', '0.50909', '-', '0.50909)']
['system', 'ROUGE-S*', 'Average_F:', '0.01928', '(95%-conf.int.', '0.01928', '-', '0.01928)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:55', 'F:28']
['Syntactic universals are a well studied concept in linguistics (Carnie, 2002; Newmeyer, 2005), and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.']
['To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.', 'Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand  using individual words as the vertices throws away the context necessary for disambiguation; on the other hand  it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences.', 'These universal POS categories not only facilitate the transfer of POS information from one language to another  but also relieve us from using controversial evaluation metrics 2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.', u'To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (\u0e22\u0e073.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.', u'To this end  we construct a bilingual graph over word types to establish a connection between the two languages (\u0e22\u0e073)  and then use graph label propagation to project syntactic information from English to the foreign language (\u0e22\u0e074).']
['system', 'ROUGE-S*', 'Average_R:', '0.00012', '(95%-conf.int.', '0.00012', '-', '0.00012)']
['system', 'ROUGE-S*', 'Average_P:', '0.00654', '(95%-conf.int.', '0.00654', '-', '0.00654)']
['system', 'ROUGE-S*', 'Average_F:', '0.00025', '(95%-conf.int.', '0.00025', '-', '0.00025)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:8001', 'P:153', 'F:1']
['(2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available.']
['Because we are interested in applying our techniques to languages for which no labeled resources are available  we paid particular attention to minimize the number of free parameters and used the same hyperparameters for all language pairs.', u'As discussed in more detail in \u0e22\u0e073  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', 'The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.', u'The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (\u0e22\u0e073.2).', 'This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4095', 'P:55', 'F:0']
['Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.']
['First  we use a novel graph-based framework for projecting syntactic information across language boundaries.', 'Because the information flow in our graph is asymmetric (from English to the foreign language)  we use different types of vertices for each language.', 'This feature ft incorporates information from the smoothed graph and prunes hidden states that are inconsistent with the thresholded vector tx.', 'For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00106', '(95%-conf.int.', '0.00106', '-', '0.00106)']
['system', 'ROUGE-S*', 'Average_P:', '0.01905', '(95%-conf.int.', '0.01905', '-', '0.01905)']
['system', 'ROUGE-S*', 'Average_F:', '0.00200', '(95%-conf.int.', '0.00200', '-', '0.00200)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:105', 'F:2']
['We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).']
['We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.', u'We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value \u03c4: We describe how we choose \u03c4 in \xa76.4.', u'The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (\xa73.2).', 'This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.', u'Because we don\u2019t have a separate development set  we used the training set to select among them and found 0.2 to work slightly better than 0.1 and 0.3.']
['system', 'ROUGE-S*', 'Average_R:', '0.00072', '(95%-conf.int.', '0.00072', '-', '0.00072)']
['system', 'ROUGE-S*', 'Average_P:', '0.01471', '(95%-conf.int.', '0.01471', '-', '0.01471)']
['system', 'ROUGE-S*', 'Average_F:', '0.00137', '(95%-conf.int.', '0.00137', '-', '0.00137)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2775', 'P:136', 'F:2']
['We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value &#964;: We describe how we choose &#964; in &#167;6.4.']
['We tried two versions of our graph-based approach: feature after the first stage of label propagation (Eq.', 'These tag distributions are used to initialize the label distributions over the English vertices in the graph.', 'We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).', 'For a sentence x and a state sequence z  a first order Markov model defines a distribution: (9) where Val(X) corresponds to the entire vocabulary.', 'We use label propagation in two stages to generate soft labels on all the vertices in the graph.']
['system', 'ROUGE-S*', 'Average_R:', '0.00063', '(95%-conf.int.', '0.00063', '-', '0.00063)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00118', '(95%-conf.int.', '0.00118', '-', '0.00118)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:105', 'F:1']
['To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.']
['We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.', u'As discussed in more detail in \u0e22\u0e073  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', 'To initialize the graph we tag the English side of the parallel text using a supervised model.', 'For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.']
['system', 'ROUGE-S*', 'Average_R:', '0.01006', '(95%-conf.int.', '0.01006', '-', '0.01006)']
['system', 'ROUGE-S*', 'Average_P:', '0.04735', '(95%-conf.int.', '0.04735', '-', '0.04735)']
['system', 'ROUGE-S*', 'Average_F:', '0.01659', '(95%-conf.int.', '0.01659', '-', '0.01659)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:528', 'F:25']
['Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).']
['We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.', u'We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value \u03c4: We describe how we choose \u03c4 in \xa76.4.', u'For graph propagation  the hyperparameter v was set to 2 x 10\u22126 and was not tuned.', 'This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.', 'We define a symmetric similarity function K(uZ7 uj) over two foreign language vertices uZ7 uj E Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00362', '(95%-conf.int.', '0.00362', '-', '0.00362)']
['system', 'ROUGE-S*', 'Average_P:', '0.01613', '(95%-conf.int.', '0.01613', '-', '0.01613)']
['system', 'ROUGE-S*', 'Average_F:', '0.00591', '(95%-conf.int.', '0.00591', '-', '0.00591)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:496', 'F:8']
['To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).']
['We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.', 'Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.', 'To optimize this function  we used L-BFGS  a quasi-Newton method (Liu and Nocedal  1989).', u'The \u201cNo LP\u201d model does not outperform direct projection for German and Greek  but performs better for six out of eight languages.', 'We define a symmetric similarity function K(uZ7 uj) over two foreign language vertices uZ7 uj E Vf based on the co-occurrence statistics of the nine feature concepts given in Table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00044', '(95%-conf.int.', '0.00044', '-', '0.00044)']
['system', 'ROUGE-S*', 'Average_P:', '0.01515', '(95%-conf.int.', '0.01515', '-', '0.01515)']
['system', 'ROUGE-S*', 'Average_F:', '0.00085', '(95%-conf.int.', '0.00085', '-', '0.00085)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:66', 'F:1']
['We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).']
['For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.', 'The feature-based model replaces the emission distribution with a log-linear model  such that: on the word identity x  features checking whether x contains digits or hyphens  whether the first letter of x is upper case  and suffix features up to length 3.', u'The edge weights between the foreign language trigrams are computed using a co-occurence based similarity function  designed to indicate how syntactically similar the middle words of the connected trigrams are (\u0e22\u0e073.2).', u'As discussed in more detail in \u0e22\u0e073  we use two types of vertices in our graph: on the foreign language side vertices correspond to trigram types  while the vertices on the English side are individual word types.', 'This formulation of the constraint feature is equivalent to the use of a tagging dictionary extracted from the graph using a threshold T on the posterior distribution of tags for a given word type (Eq.']
['system', 'ROUGE-S*', 'Average_R:', '0.00102', '(95%-conf.int.', '0.00102', '-', '0.00102)']
['system', 'ROUGE-S*', 'Average_P:', '0.04412', '(95%-conf.int.', '0.04412', '-', '0.04412)']
['system', 'ROUGE-S*', 'Average_F:', '0.00199', '(95%-conf.int.', '0.00199', '-', '0.00199)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5886', 'P:136', 'F:6']
0.0619690903457 0.00249909088637 0.00449272723188





input/ref/Task1/E03-1005_aakansha.csv
input/res/Task1/E03-1005.csv
parsing: input/ref/Task1/E03-1005_aakansha.csv
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'"]
'105'
['105']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'"]
'145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'"]
'80'
['80']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'"]
'143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid="141" ssid="6">This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'141'"]
'140'
'141'
['140', '141']
parsed_discourse_facet ['result_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="102" ssid="5">In case the shortest derivation is not unique, Bod (2000b) proposes to back off to a frequency ordering of the subtrees.</S>
    <S sid="103" ssid="6">That is, all subtrees of each root label are assigned a rank according to their frequency in the treebank: the most frequent subtree (or subtrees) of each root label gets rank 1, the second most frequent subtree gets rank 2, etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'", "'103'"]
'102'
'103'
['102', '103']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="46" ssid="43">Most previous notions of best parse tree in DOP1 were based on a probabilistic metric, with Bod (2000b) as a notable exception, who used a simplicity metric based on the shortest derivation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'"]
'46'
['46']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="37">For example, Bod (2001) samples a fixed number of subtrees of each depth, which has the effect of assigning roughly equal weight to each node in the training data, and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S>
    <S sid="86" ssid="38">Bod reports state-of-the-art results with this method, and observes no decrease in parse accuracy when larger subtrees are included (using subtrees up to depth 14).</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'", "'86'"]
'85'
'86'
['85', '86']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="115" ssid="18">The only thing that needs to be changed for Simplicity-DOP is that all subtrees should be assigned equal probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="71" ssid = "23">For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'64'", "'99'", "'71'", "'39'"]
'7'
'64'
'99'
'71'
'39'
['7', '64', '99', '71', '39']
parsed_discourse_facet ['results_citation']
<S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="27" ssid = "24">Goodman (1996  1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set  thus converting the exponential number of subtrees to a compact grammar.</S><S sid ="88" ssid = "40">In this paper  we will test a simple extension of Goodman's compact PCFG-reduction of DOP which has the same property as the normalization proposed in Bod (2001) in that it assigns roughly equal weight to each node in the training data.</S><S sid ="69" ssid = "21">Thus  rather than using the large  explicit DOP1 model  one can also use this small PCFG that generates isomorphic derivations  with identical probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'130'", "'27'", "'88'", "'69'"]
'140'
'130'
'27'
'88'
'69'
['140', '130', '27', '88', '69']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "2">The probability of a parse tree is computed from the occurrencefrequencies of the subtrees in the treebank.</S><S sid ="76" ssid = "28">Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.</S><S sid ="46" ssid = "43">Most previous notions of best parse tree in DOP1 were based on a probabilistic metric  with Bod (2000b) as a notable exception  who used a simplicity metric based on the shortest derivation.</S><S sid ="91" ssid = "43">It easy to see that this is equivalent to reducing the probability of a tree by a factor of four for each pair of nonterminals it contains  resulting in the PCFG reduction in figure 4.</S><S sid ="52" ssid = "4">The probability of a parse tree T is the sum of the probabilities of its distinct derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'76'", "'46'", "'91'", "'52'"]
'50'
'76'
'46'
'91'
'52'
['50', '76', '46', '91', '52']
parsed_discourse_facet ['method_citation']
<S sid ="74" ssid = "26">Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.</S><S sid ="124" ssid = "5">We employed the same unknown (category) word model as in Bod (2001)  based on statistics on word-endings  hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 87).</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="96" ssid = "48">However  as mentioned above  efficient parsing does not necessarily mean efficient disambiguation: the exact computation of the most probable parse remains exponential.</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'124'", "'40'", "'13'", "'96'"]
'74'
'124'
'40'
'13'
'96'
['74', '124', '40', '13', '96']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">An Efficient Implementation of a New DOP Model</S><S sid ="120" ssid = "1">For our experiments we used the standard division of the WSJ (Marcus et al. 1993)  with sections 2 through 21 for training (approx.</S><S sid ="34" ssid = "31">But even with cross-validation  ML-DOP is outperformed by the much simpler DOP1 model on both the ATIS and OVIS treebanks (Bod 2000b).</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'120'", "'34'", "'15'", "'40'"]
'0'
'120'
'34'
'15'
'40'
['0', '120', '34', '15', '40']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="3" ssid = "3">Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models  and an average processing time of 3.6 seconds per WSJ sentence.</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'3'", "'37'", "'39'", "'85'"]
'7'
'3'
'37'
'39'
'85'
['7', '3', '37', '39', '85']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'64'", "'136'", "'99'", "'140'"]
'7'
'64'
'136'
'99'
'140'
['7', '64', '136', '99', '140']
parsed_discourse_facet ['method_citation']
<S sid ="145" ssid = "10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S><S sid ="46" ssid = "43">Most previous notions of best parse tree in DOP1 were based on a probabilistic metric  with Bod (2000b) as a notable exception  who used a simplicity metric based on the shortest derivation.</S><S sid ="124" ssid = "5">We employed the same unknown (category) word model as in Bod (2001)  based on statistics on word-endings  hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 87).</S><S sid ="105" ssid = "8">The derivation with the smallest sum  or highest rank  is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP  its results are still rather impressive for such a simple model.</S><S sid ="41" ssid = "38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'", "'46'", "'124'", "'105'", "'41'"]
'145'
'46'
'124'
'105'
'41'
['145', '46', '124', '105', '41']
parsed_discourse_facet ['method_citation']
<S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="42" ssid = "39">We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'64'", "'99'", "'42'", "'136'"]
'140'
'64'
'99'
'42'
'136'
['140', '64', '99', '42', '136']
parsed_discourse_facet ['method_citation']
<S sid ="142" ssid = "7">Compared to the reranking technique in Collins (2000)  who obtained an LP of 89.9% and an LR of 89.6%  our results show a 9% relative error rate reduction.</S><S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="141" ssid = "6">This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S><S sid ="59" ssid = "11">A new nonterminal is created for each node in the training data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'7'", "'141'", "'136'", "'59'"]
'142'
'7'
'141'
'136'
'59'
['142', '7', '141', '136', '59']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="116" ssid = "19">Then the shortest derivation is equal to the most probable derivation and can be computed by standard Viterbi optimization  which can be seen as follows: if each subtree has a probability p then the probability of a derivation involving n subtrees is equal to pn  and since 0<p<1  the derivation with the fewest subtrees has the greatest probability.</S><S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'116'", "'85'", "'37'", "'39'"]
'7'
'116'
'85'
'37'
'39'
['7', '116', '85', '37', '39']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="103" ssid = "6">That is  all subtrees of each root label are assigned a rank according to their frequency in the treebank: the most frequent subtree (or subtrees) of each root label gets rank 1  the second most frequent subtree gets rank 2  etc.</S><S sid ="134" ssid = "15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'37'", "'99'", "'103'", "'134'"]
'7'
'37'
'99'
'103'
'134'
['7', '37', '99', '103', '134']
parsed_discourse_facet ['method_citation']
<S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S><S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="88" ssid = "40">In this paper  we will test a simple extension of Goodman's compact PCFG-reduction of DOP which has the same property as the normalization proposed in Bod (2001) in that it assigns roughly equal weight to each node in the training data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'85'", "'7'", "'39'", "'88'"]
'37'
'85'
'7'
'39'
'88'
['37', '85', '7', '39', '88']
parsed_discourse_facet ['results_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'37'", "'140'", "'39'", "'64'"]
'7'
'37'
'140'
'39'
'64'
['7', '37', '140', '39', '64']
parsed_discourse_facet ['method_citation']
<S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S><S sid ="42" ssid = "39">We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="141" ssid = "6">This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'99'", "'136'", "'42'", "'140'", "'141'"]
'99'
'136'
'42'
'140'
'141'
['99', '136', '42', '140', '141']
parsed_discourse_facet ['method_citation']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models  and an average processing time of 3.6 seconds per WSJ sentence.', 'Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.', "Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.", 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.', 'For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).']
['system', 'ROUGE-S*', 'Average_R:', '0.00064', '(95%-conf.int.', '0.00064', '-', '0.00064)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00127', '(95%-conf.int.', '0.00127', '-', '0.00127)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4656', 'P:78', 'F:3']
['The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.']
['We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.', "Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.", 'There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.', 'For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.', 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.']
['system', 'ROUGE-S*', 'Average_R:', '0.00633', '(95%-conf.int.', '0.00633', '-', '0.00633)']
['system', 'ROUGE-S*', 'Average_P:', '0.04667', '(95%-conf.int.', '0.04667', '-', '0.04667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01115', '(95%-conf.int.', '0.01115', '-', '0.01115)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:300', 'F:14']
['Bod reports state-of-the-art results with this method, and observes no decrease in parse accuracy when larger subtrees are included (using subtrees up to depth 14).', 'For example, Bod (2001) samples a fixed number of subtrees of each depth, which has the effect of assigning roughly equal weight to each node in the training data, and roughly exponentially less probability for larger trees (see Goodman 2002: 12).']
["Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.", 'Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.', "In this paper  we will test a simple extension of Goodman's compact PCFG-reduction of DOP which has the same property as the normalization proposed in Bod (2001) in that it assigns roughly equal weight to each node in the training data.", 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.', 'For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).']
['system', 'ROUGE-S*', 'Average_R:', '0.09841', '(95%-conf.int.', '0.09841', '-', '0.09841)']
['system', 'ROUGE-S*', 'Average_P:', '0.54386', '(95%-conf.int.', '0.54386', '-', '0.54386)']
['system', 'ROUGE-S*', 'Average_F:', '0.16667', '(95%-conf.int.', '0.16667', '-', '0.16667)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4095', 'P:741', 'F:403']
['While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.']
['The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.', "Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.", 'We employed the same unknown (category) word model as in Bod (2001)  based on statistics on word-endings  hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 87).', "Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.", 'However  as mentioned above  efficient parsing does not necessarily mean efficient disambiguation: the exact computation of the most probable parse remains exponential.']
['system', 'ROUGE-S*', 'Average_R:', '0.00746', '(95%-conf.int.', '0.00746', '-', '0.00746)']
['system', 'ROUGE-S*', 'Average_P:', '0.04558', '(95%-conf.int.', '0.04558', '-', '0.04558)']
['system', 'ROUGE-S*', 'Average_F:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:351', 'F:16']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.', "Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.", 'Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.']
['system', 'ROUGE-S*', 'Average_R:', '0.03325', '(95%-conf.int.', '0.03325', '-', '0.03325)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.06436', '(95%-conf.int.', '0.06436', '-', '0.06436)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:78', 'F:78']
['The only thing that needs to be changed for Simplicity-DOP is that all subtrees should be assigned equal probabilities.']
['We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.', 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', 'As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:28', 'F:0']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['A new nonterminal is created for each node in the training data.', 'Compared to the reranking technique in Collins (2000)  who obtained an LP of 89.9% and an LR of 89.6%  our results show a 9% relative error rate reduction.', 'As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.', 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.', 'This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00363', '(95%-conf.int.', '0.00363', '-', '0.00363)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.00693', '(95%-conf.int.', '0.00693', '-', '0.00693)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:78', 'F:6']
['DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).']
['Most previous notions of best parse tree in DOP1 were based on a probabilistic metric  with Bod (2000b) as a notable exception  who used a simplicity metric based on the shortest derivation.', "Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.", 'It easy to see that this is equivalent to reducing the probability of a tree by a factor of four for each pair of nonterminals it contains  resulting in the PCFG reduction in figure 4.', 'The probability of a parse tree is computed from the occurrencefrequencies of the subtrees in the treebank.', 'The probability of a parse tree T is the sum of the probabilities of its distinct derivations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00242', '(95%-conf.int.', '0.00242', '-', '0.00242)']
['system', 'ROUGE-S*', 'Average_P:', '0.08889', '(95%-conf.int.', '0.08889', '-', '0.08889)']
['system', 'ROUGE-S*', 'Average_F:', '0.00471', '(95%-conf.int.', '0.00471', '-', '0.00471)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:45', 'F:4']
['In case the shortest derivation is not unique, Bod (2000b) proposes to back off to a frequency ordering of the subtrees.', 'That is, all subtrees of each root label are assigned a rank according to their frequency in the treebank: the most frequent subtree (or subtrees) of each root label gets rank 1, the second most frequent subtree gets rank 2, etc.']
['We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.', 'As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.', 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.']
['system', 'ROUGE-S*', 'Average_R:', '0.00226', '(95%-conf.int.', '0.00226', '-', '0.00226)']
['system', 'ROUGE-S*', 'Average_P:', '0.00985', '(95%-conf.int.', '0.00985', '-', '0.00985)']
['system', 'ROUGE-S*', 'Average_F:', '0.00368', '(95%-conf.int.', '0.00368', '-', '0.00368)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:406', 'F:4']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.', 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', 'There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.', 'As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['system', 'ROUGE-S*', 'Average_R:', '0.04262', '(95%-conf.int.', '0.04262', '-', '0.04262)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.08176', '(95%-conf.int.', '0.08176', '-', '0.08176)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:78', 'F:78']
0.28502299715 0.019701999803 0.0353349996467





input/ref/Task1/E03-1005_sweta.csv
input/res/Task1/E03-1005.csv
parsing: input/ref/Task1/E03-1005_sweta.csv
<S sid="20" ssid="17">Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).</S>
original cit marker offset is 0
new cit marker offset is 0



["20'"]
20'
['20']
parsed_discourse_facet ['method_citation']
 <S sid="74" ssid="26">Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.</S>
original cit marker offset is 0
new cit marker offset is 0



["74'"]
74'
['74']
parsed_discourse_facet ['method_citation']
<S sid="44" ssid="41">But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["44'"]
44'
['44']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["145'"]
145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S>
original cit marker offset is 0
new cit marker offset is 0



["22'"]
22'
['22']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="14">It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).</S>
original cit marker offset is 0
new cit marker offset is 0



["133'"]
133'
['133']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier &amp; Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="35">Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
 <S sid="100" ssid="3">In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="29">However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
 <S sid="130" ssid="11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["130'"]
130'
['130']
parsed_discourse_facet ['method_citation']
 <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["140'"]
140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="24">Goodman (1996, 1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'"]
27'
['27']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="71" ssid = "23">For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'64'", "'99'", "'71'", "'39'"]
'7'
'64'
'99'
'71'
'39'
['7', '64', '99', '71', '39']
parsed_discourse_facet ['results_citation']
<S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="130" ssid = "11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ  comparable to Charniak (2000)  Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S><S sid ="27" ssid = "24">Goodman (1996  1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set  thus converting the exponential number of subtrees to a compact grammar.</S><S sid ="88" ssid = "40">In this paper  we will test a simple extension of Goodman's compact PCFG-reduction of DOP which has the same property as the normalization proposed in Bod (2001) in that it assigns roughly equal weight to each node in the training data.</S><S sid ="69" ssid = "21">Thus  rather than using the large  explicit DOP1 model  one can also use this small PCFG that generates isomorphic derivations  with identical probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'130'", "'27'", "'88'", "'69'"]
'140'
'130'
'27'
'88'
'69'
['140', '130', '27', '88', '69']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "2">The probability of a parse tree is computed from the occurrencefrequencies of the subtrees in the treebank.</S><S sid ="76" ssid = "28">Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.</S><S sid ="46" ssid = "43">Most previous notions of best parse tree in DOP1 were based on a probabilistic metric  with Bod (2000b) as a notable exception  who used a simplicity metric based on the shortest derivation.</S><S sid ="91" ssid = "43">It easy to see that this is equivalent to reducing the probability of a tree by a factor of four for each pair of nonterminals it contains  resulting in the PCFG reduction in figure 4.</S><S sid ="52" ssid = "4">The probability of a parse tree T is the sum of the probabilities of its distinct derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'76'", "'46'", "'91'", "'52'"]
'50'
'76'
'46'
'91'
'52'
['50', '76', '46', '91', '52']
parsed_discourse_facet ['method_citation']
<S sid ="74" ssid = "26">Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.</S><S sid ="124" ssid = "5">We employed the same unknown (category) word model as in Bod (2001)  based on statistics on word-endings  hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 87).</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S><S sid ="13" ssid = "10">The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.</S><S sid ="96" ssid = "48">However  as mentioned above  efficient parsing does not necessarily mean efficient disambiguation: the exact computation of the most probable parse remains exponential.</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'124'", "'40'", "'13'", "'96'"]
'74'
'124'
'40'
'13'
'96'
['74', '124', '40', '13', '96']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">An Efficient Implementation of a New DOP Model</S><S sid ="120" ssid = "1">For our experiments we used the standard division of the WSJ (Marcus et al. 1993)  with sections 2 through 21 for training (approx.</S><S sid ="34" ssid = "31">But even with cross-validation  ML-DOP is outperformed by the much simpler DOP1 model on both the ATIS and OVIS treebanks (Bod 2000b).</S><S sid ="15" ssid = "12">However  during the last few years we can observe a shift towards using more and larger corpus fragments with fewer restrictions.</S><S sid ="40" ssid = "37">Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'120'", "'34'", "'15'", "'40'"]
'0'
'120'
'34'
'15'
'40'
['0', '120', '34', '15', '40']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="3" ssid = "3">Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models  and an average processing time of 3.6 seconds per WSJ sentence.</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'3'", "'37'", "'39'", "'85'"]
'7'
'3'
'37'
'39'
'85'
['7', '3', '37', '39', '85']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'64'", "'136'", "'99'", "'140'"]
'7'
'64'
'136'
'99'
'140'
['7', '64', '136', '99', '140']
parsed_discourse_facet ['method_citation']
<S sid ="145" ssid = "10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S><S sid ="46" ssid = "43">Most previous notions of best parse tree in DOP1 were based on a probabilistic metric  with Bod (2000b) as a notable exception  who used a simplicity metric based on the shortest derivation.</S><S sid ="124" ssid = "5">We employed the same unknown (category) word model as in Bod (2001)  based on statistics on word-endings  hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 87).</S><S sid ="105" ssid = "8">The derivation with the smallest sum  or highest rank  is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP  its results are still rather impressive for such a simple model.</S><S sid ="41" ssid = "38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'", "'46'", "'124'", "'105'", "'41'"]
'145'
'46'
'124'
'105'
'41'
['145', '46', '124', '105', '41']
parsed_discourse_facet ['method_citation']
<S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="42" ssid = "39">We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'64'", "'99'", "'42'", "'136'"]
'140'
'64'
'99'
'42'
'136'
['140', '64', '99', '42', '136']
parsed_discourse_facet ['method_citation']
<S sid ="142" ssid = "7">Compared to the reranking technique in Collins (2000)  who obtained an LP of 89.9% and an LR of 89.6%  our results show a 9% relative error rate reduction.</S><S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="141" ssid = "6">This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S><S sid ="59" ssid = "11">A new nonterminal is created for each node in the training data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'7'", "'141'", "'136'", "'59'"]
'142'
'7'
'141'
'136'
'59'
['142', '7', '141', '136', '59']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="116" ssid = "19">Then the shortest derivation is equal to the most probable derivation and can be computed by standard Viterbi optimization  which can be seen as follows: if each subtree has a probability p then the probability of a derivation involving n subtrees is equal to pn  and since 0<p<1  the derivation with the fewest subtrees has the greatest probability.</S><S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'116'", "'85'", "'37'", "'39'"]
'7'
'116'
'85'
'37'
'39'
['7', '116', '85', '37', '39']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="103" ssid = "6">That is  all subtrees of each root label are assigned a rank according to their frequency in the treebank: the most frequent subtree (or subtrees) of each root label gets rank 1  the second most frequent subtree gets rank 2  etc.</S><S sid ="134" ssid = "15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained  while in the current experiment we used all subtrees as given by the PCFG-reduction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'37'", "'99'", "'103'", "'134'"]
'7'
'37'
'99'
'103'
'134'
['7', '37', '99', '103', '134']
parsed_discourse_facet ['method_citation']
<S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="85" ssid = "37">For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S><S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="88" ssid = "40">In this paper  we will test a simple extension of Goodman's compact PCFG-reduction of DOP which has the same property as the normalization proposed in Bod (2001) in that it assigns roughly equal weight to each node in the training data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'85'", "'7'", "'39'", "'88'"]
'37'
'85'
'7'
'39'
'88'
['37', '85', '7', '39', '88']
parsed_discourse_facet ['results_citation']
<S sid ="7" ssid = "4">Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.</S><S sid ="37" ssid = "34">Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="39" ssid = "36">Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.</S><S sid ="64" ssid = "16">There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'37'", "'140'", "'39'", "'64'"]
'7'
'37'
'140'
'39'
'64'
['7', '37', '140', '39', '64']
parsed_discourse_facet ['method_citation']
<S sid ="99" ssid = "2">We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.</S><S sid ="136" ssid = "1">As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.</S><S sid ="42" ssid = "39">We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.</S><S sid ="140" ssid = "5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid ="141" ssid = "6">This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'99'", "'136'", "'42'", "'140'", "'141'"]
'99'
'136'
'42'
'140'
'141'
['99', '136', '42', '140', '141']
parsed_discourse_facet ['method_citation']
['This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.']
['Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models  and an average processing time of 3.6 seconds per WSJ sentence.', 'Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.', "Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.", 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.', 'For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).']
['system', 'ROUGE-S*', 'Average_R:', '0.00494', '(95%-conf.int.', '0.00494', '-', '0.00494)']
['system', 'ROUGE-S*', 'Average_P:', '0.16912', '(95%-conf.int.', '0.16912', '-', '0.16912)']
['system', 'ROUGE-S*', 'Average_F:', '0.00960', '(95%-conf.int.', '0.00960', '-', '0.00960)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4656', 'P:136', 'F:23']
['Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).']
['We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.', "Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.", 'There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.', 'For the node in figure 1  the following eight PCFG rules are generated  where the number in parentheses following a rule is its probability.', 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.']
['system', 'ROUGE-S*', 'Average_R:', '0.00090', '(95%-conf.int.', '0.00090', '-', '0.00090)']
['system', 'ROUGE-S*', 'Average_P:', '0.01307', '(95%-conf.int.', '0.01307', '-', '0.01307)']
['system', 'ROUGE-S*', 'Average_F:', '0.00169', '(95%-conf.int.', '0.00169', '-', '0.00169)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:153', 'F:2']
["While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996)."]
["Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.", 'Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.', "In this paper  we will test a simple extension of Goodman's compact PCFG-reduction of DOP which has the same property as the normalization proposed in Bod (2001) in that it assigns roughly equal weight to each node in the training data.", 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.', 'For example  Bod (2001) samples a fixed number of subtrees of each depth  which has the effect of assigning roughly equal weight to each node in the training data  and roughly exponentially less probability for larger trees (see Goodman 2002: 12).']
['system', 'ROUGE-S*', 'Average_R:', '0.00317', '(95%-conf.int.', '0.00317', '-', '0.00317)']
['system', 'ROUGE-S*', 'Average_P:', '0.06842', '(95%-conf.int.', '0.06842', '-', '0.06842)']
['system', 'ROUGE-S*', 'Average_F:', '0.00607', '(95%-conf.int.', '0.00607', '-', '0.00607)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4095', 'P:190', 'F:13']
['While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.']
['The other innovation of DOP was to take (in principle) all corpus fragments  of any size  rather than a small subset.', "Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.", 'We employed the same unknown (category) word model as in Bod (2001)  based on statistics on word-endings  hyphenation and capitalization in combination with Good-Turing (Bod 1998: 85 87).', "Goodman (2002) furthermore showed how Bonnema et al. 's (1999) and Bod's (2001) estimators can be incorporated in his PCFGreduction  but did not report any experiments with these reductions.", 'However  as mentioned above  efficient parsing does not necessarily mean efficient disambiguation: the exact computation of the most probable parse remains exponential.']
['system', 'ROUGE-S*', 'Average_R:', '0.00746', '(95%-conf.int.', '0.00746', '-', '0.00746)']
['system', 'ROUGE-S*', 'Average_P:', '0.04558', '(95%-conf.int.', '0.04558', '-', '0.04558)']
['system', 'ROUGE-S*', 'Average_F:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:351', 'F:16']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.', "Collins & Duffy (2002) showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1's subtrees  reporting a 5.1% relative reduction in error rate over the model in Collins (1999) on the WSJ.", 'Bod (2001) used an alternative technique which samples a fixed number of subtrees of each depth and which has the effect of assigning roughly equal weight to each node in the training data.', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.']
['system', 'ROUGE-S*', 'Average_R:', '0.03325', '(95%-conf.int.', '0.03325', '-', '0.03325)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.06436', '(95%-conf.int.', '0.06436', '-', '0.06436)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:78', 'F:78']
['Goodman (1996, 1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.']
['We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.', 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', 'As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00140', '(95%-conf.int.', '0.00140', '-', '0.00140)']
['system', 'ROUGE-S*', 'Average_P:', '0.01579', '(95%-conf.int.', '0.01579', '-', '0.01579)']
['system', 'ROUGE-S*', 'Average_F:', '0.00257', '(95%-conf.int.', '0.00257', '-', '0.00257)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:190', 'F:3']
["Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003)."]
['A new nonterminal is created for each node in the training data.', 'Compared to the reranking technique in Collins (2000)  who obtained an LP of 89.9% and an LR of 89.6%  our results show a 9% relative error rate reduction.', 'As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.', 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.', 'This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00363', '(95%-conf.int.', '0.00363', '-', '0.00363)']
['system', 'ROUGE-S*', 'Average_P:', '0.03922', '(95%-conf.int.', '0.03922', '-', '0.03922)']
['system', 'ROUGE-S*', 'Average_F:', '0.00664', '(95%-conf.int.', '0.00664', '-', '0.00664)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:153', 'F:6']
["But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996)."]
['Most previous notions of best parse tree in DOP1 were based on a probabilistic metric  with Bod (2000b) as a notable exception  who used a simplicity metric based on the shortest derivation.', "Note that Goodman's reduction method does still not allow for an efficient computation of the most probable parse tree of a sentence: there may still be exponentially many derivations generating the same tree.", 'It easy to see that this is equivalent to reducing the probability of a tree by a factor of four for each pair of nonterminals it contains  resulting in the PCFG reduction in figure 4.', 'The probability of a parse tree is computed from the occurrencefrequencies of the subtrees in the treebank.', 'The probability of a parse tree T is the sum of the probabilities of its distinct derivations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00060', '(95%-conf.int.', '0.00060', '-', '0.00060)']
['system', 'ROUGE-S*', 'Average_P:', '0.00526', '(95%-conf.int.', '0.00526', '-', '0.00526)']
['system', 'ROUGE-S*', 'Average_F:', '0.00109', '(95%-conf.int.', '0.00109', '-', '0.00109)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:190', 'F:1']
['DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).']
['We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.', 'There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.', 'As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.', 'Other models started off with a predefined grammar and used a corpus only for estimating the rule probabilities (as e.g.']
['system', 'ROUGE-S*', 'Average_R:', '0.00452', '(95%-conf.int.', '0.00452', '-', '0.00452)']
['system', 'ROUGE-S*', 'Average_P:', '0.05882', '(95%-conf.int.', '0.05882', '-', '0.05882)']
['system', 'ROUGE-S*', 'Average_F:', '0.00839', '(95%-conf.int.', '0.00839', '-', '0.00839)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:136', 'F:8']
["Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002)."]
['We will refer to these models as Likelihood-DOP models  but in this paper we will specifically mean by &quot;Likelihood-DOP&quot; the PCFG-reduction of Bod (2001) given in Section 2.2.', 'We show that these PCFG-reductions result in a 60 times speedup in processing time w.r.t.', 'There are bk non-trivial subtrees headed by B@k  and there is also the trivial case where the left node is simply B.', 'As our second experimental goal  we compared the models SL-DOP and LS-DOP explained in Section 3.2.', 'The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['system', 'ROUGE-S*', 'Average_R:', '0.00164', '(95%-conf.int.', '0.00164', '-', '0.00164)']
['system', 'ROUGE-S*', 'Average_P:', '0.01000', '(95%-conf.int.', '0.01000', '-', '0.01000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00282', '(95%-conf.int.', '0.00282', '-', '0.00282)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:300', 'F:3']
0.142527998575 0.00615099993849 0.011604999884





input/ref/Task1/A00-2018_sweta.csv
input/res/Task1/A00-2018.csv
parsing: input/ref/Task1/A00-2018_sweta.csv
 <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["17'"]
17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
 <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["17'"]
17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="11">Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &amp;quot;correct&amp;quot;, and statistics were collected on the resulting parses.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="10">(It is &amp;quot;soft&amp;quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.)</S>
original cit marker offset is 0
new cit marker offset is 0



["119'"]
119'
['119']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.</S>
original cit marker offset is 0
new cit marker offset is 0



["95'"]
95'
['95']
parsed_discourse_facet ['method_citation']
 <S sid="175" ssid="2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["175'"]
175'
['175']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
original cit marker offset is 0
new cit marker offset is 0



["92'"]
92'
['92']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="17">This is indicated in Figure 2, where the model labeled &amp;quot;Best&amp;quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["126'"]
126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="1">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c, t(c) (t for &amp;quot;tag&amp;quot;), then the lexical head of c, h(c), and then the expansion of c into further constituents e(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["174'"]
174'
['174']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="32">As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &amp;quot;before&amp;quot;), and the label of the grandparent of c (la).</S>
original cit marker offset is 0
new cit marker offset is 0



["63'"]
63'
['63']
parsed_discourse_facet ['method_citation']
<S sid="78" ssid="47">With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.</S>
original cit marker offset is 0
new cit marker offset is 0



["78'"]
78'
['78']
parsed_discourse_facet ['method_citation']
 <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="7">From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.</S>
original cit marker offset is 0
new cit marker offset is 0



["180'"]
180'
['180']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2018.csv
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'23'", "'62'", "'33'", "'27'"]
'30'
'23'
'62'
'33'
'27'
['30', '23', '62', '33', '27']
parsed_discourse_facet ['results_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'27'", "'62'", "'33'", "'23'"]
'30'
'27'
'62'
'33'
'23'
['30', '27', '62', '33', '23']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'62'", "'33'", "'89'", "'23'"]
'30'
'62'
'33'
'89'
'23'
['30', '62', '33', '89', '23']
parsed_discourse_facet ['method_citation']
<S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="20" ssid = "9">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="29" ssid = "18">So  for example  in a second-order Markov PCFG  L2 would be conditioned on L1 and M. In our complete model  of course  the probability of each label in the expansions is also conditioned on other material as specified in Equation 1  e.g.  p(e t  h  H).</S><S sid ="91" ssid = "2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'", "'20'", "'174'", "'29'", "'91'"]
'27'
'20'
'174'
'29'
'91'
['27', '20', '174', '29', '91']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'23'", "'62'", "'27'", "'33'"]
'30'
'23'
'62'
'27'
'33'
['30', '23', '62', '27', '33']
parsed_discourse_facet ['method_citation']
<S sid ="101" ssid = "12">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S><S sid ="1" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid ="5" ssid = "1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40  and 89.5% for sentences of length < 100  when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="63" ssid = "32">As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'1'", "'5'", "'27'", "'63'"]
'101'
'1'
'5'
'27'
'63'
['101', '1', '5', '27', '63']
parsed_discourse_facet ['method_citation']
<S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'", "'30'", "'62'", "'89'", "'23'"]
'27'
'30'
'62'
'89'
'23'
['27', '30', '62', '89', '23']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'62'", "'23'", "'89'", "'33'"]
'30'
'62'
'23'
'89'
'33'
['30', '62', '23', '89', '33']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "7">To compute a probability in a log-linear model one first defines a set of &quot;features&quot;  functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.</S><S sid ="91" ssid = "2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="20" ssid = "9">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid ="26" ssid = "15">Thus an expansion e(c) looks like: The expansion is generated by guessing first M  then in order L1 through L „.+1 (= A)  and similarly for RI through In a pure Markov PCFG we are given the left-hand side label 1 and then probabilistically generate the right-hand side conditioning on no information other than 1 and (possibly) previously generated pieces of the right-hand side itself.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'91'", "'174'", "'20'", "'26'"]
'38'
'91'
'174'
'20'
'26'
['38', '91', '174', '20', '26']
parsed_discourse_facet ['method_citation']
<S sid ="29" ssid = "18">So  for example  in a second-order Markov PCFG  L2 would be conditioned on L1 and M. In our complete model  of course  the probability of each label in the expansions is also conditioned on other material as specified in Equation 1  e.g.  p(e t  h  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="91" ssid = "2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid ="33" ssid = "2">For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'", "'27'", "'174'", "'91'", "'33'"]
'29'
'27'
'174'
'91'
'33'
['29', '27', '174', '91', '33']
parsed_discourse_facet ['method_citation']
<S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="101" ssid = "12">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S><S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'", "'27'", "'89'", "'101'", "'30'"]
'62'
'27'
'89'
'101'
'30'
['62', '27', '89', '101', '30']
parsed_discourse_facet ['method_citation']
<S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="115" ssid = "6">As noted in [5]  that system is based upon a &quot;tree-bank grammar&quot; - a grammar read directly off the training corpus.</S><S sid ="141" ssid = "32">(For example  part-ofspeech tagging using the most probable preterminal for each word is 90% accurate [8].)</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S><S sid ="184" ssid = "11">The first is the slight  but important  improvement achieved by using this model over conventional deleted interpolation  as indicated in Figure 2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'115'", "'141'", "'62'", "'184'"]
'23'
'115'
'141'
'62'
'184'
['23', '115', '141', '62', '184']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "9">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid ="76" ssid = "45">This requires finding the appropriate Ais for Equation 3  which is accomplished using an algorithm such as iterative scaling [II] in which values for the Ai are initially &quot;guessed&quot; and then modified until they converge on stable values.</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid ="110" ssid = "1">In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach  the aspect of the parser that is most novel.</S><S sid ="101" ssid = "12">In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'76'", "'27'", "'110'", "'101'"]
'20'
'76'
'27'
'110'
'101'
['20', '76', '27', '110', '101']
parsed_discourse_facet ['results_citation']
<S sid ="74" ssid = "43">Suppose we were  in fact  going to compute a true maximum entropy model based upon the features used in Equation 7  Ii (t 1)  f2(t 1 1p)  f3(t 1 lp) .</S><S sid ="126" ssid = "17">This is indicated in Figure 2  where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%  0.4% lower than the results on the official test corpus.</S><S sid ="125" ssid = "16">For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.</S><S sid ="174" ssid = "1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S sid ="27" ssid = "16">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / — that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S>
original cit marker offset is 0
new cit marker offset is 0



["'74'", "'126'", "'125'", "'174'", "'27'"]
'74'
'126'
'125'
'174'
'27'
['74', '126', '125', '174', '27']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "19">Thus we would use p(L2 I L1  M  1  t  h  H).</S><S sid ="89" ssid = "58">(Actually  we use a minor variant described in [4].)</S><S sid ="23" ssid = "12">For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).</S><S sid ="42" ssid = "11">This feature is obviously composed of two sub-features  one recognizing t  the other 1.</S><S sid ="62" ssid = "31">In Equation 1 we wrote this as p(t I 1  H).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'89'", "'23'", "'42'", "'62'"]
'30'
'89'
'23'
'42'
'62'
['30', '89', '23', '42', '62']
parsed_discourse_facet ['method_citation']
<S sid ="176" ssid = "3">That the previous three best parsers on this test [5 9 17] all perform within a percentage point of each other  despite quite different basic mechanisms  led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training  and to conjecture that perhaps we were at it.</S><S sid ="58" ssid = "27">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid ="180" ssid = "7">From our perspective  perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.</S><S sid ="44" ssid = "13">Now consider computing a conditional probability p(a H) with a set of features h that connect a to the history H. In a log-linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability  the higher the absolute value of the associated A.</S><S sid ="13" ssid = "2">Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'", "'58'", "'180'", "'44'", "'13'"]
'176'
'58'
'180'
'44'
'13'
['176', '58', '180', '44', '13']
parsed_discourse_facet ['method_citation']
['In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.']
['For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).', u'In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / \xe2\u20ac\u201d that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).', 'In Equation 1 we wrote this as p(t I 1  H).', 'Thus we would use p(L2 I L1  M  1  t  h  H).', 'For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:105', 'F:0']
['We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank.']
['As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.', 'We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ', 'To compute a probability in a log-linear model one first defines a set of &quot;features&quot;  functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.']
['system', 'ROUGE-S*', 'Average_R:', '0.05734', '(95%-conf.int.', '0.05734', '-', '0.05734)']
['system', 'ROUGE-S*', 'Average_P:', '0.19524', '(95%-conf.int.', '0.19524', '-', '0.19524)']
['system', 'ROUGE-S*', 'Average_F:', '0.08865', '(95%-conf.int.', '0.08865', '-', '0.08865)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:630', 'F:123']
['Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &quot;correct&quot;, and statistics were collected on the resulting parses.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ', u'In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / \xe2\u20ac\u201d that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).', 'In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:171', 'F:0']
['This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].']
['For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).', 'In Equation 1 we wrote this as p(t I 1  H).', u'In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / \xe2\u20ac\u201d that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).', '(Actually  we use a minor variant described in [4].)', 'Thus we would use p(L2 I L1  M  1  t  h  H).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:78', 'F:0']
['As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.']
['This feature is obviously composed of two sub-features  one recognizing t  the other 1.', 'In Equation 1 we wrote this as p(t I 1  H).', 'Thus we would use p(L2 I L1  M  1  t  h  H).', '(Actually  we use a minor variant described in [4].)', 'For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:231', 'F:0']
['We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.']
['We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.', 'In keeping with the standard methodology [5  9 10 15 17]  we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training  section 23 for testing  and section 24 for development (debugging and tuning).', 'We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.00031', '(95%-conf.int.', '0.00031', '-', '0.00031)']
['system', 'ROUGE-S*', 'Average_P:', '0.00526', '(95%-conf.int.', '0.00526', '-', '0.00526)']
['system', 'ROUGE-S*', 'Average_F:', '0.00058', '(95%-conf.int.', '0.00058', '-', '0.00058)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3240', 'P:190', 'F:1']
['In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.']
['For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).', 'In Equation 1 we wrote this as p(t I 1  H).', 'Thus we would use p(L2 I L1  M  1  t  h  H).', '(Actually  we use a minor variant described in [4].)', 'For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:105', 'F:0']
['(It is &quot;soft&quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.)']
['For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).', u'In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / \xe2\u20ac\u201d that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).', 'In Equation 1 we wrote this as p(t I 1  H).', 'Thus we would use p(L2 I L1  M  1  t  h  H).', 'For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).']
['system', 'ROUGE-S*', 'Average_R:', '0.00101', '(95%-conf.int.', '0.00101', '-', '0.00101)']
['system', 'ROUGE-S*', 'Average_P:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00180', '(95%-conf.int.', '0.00180', '-', '0.00180)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:120', 'F:1']
['We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.']
['For example  in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1  M 1  t  h  H).', u'In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / \xe2\u20ac\u201d that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).', 'In Equation 1 we wrote this as p(t I 1  H).', 'Thus we would use p(L2 I L1  M  1  t  h  H).', 'For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).']
['system', 'ROUGE-S*', 'Average_R:', '0.00303', '(95%-conf.int.', '0.00303', '-', '0.00303)']
['system', 'ROUGE-S*', 'Average_P:', '0.00317', '(95%-conf.int.', '0.00317', '-', '0.00317)']
['system', 'ROUGE-S*', 'Average_F:', '0.00310', '(95%-conf.int.', '0.00310', '-', '0.00310)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:946', 'F:3']
["From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head."]
['That the previous three best parsers on this test [5 9 17] all perform within a percentage point of each other  despite quite different basic mechanisms  led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training  and to conjecture that perhaps we were at it.', "From our perspective  perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.", u'Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g.  whether it is a noun phrase (np)  verb-phrase  etc.) and H (c) is the relevant history of c \xe2\u20ac\u201d information outside c that our probability model deems important in determining the probability in question.', 'Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.', 'Now consider computing a conditional probability p(a H) with a set of features h that connect a to the history H. In a log-linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability  the higher the absolute value of the associated A.']
['system', 'ROUGE-S*', 'Average_R:', '0.03802', '(95%-conf.int.', '0.03802', '-', '0.03802)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.07325', '(95%-conf.int.', '0.07325', '-', '0.07325)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:7260', 'P:276', 'F:276']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.']
['As noted in [5]  that system is based upon a &quot;tree-bank grammar&quot; - a grammar read directly off the training corpus.', 'In Equation 1 we wrote this as p(t I 1  H).', '(For example  part-ofspeech tagging using the most probable preterminal for each word is 90% accurate [8].)', 'The first is the slight  but important  improvement achieved by using this model over conventional deleted interpolation  as indicated in Figure 2.', 'For each expansion we distinguish one of the right-hand side labels as the &quot;middle&quot; or &quot;head&quot; symbol M(c).']
['system', 'ROUGE-S*', 'Average_R:', '0.00408', '(95%-conf.int.', '0.00408', '-', '0.00408)']
['system', 'ROUGE-S*', 'Average_P:', '0.01232', '(95%-conf.int.', '0.01232', '-', '0.01232)']
['system', 'ROUGE-S*', 'Average_F:', '0.00613', '(95%-conf.int.', '0.00613', '-', '0.00613)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:406', 'F:5']
['With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ', 'Suppose we were  in fact  going to compute a true maximum entropy model based upon the features used in Equation 7  Ii (t 1)  f2(t 1 1p)  f3(t 1 lp) .', 'For example  the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.', 'This is indicated in Figure 2  where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%  0.4% lower than the results on the official test corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:45', 'F:0']
0.102026665816 0.00864916659459 0.0144591665462





input/ref/Task1/W11-2123_swastika.csv
input/res/Task1/W11-2123.csv
parsing: input/ref/Task1/W11-2123_swastika.csv
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



['52']
52
['52']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="177" ssid="49">However, TRIE partitions storage by n-gram length, so walking the trie reads N disjoint pages.</S>
original cit marker offset is 0
new cit marker offset is 0



['177']
177
['177']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="278" ssid="5">We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns.</S>
original cit marker offset is 0
new cit marker offset is 0



['278']
278
['278']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/W11-2123.csv
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="27" ssid = "5">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'183'", "'69'", "'27'", "'231'"]
'0'
'183'
'69'
'27'
'231'
['0', '183', '69', '27', '231']
parsed_discourse_facet ['results_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="149" ssid = "21">RandLM and SRILM also remove context that will not extend  but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.</S><S sid ="193" ssid = "12">It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'183'", "'69'", "'149'", "'193'"]
'0'
'183'
'69'
'149'
'193'
['0', '183', '69', '149', '193']
parsed_discourse_facet ['method_citation']
<S sid ="62" ssid = "40">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S><S sid ="142" ssid = "14">Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn−1 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.</S><S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="216" ssid = "35">Though we are not able to calculate their memory usage on our model  results reported in their paper suggest lower memory consumption than TRIE on large-scale models  at the expense of CPU time.</S><S sid ="50" ssid = "28">Given counts cn1 where e.g. c1 is the vocabulary size  total memory consumption  in bits  is Our PROBING data structure places all n-grams of the same order into a single giant hash table.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'", "'142'", "'3'", "'216'", "'50'"]
'62'
'142'
'3'
'216'
'50'
['62', '142', '3', '216', '50']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="137" ssid = "9">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="157" ssid = "29">Each visited entry wni stores backoff b(wni ).</S><S sid ="27" ssid = "5">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'137'", "'183'", "'157'", "'27'"]
'0'
'137'
'183'
'157'
'27'
['0', '137', '183', '157', '27']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="233" ssid = "52">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid ="193" ssid = "12">It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'3'", "'233'", "'193'", "'69'"]
'183'
'3'
'233'
'193'
'69'
['183', '3', '233', '193', '69']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="38" ssid = "16">The ratio of buckets to entries is controlled by space multiplier m > 1.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="71" ssid = "49">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'0'", "'38'", "'88'", "'71'"]
'183'
'0'
'38'
'88'
'71'
['183', '0', '38', '88', '71']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="71" ssid = "49">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'0'", "'14'", "'71'", "'88'"]
'69'
'0'
'14'
'71'
'88'
['69', '0', '14', '71', '88']
parsed_discourse_facet ['method_citation']
<S sid ="124" ssid = "28">Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="227" ssid = "46">The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.</S><S sid ="21" ssid = "16">Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'69'", "'227'", "'21'", "'13'"]
'124'
'69'
'227'
'21'
'13'
['124', '69', '227', '21', '13']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="137" ssid = "9">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid ="116" ssid = "20">Both implementations employ a state object  opaque to the application  that carries information from one query to the next; we discuss both further in Section 4.2.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'0'", "'137'", "'116'", "'231'"]
'183'
'0'
'137'
'116'
'231'
['183', '0', '137', '116', '231']
parsed_discourse_facet ['method_citation']
<S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="118" ssid = "22">The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'183'", "'69'", "'118'", "'14'"]
'88'
'183'
'69'
'118'
'14'
['88', '183', '69', '118', '14']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="227" ssid = "46">The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.</S><S sid ="124" ssid = "28">Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'231'", "'183'", "'227'", "'124'"]
'69'
'231'
'183'
'227'
'124'
['69', '231', '183', '227', '124']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "29">Each visited entry wni stores backoff b(wni ).</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="53" ssid = "31">Sorted arrays store key-value pairs in an array sorted by key  incurring no space overhead.</S><S sid ="27" ssid = "5">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S><S sid ="118" ssid = "22">The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'0'", "'53'", "'27'", "'118'"]
'157'
'0'
'53'
'27'
'118'
['157', '0', '53', '27', '118']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="195" ssid = "14">Interpolation search has a more expensive pivot but performs less pivoting and reads  so it is slow on small data and faster on large data.</S><S sid ="116" ssid = "20">Both implementations employ a state object  opaque to the application  that carries information from one query to the next; we discuss both further in Section 4.2.</S><S sid ="137" ssid = "9">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid ="62" ssid = "40">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'195'", "'116'", "'137'", "'62'"]
'183'
'195'
'116'
'137'
'62'
['183', '195', '116', '137', '62']
parsed_discourse_facet ['results_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S><S sid ="233" ssid = "52">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid ="193" ssid = "12">It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'69'", "'231'", "'233'", "'193'"]
'183'
'69'
'231'
'233'
'193'
['183', '69', '231', '233', '193']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="201" ssid = "20">Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'69'", "'88'", "'183'", "'201'"]
'0'
'69'
'88'
'183'
'201'
['0', '69', '88', '183', '201']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="62" ssid = "40">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S><S sid ="102" ssid = "6">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie)  allocating memory all at once (eliminating the need for full pointers)  and being easy to compile.</S><S sid ="179" ssid = "51">We do not experiment with models larger than physical memory in this paper because TPT is unreleased  factors such as disk speed are hard to replicate  and in such situations we recommend switching to a more compact representation  such as RandLM.</S><S sid ="142" ssid = "14">Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn−1 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'62'", "'102'", "'179'", "'142'"]
'3'
'62'
'102'
'179'
'142'
['3', '62', '102', '179', '142']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="2" ssid = "2">The structure uses linear probing hash tables and is designed for speed.</S><S sid ="129" ssid = "1">In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.</S><S sid ="71" ssid = "49">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S><S sid ="201" ssid = "20">Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'2'", "'129'", "'71'", "'201'"]
'69'
'2'
'129'
'71'
'201'
['69', '2', '129', '71', '201']
parsed_discourse_facet ['method_citation']
<S sid ="199" ssid = "18">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid ="205" ssid = "24">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al.  2009).</S><S sid ="182" ssid = "1">This section measures performance on shared tasks in order of increasing complexity: sparse lookups  evaluating perplexity of a large file  and translation with Moses.</S><S sid ="67" ssid = "45">While sorted arrays could be used to implement the same data structure as PROBING  effectively making m = 1  we abandoned this implementation because it is slower and larger than a trie implementation.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'", "'205'", "'182'", "'67'", "'45'"]
'199'
'205'
'182'
'67'
'45'
['199', '205', '182', '67', '45']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="149" ssid = "21">RandLM and SRILM also remove context that will not extend  but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.</S><S sid ="201" ssid = "20">Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'183'", "'69'", "'149'", "'201'"]
'0'
'183'
'69'
'149'
'201'
['0', '183', '69', '149', '201']
parsed_discourse_facet ['aim_citation', 'results_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="118" ssid = "22">The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'0'", "'69'", "'88'", "'118'"]
'183'
'0'
'69'
'88'
'118'
['183', '0', '69', '88', '118']
parsed_discourse_facet ['method_citation']
['The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.']
['Interpolation search has a more expensive pivot but performs less pivoting and reads  so it is slow on small data and faster on large data.', 'Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.', u'If the key distribution\u2019s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |\u2212 1] to estimate pivots; this optimization alone led to a 24% speed improvement.', 'Both implementations employ a state object  opaque to the application  that carries information from one query to the next; we discuss both further in Section 4.2.', u'The state function is integrated into the query process so that  in lieu of the query p(wnjwn\u22121 1 )  the application issues query p(wnjs(wn\u22121 1 )) which also returns s(wn1 ).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3916', 'P:120', 'F:0']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['KenLM: Faster and Smaller Language Model Queries', u'Hash tables are a common sparse mapping technique used by SRILM\u2019s default and BerkeleyLM\u2019s hashed variant.', 'Sorted arrays store key-value pairs in an array sorted by key  incurring no space overhead.', 'Each visited entry wni stores backoff b(wni ).', u'The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM\u2019s default.']
['system', 'ROUGE-S*', 'Average_R:', '0.00707', '(95%-conf.int.', '0.00707', '-', '0.00707)']
['system', 'ROUGE-S*', 'Average_P:', '0.08974', '(95%-conf.int.', '0.08974', '-', '0.08974)']
['system', 'ROUGE-S*', 'Average_F:', '0.01311', '(95%-conf.int.', '0.01311', '-', '0.01311)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:78', 'F:7']
['We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns.']
['This section measures performance on shared tasks in order of increasing complexity: sparse lookups  evaluating perplexity of a large file  and translation with Moses.', 'We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al.  2009).', 'The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.', 'For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.', 'While sorted arrays could be used to implement the same data structure as PROBING  effectively making m = 1  we abandoned this implementation because it is slower and larger than a trie implementation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00056', '(95%-conf.int.', '0.00056', '-', '0.00056)']
['system', 'ROUGE-S*', 'Average_P:', '0.02198', '(95%-conf.int.', '0.02198', '-', '0.02198)']
['system', 'ROUGE-S*', 'Average_F:', '0.00109', '(95%-conf.int.', '0.00109', '-', '0.00109)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3570', 'P:91', 'F:2']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.', 'Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\xe2\u20ac\u2122s inverted variant  and BerkeleyLM except for the scrolling variant.', 'It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.', 'The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.']
['system', 'ROUGE-S*', 'Average_R:', '0.00032', '(95%-conf.int.', '0.00032', '-', '0.00032)']
['system', 'ROUGE-S*', 'Average_P:', '0.02564', '(95%-conf.int.', '0.02564', '-', '0.02564)']
['system', 'ROUGE-S*', 'Average_F:', '0.00062', '(95%-conf.int.', '0.00062', '-', '0.00062)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6328', 'P:78', 'F:2']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['Though we are not able to calculate their memory usage on our model  results reported in their paper suggest lower memory consumption than TRIE on large-scale models  at the expense of CPU time.', u'If the key distribution\u2019s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |\u2212 1] to estimate pivots; this optimization alone led to a 24% speed improvement.', u'Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn\u22121 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.', 'Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', 'Given counts cn1 where e.g. c1 is the vocabulary size  total memory consumption  in bits  is Our PROBING data structure places all n-grams of the same order into a single giant hash table.']
['system', 'ROUGE-S*', 'Average_R:', '0.00161', '(95%-conf.int.', '0.00161', '-', '0.00161)']
['system', 'ROUGE-S*', 'Average_P:', '0.18182', '(95%-conf.int.', '0.18182', '-', '0.18182)']
['system', 'ROUGE-S*', 'Average_F:', '0.00319', '(95%-conf.int.', '0.00319', '-', '0.00319)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6216', 'P:55', 'F:10']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['KenLM: Faster and Smaller Language Model Queries', u'Hash tables are a common sparse mapping technique used by SRILM\u2019s default and BerkeleyLM\u2019s hashed variant.', u'The state function is integrated into the query process so that  in lieu of the query p(wnjwn\u22121 1 )  the application issues query p(wnjs(wn\u22121 1 )) which also returns s(wn1 ).', 'Each visited entry wni stores backoff b(wni ).', 'Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.']
['system', 'ROUGE-S*', 'Average_R:', '0.00298', '(95%-conf.int.', '0.00298', '-', '0.00298)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.00573', '(95%-conf.int.', '0.00573', '-', '0.00573)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:78', 'F:6']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\u2019s inverted variant  and BerkeleyLM except for the scrolling variant.', 'Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.', u'The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI\u2019s speed reported in Guthrie and Hepple (2010) under different conditions.', 'Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.']
['system', 'ROUGE-S*', 'Average_R:', '0.00027', '(95%-conf.int.', '0.00027', '-', '0.00027)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00054', '(95%-conf.int.', '0.00054', '-', '0.00054)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3655', 'P:55', 'F:1']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\xe2\u20ac\u2122s inverted variant  and BerkeleyLM except for the scrolling variant.', 'The structure uses linear probing hash tables and is designed for speed.', 'Nodes in the trie are based on arrays sorted by vocabulary identifier.', 'In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.']
['system', 'ROUGE-S*', 'Average_R:', '0.00210', '(95%-conf.int.', '0.00210', '-', '0.00210)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00404', '(95%-conf.int.', '0.00404', '-', '0.00404)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:55', 'F:3']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['KenLM: Faster and Smaller Language Model Queries', 'RandLM and SRILM also remove context that will not extend  but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\xe2\u20ac\u2122s inverted variant  and BerkeleyLM except for the scrolling variant.', 'It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.', 'Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.']
['system', 'ROUGE-S*', 'Average_R:', '0.00348', '(95%-conf.int.', '0.00348', '-', '0.00348)']
['system', 'ROUGE-S*', 'Average_P:', '0.14103', '(95%-conf.int.', '0.14103', '-', '0.14103)']
['system', 'ROUGE-S*', 'Average_F:', '0.00679', '(95%-conf.int.', '0.00679', '-', '0.00679)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:78', 'F:11']
['The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.']
['The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie)  allocating memory all at once (eliminating the need for full pointers)  and being easy to compile.', 'Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', u'Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn\u22121 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.', u'If the key distribution\u2019s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |\u2212 1] to estimate pivots; this optimization alone led to a 24% speed improvement.', 'We do not experiment with models larger than physical memory in this paper because TPT is unreleased  factors such as disk speed are hard to replicate  and in such situations we recommend switching to a more compact representation  such as RandLM.']
['system', 'ROUGE-S*', 'Average_R:', '0.00016', '(95%-conf.int.', '0.00016', '-', '0.00016)']
['system', 'ROUGE-S*', 'Average_P:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00032', '(95%-conf.int.', '0.00032', '-', '0.00032)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6216', 'P:120', 'F:1']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['KenLM: Faster and Smaller Language Model Queries', u'Hash tables are a common sparse mapping technique used by SRILM\u2019s default and BerkeleyLM\u2019s hashed variant.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\u2019s inverted variant  and BerkeleyLM except for the scrolling variant.', u'The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI\u2019s speed reported in Guthrie and Hepple (2010) under different conditions.', 'Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.']
['system', 'ROUGE-S*', 'Average_R:', '0.00266', '(95%-conf.int.', '0.00266', '-', '0.00266)']
['system', 'ROUGE-S*', 'Average_P:', '0.08974', '(95%-conf.int.', '0.08974', '-', '0.08974)']
['system', 'ROUGE-S*', 'Average_F:', '0.00517', '(95%-conf.int.', '0.00517', '-', '0.00517)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:78', 'F:7']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
[u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\u2019s inverted variant  and BerkeleyLM except for the scrolling variant.', u'By contrast  BerkeleyLM\u2019s hash and compressed variants will return incorrect results based on an n \u22121-gram.', 'MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.', 'Nodes in the trie are based on arrays sorted by vocabulary identifier.', 'KenLM: Faster and Smaller Language Model Queries']
['system', 'ROUGE-S*', 'Average_R:', '0.00653', '(95%-conf.int.', '0.00653', '-', '0.00653)']
['system', 'ROUGE-S*', 'Average_P:', '0.11538', '(95%-conf.int.', '0.11538', '-', '0.11538)']
['system', 'ROUGE-S*', 'Average_F:', '0.01236', '(95%-conf.int.', '0.01236', '-', '0.01236)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:78', 'F:9']
['Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.']
['Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.', u'By contrast  BerkeleyLM\u2019s hash and compressed variants will return incorrect results based on an n \u22121-gram.', 'The ratio of buckets to entries is controlled by space multiplier m > 1.', 'Nodes in the trie are based on arrays sorted by vocabulary identifier.', 'KenLM: Faster and Smaller Language Model Queries']
['system', 'ROUGE-S*', 'Average_R:', '0.00218', '(95%-conf.int.', '0.00218', '-', '0.00218)']
['system', 'ROUGE-S*', 'Average_P:', '0.04545', '(95%-conf.int.', '0.04545', '-', '0.04545)']
['system', 'ROUGE-S*', 'Average_F:', '0.00416', '(95%-conf.int.', '0.00416', '-', '0.00416)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:66', 'F:3']
['The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.']
['Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.', 'KenLM: Faster and Smaller Language Model Queries', u'By contrast  BerkeleyLM\u2019s hash and compressed variants will return incorrect results based on an n \u22121-gram.', 'Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\u2019s inverted variant  and BerkeleyLM except for the scrolling variant.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:120', 'F:0']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.', u'By contrast  BerkeleyLM\u2019s hash and compressed variants will return incorrect results based on an n \u22121-gram.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\u2019s inverted variant  and BerkeleyLM except for the scrolling variant.', u'The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM\u2019s default.', 'KenLM: Faster and Smaller Language Model Queries']
['system', 'ROUGE-S*', 'Average_R:', '0.00136', '(95%-conf.int.', '0.00136', '-', '0.00136)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00265', '(95%-conf.int.', '0.00265', '-', '0.00265)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:55', 'F:3']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\xe2\u20ac\u2122s inverted variant  and BerkeleyLM except for the scrolling variant.', 'IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.', 'Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.', 'Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.']
['system', 'ROUGE-S*', 'Average_R:', '0.00032', '(95%-conf.int.', '0.00032', '-', '0.00032)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00062', '(95%-conf.int.', '0.00062', '-', '0.00062)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:78', 'F:1']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.', 'The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.', u'The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI\u2019s speed reported in Guthrie and Hepple (2010) under different conditions.', 'It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\u2019s inverted variant  and BerkeleyLM except for the scrolling variant.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4851', 'P:55', 'F:0']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.', 'KenLM: Faster and Smaller Language Model Queries', 'RandLM and SRILM also remove context that will not extend  but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\xe2\u20ac\u2122s inverted variant  and BerkeleyLM except for the scrolling variant.', 'Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.']
['system', 'ROUGE-S*', 'Average_R:', '0.00373', '(95%-conf.int.', '0.00373', '-', '0.00373)']
['system', 'ROUGE-S*', 'Average_P:', '0.16364', '(95%-conf.int.', '0.16364', '-', '0.16364)']
['system', 'ROUGE-S*', 'Average_F:', '0.00729', '(95%-conf.int.', '0.00729', '-', '0.00729)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:55', 'F:9']
0.0610983329939 0.00196277776687 0.00375999997911





input/ref/Task1/P08-1043_sweta.csv
input/res/Task1/P08-1043.csv
parsing: input/ref/Task1/P08-1043_sweta.csv
<S sid="156" ssid="34">To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["4'"]
4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="2">Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.</S>
original cit marker offset is 0
new cit marker offset is 0



["70'"]
70'
['70']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="3">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="9">Tsarfaty (2006) used a morphological analyzer (Segal, 2000), a PoS tagger (Bar-Haim et al., 2005), and a general purpose parser (Schmid, 2000) in an integrated framework in which morphological and syntactic components interact to share information, leading to improved performance on the joint task.</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="107" ssid="39">Firstly, Hebrew unknown tokens are doubly unknown: each unknown token may correspond to several segmentation possibilities, and each segment in such sequences may be able to admit multiple PoS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["107'"]
107'
['107']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="10">The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["14'"]
14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="12">The current work treats both segmental and super-segmental phenomena, yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg, 2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="53" ssid="11">Both (Tsarfaty, 2006; Cohen and Smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has shown a single generative model which handles both tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["53'"]
53'
['53']
parsed_discourse_facet ['method_citation']
 <S sid="48" ssid="6">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["48'"]
48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="141" ssid="19">This analyzer setting is similar to that of (Cohen and Smith, 2007), and models using it are denoted nohsp, Parser and Grammar We used BitPar (Schmid, 2004), an efficient general purpose parser,10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["141'"]
141'
['141']
parsed_discourse_facet ['method_citation']
<S sid="188" ssid="2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



["188'"]
188'
['188']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["94'"]
94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="12">Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training data similar to (Cohen and Smith, 2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
    <S sid="156" ssid="34">To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth, &#8216;tokens&#8217;) that constitute the unanalyzed surface forms (utterances).</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="31" ssid = "10">Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'92'", "'86'", "'192'", "'31'"]
'183'
'92'
'86'
'192'
'31'
['183', '92', '86', '192', '31']
parsed_discourse_facet ['results_citation']
<S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="173" ssid = "11">The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter (α) which is tuned separately for each of the tasks.</S><S sid ="77" ssid = "9">Segments with the same surface form but different PoS tags are treated as different lexemes  and are represented as separate arcs (e.g. the two arcs labeled neim from node 6 to 7).</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'192'", "'173'", "'77'", "'133'"]
'183'
'192'
'173'
'77'
'133'
['183', '192', '173', '77', '133']
parsed_discourse_facet ['method_citation']
<S sid ="141" ssid = "19">This analyzer setting is similar to that of (Cohen and Smith  2007)  and models using it are denoted nohsp  Parser and Grammar We used BitPar (Schmid  2004)  an efficient general purpose parser 10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.</S><S sid ="89" ssid = "21">Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree π s.t.</S><S sid ="51" ssid = "9">Tsarfaty (2006) used a morphological analyzer (Segal  2000)  a PoS tagger (Bar-Haim et al.  2005)  and a general purpose parser (Schmid  2000) in an integrated framework in which morphological and syntactic components interact to share information  leading to improved performance on the joint task.</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="82" ssid = "14">In sequential tagging models such as (Adler and Elhadad  2006; Bar-Haim et al.  2007; Smith et al.  2005) weights are assigned according to a language model The input for the joint task is a sequence W = w1  ...   wn of space-delimited tokens.</S>
original cit marker offset is 0
new cit marker offset is 0



["'141'", "'89'", "'51'", "'156'", "'82'"]
'141'
'89'
'51'
'156'
'82'
['141', '89', '51', '156', '82']
parsed_discourse_facet ['method_citation']
<S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'183'", "'86'", "'0'", "'92'"]
'192'
'183'
'86'
'0'
'92'
['192', '183', '86', '0', '92']
parsed_discourse_facet ['method_citation']
<S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'183'", "'0'", "'86'", "'92'"]
'192'
'183'
'0'
'86'
'92'
['192', '183', '0', '86', '92']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="89" ssid = "21">Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree π s.t.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'86'", "'192'", "'89'", "'92'"]
'183'
'86'
'192'
'89'
'92'
['183', '86', '192', '89', '92']
parsed_discourse_facet ['method_citation']
<S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'86'", "'57'", "'192'", "'183'"]
'92'
'86'
'57'
'192'
'183'
['92', '86', '57', '192', '183']
parsed_discourse_facet ['method_citation']
<S sid ="31" ssid = "10">Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="186" ssid = "24">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'", "'92'", "'133'", "'0'", "'186'"]
'31'
'92'
'133'
'0'
'186'
['31', '92', '133', '0', '186']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'92'", "'192'", "'183'", "'133'"]
'86'
'92'
'192'
'183'
'133'
['86', '92', '192', '183', '133']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'192'", "'183'", "'0'", "'92'"]
'86'
'192'
'183'
'0'
'92'
['86', '192', '183', '0', '92']
parsed_discourse_facet ['method_citation']
<S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="123" ssid = "1">Previous work on morphological and syntactic disambiguation in Hebrew used different sets of data  different splits  differing annotation schemes  and different evaluation measures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'57'", "'192'", "'86'", "'123'"]
'92'
'57'
'192'
'86'
'123'
['92', '57', '192', '86', '123']
parsed_discourse_facet ['method_citation']
<S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="69" ssid = "1">We represent all morphological analyses of a given utterance using a lattice structure.</S><S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="113" ssid = "45">The remaining arcs are marked OOV.</S><S sid ="50" ssid = "8">The joint morphological and syntactic hypothesis was first discussed in (Tsarfaty  2006; Tsarfaty and Sima’an  2004) and empirically explored in (Tsarfaty  2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'", "'69'", "'14'", "'113'", "'50'"]
'57'
'69'
'14'
'113'
'50'
['57', '69', '14', '113', '50']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'92'", "'192'", "'183'", "'57'"]
'86'
'92'
'192'
'183'
'57'
['86', '92', '192', '183', '57']
parsed_discourse_facet ['results_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'192'", "'0'", "'92'", "'183'"]
'86'
'192'
'0'
'92'
'183'
['86', '192', '0', '92', '183']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'183'", "'192'", "'92'", "'0'"]
'86'
'183'
'192'
'92'
'0'
['86', '183', '192', '92', '0']
parsed_discourse_facet ['method_citation']
<S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S><S sid ="18" ssid = "14">Cohen and Smith (2007) followed up on these results and proposed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'92'", "'183'", "'133'", "'18'"]
'192'
'92'
'183'
'133'
'18'
['192', '92', '183', '133', '18']
parsed_discourse_facet ['method_citation']
['The current work treats both segmental and super-segmental phenomena, yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg, 2008).']
['Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', 'This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.']
['system', 'ROUGE-S*', 'Average_R:', '0.00226', '(95%-conf.int.', '0.00226', '-', '0.00226)']
['system', 'ROUGE-S*', 'Average_P:', '0.01429', '(95%-conf.int.', '0.01429', '-', '0.01429)']
['system', 'ROUGE-S*', 'Average_F:', '0.00391', '(95%-conf.int.', '0.00391', '-', '0.00391)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:210', 'F:3']
['To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).']
[u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.', 'Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', u'A morphological analyzer M : W\u2014* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00260', '(95%-conf.int.', '0.00260', '-', '0.00260)']
['system', 'ROUGE-S*', 'Average_P:', '0.00635', '(95%-conf.int.', '0.00635', '-', '0.00635)']
['system', 'ROUGE-S*', 'Average_F:', '0.00369', '(95%-conf.int.', '0.00369', '-', '0.00369)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:630', 'F:4']
['Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.']
[u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', u'A morphological analyzer M : W\u2014* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00145', '(95%-conf.int.', '0.00145', '-', '0.00145)']
['system', 'ROUGE-S*', 'Average_P:', '0.02564', '(95%-conf.int.', '0.02564', '-', '0.02564)']
['system', 'ROUGE-S*', 'Average_F:', '0.00275', '(95%-conf.int.', '0.00275', '-', '0.00275)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:78', 'F:2']
['Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.']
[u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', u'A morphological analyzer M : W\u2014* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.01234', '(95%-conf.int.', '0.01234', '-', '0.01234)']
['system', 'ROUGE-S*', 'Average_P:', '0.30909', '(95%-conf.int.', '0.30909', '-', '0.30909)']
['system', 'ROUGE-S*', 'Average_F:', '0.02373', '(95%-conf.int.', '0.02373', '-', '0.02373)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:55', 'F:17']
['Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.']
['In sequential tagging models such as (Adler and Elhadad  2006; Bar-Haim et al.  2007; Smith et al.  2005) weights are assigned according to a language model The input for the joint task is a sequence W = w1  ...   wn of space-delimited tokens.', u'Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree \xcf\u20ac s.t.', 'Tsarfaty (2006) used a morphological analyzer (Segal  2000)  a PoS tagger (Bar-Haim et al.  2005)  and a general purpose parser (Schmid  2000) in an integrated framework in which morphological and syntactic components interact to share information  leading to improved performance on the joint task.', 'To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).', 'This analyzer setting is similar to that of (Cohen and Smith  2007)  and models using it are denoted nohsp  Parser and Grammar We used BitPar (Schmid  2004)  an efficient general purpose parser 10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.']
['system', 'ROUGE-S*', 'Average_R:', '0.00193', '(95%-conf.int.', '0.00193', '-', '0.00193)']
['system', 'ROUGE-S*', 'Average_P:', '0.24359', '(95%-conf.int.', '0.24359', '-', '0.24359)']
['system', 'ROUGE-S*', 'Average_F:', '0.00382', '(95%-conf.int.', '0.00382', '-', '0.00382)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:9870', 'P:78', 'F:19']
['Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['Segments with the same surface form but different PoS tags are treated as different lexemes  and are represented as separate arcs (e.g. the two arcs labeled neim from node 6 to 7).', u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.', u'The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter (\u03b1) which is tuned separately for each of the tasks.', 'Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00161', '(95%-conf.int.', '0.00161', '-', '0.00161)']
['system', 'ROUGE-S*', 'Average_P:', '0.01058', '(95%-conf.int.', '0.01058', '-', '0.01058)']
['system', 'ROUGE-S*', 'Average_F:', '0.00279', '(95%-conf.int.', '0.00279', '-', '0.00279)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:378', 'F:4']
['The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006).']
[u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.', u'For brevity we omit the segments from the analysis  and so analysis of the form \u201cfmnh\u201d as f/REL mnh/VB is represented simply as REL VB.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', u'A morphological analyzer M : W\u2014* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00351', '(95%-conf.int.', '0.00351', '-', '0.00351)']
['system', 'ROUGE-S*', 'Average_P:', '0.03509', '(95%-conf.int.', '0.03509', '-', '0.03509)']
['system', 'ROUGE-S*', 'Average_F:', '0.00638', '(95%-conf.int.', '0.00638', '-', '0.00638)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:171', 'F:6']
['Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth, &#8216;tokens&#8217;) that constitute the unanalyzed surface forms (utterances).']
['Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.', u'Cohen and Smith approach this by introducing the \xce\xb1 hyperparameter  which performs best when optimized independently for each sentence (cf.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', 'Cohen and Smith (2007) followed up on these results and proposed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own.', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00205', '(95%-conf.int.', '0.00205', '-', '0.00205)']
['system', 'ROUGE-S*', 'Average_P:', '0.00920', '(95%-conf.int.', '0.00920', '-', '0.00920)']
['system', 'ROUGE-S*', 'Average_F:', '0.00335', '(95%-conf.int.', '0.00335', '-', '0.00335)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:435', 'F:4']
['Tsarfaty (2006) used a morphological analyzer (Segal, 2000), a PoS tagger (Bar-Haim et al., 2005), and a general purpose parser (Schmid, 2000) in an integrated framework in which morphological and syntactic components interact to share information, leading to improved performance on the joint task.']
[u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', u'A morphological analyzer M : W\u2014* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.01524', '(95%-conf.int.', '0.01524', '-', '0.01524)']
['system', 'ROUGE-S*', 'Average_P:', '0.04828', '(95%-conf.int.', '0.04828', '-', '0.04828)']
['system', 'ROUGE-S*', 'Average_F:', '0.02317', '(95%-conf.int.', '0.02317', '-', '0.02317)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:435', 'F:21']
['Both (Tsarfaty, 2006; Cohen and Smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has shown a single generative model which handles both tasks.']
['Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.', u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', u'A morphological analyzer M : W\u2014* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00067', '(95%-conf.int.', '0.00067', '-', '0.00067)']
['system', 'ROUGE-S*', 'Average_P:', '0.00585', '(95%-conf.int.', '0.00585', '-', '0.00585)']
['system', 'ROUGE-S*', 'Average_F:', '0.00121', '(95%-conf.int.', '0.00121', '-', '0.00121)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:171', 'F:1']
0.070795999292 0.00436599995634 0.0074799999252





input/ref/Task1/P04-1036_sweta.csv
input/res/Task1/P04-1036.csv
parsing: input/ref/Task1/P04-1036_sweta.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="82" ssid="11">We also calculate the WSD accuracy that would be obtained on SemCor, when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["82'"]
82'
['82']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="20">We briefly summarise the two measures here; for a more detailed summary see (Patwardhan et al., 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="172" ssid="20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="1">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="165" ssid="13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus, whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S>
original cit marker offset is 0
new cit marker offset is 0



["165'"]
165'
['165']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["171'"]
171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["89'"]
89'
['89']
parsed_discourse_facet ['method_citation']
<S sid="172" ssid="20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="12">Additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["189'"]
189'
['189']
parsed_discourse_facet ['method_citation']
<S sid="87" ssid="16">Again, the automatic ranking outperforms this by a large margin.</S>
original cit marker offset is 0
new cit marker offset is 0



["87'"]
87'
['87']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["89'"]
89'
['89']
parsed_discourse_facet ['method_citation']
 <S sid="137" ssid="14">Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["137'"]
137'
['137']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="19">We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline, but because of space limitations give results for the two which perform the best.</S>
original cit marker offset is 0
new cit marker offset is 0



["63'"]
63'
['63']
parsed_discourse_facet ['method_citation']
<S sid="159" ssid="7">Magnini and Cavagli`a (2000) have identified WordNet word senses with particular domains, and this has proven useful for high precision WSD (Magnini et al., 2001); indeed in section 5 we used these domain labels for evaluation.</S>
original cit marker offset is 0
new cit marker offset is 0



["159'"]
159'
['159']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="115" ssid = "13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor  which is very encouraging given that our method only uses raw text  with no manual labelling.</S><S sid ="12" ssid = "5">The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor  from those that do not (without HTD).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'123'", "'14'", "'115'", "'12'"]
'8'
'123'
'14'
'115'
'12'
['8', '123', '14', '115', '12']
parsed_discourse_facet ['results_citation']
<S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="105" ssid = "3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents  rather than a lexical sample task  where the target words are manually determined and the results will depend on the skew of the words in the sample.</S><S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="145" ssid = "22">It is not always intuitively clear which of the senses to expect as predominant sense for either a particular domain or for the BNC  but the first senses of words like division and goal shift towards the more specific senses (league and score respectively).</S><S sid ="155" ssid = "3">A major benefit of our work  rather than reliance on hand-tagged training data such as SemCor  is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'105'", "'165'", "'145'", "'155'"]
'15'
'105'
'165'
'145'
'155'
['15', '105', '165', '145', '155']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="174" ssid = "22">We have restricted ourselves to nouns in this work  since this PoS is perhaps most affected by domain.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of “nearest neighbours”.</S><S sid ="133" ssid = "10">We acquired thesauruses for these corpora using the procedure described in section 2.1.</S><S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'174'", "'34'", "'133'", "'7'"]
'100'
'174'
'34'
'133'
'7'
['100', '174', '34', '133', '7']
parsed_discourse_facet ['method_citation']
<S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="70" ssid = "26">Jiang and Conrath specify a distance measure:   where the third class ( ) is the most informative  or most specific  superordinate synset of the two senses and .</S><S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="71" ssid = "27">This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal:</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'50'", "'70'", "'61'", "'71'"]
'44'
'50'
'70'
'61'
'71'
['44', '50', '70', '61', '71']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S><S sid ="72" ssid = "1">In order to evaluate our method we use the data in SemCor as a gold-standard.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'72'", "'91'", "'10'", "'45'"]
'30'
'72'
'91'
'10'
'45'
['30', '72', '91', '10', '45']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S><S sid ="81" ssid = "10">4 We calculate the accuracy of finding the predominant sense  when there is indeed one sense with a higher frequency than the others for this word in SemCor ( ).</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'50'", "'82'", "'81'", "'44'"]
'69'
'50'
'82'
'81'
'44'
['69', '50', '82', '81', '44']
parsed_discourse_facet ['method_citation']
<S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S><S sid ="58" ssid = "14">A noun    is thus described by a set of co-occurrence triples and associated frequencies  where is a grammatical relation and is a possible cooccurrence with in that relation.</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S><S sid ="56" ssid = "12">For each noun we considered the co-occurring verbs in the direct object and subject relation  the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations.</S><S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'58'", "'45'", "'56'", "'7'"]
'55'
'58'
'45'
'56'
'7'
['55', '58', '45', '56', '7']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S><S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="53" ssid = "9">This weight is the WordNet similarity score ( ) between the target sense ( ) and the sense of ( ) that maximises this score  divided by the sum of all such WordNet similarity scores for and .</S><S sid ="52" ssid = "8">For each sense of ( ) we obtain a ranking score by summing over the of each neighbour ( ) multiplied by a weight.</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'136'", "'53'", "'52'", "'82'"]
'30'
'136'
'53'
'52'
'82'
['30', '136', '53', '52', '82']
parsed_discourse_facet ['method_citation']
<S sid ="135" ssid = "12">We therefore decided to select a limited number of words and to evaluate these words qualitatively.</S><S sid ="179" ssid = "2">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S sid ="111" ssid = "9">We give the results for this WSD task in table 2.</S><S sid ="54" ssid = "10">Thus we rank each sense using: parser (Briscoe and Carroll  2002).</S><S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'135'", "'179'", "'111'", "'54'", "'6'"]
'135'
'179'
'111'
'54'
'6'
['135', '179', '111', '54', '6']
parsed_discourse_facet ['method_citation']
<S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="156" ssid = "4">Buitelaar and Sacaleanu (2001) have previously explored ranking and selection of synsets in GermaNet for specific domains using the words in a given synset  and those related by hyponymy  and a term relevance measure taken from information retrieval.</S><S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'156'", "'61'", "'65'", "'142'"]
'137'
'156'
'61'
'65'
'142'
['137', '156', '61', '65', '142']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="71" ssid = "27">This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal:</S><S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'69'", "'44'", "'71'", "'30'"]
'50'
'69'
'44'
'71'
'30'
['50', '69', '44', '71', '30']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="70" ssid = "26">Jiang and Conrath specify a distance measure:   where the third class ( ) is the most informative  or most specific  superordinate synset of the two senses and .</S><S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'70'", "'137'", "'69'", "'44'"]
'50'
'70'
'137'
'69'
'44'
['50', '70', '137', '69', '44']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">SemCor comprises a relatively small sample of 250 000 words.</S><S sid ="161" ssid = "9">Our approach is complementary to this.</S><S sid ="29" ssid = "22">We discuss our method in the following section.</S><S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of “nearest neighbours”.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'161'", "'29'", "'6'", "'34'"]
'16'
'161'
'29'
'6'
'34'
['16', '161', '29', '6', '34']
parsed_discourse_facet ['results_citation']
<S sid ="157" ssid = "5">Buitelaar and Sacaleanu have evaluated their method on identifying domain specific concepts using human judgements on 100 items.</S><S sid ="185" ssid = "8">In the future  we will perform a large scale evaluation on domain specific corpora.</S><S sid ="193" ssid = "2">This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.</S><S sid ="39" ssid = "32">We expect that the quantity and similarity of the neighbours pertaining to different senses will reflect the dominance of the sense to which they pertain.</S><S sid ="128" ssid = "5">The Reuters corpus (Rose et al.  2002) is a collection of about 810 000 Reuters  English Language News stories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'185'", "'193'", "'39'", "'128'"]
'157'
'185'
'193'
'39'
'128'
['157', '185', '193', '39', '128']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "24">We are of course able to apply the method to other versions of WordNet. synset  is incremented with the frequency counts from the corpus of all words belonging to that synset  directly or via the hyponymy relation.</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'50'", "'65'", "'44'", "'82'"]
'68'
'50'
'65'
'44'
'82'
['68', '50', '65', '44', '82']
parsed_discourse_facet ['method_citation']
<S sid ="90" ssid = "19">From manual analysis  there are cases where the acquired first sense disagrees with SemCor  yet is intuitively plausible.</S><S sid ="175" ssid = "23">We are currently investigating the performance of the first sense heuristic  and this method  for other PoS on SENSEVAL-3 data (McCarthy et al.  2004)  although not yet with rankings from domain specific corpora.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="2" ssid = "2">The problem with using the predominant  or first sense heuristic  aside from the fact that it does not take surrounding context into account  is that it assumes some quantity of handtagged data.</S><S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'175'", "'8'", "'2'", "'1'"]
'90'
'175'
'8'
'2'
'1'
['90', '175', '8', '2', '1']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="0" ssid = "0">Finding Predominant Word Senses in Untagged Text</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'10'", "'0'", "'50'", "'30'"]
'69'
'10'
'0'
'50'
'30'
['69', '10', '0', '50', '30']
parsed_discourse_facet ['method_citation']
['We briefly summarise the two measures here; for a more detailed summary see (Patwardhan et al., 2003).']
['We acquired thesauruses for these corpora using the procedure described in section 2.1.', 'In the English all-words SENSEVAL-2  25% of the noun data was monosemous.', 'We have restricted ourselves to nouns in this work  since this PoS is perhaps most affected by domain.', 'Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.', u'In these each target word is entered with an ordered list of \u201cnearest neighbours\u201d.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:28', 'F:0']
['We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline, but because of space limitations give results for the two which perform the best.']
['word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.', 'From manual analysis  there are cases where the acquired first sense disagrees with SemCor  yet is intuitively plausible.', 'We are currently investigating the performance of the first sense heuristic  and this method  for other PoS on SENSEVAL-3 data (McCarthy et al.  2004)  although not yet with rankings from domain specific corpora.', 'The problem with using the predominant  or first sense heuristic  aside from the fact that it does not take surrounding context into account  is that it assumes some quantity of handtagged data.', 'The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:66', 'F:0']
['In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.']
['We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).', 'Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.', 'The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.', 'This weight is the WordNet similarity score ( ) between the target sense ( ) and the sense of ( ) that maximises this score  divided by the sum of all such WordNet similarity scores for and .', 'For each sense of ( ) we obtain a ranking score by summing over the of each neighbour ( ) multiplied by a weight.']
['system', 'ROUGE-S*', 'Average_R:', '0.00484', '(95%-conf.int.', '0.00484', '-', '0.00484)']
['system', 'ROUGE-S*', 'Average_P:', '0.14545', '(95%-conf.int.', '0.14545', '-', '0.14545)']
['system', 'ROUGE-S*', 'Average_F:', '0.00937', '(95%-conf.int.', '0.00937', '-', '0.00937)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:55', 'F:8']
['Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.']
['The Reuters corpus (Rose et al.  2002) is a collection of about 810 000 Reuters  English Language News stories.', 'We expect that the quantity and similarity of the neighbours pertaining to different senses will reflect the dominance of the sense to which they pertain.', 'This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.', 'Buitelaar and Sacaleanu have evaluated their method on identifying domain specific concepts using human judgements on 100 items.', 'In the future  we will perform a large scale evaluation on domain specific corpora.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:171', 'F:0']
['Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.']
['We are of course able to apply the method to other versions of WordNet. synset  is incremented with the frequency counts from the corpus of all words belonging to that synset  directly or via the hyponymy relation.', 'We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).', 'We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within', 'For input we used grammatical relation data extracted using an automatic where: .', 'The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.']
['system', 'ROUGE-S*', 'Average_R:', '0.00169', '(95%-conf.int.', '0.00169', '-', '0.00169)']
['system', 'ROUGE-S*', 'Average_P:', '0.02206', '(95%-conf.int.', '0.02206', '-', '0.02206)']
['system', 'ROUGE-S*', 'Average_F:', '0.00315', '(95%-conf.int.', '0.00315', '-', '0.00315)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:136', 'F:3']
['word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.']
['We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).', u'The frequency data is used to calculate the \u201cinformation content\u201d (IC) of a class .', 'We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within', '4 We calculate the accuracy of finding the predominant sense  when there is indeed one sense with a higher frequency than the others for this word in SemCor ( ).', 'For input we used grammatical relation data extracted using an automatic where: .']
['system', 'ROUGE-S*', 'Average_R:', '0.00664', '(95%-conf.int.', '0.00664', '-', '0.00664)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.01223', '(95%-conf.int.', '0.01223', '-', '0.01223)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:78', 'F:6']
['Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.']
['Thus we rank each sense using: parser (Briscoe and Carroll  2002).', 'We give the results for this WSD task in table 2.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.', 'We therefore decided to select a limited number of words and to evaluate these words qualitatively.', 'This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.']
['system', 'ROUGE-S*', 'Average_R:', '0.00357', '(95%-conf.int.', '0.00357', '-', '0.00357)']
['system', 'ROUGE-S*', 'Average_P:', '0.01170', '(95%-conf.int.', '0.01170', '-', '0.01170)']
['system', 'ROUGE-S*', 'Average_F:', '0.00546', '(95%-conf.int.', '0.00546', '-', '0.00546)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:171', 'F:2']
['Magnini and Cavagli`a (2000) have identified WordNet word senses with particular domains, and this has proven useful for high precision WSD (Magnini et al., 2001); indeed in section 5 we used these domain labels for evaluation.']
['For input we used grammatical relation data extracted using an automatic where: .', u'The frequency data is used to calculate the \u201cinformation content\u201d (IC) of a class .', 'Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.', 'Finding Predominant Word Senses in Untagged Text', 'The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).']
['system', 'ROUGE-S*', 'Average_R:', '0.00555', '(95%-conf.int.', '0.00555', '-', '0.00555)']
['system', 'ROUGE-S*', 'Average_P:', '0.03158', '(95%-conf.int.', '0.03158', '-', '0.03158)']
['system', 'ROUGE-S*', 'Average_F:', '0.00944', '(95%-conf.int.', '0.00944', '-', '0.00944)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:190', 'F:6']
['word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.']
[u'In these each target word is entered with an ordered list of \u201cnearest neighbours\u201d.', 'This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.', 'Our approach is complementary to this.', 'We discuss our method in the following section.', 'SemCor comprises a relatively small sample of 250 000 words.']
['system', 'ROUGE-S*', 'Average_R:', '0.00285', '(95%-conf.int.', '0.00285', '-', '0.00285)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00466', '(95%-conf.int.', '0.00466', '-', '0.00466)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:78', 'F:1']
['Again, the automatic ranking outperforms this by a large margin.']
[u'The frequency data is used to calculate the \u201cinformation content\u201d (IC) of a class .', 'We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within', 'Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.', 'For input we used grammatical relation data extracted using an automatic where: .', 'Jiang and Conrath specify a distance measure:   where the third class ( ) is the most informative  or most specific  superordinate synset of the two senses and .']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:10', 'F:0']
0.0300529996995 0.00251399997486 0.00443099995569





input/ref/Task1/P87-1015_sweta.csv
input/res/Task1/P87-1015.csv
parsing: input/ref/Task1/P87-1015_sweta.csv
<S sid="205" ssid="11">Independence of paths at this level reflects context freeness of rewriting and suggests why they can be recognized efficiently.</S>
original cit marker offset is 0
new cit marker offset is 0



["205'"]
205'
['205']
parsed_discourse_facet ['method_citation']
<S sid="229" ssid="35">In addition, the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars, and the hierarchy defined by Weir).</S>
original cit marker offset is 0
new cit marker offset is 0



["229'"]
229'
['229']
parsed_discourse_facet ['method_citation']
<S sid="146" ssid="31">Since every CFL is known to be semilinear (Parikh, 1966), in order to show semilinearity of some language, we need only show the existence of a letter equivalent CFL Our definition of LCFRS's insists that the composition operations are linear and nonerasing.</S>
original cit marker offset is 0
new cit marker offset is 0



["146'"]
146'
['146']
parsed_discourse_facet ['method_citation']
<S sid="201" ssid="7">It is interesting to note, however, that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG, FUG and IG's.</S>
original cit marker offset is 0
new cit marker offset is 0



["201'"]
201'
['201']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
 <S sid="222" ssid="28">However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



["222'"]
222'
['222']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="7">Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.</S>
    <S sid="23" ssid="8">He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["22'", "'23'"]
22'
'23'
['22', '23']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="221" ssid="27">Members of LCFRS whose operations have this property can be translated into the ILFP notation (Rounds, 1985).</S>
original cit marker offset is 0
new cit marker offset is 0



["221'"]
221'
['221']
parsed_discourse_facet ['method_citation']
<S sid="54" ssid="39">An IG can be viewed as a CFG in which each nonterminal is associated with a stack.</S>
original cit marker offset is 0
new cit marker offset is 0



["54'"]
54'
['54']
parsed_discourse_facet ['method_citation']
<S sid="128" ssid="13">As in the case of the derivation trees of CFG's, nodes are labeled by a member of some finite set of symbols (perhaps only implicit in the grammar as in TAG's) used to denote derived structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["128'"]
128'
['128']
parsed_discourse_facet ['method_citation']
<S sid="217" ssid="23">In considering the recognition of these languages, we were forced to be more specific regarding the relationship between the structures derived by these formalisms and the substrings they span.</S>
original cit marker offset is 0
new cit marker offset is 0



[";217'"]
;217'
[';217']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="1">From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="1">From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="214" ssid="20">LCFRS's share several properties possessed by the class of mildly context-sensitive formalisms discussed by Joshi (1983/85).</S>
original cit marker offset is 0
new cit marker offset is 0



["214'"]
214'
['214']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="20">TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish, for cross serial dependencies in Dutch subordinate clauses, and for the nestings of paired English complementizers.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P87-1015.csv
<S sid ="115" ssid = "21">From the point of view of recognition  independent paths in the derivation structures suggests that a top-down parser (for example) can work on each branch independently  which may lead to efficient parsing using an algorithm based on the Divide and Conquer technique.</S><S sid ="222" ssid = "28">However  in order to capture the properties of various grammatical systems under consideration  our notation is more restrictive that ILFP  which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="67" ssid = "52">On the one hand  the definition of composition in Steedman (1985)  which technically permits composition of functions with unbounded number of arguments  generates tree sets with dependent paths such as those shown in Figure 6.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'222'", "'195'", "'67'", "'153'"]
'115'
'222'
'195'
'67'
'153'
['115', '222', '195', '67', '153']
parsed_discourse_facet ['results_citation']
<S sid ="37" ssid = "22">Like CFG's  the choice is predetermined by a finite number of rules encapsulated in the grammar.</S><S sid ="231" ssid = "37">In this paper  our goal has been to use the notion of LCFRS's to classify grammatical systems on the basis of their strong generative capacity.</S><S sid ="197" ssid = "3">We address the question of whether or not a formalism can generate only structural descriptions with independent paths.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S><S sid ="92" ssid = "77">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'231'", "'197'", "'116'", "'92'"]
'37'
'231'
'197'
'116'
'92'
['37', '231', '197', '116', '92']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.</S><S sid ="131" ssid = "16">The composition operations in the case of CFG's are parameterized by the productions.</S><S sid ="65" ssid = "50">While the generative power of CG's is greater that of CFG's  it appears to be highly constrained.</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="179" ssid = "64">It can be seen that M performs a top-down recognition of the input al ... nin logspace.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'131'", "'65'", "'227'", "'179'"]
'204'
'131'
'65'
'227'
'179'
['204', '131', '65', '227', '179']
parsed_discourse_facet ['method_citation']
<S sid ="194" ssid = "79">Thus  M works in logspace and recognition can be done on a deterministic TM in polynomial tape.</S><S sid ="92" ssid = "77">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S><S sid ="65" ssid = "50">While the generative power of CG's is greater that of CFG's  it appears to be highly constrained.</S><S sid ="20" ssid = "5">As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.</S><S sid ="131" ssid = "16">The composition operations in the case of CFG's are parameterized by the productions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'92'", "'65'", "'20'", "'131'"]
'194'
'92'
'65'
'20'
'131'
['194', '92', '65', '20', '131']
parsed_discourse_facet ['method_citation']
<S sid ="155" ssid = "40">CFG's  TAG's  MCTAG's and HG's are all members of this class since they satisfy these restrictions.</S><S sid ="37" ssid = "22">Like CFG's  the choice is predetermined by a finite number of rules encapsulated in the grammar.</S><S sid ="117" ssid = "2">Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.</S><S sid ="92" ssid = "77">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S><S sid ="48" ssid = "33">There has been recent interest in the application of Indexed Grammars (IG's) to natural languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'", "'37'", "'117'", "'92'", "'48'"]
'155'
'37'
'117'
'92'
'48'
['155', '37', '117', '92', '48']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "5">As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.</S><S sid ="216" ssid = "22">Having defined LCFRS's  in Section 4.2 we established the semilinearity (and hence constant growth property) of the languages generated.</S><S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="179" ssid = "64">It can be seen that M performs a top-down recognition of the input al ... nin logspace.</S><S sid ="131" ssid = "16">The composition operations in the case of CFG's are parameterized by the productions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'216'", "'59'", "'179'", "'131'"]
'20'
'216'
'59'
'179'
'131'
['20', '216', '59', '179', '131']
parsed_discourse_facet ['method_citation']
<S sid ="162" ssid = "47">Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="219" ssid = "25">The composition operations are mapped onto operations that use concatenation to define the substrings spanned by the resulting structures.</S><S sid ="0" ssid = "0">CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*</S><S sid ="193" ssid = "78">Since the work tapes store integers (which can be written in binary) that never exceed the size of the input  no configuration has space exceeding 0(log n).</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'", "'195'", "'219'", "'0'", "'193'"]
'162'
'195'
'219'
'0'
'193'
['162', '195', '219', '0', '193']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "9">By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.</S><S sid ="115" ssid = "21">From the point of view of recognition  independent paths in the derivation structures suggests that a top-down parser (for example) can work on each branch independently  which may lead to efficient parsing using an algorithm based on the Divide and Conquer technique.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="1" ssid = "1">We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'115'", "'153'", "'1'", "'195'"]
'11'
'115'
'153'
'1'
'195'
['11', '115', '153', '1', '195']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*</S><S sid ="219" ssid = "25">The composition operations are mapped onto operations that use concatenation to define the substrings spanned by the resulting structures.</S><S sid ="125" ssid = "10">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S sid ="58" ssid = "43">Unification is used in LFG's to produce structures having two dependent spines of unbounded length as in Figure 5.</S><S sid ="192" ssid = "77">Thus  the ATM has no more than 6km&quot; + 1 work tapes  where km&quot; is the maximum number of substrings spanned by a derived structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'219'", "'125'", "'58'", "'192'"]
'0'
'219'
'125'
'58'
'192'
['0', '219', '125', '58', '192']
parsed_discourse_facet ['method_citation']
<S sid ="193" ssid = "78">Since the work tapes store integers (which can be written in binary) that never exceed the size of the input  no configuration has space exceeding 0(log n).</S><S sid ="129" ssid = "14">Frontier nodes are annotated by zero arty functions corresponding to elementary structures.</S><S sid ="127" ssid = "12">Nodes are annotated by the name of the composition operation used at that step in the derivation.</S><S sid ="172" ssid = "57">A step of an ATM consists of reading a symbol from each tape and optionally moving each head to the left or right one tape cell.</S><S sid ="173" ssid = "58">A configuration of M consists of a state of the finite control  the nonblank contents of the input tape and k work tapes  and the position of each head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'193'", "'129'", "'127'", "'172'", "'173'"]
'193'
'129'
'127'
'172'
'173'
['193', '129', '127', '172', '173']
parsed_discourse_facet ['method_citation']
<S sid ="189" ssid = "74">For rules p : A fpo such that fp is constant function  giving an elementary structure  fp is defined such that fp() = (Si ... xi() where each z is a constant string.</S><S sid ="183" ssid = "68">We assume that M is in an existential state qA  with integers i1 and i2 representing zi in the (2i — 1)th and 22th work tape  for 1 < i < k. For each rule p : A fp(B  C) such that fp is mapped onto the function fp defined by the following rule. jp((xi .. •  rnt)  (1ii  • • • • Yn3))= (Zi   • • •   Zk) M breaks xi   zk into substrings xi    xn  and yi ... y&quot; conforming to the definition of fp.</S><S sid ="160" ssid = "45">A derived structure will be mapped onto a sequence zi of substrings (not necessarily contiguous in the input)  and the composition operations will be mapped onto functions that can defined as follows3. f((zi • • •   zni)  (m. • • •  Yn3)) = (Z1  • • •   Zn3) where each z  is the concatenation of strings from z 's and yk's.</S><S sid ="148" ssid = "33">If 0(A) gives the number of occurrences of each terminal in the structure named by A  then  given the constraints imposed on the formalism  for each rule A --. fp(Ai    An) we have the equality where c„ is some constant.</S><S sid ="201" ssid = "7">It is interesting to note  however  that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG  FUG and IG's.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'183'", "'160'", "'148'", "'201'"]
'189'
'183'
'160'
'148'
'201'
['189', '183', '160', '148', '201']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*</S><S sid ="162" ssid = "47">Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="196" ssid = "2">We contrasted formalisms such as CFG's  HG's  TAG's and MCTAG's  with formalisms such as IG's and unificational systems such as LFG's and FUG's.</S><S sid ="204" ssid = "10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'162'", "'227'", "'196'", "'204'"]
'0'
'162'
'227'
'196'
'204'
['0', '162', '227', '196', '204']
parsed_discourse_facet ['method_citation']
<S sid ="58" ssid = "43">Unification is used in LFG's to produce structures having two dependent spines of unbounded length as in Figure 5.</S><S sid ="214" ssid = "20">LCFRS's share several properties possessed by the class of mildly context-sensitive formalisms discussed by Joshi (1983/85).</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="196" ssid = "2">We contrasted formalisms such as CFG's  HG's  TAG's and MCTAG's  with formalisms such as IG's and unificational systems such as LFG's and FUG's.</S><S sid ="163" ssid = "48">This representation of structures by substrings and the composition operation by its effect on substrings is related to the work of Rounds (1985).</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'214'", "'227'", "'196'", "'163'"]
'58'
'214'
'227'
'196'
'163'
['58', '214', '227', '196', '163']
parsed_discourse_facet ['results_citation']
<S sid ="50" ssid = "35">The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.</S><S sid ="202" ssid = "8">As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.</S><S sid ="7" ssid = "5">The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG's and IG's.</S><S sid ="230" ssid = "36">LCFRS's have only been loosely defined in this paper; we have yet to provide a complete set of formal properties associated with members of this class.</S><S sid ="110" ssid = "16">The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'202'", "'7'", "'230'", "'110'"]
'50'
'202'
'7'
'230'
'110'
['50', '202', '7', '230', '110']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "42">For example  in TAG's a derived auxiliary tree spans two substrings (to the left and right of the foot node)  and the adjunction operation inserts another substring (spanned by the subtree under the node where adjunction takes place) between them (see Figure 3).</S><S sid ="162" ssid = "47">Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .</S><S sid ="83" ssid = "68">The following context-free production captures the derivation step of the grammar shown in Figure 7  in which the trees in the auxiliary tree set are adjoined into themselves at the root node (address c).</S><S sid ="67" ssid = "52">On the one hand  the definition of composition in Steedman (1985)  which technically permits composition of functions with unbounded number of arguments  generates tree sets with dependent paths such as those shown in Figure 6.</S><S sid ="154" ssid = "39">Since each composition operation is linear and nonerasing  a bounded sequences of substrings associated with the resulting structure is obtained by combining the substrings in each of its arguments using only the concatenation operation  including each substring exactly once.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'162'", "'83'", "'67'", "'154'"]
'157'
'162'
'83'
'67'
'154'
['157', '162', '83', '67', '154']
parsed_discourse_facet ['method_citation']
<S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="156" ssid = "41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S><S sid ="204" ssid = "10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.</S><S sid ="213" ssid = "19">By sharing stacks (in IG's) or by using nonlinear equations over f-structures (in FUG's and LFG's)  structures with unbounded dependencies between paths can be generated.</S><S sid ="125" ssid = "10">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'227'", "'156'", "'204'", "'213'", "'125'"]
'227'
'156'
'204'
'213'
'125'
['227', '156', '204', '213', '125']
parsed_discourse_facet ['method_citation']
['Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.', 'He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.']
['We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.', 'CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*', 'Since the work tapes store integers (which can be written in binary) that never exceed the size of the input  no configuration has space exceeding 0(log n).', 'Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .', 'The composition operations are mapped onto operations that use concatenation to define the substrings spanned by the resulting structures.']
['system', 'ROUGE-S*', 'Average_R:', '0.00302', '(95%-conf.int.', '0.00302', '-', '0.00302)']
['system', 'ROUGE-S*', 'Average_P:', '0.02381', '(95%-conf.int.', '0.02381', '-', '0.02381)']
['system', 'ROUGE-S*', 'Average_F:', '0.00537', '(95%-conf.int.', '0.00537', '-', '0.00537)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:210', 'F:5']
['Independence of paths at this level reflects context freeness of rewriting and suggests why they can be recognized efficiently.']
['From the point of view of recognition  independent paths in the derivation structures suggests that a top-down parser (for example) can work on each branch independently  which may lead to efficient parsing using an algorithm based on the Divide and Conquer technique.', 'We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.', 'In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.', 'However  in order to capture the properties of various grammatical systems under consideration  our notation is more restrictive that ILFP  which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.', 'On the one hand  the definition of composition in Steedman (1985)  which technically permits composition of functions with unbounded number of arguments  generates tree sets with dependent paths such as those shown in Figure 6.']
['system', 'ROUGE-S*', 'Average_R:', '0.00134', '(95%-conf.int.', '0.00134', '-', '0.00134)']
['system', 'ROUGE-S*', 'Average_P:', '0.13333', '(95%-conf.int.', '0.13333', '-', '0.13333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00266', '(95%-conf.int.', '0.00266', '-', '0.00266)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4465', 'P:45', 'F:6']
["In addition, the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars, and the hierarchy defined by Weir)."]
['We address the question of whether or not a formalism can generate only structural descriptions with independent paths.', 'From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).', "Like CFG's  the choice is predetermined by a finite number of rules encapsulated in the grammar.", "In this paper  our goal has been to use the notion of LCFRS's to classify grammatical systems on the basis of their strong generative capacity.", 'We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.']
['system', 'ROUGE-S*', 'Average_R:', '0.03656', '(95%-conf.int.', '0.03656', '-', '0.03656)']
['system', 'ROUGE-S*', 'Average_P:', '0.09885', '(95%-conf.int.', '0.09885', '-', '0.09885)']
['system', 'ROUGE-S*', 'Average_F:', '0.05338', '(95%-conf.int.', '0.05338', '-', '0.05338)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:435', 'F:43']
["TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish, for cross serial dependencies in Dutch subordinate clauses, and for the nestings of paired English complementizers."]
['Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.', "By sharing stacks (in IG's) or by using nonlinear equations over f-structures (in FUG's and LFG's)  structures with unbounded dependencies between paths can be generated.", "The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.", 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', "Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings."]
['system', 'ROUGE-S*', 'Average_R:', '0.00512', '(95%-conf.int.', '0.00512', '-', '0.00512)']
['system', 'ROUGE-S*', 'Average_P:', '0.05714', '(95%-conf.int.', '0.05714', '-', '0.05714)']
['system', 'ROUGE-S*', 'Average_F:', '0.00939', '(95%-conf.int.', '0.00939', '-', '0.00939)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:210', 'F:12']
["LCFRS's share several properties possessed by the class of mildly context-sensitive formalisms discussed by Joshi (1983/85)."]
['Since each composition operation is linear and nonerasing  a bounded sequences of substrings associated with the resulting structure is obtained by combining the substrings in each of its arguments using only the concatenation operation  including each substring exactly once.', 'The following context-free production captures the derivation step of the grammar shown in Figure 7  in which the trees in the auxiliary tree set are adjoined into themselves at the root node (address c).', 'Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .', "For example  in TAG's a derived auxiliary tree spans two substrings (to the left and right of the foot node)  and the adjunction operation inserts another substring (spanned by the subtree under the node where adjunction takes place) between them (see Figure 3).", 'On the one hand  the definition of composition in Steedman (1985)  which technically permits composition of functions with unbounded number of arguments  generates tree sets with dependent paths such as those shown in Figure 6.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3828', 'P:78', 'F:0']
["From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'."]
["As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.", "LCFRS's have only been loosely defined in this paper; we have yet to provide a complete set of formal properties associated with members of this class.", "The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.", "The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG's and IG's.", 'The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .']
['system', 'ROUGE-S*', 'Average_R:', '0.01748', '(95%-conf.int.', '0.01748', '-', '0.01748)']
['system', 'ROUGE-S*', 'Average_P:', '0.34167', '(95%-conf.int.', '0.34167', '-', '0.34167)']
['system', 'ROUGE-S*', 'Average_F:', '0.03325', '(95%-conf.int.', '0.03325', '-', '0.03325)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:120', 'F:41']
['However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.']
["Having defined LCFRS's  in Section 4.2 we established the semilinearity (and hence constant growth property) of the languages generated.", 'Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.', "The composition operations in the case of CFG's are parameterized by the productions.", "As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.", 'It can be seen that M performs a top-down recognition of the input al ... nin logspace.']
['system', 'ROUGE-S*', 'Average_R:', '0.00163', '(95%-conf.int.', '0.00163', '-', '0.00163)']
['system', 'ROUGE-S*', 'Average_P:', '0.01053', '(95%-conf.int.', '0.01053', '-', '0.01053)']
['system', 'ROUGE-S*', 'Average_F:', '0.00283', '(95%-conf.int.', '0.00283', '-', '0.00283)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:190', 'F:2']
["As in the case of the derivation trees of CFG's, nodes are labeled by a member of some finite set of symbols (perhaps only implicit in the grammar as in TAG's) used to denote derived structures."]
[u'We assume that M is in an existential state qA  with integers i1 and i2 representing zi in the (2i \u2014 1)th and 22th work tape  for 1 ', 'For rules p : A fpo such that fp is constant function  giving an elementary structure  fp is defined such that fp() = (Si ... xi() where each z is a constant string.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:120', 'F:0']
["Since every CFL is known to be semilinear (Parikh, 1966), in order to show semilinearity of some language, we need only show the existence of a letter equivalent CFL Our definition of LCFRS's insists that the composition operations are linear and nonerasing."]
['Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.', "The composition operations in the case of CFG's are parameterized by the productions.", "The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.", 'It can be seen that M performs a top-down recognition of the input al ... nin logspace.', "While the generative power of CG's is greater that of CFG's  it appears to be highly constrained."]
['system', 'ROUGE-S*', 'Average_R:', '0.00073', '(95%-conf.int.', '0.00073', '-', '0.00073)']
['system', 'ROUGE-S*', 'Average_P:', '0.00526', '(95%-conf.int.', '0.00526', '-', '0.00526)']
['system', 'ROUGE-S*', 'Average_F:', '0.00128', '(95%-conf.int.', '0.00128', '-', '0.00128)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:190', 'F:1']
["It is interesting to note, however, that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG, FUG and IG's."]
['Thus  M works in logspace and recognition can be done on a deterministic TM in polynomial tape.', "The composition operations in the case of CFG's are parameterized by the productions.", 'We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.', "As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.", "While the generative power of CG's is greater that of CFG's  it appears to be highly constrained."]
['system', 'ROUGE-S*', 'Average_R:', '0.00385', '(95%-conf.int.', '0.00385', '-', '0.00385)']
['system', 'ROUGE-S*', 'Average_P:', '0.01579', '(95%-conf.int.', '0.01579', '-', '0.01579)']
['system', 'ROUGE-S*', 'Average_F:', '0.00619', '(95%-conf.int.', '0.00619', '-', '0.00619)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:190', 'F:3']
['Members of LCFRS whose operations have this property can be translated into the ILFP notation (Rounds, 1985).']
["Unification is used in LFG's to produce structures having two dependent spines of unbounded length as in Figure 5.", 'CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*', 'Thus  the ATM has no more than 6km&quot; + 1 work tapes  where km&quot; is the maximum number of substrings spanned by a derived structure.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'The composition operations are mapped onto operations that use concatenation to define the substrings spanned by the resulting structures.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:36', 'F:0']
0.0623981812509 0.00633909085146 0.010395454451





input/ref/Task1/D10-1044_sweta.csv
input/res/Task1/D10-1044.csv
parsing: input/ref/Task1/D10-1044_sweta.csv
<S sid="4" ssid="1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S>
original cit marker offset is 0
new cit marker offset is 0



["4'"]
4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="1">We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="4">For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components (word-alignment model, language model, translation model, etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["7'"]
7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="14">Linear weights are difficult to incorporate into the standard MERT procedure because they are &#8220;hidden&#8221; within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn, 2007), we circumvent this problem by choosing weights to optimize corpus loglikelihood, which is roughly speaking the training criterion used by the LM and TM themselves.</S>
original cit marker offset is 0
new cit marker offset is 0



["50'"]
50'
['50']
parsed_discourse_facet ['method_citation']
<S sid="152" ssid="9">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["152'"]
152'
['152']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
 <S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="97" ssid="1">We carried out translation experiments in two different settings.</S>
original cit marker offset is 0
new cit marker offset is 0



["97'"]
97'
['97']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="12">Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="10">Finally, we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="141" ssid="10">Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L&#168;u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L&#168;u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["141'"]
141'
['141']
parsed_discourse_facet ['method_citation']
 <S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="37" ssid="1">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S>
original cit marker offset is 0
new cit marker offset is 0



["37'"]
37'
['37']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D10-1044.csv
<S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S><S sid ="10" ssid = "7">This is a standard adaptation problem for SMT.</S><S sid ="4" ssid = "1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S><S sid ="136" ssid = "5">It is also worth pointing out a connection with Daum´e’s (2007) work that splits each feature into domain-specific and general copies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'128'", "'140'", "'10'", "'4'", "'136'"]
'128'
'140'
'10'
'4'
'136'
['128', '140', '10', '4', '136']
parsed_discourse_facet ['results_citation']
<S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="82" ssid = "19">This is not unreasonable given the application to phrase pairs from OUT  but it suggests that an interesting alternative might be to use a plain log-linear weighting function exp(Ei Aifi(s  t))  with outputs in [0  oo].</S><S sid ="142" ssid = "11">There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'38'", "'82'", "'142'", "'130'"]
'143'
'38'
'82'
'142'
'130'
['143', '38', '82', '142', '130']
parsed_discourse_facet ['method_citation']
<S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S><S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="31" ssid = "28">For comparison to information-retrieval inspired baselines  eg (L¨u et al.  2007)  we select sentences from OUT using language model perplexities from IN.</S><S sid ="128" ssid = "32">Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'7'", "'31'", "'128'", "'78'"]
'140'
'7'
'31'
'128'
'78'
['140', '7', '31', '128', '78']
parsed_discourse_facet ['method_citation']
<S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="102" ssid = "6">The dev corpus was taken from the NIST05 evaluation set  augmented with some randomly-selected material reserved from the training set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'", "'127'", "'143'", "'65'", "'102'"]
'130'
'127'
'143'
'65'
'102'
['130', '127', '143', '65', '102']
parsed_discourse_facet ['method_citation']
<S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="55" ssid = "19">This suggests a direct parallel to (1): where ˜p(s  t) is a joint empirical distribution extracted from the IN dev set using the standard procedure.2 An alternative form of linear combination is a maximum a posteriori (MAP) combination (Bacchiani et al.  2004).</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'21'", "'49'", "'55'", "'127'"]
'75'
'21'
'49'
'55'
'127'
['75', '21', '49', '55', '127']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'130'", "'49'", "'38'", "'143'"]
'127'
'130'
'49'
'38'
'143'
['127', '130', '49', '38', '143']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'20'", "'38'", "'130'", "'49'"]
'127'
'20'
'38'
'130'
'49'
['127', '20', '38', '130', '49']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="111" ssid = "15">We used a standard one-pass phrase-based system (Koehn et al.  2003)  with the following features: relative-frequency TM probabilities in both directions; a 4-gram LM with Kneser-Ney smoothing; word-displacement distortion model; and word count.</S><S sid ="37" ssid = "1">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features  interpreted as log probabilities  many of which have their own internal parameters and objectives.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'49'", "'111'", "'37'", "'21'"]
'127'
'49'
'111'
'37'
'21'
['127', '49', '111', '37', '21']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'81'", "'151'", "'75'", "'143'"]
'7'
'81'
'151'
'75'
'143'
['7', '81', '151', '75', '143']
parsed_discourse_facet ['method_citation']
<S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="151" ssid = "8">In future work we plan to try this approach with more competitive SMT systems  and to extend instance weighting to other standard SMT components such as the LM  lexical phrase weights  and lexicalized distortion.</S><S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'75'", "'127'", "'151'", "'143'"]
'21'
'75'
'127'
'151'
'143'
['21', '75', '127', '151', '143']
parsed_discourse_facet ['method_citation']
<S sid ="97" ssid = "1">We carried out translation experiments in two different settings.</S><S sid ="53" ssid = "17">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).</S><S sid ="106" ssid = "10">The corpora for both settings are summarized in table 1.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="26" ssid = "23">Phrase-level granularity distinguishes our work from previous work by Matsoukas et al (2009)  who weight sentences according to sub-corpus and genre membership.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'53'", "'106'", "'81'", "'26'"]
'97'
'53'
'106'
'81'
'26'
['97', '53', '106', '81', '26']
parsed_discourse_facet ['method_citation']
<S sid ="7" ssid = "4">For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="31" ssid = "28">For comparison to information-retrieval inspired baselines  eg (L¨u et al.  2007)  we select sentences from OUT using language model perplexities from IN.</S><S sid ="78" ssid = "15">This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.</S><S sid ="140" ssid = "9">Recent work by Finkel and Manning (2009) which re-casts Daum´e’s approach in a hierarchical MAP framework may be applicable to this problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'", "'81'", "'31'", "'78'", "'140'"]
'7'
'81'
'31'
'78'
'140'
['7', '81', '31', '78', '140']
parsed_discourse_facet ['method_citation']
<S sid ="60" ssid = "24">The matching sentence pairs are then added to the IN corpus  and the system is re-trained.</S><S sid ="30" ssid = "27">A similar maximumlikelihood approach was used by Foster and Kuhn (2007)  but for language models only.</S><S sid ="24" ssid = "21">Sentence pairs are the natural instances for SMT  but sentences often contain a mix of domain-specific and general language.</S><S sid ="65" ssid = "2">Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.</S><S sid ="145" ssid = "2">Each out-of-domain phrase pair is characterized by a set of simple features intended to reflect how useful it will be.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'30'", "'24'", "'65'", "'145'"]
'60'
'30'
'24'
'65'
'145'
['60', '30', '24', '65', '145']
parsed_discourse_facet ['results_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="22" ssid = "19">Within this framework  we use features intended to capture degree of generality  including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'20'", "'130'", "'49'", "'22'"]
'127'
'20'
'130'
'49'
'22'
['127', '20', '130', '49', '22']
parsed_discourse_facet ['method_citation']
<S sid ="127" ssid = "31">The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the “flattened” variant described in section 3.2.</S><S sid ="130" ssid = "34">The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.</S><S sid ="20" ssid = "17">Daum´e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.</S><S sid ="49" ssid = "13">This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.</S><S sid ="119" ssid = "23">The 2nd block contains the IR system  which was tuned by selecting text in multiples of the size of the EMEA training corpus  according to dev set performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'", "'130'", "'20'", "'49'", "'119'"]
'127'
'130'
'20'
'49'
'119'
['127', '130', '20', '49', '119']
parsed_discourse_facet ['method_citation']
<S sid ="143" ssid = "12">Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).</S><S sid ="66" ssid = "3">The weight on each sentence is a value in [0  1] computed by a perceptron with Boolean features that indicate collection and genre membership.</S><S sid ="75" ssid = "12">However  it is robust  efficient  and easy to implement.4 To perform the maximization in (7)  we used the popular L-BFGS algorithm (Liu and Nocedal  1989)  which requires gradient information.</S><S sid ="113" ssid = "17">The corpus was wordaligned using both HMM and IBM2 models  and the phrase table was the union of phrases extracted from these separate alignments  with a length limit of 7.</S><S sid ="102" ssid = "6">The dev corpus was taken from the NIST05 evaluation set  augmented with some randomly-selected material reserved from the training set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'", "'66'", "'75'", "'113'", "'102'"]
'143'
'66'
'75'
'113'
'102'
['143', '66', '75', '113', '102']
parsed_discourse_facet ['method_citation']
<S sid ="44" ssid = "8">When OUT is large and distinct  its contribution can be controlled by training separate IN and OUT models  and weighting their combination.</S><S sid ="79" ssid = "16">This combination generalizes (2) and (3): we use either at = a to obtain a fixed-weight linear combination  or at = cI(t)/(cI(t) + 0) to obtain a MAP combination.</S><S sid ="62" ssid = "26">To approximate these baselines  we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S><S sid ="115" ssid = "19">Table 2 shows results for both settings and all methods described in sections 2 and 3.</S><S sid ="13" ssid = "10">The techniques we develop can be extended in a relatively straightforward manner to the more general case when OUT consists of multiple sub-domains.</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'79'", "'62'", "'115'", "'13'"]
'44'
'79'
'62'
'115'
'13'
['44', '79', '62', '115', '13']
parsed_discourse_facet ['method_citation']
<S sid ="106" ssid = "10">The corpora for both settings are summarized in table 1.</S><S sid ="60" ssid = "24">The matching sentence pairs are then added to the IN corpus  and the system is re-trained.</S><S sid ="81" ssid = "18">The logistic function  whose outputs are in [0  1]  forces pp(s  t) <_ po(s  t).</S><S sid ="38" ssid = "2">The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.</S><S sid ="53" ssid = "17">This has led previous workers to adopt ad hoc linear weighting schemes (Finch and Sumita  2008; Foster and Kuhn  2007; L¨u et al.  2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'60'", "'81'", "'38'", "'53'"]
'106'
'60'
'81'
'38'
'53'
['106', '60', '81', '38', '53']
parsed_discourse_facet ['method_citation']
<S sid ="95" ssid = "32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S><S sid ="50" ssid = "14">Linear weights are difficult to incorporate into the standard MERT procedure because they are “hidden” within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.</S><S sid ="21" ssid = "18">This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.</S><S sid ="59" ssid = "23">To set β  we used the same criterion as for α  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose “relevant” sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L¨u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).</S><S sid ="51" ssid = "15">For the LM  adaptive weights are set as follows: where α is a weight vector containing an element αi for each domain (just IN and OUT in our case)  pi are the corresponding domain-specific models  and ˜p(w  h) is an empirical distribution from a targetlanguage training corpus—we used the IN dev set for this.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'50'", "'21'", "'59'", "'51'"]
'95'
'50'
'21'
'59'
'51'
['95', '50', '21', '59', '51']
parsed_discourse_facet ['aim_citation', 'results_citation']
['In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.']
['The 2nd block contains the IR system  which was tuned by selecting text in multiples of the size of the EMEA training corpus  according to dev set performance.', 'The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.', u'Daum\xb4e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.', 'This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.', u'The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the \u201cflattened\u201d variant described in section 3.2.']
['system', 'ROUGE-S*', 'Average_R:', '0.00132', '(95%-conf.int.', '0.00132', '-', '0.00132)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00255', '(95%-conf.int.', '0.00255', '-', '0.00255)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:78', 'F:3']
['To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.']
['The dev corpus was taken from the NIST05 evaluation set  augmented with some randomly-selected material reserved from the training set.', 'The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.', 'Matsoukas et al (2009) generalize it by learning weights on sentence pairs that are used when estimating relative-frequency phrase-pair probabilities.', 'Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).', u'The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the \u201cflattened\u201d variant described in section 3.2.']
['system', 'ROUGE-S*', 'Average_R:', '0.00433', '(95%-conf.int.', '0.00433', '-', '0.00433)']
['system', 'ROUGE-S*', 'Average_P:', '0.10833', '(95%-conf.int.', '0.10833', '-', '0.10833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:120', 'F:13']
['We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.']
['This is not unreasonable given the application to phrase pairs from OUT  but it suggests that an interesting alternative might be to use a plain log-linear weighting function exp(Ei Aifi(s  t))  with outputs in [0  oo].', 'The toplevel weights are trained to maximize a metric such as BLEU on a small development set of approximately 1000 sentence pairs.', 'There has also been some work on adapting the word alignment model prior to phrase extraction (Civera and Juan  2007; Wu et al.  2005)  and on dynamically choosing a dev set (Xu et al.  2007).', 'Other work includes transferring latent topic distributions from source to target language for LM adaptation  (Tam et al.  2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita  2008).', 'The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.']
['system', 'ROUGE-S*', 'Average_R:', '0.00131', '(95%-conf.int.', '0.00131', '-', '0.00131)']
['system', 'ROUGE-S*', 'Average_P:', '0.04167', '(95%-conf.int.', '0.04167', '-', '0.04167)']
['system', 'ROUGE-S*', 'Average_F:', '0.00253', '(95%-conf.int.', '0.00253', '-', '0.00253)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3828', 'P:120', 'F:5']
['Domain adaptation is a common concern when optimizing empirical NLP applications.']
['Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.', 'This is a standard adaptation problem for SMT.', 'Domain adaptation is a common concern when optimizing empirical NLP applications.', u'Recent work by Finkel and Manning (2009) which re-casts Daum\xb4e\u2019s approach in a hierarchical MAP framework may be applicable to this problem.', u'It is also worth pointing out a connection with Daum\xb4e\u2019s (2007) work that splits each feature into domain-specific and general copies.']
['system', 'ROUGE-S*', 'Average_R:', '0.02381', '(95%-conf.int.', '0.02381', '-', '0.02381)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.04651', '(95%-conf.int.', '0.04651', '-', '0.04651)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:28', 'F:28']
['Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.']
[u'To set \u03b2  we used the same criterion as for \u03b1  over a dev corpus: The MAP combination was used for TM probabilities only  in part due to a technical difficulty in formulating coherent counts when using standard LM smoothing techniques (Kneser and Ney  1995).3 Motivated by information retrieval  a number of approaches choose \u201crelevant\u201d sentence pairs from OUT by matching individual source sentences from IN (Hildebrand et al.  2005; L\xa8u et al.  2007)  or individual target hypotheses (Zhao et al.  2004).', u'For the LM  adaptive weights are set as follows: where \u03b1 is a weight vector containing an element \u03b1i for each domain (just IN and OUT in our case)  pi are the corresponding domain-specific models  and \u02dcp(w  h) is an empirical distribution from a targetlanguage training corpus\u2014we used the IN dev set for this.', 'Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models)  and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples  with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.', u'Linear weights are difficult to incorporate into the standard MERT procedure because they are \u201chidden\u201d within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn  2007)  we circumvent this problem by choosing weights to optimize corpus loglikelihood  which is roughly speaking the training criterion used by the LM and TM themselves.', 'This highly effective approach is not directly applicable to the multinomial models used for core SMT components  which have no natural method for combining split features  so we rely on an instance-weighting approach (Jiang and Zhai  2007) to downweight domain-specific examples in OUT.']
['system', 'ROUGE-S*', 'Average_R:', '0.00306', '(95%-conf.int.', '0.00306', '-', '0.00306)']
['system', 'ROUGE-S*', 'Average_P:', '0.15584', '(95%-conf.int.', '0.15584', '-', '0.15584)']
['system', 'ROUGE-S*', 'Average_F:', '0.00599', '(95%-conf.int.', '0.00599', '-', '0.00599)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:11781', 'P:231', 'F:36']
['However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.']
['The logistic function  whose outputs are in [0  1]  forces pp(s  t) ', 'For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:325', 'P:136', 'F:0']
['Finally, we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.']
['The final block in table 2 shows models trained on feature subsets and on the SVM feature described in 3.4.', u'Daum\xb4e (2007) applies a related idea in a simpler way  by splitting features into general and domain-specific versions.', 'This leads to a linear combination of domain-specific probabilities  with weights in [0  1]  normalized to sum to 1.', 'Within this framework  we use features intended to capture degree of generality  including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.', u'The iw all map variant uses a non-0 y weight on a uniform prior in p  (s t)  and outperforms a version with y = 0 (iw all) and the \u201cflattened\u201d variant described in section 3.2.']
['system', 'ROUGE-S*', 'Average_R:', '0.00653', '(95%-conf.int.', '0.00653', '-', '0.00653)']
['system', 'ROUGE-S*', 'Average_P:', '0.25455', '(95%-conf.int.', '0.25455', '-', '0.25455)']
['system', 'ROUGE-S*', 'Average_F:', '0.01273', '(95%-conf.int.', '0.01273', '-', '0.01273)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:55', 'F:14']
['We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.']
['The matching sentence pairs are then added to the IN corpus  and the system is re-trained.', 'The logistic function  whose outputs are in [0  1]  forces pp(s  t) ', 'The corpora for both settings are summarized in table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.01170', '(95%-conf.int.', '0.01170', '-', '0.01170)']
['system', 'ROUGE-S*', 'Average_P:', '0.01307', '(95%-conf.int.', '0.01307', '-', '0.01307)']
['system', 'ROUGE-S*', 'Average_F:', '0.01235', '(95%-conf.int.', '0.01235', '-', '0.01235)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:171', 'P:153', 'F:2']
['For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components (word-alignment model, language model, translation model, etc.']
['Clearly  retaining the original frequencies is important for good performance  and globally smoothing the final weighted frequencies is crucial.', u'For comparison to information-retrieval inspired baselines  eg (L\xa8u et al.  2007)  we select sentences from OUT using language model perplexities from IN.', 'For developers of Statistical Machine Translation (SMT) systems  an additional complication is the heterogeneous nature of SMT components (word-alignment model  language model  translation model  etc.', u'Recent work by Finkel and Manning (2009) which re-casts Daum\xb4e\u2019s approach in a hierarchical MAP framework may be applicable to this problem.', 'This has solutions: where pI(s|t) is derived from the IN corpus using relative-frequency estimates  and po(s|t) is an instance-weighted model derived from the OUT corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.07289', '(95%-conf.int.', '0.07289', '-', '0.07289)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.13588', '(95%-conf.int.', '0.13588', '-', '0.13588)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:171', 'F:171']
0.290213330109 0.0138833331791 0.0252077774977





input/ref/Task1/P08-1028_swastika.csv
input/res/Task1/P08-1028.csv
parsing: input/ref/Task1/P08-1028_swastika.csv
<S sid="21" ssid="17">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="23">Our results show that the multiplicative models are superior and correlate significantly with behavioral data.</S>
original cit marker offset is 0
new cit marker offset is 0



['27']
27
['27']
parsed_discourse_facet ['result_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['189']
189
['189']
parsed_discourse_facet ['aim_citation']
<S sid="53" ssid="1">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.</S>
original cit marker offset is 0
new cit marker offset is 0



['53']
53
['53']
parsed_discourse_facet ['method_citation']
<S sid="76" ssid="24">The models considered so far assume that components do not &#8216;interfere&#8217; with each other, i.e., that It is also possible to re-introduce the dependence on K into the model of vector composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['76']
76
['76']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['57']
57
['57']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['57']
57
['57']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="60" ssid="8">To derive specific models from this general framework requires the identification of appropriate constraints to narrow the space of functions being considered.</S>
original cit marker offset is 0
new cit marker offset is 0



['60']
60
['60']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="73" ssid="21">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware, since semantically important constituents can participate more actively in the composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['73']
73
['73']
parsed_discourse_facet ['result_citation']
<S sid="99" ssid="12">In order to establish an independent measure of sentence similarity, we assembled a set of experimental materials and elicited similarity ratings from human subjects.</S>
original cit marker offset is 0
new cit marker offset is 0



['99']
99
['99']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['189']
189
['189']
parsed_discourse_facet ['aim_citation']
<S sid="191" ssid="3">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S>
original cit marker offset is 0
new cit marker offset is 0



['191']
191
['191']
parsed_discourse_facet ['result_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P08-1028.csv
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="140" ssid = "53">Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid ="40" ssid = "13">Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.</S><S sid ="155" ssid = "68">Secondly  we optimized the weightings in the combined model (11) with a similar grid search over its three parameters.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'140'", "'51'", "'40'", "'155'"]
'194'
'140'
'51'
'40'
'155'
['194', '140', '51', '40', '155']
parsed_discourse_facet ['results_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="58" ssid = "6">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="10" ssid = "6">Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'58'", "'59'", "'10'", "'51'"]
'194'
'58'
'59'
'10'
'51'
['194', '58', '59', '10', '51']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="38" ssid = "11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S><S sid ="141" ssid = "54">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S><S sid ="36" ssid = "9">To overcome this problem  other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'194'", "'38'", "'141'", "'36'"]
'108'
'194'
'38'
'141'
'36'
['108', '194', '38', '141', '36']
parsed_discourse_facet ['method_citation']
<S sid ="160" ssid = "73">Specifically  m was set to 20 and m to 1.</S><S sid ="156" ssid = "69">This yielded a weighted sum consisting of 95% verb  0% noun and 5% of their multiplicative combination.</S><S sid ="34" ssid = "7">Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.</S><S sid ="190" ssid = "2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S sid ="117" ssid = "30">This yielded a reduced set of experimental items (120 in total) consisting of 15 reference verbs  each with 4 nouns  and 2 landmarks.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'", "'156'", "'34'", "'190'", "'117'"]
'160'
'156'
'34'
'190'
'117'
['160', '156', '34', '190', '117']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">Vector-based Models of Semantic Composition</S><S sid ="36" ssid = "9">To overcome this problem  other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid ="142" ssid = "55">We experimented with a variety of dimensions (ranging from 50 to 500 000)  vector component definitions (e.g.  pointwise mutual information or log likelihood ratio) and similarity measures (e.g.  cosine or confusion probability).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'36'", "'51'", "'142'", "'194'"]
'0'
'36'
'51'
'142'
'194'
['0', '36', '51', '142', '194']
parsed_discourse_facet ['method_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="59" ssid = "7">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.</S><S sid ="51" ssid = "24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).</S><S sid ="58" ssid = "6">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.</S><S sid ="201" ssid = "13">We intend to assess the potential of our composition models on context sensitive semantic priming (Till et al.  1988) and inductive inference (Heit and Rubinstein  1994).</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'59'", "'51'", "'58'", "'201'"]
'194'
'59'
'51'
'58'
'201'
['194', '59', '51', '58', '201']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "3">For the hierarchical structure of natural language this binding problem becomes particularly acute.</S><S sid ="96" ssid = "9">Any adequate model of composition must be able to represent argument-verb meaning.</S><S sid ="1" ssid = "1">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S><S sid ="24" ssid = "20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S><S sid ="44" ssid = "17">For example  assuming that individual words are represented by vectors  we can compute the meaning of a sentence by taking their mean (Foltz et al.  1998; Landauer and Dumais  1997).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'96'", "'1'", "'24'", "'44'"]
'30'
'96'
'1'
'24'
'44'
['30', '96', '1', '24', '44']
parsed_discourse_facet ['method_citation']
<S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="37" ssid = "10">Holographic reduced representations (Plate  1991) are one implementation of this idea where the tensor product is projected back onto the space of its components.</S><S sid ="141" ssid = "54">The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'3'", "'101'", "'37'", "'141'"]
'194'
'3'
'101'
'37'
'141'
['194', '3', '101', '37', '141']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid ="140" ssid = "53">Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.</S><S sid ="144" ssid = "57">We obtained best results with a model using a context window of five words on either side of the target word  the cosine measure  and 2 000 vector components.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="37" ssid = "10">Holographic reduced representations (Plate  1991) are one implementation of this idea where the tensor product is projected back onto the space of its components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'140'", "'144'", "'194'", "'37'"]
'39'
'140'
'144'
'194'
'37'
['39', '140', '144', '194', '37']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="168" ssid = "2">These included three additive models  i.e.  simple addition (equation (5)  Add)  weighted addition (equation (7)  WeightAdd)  and Kintsch’s (2001) model (equation (10)  Kintsch)  a multiplicative model (equation (6)  Multiply)  and also a model which combines multiplication with addition (equation (11)  Combined).</S><S sid ="35" ssid = "8">The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely  the tensor product has dimensionality m x n).</S><S sid ="144" ssid = "57">We obtained best results with a model using a context window of five words on either side of the target word  the cosine measure  and 2 000 vector components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'194'", "'168'", "'35'", "'144'"]
'108'
'194'
'168'
'35'
'144'
['108', '194', '168', '35', '144']
parsed_discourse_facet ['method_citation']
<S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid ="117" ssid = "30">This yielded a reduced set of experimental items (120 in total) consisting of 15 reference verbs  each with 4 nouns  and 2 landmarks.</S><S sid ="35" ssid = "8">The tensor product u ® v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely  the tensor product has dimensionality m x n).</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'117'", "'35'", "'194'", "'108'"]
'39'
'117'
'35'
'194'
'108'
['39', '117', '35', '194', '108']
parsed_discourse_facet ['method_citation']
<S sid ="108" ssid = "21">Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="153" ssid = "66">Specifically  we considered eleven models  varying in their weightings  in steps of 10%  from 100% noun through 50% of both verb and noun to 100% verb.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'", "'101'", "'3'", "'153'", "'194'"]
'108'
'101'
'3'
'153'
'194'
['108', '101', '3', '153', '194']
parsed_discourse_facet ['method_citation']
<S sid ="148" ssid = "61">In addition  Bullinaria and Levy (2007) found that these parameters perform well on a number of other tasks such as the synonymy task from the Test ofEnglish as a Foreign Language (TOEFL).</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S><S sid ="177" ssid = "11">The difference between High and Low similarity values estimated by these models are statistically significant (p < 0.01 using the Wilcoxon rank sum test).</S><S sid ="4" ssid = "4">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S><S sid ="127" ssid = "40">Analysis of Similarity Ratings The reliability of the collected judgments is important for our evaluation experiments; we therefore performed several tests to validate the quality of the ratings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'148'", "'164'", "'177'", "'4'", "'127'"]
'148'
'164'
'177'
'4'
'127'
['148', '164', '177', '4', '127']
parsed_discourse_facet ['results_citation']
<S sid ="115" ssid = "28">Each cell recorded the number of times our subjects selected the landmark as compatible with the noun or not.</S><S sid ="131" ssid = "44">A Wilcoxon rank sum test confirmed that the difference is statistically significant (p < 0.01).</S><S sid ="147" ssid = "60">This configuration gave high correlations with the WordSim353 similarity judgments using the cosine measure.</S><S sid ="173" ssid = "7">Model similarities have been estimated using cosine which ranges from 0 to 1  whereas our subjects rated the sentences on a scale from 1 to 7.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'131'", "'147'", "'173'", "'164'"]
'115'
'131'
'147'
'173'
'164'
['115', '131', '147', '173', '164']
parsed_discourse_facet ['method_citation']
<S sid ="140" ssid = "53">Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.</S><S sid ="3" ssid = "3">Under this framework  we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S sid ="101" ssid = "14">Materials and Design Our materials consisted of sentences with an an intransitive verb and its subject.</S><S sid ="2" ssid = "2">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S><S sid ="139" ssid = "52">The semantic space we used in our experiments was built on a lemmatised version of the BNC.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'3'", "'101'", "'2'", "'139'"]
'140'
'3'
'101'
'2'
'139'
['140', '3', '101', '2', '139']
parsed_discourse_facet ['method_citation']
<S sid ="115" ssid = "28">Each cell recorded the number of times our subjects selected the landmark as compatible with the noun or not.</S><S sid ="147" ssid = "60">This configuration gave high correlations with the WordSim353 similarity judgments using the cosine measure.</S><S sid ="39" ssid = "12">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S><S sid ="173" ssid = "7">Model similarities have been estimated using cosine which ranges from 0 to 1  whereas our subjects rated the sentences on a scale from 1 to 7.</S><S sid ="164" ssid = "77">A more scrupulous evaluation requires directly correlating all the individual participants’ similarity judgments with those of the models.6 We used Spearman’s p for our correlation analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'147'", "'39'", "'173'", "'164'"]
'115'
'147'
'39'
'173'
'164'
['115', '147', '39', '173', '164']
parsed_discourse_facet ['method_citation']
<S sid ="38" ssid = "11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S><S sid ="153" ssid = "66">Specifically  we considered eleven models  varying in their weightings  in steps of 10%  from 100% noun through 50% of both verb and noun to 100% verb.</S><S sid ="194" ssid = "6">Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.</S><S sid ="183" ssid = "17">The weighted additive model (p = 0.09) is not significantly different from the baseline either or Kintsch (2001) (p = 0.09).</S><S sid ="40" ssid = "13">Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'153'", "'194'", "'183'", "'40'"]
'38'
'153'
'194'
'183'
'40'
['38', '153', '194', '183', '40']
parsed_discourse_facet ['method_citation']
['Our results show that the multiplicative models are superior and correlate significantly with behavioral data.']
['Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.', 'Secondly  we optimized the weightings in the combined model (11) with a similar grid search over its three parameters.', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).', 'Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.', 'Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.']
['system', 'ROUGE-S*', 'Average_R:', '0.00154', '(95%-conf.int.', '0.00154', '-', '0.00154)']
['system', 'ROUGE-S*', 'Average_P:', '0.08333', '(95%-conf.int.', '0.08333', '-', '0.08333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00302', '(95%-conf.int.', '0.00302', '-', '0.00302)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:36', 'F:3']
['We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.']
['This yielded a reduced set of experimental items (120 in total) consisting of 15 reference verbs  each with 4 nouns  and 2 landmarks.', 'Smolensky (1990) proposed the use of tensor products as a means of binding one vector to another.', 'Specifically  m was set to 20 and m to 1.', 'We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.', 'This yielded a weighted sum consisting of 95% verb  0% noun and 5% of their multiplicative combination.']
['system', 'ROUGE-S*', 'Average_R:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_P:', '0.02769', '(95%-conf.int.', '0.02769', '-', '0.02769)']
['system', 'ROUGE-S*', 'Average_F:', '0.01280', '(95%-conf.int.', '0.01280', '-', '0.01280)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:325', 'F:9']
['In order to establish an independent measure of sentence similarity, we assembled a set of experimental materials and elicited similarity ratings from human subjects.']
['Each cell recorded the number of times our subjects selected the landmark as compatible with the noun or not.', 'A Wilcoxon rank sum test confirmed that the difference is statistically significant (p ']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:136', 'P:105', 'F:0']
['The models considered so far assume that components do not &#8216;interfere&#8217; with each other, i.e., that It is also possible to re-introduce the dependence on K into the model of vector composition.']
['Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.', 'Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).', 'To overcome this problem  other techniques have been proposed in which the binding of two vectors results in a vector which has the same dimensionality as its components.', 'We experimented with a variety of dimensions (ranging from 50 to 500 000)  vector component definitions (e.g.  pointwise mutual information or log likelihood ratio) and similarity measures (e.g.  cosine or confusion probability).']
['system', 'ROUGE-S*', 'Average_R:', '0.00466', '(95%-conf.int.', '0.00466', '-', '0.00466)']
['system', 'ROUGE-S*', 'Average_P:', '0.15152', '(95%-conf.int.', '0.15152', '-', '0.15152)']
['system', 'ROUGE-S*', 'Average_F:', '0.00905', '(95%-conf.int.', '0.00905', '-', '0.00905)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:66', 'F:10']
['We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.', 'The compression is achieved by summing along the transdiagonal elements of the tensor product.', 'Holographic reduced representations (Plate  1991) are one implementation of this idea where the tensor product is projected back onto the space of its components.', 'We obtained best results with a model using a context window of five words on either side of the target word  the cosine measure  and 2 000 vector components.', 'Following previous work (Bullinaria and Levy  2007)  we optimized its parameters on a word-based semantic similarity task.']
['system', 'ROUGE-S*', 'Average_R:', '0.00529', '(95%-conf.int.', '0.00529', '-', '0.00529)']
['system', 'ROUGE-S*', 'Average_P:', '0.27778', '(95%-conf.int.', '0.27778', '-', '0.27778)']
['system', 'ROUGE-S*', 'Average_F:', '0.01038', '(95%-conf.int.', '0.01038', '-', '0.01038)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:36', 'F:10']
['We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.', 'Specifically  we considered eleven models  varying in their weightings  in steps of 10%  from 100% noun through 50% of both verb and noun to 100% verb.', 'The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.', 'Noisy versions of the original vectors can be recovered by means of circular correlation which is the approximate inverse of circular convolution.', 'The weighted additive model (p = 0.09) is not significantly different from the baseline either or Kintsch (2001) (p = 0.09).']
['system', 'ROUGE-S*', 'Average_R:', '0.00694', '(95%-conf.int.', '0.00694', '-', '0.00694)']
['system', 'ROUGE-S*', 'Average_P:', '0.38889', '(95%-conf.int.', '0.38889', '-', '0.38889)']
['system', 'ROUGE-S*', 'Average_F:', '0.01365', '(95%-conf.int.', '0.01365', '-', '0.01365)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:36', 'F:14']
['We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['Specifically  they belonged to different synsets and were maximally dissimilar as measured by the Jiang and Conrath (1997) measure.3 Our initial set of candidate materials consisted of 20 verbs  each paired with 10 nouns  and 2 landmarks (400 pairs of sentences in total).', 'Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.', 'The compression is achieved by summing along the transdiagonal elements of the tensor product.', 'This yielded a reduced set of experimental items (120 in total) consisting of 15 reference verbs  each with 4 nouns  and 2 landmarks.', u'The tensor product u \xc2\xae v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely  the tensor product has dimensionality m x n).']
['system', 'ROUGE-S*', 'Average_R:', '0.00196', '(95%-conf.int.', '0.00196', '-', '0.00196)']
['system', 'ROUGE-S*', 'Average_P:', '0.19444', '(95%-conf.int.', '0.19444', '-', '0.19444)']
['system', 'ROUGE-S*', 'Average_F:', '0.00388', '(95%-conf.int.', '0.00388', '-', '0.00388)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3570', 'P:36', 'F:7']
['In this paper we presented a general framework for vector-based semantic composition.']
['Importantly  additive models capture composition by considering all vector components representing the meaning of the verb and its subject  whereas multiplicative models consider a subset  namely non-zero components.', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations  multiplication and addition (and their combination).', 'It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence  via the argument R  on syntax.', 'We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v  as is the case for tensor products.', 'Moreover  the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald  2000) and word association norms (Denhire and Lemaire  2004).']
['system', 'ROUGE-S*', 'Average_R:', '0.00296', '(95%-conf.int.', '0.00296', '-', '0.00296)']
['system', 'ROUGE-S*', 'Average_P:', '0.28571', '(95%-conf.int.', '0.28571', '-', '0.28571)']
['system', 'ROUGE-S*', 'Average_F:', '0.00586', '(95%-conf.int.', '0.00586', '-', '0.00586)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2701', 'P:28', 'F:8']
['Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.']
['This paper proposes a framework for representing the meaning of phrases and sentences in vector space.', 'In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.', 'For example  assuming that individual words are represented by vectors  we can compute the meaning of a sentence by taking their mean (Foltz et al.  1998; Landauer and Dumais  1997).', 'For the hierarchical structure of natural language this binding problem becomes particularly acute.', 'Any adequate model of composition must be able to represent argument-verb meaning.']
['system', 'ROUGE-S*', 'Average_R:', '0.01152', '(95%-conf.int.', '0.01152', '-', '0.01152)']
['system', 'ROUGE-S*', 'Average_P:', '0.09559', '(95%-conf.int.', '0.09559', '-', '0.09559)']
['system', 'ROUGE-S*', 'Average_F:', '0.02057', '(95%-conf.int.', '0.02057', '-', '0.02057)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:136', 'F:13']
0.167216664809 0.00479999994667 0.00880111101332





input/ref/Task1/P11-1060_aakansha.csv
input/res/Task1/P11-1060.csv
parsing: input/ref/Task1/P11-1060_aakansha.csv
<S sid="11" ssid="7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'"]
'2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="5">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'"]
'112'
['112']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="11">Although a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">However, we still model the logical form (now as a latent variable) to capture the complexities of language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="12">It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="21">The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="88" ssid="64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets, a restrictor A and a nuclear scope B.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'"]
'88'
['88']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="158" ssid = "43">5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation  DCS  which offers a new perspective lexical semantics.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="23" ssid = "19">We trained our model using an EM-like algorithm (Section 3) on two benchmarks  GEO and JOBS (Section 4).</S><S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'158'", "'21'", "'23'", "'132'"]
'26'
'158'
'21'
'23'
'132'
['26', '158', '21', '23', '132']
parsed_discourse_facet ['results_citation']
<S sid ="39" ssid = "15">Such a z defines a constraint satisfaction problem (CSP) with nodes as variables.</S><S sid ="56" ssid = "32">The basic version of DCS described thus far handles a core subset of language.</S><S sid ="4" ssid = "4">On two stansemantic parsing benchmarks our system obtains the highest published accuracies  despite requiring no annotated logical forms.</S><S sid ="136" ssid = "21">In fact  DCS performs comparably to even the version of SEMRESP trained using logical forms.</S><S sid ="83" ssid = "59">It suffices to define Xi(d) for a single column i.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'", "'56'", "'4'", "'136'", "'83'"]
'39'
'56'
'4'
'136'
'83'
['39', '56', '4', '136', '83']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S><S sid ="148" ssid = "33">This bootstrapping behavior occurs naturally: The “easy” examples are processed first  where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'100'", "'115'", "'88'", "'148'"]
'26'
'100'
'115'
'88'
'148'
['26', '100', '115', '88', '148']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="138" ssid = "23">Next  we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="53" ssid = "29">Having instantiated s as a value  everything above this node is an ordinary CSP: s constrains the count node  which in turns constrains the root node to |s|.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'138'", "'26'", "'53'"]
'100'
'21'
'138'
'26'
'53'
['100', '21', '138', '26', '53']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).</S><S sid ="94" ssid = "70">We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="58" ssid = "34">We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'94'", "'134'", "'8'", "'58'"]
'18'
'94'
'134'
'8'
'58'
['18', '94', '134', '8', '58']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'21'", "'100'", "'115'", "'42'"]
'26'
'21'
'100'
'115'
'42'
['26', '21', '100', '115', '42']
parsed_discourse_facet ['method_citation']
<S sid ="87" ssid = "63">For example  in Figure 4(a)  before execution  the denotation of the DCS tree is hh{[(CA  OR)  (OR)] ... }; ø; (E  Qhstatei�w  ø)ii; after applying X1  we have hh{[(OR)]  ... }; øii.</S><S sid ="113" ssid = "89">Formally  let ˜O(θ  θ') be the objective function O(θ) with ZL(x) ˜ZL θI(x).</S><S sid ="20" ssid = "16">At the same time  representations such as FunQL (Kate et al.  2005)  which was used in Clarke et al. (2010)  are simpler but lack the full expressive power of lambda calculus.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="74" ssid = "50">Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'", "'113'", "'20'", "'154'", "'74'"]
'87'
'113'
'20'
'154'
'74'
['87', '113', '20', '154', '74']
parsed_discourse_facet ['method_citation']
<S sid ="117" ssid = "2">In each dataset  each sentence x is annotated with a Prolog logical form  which we use only to evaluate and get an answer y.</S><S sid ="1" ssid = "1">Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid ="15" ssid = "11">Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.</S><S sid ="88" ssid = "64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'117'", "'1'", "'15'", "'88'", "'62'"]
'117'
'1'
'15'
'88'
'62'
['117', '1', '15', '88', '62']
parsed_discourse_facet ['method_citation']
<S sid ="83" ssid = "59">It suffices to define Xi(d) for a single column i.</S><S sid ="169" ssid = "54">The combination rules are encoded in the tems  despite using no annotated logical forms.</S><S sid ="8" ssid = "4">On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.</S><S sid ="101" ssid = "77">Formally  pθ(z  |x) ∝ eφ(x z)Tθ  where θ and φ(x  z) are parameter and feature vectors  respectively.</S><S sid ="134" ssid = "19">In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'83'", "'169'", "'8'", "'101'", "'134'"]
'83'
'169'
'8'
'101'
'134'
['83', '169', '8', '101', '134']
parsed_discourse_facet ['method_citation']
<S sid ="1" ssid = "1">Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.</S><S sid ="99" ssid = "75">To further reduce the search space  F imposes a few additional constraints  e.g.  limiting the number of marked nodes to 2 and only allowing trace predicates between arity 1 predicates.</S><S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S><S sid ="129" ssid = "14">We also define an augmented lexicon L+ which includes a prototype word x for each predicate appearing in (iii) above (e.g.  (large  size))  which cancels the predicates triggered by x’s POS tag.</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'99'", "'132'", "'129'", "'26'"]
'1'
'99'
'132'
'129'
'26'
['1', '99', '132', '129', '26']
parsed_discourse_facet ['method_citation']
<S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="54" ssid = "30">A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S><S sid ="166" ssid = "51">Our employed (Zettlemoyer and Collins  2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language—think grounded al.  2010). compositional semantics.</S><S sid ="132" ssid = "17">Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'54'", "'42'", "'166'", "'132'"]
'21'
'54'
'42'
'166'
'132'
['21', '54', '42', '166', '132']
parsed_discourse_facet ['method_citation']
<S sid ="59" ssid = "35">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S><S sid ="54" ssid = "30">A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="45" ssid = "21">The logical forms in DCS are called DCS trees  where nodes are labeled with predicates  and edges are labeled with relations.</S><S sid ="43" ssid = "19">Computation We can compute the denotation JzKw of a DCS tree z by exploiting dynamic programming on trees (Dechter  2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'54'", "'154'", "'45'", "'43'"]
'59'
'54'
'154'
'45'
'43'
['59', '54', '154', '45', '43']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "30">A DCS tree that contains only join and aggregate relations can be viewed as a collection of treestructured CSPs connected via aggregate relations.</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="74" ssid = "50">Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S><S sid ="107" ssid = "83">Our model is arc-factored  so we can sum over all DCS trees in ZL(x) using dynamic programming.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'42'", "'21'", "'74'", "'107'"]
'54'
'42'
'21'
'74'
'107'
['54', '42', '21', '74', '107']
parsed_discourse_facet ['results_citation']
<S sid ="59" ssid = "35">The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).</S><S sid ="148" ssid = "33">This bootstrapping behavior occurs naturally: The “easy” examples are processed first  where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S><S sid ="47" ssid = "23">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.</S><S sid ="103" ssid = "79">To define the features  we technically need to augment each tree z ∈ ZL(x) with alignment information—namely  for each predicate in z  the span in x (if any) that triggered it.</S><S sid ="62" ssid = "38">Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.</S>
original cit marker offset is 0
new cit marker offset is 0



["'59'", "'148'", "'47'", "'103'", "'62'"]
'59'
'148'
'47'
'103'
'62'
['59', '148', '47', '103', '62']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="23" ssid = "19">We trained our model using an EM-like algorithm (Section 3) on two benchmarks  GEO and JOBS (Section 4).</S><S sid ="86" ssid = "62">Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  −(i  ø)]{α1 = ø}.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'23'", "'86'", "'115'", "'100'"]
'26'
'23'
'86'
'115'
'100'
['26', '23', '86', '115', '100']
parsed_discourse_facet ['method_citation']
<S sid ="113" ssid = "89">Formally  let ˜O(θ  θ') be the objective function O(θ) with ZL(x) ˜ZL θI(x).</S><S sid ="78" ssid = "54">The stores are also concatenated (α + α').</S><S sid ="74" ssid = "50">Extending this notation to denotations  let (hA; αii[i] = hh{ai : a ∈ A}; αiii.</S><S sid ="154" ssid = "39">The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).</S><S sid ="111" ssid = "87">Let ˜ZL θ(x) be this approximation of ZL(x).</S>
original cit marker offset is 0
new cit marker offset is 0



["'113'", "'78'", "'74'", "'154'", "'111'"]
'113'
'78'
'74'
'154'
'111'
['113', '78', '74', '154', '111']
parsed_discourse_facet ['method_citation']
<S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="63" ssid = "39">Think of d as consisting of n columns  one for each active node according to a pre-order traversal of z.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="86" ssid = "62">Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  −(i  ø)]{α1 = ø}.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'100'", "'63'", "'115'", "'86'"]
'26'
'100'
'63'
'115'
'86'
['26', '100', '63', '115', '86']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "76">Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S sid ="21" ssid = "17">The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).</S><S sid ="26" ssid = "2">We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.</S><S sid ="115" ssid = "91">After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy pθ(T)(y  |x  z ∈ ˜ZL θ(T)).</S><S sid ="42" ssid = "18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'26'", "'115'", "'42'"]
'100'
'21'
'26'
'115'
'42'
['100', '21', '26', '115', '42']
parsed_discourse_facet ['method_citation']
<S sid ="151" ssid = "36">Inspecting the final parameters calculus formulae.</S><S sid ="56" ssid = "32">The basic version of DCS described thus far handles a core subset of language.</S><S sid ="16" ssid = "12">Which one should we use?</S><S sid ="118" ssid = "3">This evaluation is done with respect to a world w. Recall that a world w maps each predicate p ∈ P to a set of tuples w(p).</S><S sid ="33" ssid = "9">As another example  w(average) = {(S  ¯x) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'56'", "'16'", "'118'", "'33'"]
'151'
'56'
'16'
'118'
'33'
['151', '56', '16', '118', '33']
parsed_discourse_facet ['aim_citation', 'results_citation']
['The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).']
['This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure  which will facilitate parsing.', 'Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.', 'The key idea that allows us to give semanticallyscoped denotations to syntactically-scoped trees is as follows: We mark a node low in the tree with a mark relation (one of E  Q  or C).', u'This bootstrapping behavior occurs naturally: The \u201ceasy\u201d examples are processed first  where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.', u'To define the features  we technically need to augment each tree z \u2208 ZL(x) with alignment information\u2014namely  for each predicate in z  the span in x (if any) that triggered it.']
['system', 'ROUGE-S*', 'Average_R:', '0.00205', '(95%-conf.int.', '0.00205', '-', '0.00205)']
['system', 'ROUGE-S*', 'Average_P:', '0.13333', '(95%-conf.int.', '0.13333', '-', '0.13333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00404', '(95%-conf.int.', '0.00404', '-', '0.00404)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:45', 'F:6']
['The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['We now present the full version of DCS which handles this type of divergence between syntactic and semantic scope.', 'CCG is one instantiation (Steedman  2000)  which is used by many semantic parsers  e.g.  Zettlemoyer and Collins (2005).', 'On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.', 'We now turn to the task of mapping natural language For the example in Figure 4(b)  the de- utterances to DCS trees.', 'In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00218', '(95%-conf.int.', '0.00218', '-', '0.00218)']
['system', 'ROUGE-S*', 'Average_P:', '0.02857', '(95%-conf.int.', '0.02857', '-', '0.02857)']
['system', 'ROUGE-S*', 'Average_F:', '0.00405', '(95%-conf.int.', '0.00405', '-', '0.00405)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:105', 'F:3']
['The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.', 'To further reduce the search space  F imposes a few additional constraints  e.g.  limiting the number of marked nodes to 2 and only allowing trace predicates between arity 1 predicates.', 'We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.', u'We also define an augmented lexicon L+ which includes a prototype word x for each predicate appearing in (iii) above (e.g.  (large  size))  which cancels the predicates triggered by x\xe2\u20ac\u2122s POS tag.', 'Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00241', '(95%-conf.int.', '0.00241', '-', '0.00241)']
['system', 'ROUGE-S*', 'Average_P:', '0.07619', '(95%-conf.int.', '0.07619', '-', '0.07619)']
['system', 'ROUGE-S*', 'Average_F:', '0.00467', '(95%-conf.int.', '0.00467', '-', '0.00467)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3321', 'P:105', 'F:8']
['Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.']
['Compositional question answering begins by mapping questions to logical forms  but training a semantic parser to perform this mapping typically requires the costly annotation of the target logical forms.', 'In each dataset  each sentence x is annotated with a Prolog logical form  which we use only to evaluate and get an answer y.', 'Unlike standard semantic parsing  our end goal is only to generate the correct y  so we are free to choose the representation for z.', 'Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets  a restrictor A and a nuclear scope B.', 'Now d contains the consistent joint assignments to the active nodes (which include the root and all marked nodes)  as well as information stored about each marked node.']
['system', 'ROUGE-S*', 'Average_R:', '0.00423', '(95%-conf.int.', '0.00423', '-', '0.00423)']
['system', 'ROUGE-S*', 'Average_P:', '0.02667', '(95%-conf.int.', '0.02667', '-', '0.02667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00730', '(95%-conf.int.', '0.00730', '-', '0.00730)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1891', 'P:300', 'F:8']
['This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.']
['Inspecting the final parameters calculus formulae.', u'As another example  w(average) = {(S  \xafx) : We write a DCS tree z as hp; r1 : c1; ... ; rm : cmi.', 'The basic version of DCS described thus far handles a core subset of language.', u'This evaluation is done with respect to a world w. Recall that a world w maps each predicate p \u2208 P to a set of tuples w(p).', 'Which one should we use?']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:190', 'F:0']
['Although a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words.']
['In fact  although neither DCS nor SEMRESP uses logical forms  DCS uses even less supervision than SEMRESP.', 'It suffices to define Xi(d) for a single column i.', u'Formally  p\u03b8(z  |x) \u221d e\u03c6(x z)T\u03b8  where \u03b8 and \u03c6(x  z) are parameter and feature vectors  respectively.', 'The combination rules are encoded in the tems  despite using no annotated logical forms.', 'On the other hand  existing unsupervised semantic parsers (Poon and Domingos  2009) do not handle deeper linguistic phenomena such as quantification  negation  and superlatives.']
['system', 'ROUGE-S*', 'Average_R:', '0.00405', '(95%-conf.int.', '0.00405', '-', '0.00405)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00754', '(95%-conf.int.', '0.00754', '-', '0.00754)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:55', 'F:3']
['The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.']
[u'Extending this notation to denotations  let (hA; \u03b1ii[i] = hh{ai : a \u2208 A}; \u03b1iii.', u'Let \u02dcZL \u03b8(x) be this approximation of ZL(x).', 'The restriction to present (for example  [in  loc] has high weight). trees is similar to economical DRT (Bos  2009).', u"Formally  let \u02dcO(\u03b8  \u03b8') be the objective function O(\u03b8) with ZL(x) \u02dcZL \u03b8I(x).", u"The stores are also concatenated (\u03b1 + \u03b1')."]
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:406', 'P:66', 'F:0']
['Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.']
['5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation  DCS  which offers a new perspective lexical semantics.', 'We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.', 'Results We first compare our system with Clarke et al. (2010) (henceforth  SEMRESP)  which also learns a semantic parser from question-answer pairs.', 'The main technical contribution of this work is a new semantic representation  dependency-based compositional semantics (DCS)  which is both simple and expressive (Section 2).', 'We trained our model using an EM-like algorithm (Section 3) on two benchmarks  GEO and JOBS (Section 4).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3240', 'P:55', 'F:0']
['Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets, a restrictor A and a nuclear scope B.']
[u'After training  given a new utterance x  our system outputs the most likely y  summing out the latent logical form z: argmaxy p\u03b8(T)(y  |x  z \u2208 \u02dcZL \u03b8(T)).', 'We then introduce the full version (Section 2.2)  which handles linguistic phenomena such as quantification  where syntactic and semantic scope diverge.', 'Think of d as consisting of n columns  one for each active node according to a pre-order traversal of z.', u'Formally  extraction simply moves the i-th column to the front: Xi(d) = d[i  \u2212(i  \xf8)]{\u03b11 = \xf8}.', u'Model We now present our discriminative semantic parsing model  which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.']
['system', 'ROUGE-S*', 'Average_R:', '0.00078', '(95%-conf.int.', '0.00078', '-', '0.00078)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00152', '(95%-conf.int.', '0.00152', '-', '0.00152)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:45', 'F:1']
0.0379477773561 0.00174444442506 0.0032355555196





input/ref/Task1/W06-3114_swastika.csv
input/res/Task1/W06-3114.csv
parsing: input/ref/Task1/W06-3114_swastika.csv
<S sid="47" ssid="13">Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data.</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
    <S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="37">Our suspicion is that BLEU is very sensitive to jargon, to selecting exactly the right words, and not synonyms that human judges may appreciate as equally good.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
<S sid="145" ssid="38">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S>
original cit marker offset is 0
new cit marker offset is 0



['145']
145
['145']
parsed_discourse_facet ['result_citation']
    <S sid="103" ssid="19">Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['103']
103
['103']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="16">Following this method, we repeatedly &#8212; say, 1000 times &#8212; sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.</S>
original cit marker offset is 0
new cit marker offset is 0



['50']
50
['50']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="7">We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.</S>
original cit marker offset is 0
new cit marker offset is 0



['68']
68
['68']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="8">The way judgements are collected, human judges tend to use the scores to rank systems against each other.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['result_citation']
<S sid="145" ssid="38">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S>
original cit marker offset is 0
new cit marker offset is 0



['145']
145
['145']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="80" ssid = "19">We collected around 300–400 judgements per judgement type (adequacy or fluency)  per system  per language pair.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'126'", "'80'", "'136'", "'0'"]
'134'
'126'
'80'
'136'
'0'
['134', '126', '80', '136', '0']
parsed_discourse_facet ['results_citation']
<S sid ="159" ssid = "52">(b) does the translation have the same meaning  including connotations?</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'159'", "'0'", "'84'", "'17'", "'170'"]
'159'
'0'
'84'
'17'
'170'
['159', '0', '84', '17', '170']
parsed_discourse_facet ['method_citation']
<S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'", "'18'", "'16'", "'126'", "'84'"]
'170'
'18'
'16'
'126'
'84'
['170', '18', '16', '126', '84']
parsed_discourse_facet ['method_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'17'", "'170'", "'84'", "'38'"]
'126'
'17'
'170'
'84'
'38'
['126', '17', '170', '84', '38']
parsed_discourse_facet ['method_citation']
<S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'", "'38'", "'126'", "'17'", "'84'"]
'170'
'38'
'126'
'17'
'84'
['170', '38', '126', '17', '84']
parsed_discourse_facet ['method_citation']
<S sid ="52" ssid = "18">Pairwise comparison: We can use the same method to assess the statistical significance of one system outperforming another.</S><S sid ="21" ssid = "14">The out-of-domain test set differs from the Europarl data in various ways.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="129" ssid = "22">All systems (except for Systran  which was not tuned to Europarl) did considerably worse on outof-domain training data.</S><S sid ="35" ssid = "1">For the automatic evaluation  we used BLEU  since it is the most established metric in the field.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'", "'21'", "'28'", "'129'", "'35'"]
'52'
'21'
'28'
'129'
'35'
['52', '21', '28', '129', '35']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="113" ssid = "6">The confidence intervals are computed by bootstrap resampling for BLEU  and by standard significance testing for the manual scores  as described earlier in the paper.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'113'", "'126'", "'28'", "'0'"]
'16'
'113'
'126'
'28'
'0'
['16', '113', '126', '28', '0']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "11">In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="109" ssid = "2">The scores and confidence intervals are detailed first in the Figures 7–10 in table form (including ranks)  and then in graphical form in Figures 11–16.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'16'", "'38'", "'84'", "'109'"]
'18'
'16'
'38'
'84'
'109'
['18', '16', '38', '84', '109']
parsed_discourse_facet ['method_citation']
<S sid ="9" ssid = "2">Training and testing is based on the Europarl corpus.</S><S sid ="44" ssid = "10">We computed BLEU scores for each submission with a single reference translation.</S><S sid ="143" ssid = "36">For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.</S><S sid ="25" ssid = "18">We received submissions from 14 groups from 11 institutions  as listed in Figure 2.</S><S sid ="35" ssid = "1">For the automatic evaluation  we used BLEU  since it is the most established metric in the field.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'44'", "'143'", "'25'", "'35'"]
'9'
'44'
'143'
'25'
'35'
['9', '44', '143', '25', '35']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="68" ssid = "7">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="28" ssid = "21">Systran submitted their commercial rule-based system that was not tuned to the Europarl corpus.</S><S sid ="136" ssid = "29">The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'68'", "'0'", "'28'", "'136'"]
'134'
'68'
'0'
'28'
'136'
['134', '68', '0', '28', '136']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="68" ssid = "7">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'0'", "'68'", "'126'", "'17'"]
'134'
'0'
'68'
'126'
'17'
['134', '0', '68', '126', '17']
parsed_discourse_facet ['method_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="159" ssid = "52">(b) does the translation have the same meaning  including connotations?</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'134'", "'17'", "'84'", "'159'"]
'126'
'134'
'17'
'84'
'159'
['126', '134', '17', '84', '159']
parsed_discourse_facet ['method_citation']
<S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="6" ssid = "4">English was again paired with German  French  and Spanish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'", "'38'", "'155'", "'170'", "'6'"]
'84'
'38'
'155'
'170'
'6'
['84', '38', '155', '170', '6']
parsed_discourse_facet ['results_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'134'", "'0'", "'17'", "'84'"]
'126'
'134'
'0'
'17'
'84'
['126', '134', '0', '17', '84']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "27">Again  we can compute average scores for all systems for the different language pairs (Figure 6).</S><S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="68" ssid = "7">We asked participants to each judge 200–300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'0'", "'126'", "'68'", "'17'"]
'134'
'0'
'126'
'68'
'17'
['134', '0', '126', '68', '17']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'170'", "'84'", "'126'", "'17'"]
'0'
'170'
'84'
'126'
'17'
['0', '170', '84', '126', '17']
parsed_discourse_facet ['method_citation']
<S sid ="80" ssid = "19">We collected around 300–400 judgements per judgement type (adequacy or fluency)  per system  per language pair.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="150" ssid = "43">For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French)  at least as measured by BLEU.</S><S sid ="16" ssid = "9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.</S><S sid ="155" ssid = "48">For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'84'", "'150'", "'16'", "'155'"]
'80'
'84'
'150'
'16'
'155'
['80', '84', '150', '16', '155']
parsed_discourse_facet ['method_citation']
<S sid ="126" ssid = "19">The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.</S><S sid ="84" ssid = "23">The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:</S><S sid ="38" ssid = "4">The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).</S><S sid ="17" ssid = "10">Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.</S><S sid ="170" ssid = "1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'", "'84'", "'38'", "'17'", "'170'"]
'126'
'84'
'38'
'17'
'170'
['126', '84', '38', '17', '170']
parsed_discourse_facet ['method_citation']
['This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.']
['The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00468', '(95%-conf.int.', '0.00468', '-', '0.00468)']
['system', 'ROUGE-S*', 'Average_P:', '0.05882', '(95%-conf.int.', '0.05882', '-', '0.05882)']
['system', 'ROUGE-S*', 'Average_F:', '0.00866', '(95%-conf.int.', '0.00866', '-', '0.00866)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:136', 'F:8']
['Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data.']
['Manual and Automatic Evaluation of Machine Translation between European Languages', u'We collected around 300\xe2\u20ac\u201c400 judgements per judgement type (adequacy or fluency)  per system  per language pair.', 'The easiest language pair according to BLEU (English-French: 28.33) received worse manual scores than the hardest (English-German: 14.01).', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'Again  we can compute average scores for all systems for the different language pairs (Figure 6).']
['system', 'ROUGE-S*', 'Average_R:', '0.00067', '(95%-conf.int.', '0.00067', '-', '0.00067)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00131', '(95%-conf.int.', '0.00131', '-', '0.00131)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:36', 'F:1']
['Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.']
['Training and testing is based on the Europarl corpus.', 'We received submissions from 14 groups from 11 institutions  as listed in Figure 2.', 'We computed BLEU scores for each submission with a single reference translation.', 'For instance  for out-ofdomain English-French  Systran has the best BLEU and manual scores.', 'For the automatic evaluation  we used BLEU  since it is the most established metric in the field.']
['system', 'ROUGE-S*', 'Average_R:', '0.01008', '(95%-conf.int.', '0.01008', '-', '0.01008)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00980', '(95%-conf.int.', '0.00980', '-', '0.00980)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:630', 'F:6']
['In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.']
['In addition to the Europarl test set  we also collected 29 editorials from the Project Syndicate website2  which are published in all the four languages of the shared task.', 'The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.05702', '(95%-conf.int.', '0.05702', '-', '0.05702)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.10788', '(95%-conf.int.', '0.10788', '-', '0.10788)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:91', 'F:91']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
[u'We asked participants to each judge 200\xe2\u20ac\u201c300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.', 'Manual and Automatic Evaluation of Machine Translation between European Languages', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'Again  we can compute average scores for all systems for the different language pairs (Figure 6).']
['system', 'ROUGE-S*', 'Average_R:', '0.02196', '(95%-conf.int.', '0.02196', '-', '0.02196)']
['system', 'ROUGE-S*', 'Average_P:', '0.50909', '(95%-conf.int.', '0.50909', '-', '0.50909)']
['system', 'ROUGE-S*', 'Average_F:', '0.04211', '(95%-conf.int.', '0.04211', '-', '0.04211)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:55', 'F:28']
['In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.']
['The BLEU score has been shown to correlate well with human judgement  when statistical machine translation systems are compared (Doddington  2002; Przybocki  2004; Li  2005).', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00526', '(95%-conf.int.', '0.00526', '-', '0.00526)']
['system', 'ROUGE-S*', 'Average_P:', '0.09890', '(95%-conf.int.', '0.09890', '-', '0.09890)']
['system', 'ROUGE-S*', 'Average_F:', '0.00999', '(95%-conf.int.', '0.00999', '-', '0.00999)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:91', 'F:9']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['(b) does the translation have the same meaning  including connotations?', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'Again  we can compute average scores for all systems for the different language pairs (Figure 6).']
['system', 'ROUGE-S*', 'Average_R:', '0.00332', '(95%-conf.int.', '0.00332', '-', '0.00332)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00626', '(95%-conf.int.', '0.00626', '-', '0.00626)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:55', 'F:3']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['Manual and Automatic Evaluation of Machine Translation between European Languages', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.04677', '(95%-conf.int.', '0.04677', '-', '0.04677)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.08936', '(95%-conf.int.', '0.08936', '-', '0.08936)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:55', 'F:55']
['We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.']
[u'We asked participants to each judge 200\xe2\u20ac\u201c300 sentences in terms of fluency and adequacy  the most commonly used manual evaluation metrics.', 'Manual and Automatic Evaluation of Machine Translation between European Languages', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The test set included 2000 sentences from the Europarl corpus  but also 1064 sentences out-ofdomain test data.', 'Again  we can compute average scores for all systems for the different language pairs (Figure 6).']
['system', 'ROUGE-S*', 'Average_R:', '0.06118', '(95%-conf.int.', '0.06118', '-', '0.06118)']
['system', 'ROUGE-S*', 'Average_P:', '0.85714', '(95%-conf.int.', '0.85714', '-', '0.85714)']
['system', 'ROUGE-S*', 'Average_F:', '0.11420', '(95%-conf.int.', '0.11420', '-', '0.11420)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:91', 'F:78']
['The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.']
['(b) does the translation have the same meaning  including connotations?', 'Manual and Automatic Evaluation of Machine Translation between European Languages', 'Participants were also provided with two sets of 2 000 sentences of parallel text to be used for system development and tuning.', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:45', 'F:0']
['The way judgements are collected, human judges tend to use the scores to rank systems against each other.']
['For some language pairs (such as GermanEnglish) system performance is more divergent than for others (such as English-French)  at least as measured by BLEU.', u'We collected around 300\xe2\u20ac\u201c400 judgements per judgement type (adequacy or fluency)  per system  per language pair.', 'For instance  in the recent IWSLT evaluation  first fluency annotations were solicited (while withholding the source sentence)  and then adequacy annotations.', 'The human judges were presented with the following definition of adequacy and fluency  but no additional instructions:', 'The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000  which is excluded from the training data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00489', '(95%-conf.int.', '0.00489', '-', '0.00489)']
['system', 'ROUGE-S*', 'Average_P:', '0.25000', '(95%-conf.int.', '0.25000', '-', '0.25000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00960', '(95%-conf.int.', '0.00960', '-', '0.00960)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:28', 'F:7']
0.351436360441 0.0196209089125 0.0362881814883





input/ref/Task1/W99-0623_sweta.csv
input/res/Task1/W99-0623.csv
parsing: input/ref/Task1/W99-0623_sweta.csv
 <S sid="144" ssid="6">Combining multiple highly-accurate independent parsers yields promising results.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
 <S sid="125" ssid="54">The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["125'"]
125'
['125']
parsed_discourse_facet ['method_citation']
<S sid="125" ssid="54">The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["125'"]
125'
['125']
parsed_discourse_facet ['method_citation']
    <S sid="48" ssid="34">&#8226; Similarly, when the na&#239;ve Bayes classifier is configured such that the constituents require estimated probabilities strictly larger than 0.5 to be accepted, there is not enough probability mass remaining on crossing brackets for them to be included in the hypothesis.</S>
original cit marker offset is 0
new cit marker offset is 0



["48'"]
48'
['48']
parsed_discourse_facet ['method_citation']
 <S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="63">As seen by the drop in average individual parser performance baseline, the introduced parser does not perform very well.</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
    <S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="13" ssid="9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993).</S>
original cit marker offset is 0
new cit marker offset is 0



["13'"]
13'
['13']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="37">From this we see that a finer-grained model for parser combination, at least for the features we have examined, will not give us any additional power.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="98" ssid="27">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["98'"]
98'
['98']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="13">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'"]
27'
['27']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="9">For our experiments we also report the mean of precision and recall, which we denote by (P + R)I2 and F-measure.</S>
    <S sid="81" ssid="10">F-measure is the harmonic mean of precision and recall, 2PR/(P + R).</S>
    <S sid="82" ssid="11">It is closer to the smaller value of precision and recall when there is a large skew in their values.</S>
original cit marker offset is 0
new cit marker offset is 0



["80'", "'81'", "'82'"]
80'
'81'
'82'
['80', '81', '82']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="35">In general, the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["49'"]
49'
['49']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="7">Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["11'"]
11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="98" ssid="27">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["98'"]
98'
['98']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0623.csv
<S sid ="142" ssid = "4">Both of the switching techniques  as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.</S><S sid ="141" ssid = "3">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'141'", "'18'", "'81'", "'57'"]
'142'
'141'
'18'
'81'
'57'
['142', '141', '18', '81', '57']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'61'", "'81'", "'78'", "'139'"]
'112'
'61'
'81'
'78'
'139'
['112', '61', '81', '78', '139']
Error in Discourse Facet
<S sid ="62" ssid = "48">This is the parse that is closest to the centroid of the observed parses under the similarity metric.</S><S sid ="88" ssid = "17">For example  one parser could be more accurate at predicting noun phrases than the other parsers.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid ="115" ssid = "44">It is the performance we could achieve if an omniscient observer told us which parser to pick for each of the sentences.</S><S sid ="22" ssid = "8">If enough parsers suggest that a particular constituent belongs in the parse  we include it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'", "'88'", "'27'", "'115'", "'22'"]
'62'
'88'
'27'
'115'
'22'
['62', '88', '27', '115', '22']
Error in Discourse Facet
<S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="125" ssid = "54">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S><S sid ="41" ssid = "27">IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.</S><S sid ="44" ssid = "30">Each of the constituents must have received at least 1 votes from the k parsers  so a > I1 and 2 — 2k±-1 b > ri-5-111.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'21'", "'125'", "'41'", "'44'"]
'38'
'21'
'125'
'41'
'44'
['38', '21', '125', '41', '44']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'81'", "'15'", "'78'", "'139'"]
'112'
'81'
'15'
'78'
'139'
['112', '81', '15', '78', '139']
Error in Discourse Facet
<S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="136" ssid = "65">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S><S sid ="131" ssid = "60">It was then tested on section 22 of the Treebank in conjunction with the other parsers.</S><S sid ="103" ssid = "32">The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.</S><S sid ="132" ssid = "61">The results of this experiment can be seen in Table 5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'136'", "'131'", "'103'", "'132'"]
'21'
'136'
'131'
'103'
'132'
['21', '136', '131', '103', '132']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'78'", "'81'", "'61'", "'130'"]
'112'
'78'
'81'
'61'
'130'
['112', '78', '81', '61', '130']
Error in Discourse Facet
<S sid ="125" ssid = "54">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="1" ssid = "1">Three state-of-the-art statistical parsers are combined to produce more accurate parses  as well as new bounds on achievable Treebank parsing accuracy.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'27'", "'21'", "'1'", "'38'"]
'125'
'27'
'21'
'1'
'38'
['125', '27', '21', '1', '38']
Error in Discourse Facet
<S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="51" ssid = "37">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes  a completely flat structure.</S><S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'12'", "'51'", "'18'", "'57'"]
'55'
'12'
'51'
'18'
'57'
['55', '12', '51', '18', '57']
Error in Discourse Facet
<S sid ="142" ssid = "4">Both of the switching techniques  as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.</S><S sid ="127" ssid = "56">Parser 3  the most accurate parser  was chosen 71% of the time  and Parser 1  the least accurate parser was chosen 16% of the time.</S><S sid ="141" ssid = "3">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="140" ssid = "2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'127'", "'141'", "'12'", "'140'"]
'142'
'127'
'141'
'12'
'140'
['142', '127', '141', '12', '140']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'78'", "'61'", "'130'", "'15'"]
'112'
'78'
'61'
'130'
'15'
['112', '78', '61', '130', '15']
Error in Discourse Facet
<S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="51" ssid = "37">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes  a completely flat structure.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S><S sid ="26" ssid = "12">This technique has the advantage of requiring no training  but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'51'", "'52'", "'50'", "'26'"]
'55'
'51'
'52'
'50'
'26'
['55', '51', '52', '50', '26']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="114" ssid = "43">The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'81'", "'78'", "'61'", "'114'"]
'112'
'81'
'78'
'61'
'114'
['112', '81', '78', '61', '114']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'81'", "'12'", "'15'", "'130'"]
'112'
'81'
'12'
'15'
'130'
['112', '81', '12', '15', '130']
Error in Discourse Facet
<S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="114" ssid = "43">The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.</S><S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S>
original cit marker offset is 0
new cit marker offset is 0



["'78'", "'61'", "'114'", "'112'", "'81'"]
'78'
'61'
'114'
'112'
'81'
['78', '61', '114', '112', '81']
Error in Discourse Facet
<S sid ="129" ssid = "58">In the interest of testing the robustness of these combining techniques  we added a fourth  simple nonlexicalized PCFG parser.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'129'", "'11'", "'130'", "'15'", "'9'"]
'129'
'11'
'130'
'15'
'9'
['129', '11', '130', '15', '9']
Error in Discourse Facet
<S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S><S sid ="114" ssid = "43">The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.</S><S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["'81'", "'57'", "'114'", "'112'", "'78'"]
'81'
'57'
'114'
'112'
'78'
['81', '57', '114', '112', '78']
Error in Discourse Facet
['Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.']
['The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.', 'The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.', 'F-measure is the harmonic mean of precision and recall  2PR/(P + R).', 'The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.', "The second row is the accuracy of the best of the three parsers.'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00675', '(95%-conf.int.', '0.00675', '-', '0.00675)']
['system', 'ROUGE-S*', 'Average_P:', '0.04762', '(95%-conf.int.', '0.04762', '-', '0.04762)']
['system', 'ROUGE-S*', 'Average_F:', '0.01182', '(95%-conf.int.', '0.01182', '-', '0.01182)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:105', 'F:5']
['It is closer to the smaller value of precision and recall when there is a large skew in their values.', 'For our experiments we also report the mean of precision and recall, which we denote by (P + R)I2 and F-measure.', 'F-measure is the harmonic mean of precision and recall, 2PR/(P + R).']
['The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.', 'We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.', "The second row is the accuracy of the best of the three parsers.'", 'F-measure is the harmonic mean of precision and recall  2PR/(P + R).', 'The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.']
['system', 'ROUGE-S*', 'Average_R:', '0.02418', '(95%-conf.int.', '0.02418', '-', '0.02418)']
['system', 'ROUGE-S*', 'Average_P:', '0.09942', '(95%-conf.int.', '0.09942', '-', '0.09942)']
['system', 'ROUGE-S*', 'Average_F:', '0.03890', '(95%-conf.int.', '0.03890', '-', '0.03890)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:171', 'F:17']
['We have presented two general approaches to studying parser combination: parser switching and parse hybridization.']
['We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.', 'This technique has the advantage of requiring no training  but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.', 'One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes  a completely flat structure.', 'There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.', 'This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.']
['system', 'ROUGE-S*', 'Average_R:', '0.00940', '(95%-conf.int.', '0.00940', '-', '0.00940)']
['system', 'ROUGE-S*', 'Average_P:', '0.33333', '(95%-conf.int.', '0.33333', '-', '0.33333)']
['system', 'ROUGE-S*', 'Average_F:', '0.01828', '(95%-conf.int.', '0.01828', '-', '0.01828)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:45', 'F:15']
['In general, the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.']
['Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).', 'In the interest of testing the robustness of these combining techniques  we added a fourth  simple nonlexicalized PCFG parser.', 'The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.', 'We are interested in combining the substructures of the input parses to produce a better parse.', 'Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).']
['system', 'ROUGE-S*', 'Average_R:', '0.00302', '(95%-conf.int.', '0.00302', '-', '0.00302)']
['system', 'ROUGE-S*', 'Average_P:', '0.07576', '(95%-conf.int.', '0.07576', '-', '0.07576)']
['system', 'ROUGE-S*', 'Average_F:', '0.00582', '(95%-conf.int.', '0.00582', '-', '0.00582)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:66', 'F:5']
['The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.']
['We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.', 'We have presented two general approaches to studying parser combination: parser switching and parse hybridization.', 'F-measure is the harmonic mean of precision and recall  2PR/(P + R).', 'The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.', "The second row is the accuracy of the best of the three parsers.'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00150', '(95%-conf.int.', '0.00150', '-', '0.00150)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00259', '(95%-conf.int.', '0.00259', '-', '0.00259)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:105', 'F:1']
['Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.']
['The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.', 'We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.', 'The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.', 'F-measure is the harmonic mean of precision and recall  2PR/(P + R).', "The second row is the accuracy of the best of the three parsers.'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:528', 'P:120', 'F:0']
['The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.']
['Three state-of-the-art statistical parsers are combined to produce more accurate parses  as well as new bounds on achievable Treebank parsing accuracy.', u'Under certain conditions the constituent voting and na\xefve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.', u'Another technique for parse hybridization is to use a na\xefve Bayes classifier to determine which constituents to include in the parse.', u'The constituent voting and na\xefve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.', "One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set."]
['system', 'ROUGE-S*', 'Average_R:', '0.02131', '(95%-conf.int.', '0.02131', '-', '0.02131)']
['system', 'ROUGE-S*', 'Average_P:', '0.13000', '(95%-conf.int.', '0.13000', '-', '0.13000)']
['system', 'ROUGE-S*', 'Average_F:', '0.03662', '(95%-conf.int.', '0.03662', '-', '0.03662)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:300', 'F:39']
['The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.']
['It is the performance we could achieve if an omniscient observer told us which parser to pick for each of the sentences.', 'This is the parse that is closest to the centroid of the observed parses under the similarity metric.', u'Another technique for parse hybridization is to use a na\xc3\xafve Bayes classifier to determine which constituents to include in the parse.', 'For example  one parser could be more accurate at predicting noun phrases than the other parsers.', 'If enough parsers suggest that a particular constituent belongs in the parse  we include it.']
['system', 'ROUGE-S*', 'Average_R:', '0.01138', '(95%-conf.int.', '0.01138', '-', '0.01138)']
['system', 'ROUGE-S*', 'Average_P:', '0.07619', '(95%-conf.int.', '0.07619', '-', '0.07619)']
['system', 'ROUGE-S*', 'Average_F:', '0.01980', '(95%-conf.int.', '0.01980', '-', '0.01980)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:105', 'F:8']
['From this we see that a finer-grained model for parser combination, at least for the features we have examined, will not give us any additional power.']
['The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.', 'We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.', 'The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.', 'We are interested in combining the substructures of the input parses to produce a better parse.', "The second row is the accuracy of the best of the three parsers.'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00168', '(95%-conf.int.', '0.00168', '-', '0.00168)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00312', '(95%-conf.int.', '0.00313', '-', '0.00313)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:45', 'F:1']
0.0882288879086 0.00880222212442 0.0152166664976





input/ref/Task1/P87-1015_swastika.csv
input/res/Task1/P87-1015.csv
parsing: input/ref/Task1/P87-1015_swastika.csv
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="149" ssid="34">We can obtain a letter equivalent CFL defined by a CFG in which the for each rule as above, we have the production A &#8212;* A1 Anup where tk (up) = cp.</S>
original cit marker offset is 0
new cit marker offset is 0



['149']
149
['149']
parsed_discourse_facet ['method_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
    <S sid="2" ssid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['2']
2
['2']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="222" ssid="28">However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



['222']
222
['222']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['???']
???
['???']
parsed_discourse_facet ['aim_citation']
<S sid="207" ssid="13">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['207']
207
['207']
parsed_discourse_facet ['result_citation']
<S sid="207" ssid="13">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['207']
207
['207']
parsed_discourse_facet ['result_citation']
<S sid="19" ssid="4">It can be easily shown from Thatcher's result that the path set of every local set is a regular set.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/P87-1015.csv
<S sid ="115" ssid = "21">From the point of view of recognition  independent paths in the derivation structures suggests that a top-down parser (for example) can work on each branch independently  which may lead to efficient parsing using an algorithm based on the Divide and Conquer technique.</S><S sid ="222" ssid = "28">However  in order to capture the properties of various grammatical systems under consideration  our notation is more restrictive that ILFP  which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="67" ssid = "52">On the one hand  the definition of composition in Steedman (1985)  which technically permits composition of functions with unbounded number of arguments  generates tree sets with dependent paths such as those shown in Figure 6.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'", "'222'", "'195'", "'67'", "'153'"]
'115'
'222'
'195'
'67'
'153'
['115', '222', '195', '67', '153']
parsed_discourse_facet ['results_citation']
<S sid ="37" ssid = "22">Like CFG's  the choice is predetermined by a finite number of rules encapsulated in the grammar.</S><S sid ="231" ssid = "37">In this paper  our goal has been to use the notion of LCFRS's to classify grammatical systems on the basis of their strong generative capacity.</S><S sid ="197" ssid = "3">We address the question of whether or not a formalism can generate only structural descriptions with independent paths.</S><S sid ="116" ssid = "1">From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).</S><S sid ="92" ssid = "77">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'231'", "'197'", "'116'", "'92'"]
'37'
'231'
'197'
'116'
'92'
['37', '231', '197', '116', '92']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.</S><S sid ="131" ssid = "16">The composition operations in the case of CFG's are parameterized by the productions.</S><S sid ="65" ssid = "50">While the generative power of CG's is greater that of CFG's  it appears to be highly constrained.</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="179" ssid = "64">It can be seen that M performs a top-down recognition of the input al ... nin logspace.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'131'", "'65'", "'227'", "'179'"]
'204'
'131'
'65'
'227'
'179'
['204', '131', '65', '227', '179']
parsed_discourse_facet ['method_citation']
<S sid ="194" ssid = "79">Thus  M works in logspace and recognition can be done on a deterministic TM in polynomial tape.</S><S sid ="92" ssid = "77">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S><S sid ="65" ssid = "50">While the generative power of CG's is greater that of CFG's  it appears to be highly constrained.</S><S sid ="20" ssid = "5">As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.</S><S sid ="131" ssid = "16">The composition operations in the case of CFG's are parameterized by the productions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'194'", "'92'", "'65'", "'20'", "'131'"]
'194'
'92'
'65'
'20'
'131'
['194', '92', '65', '20', '131']
parsed_discourse_facet ['method_citation']
<S sid ="155" ssid = "40">CFG's  TAG's  MCTAG's and HG's are all members of this class since they satisfy these restrictions.</S><S sid ="37" ssid = "22">Like CFG's  the choice is predetermined by a finite number of rules encapsulated in the grammar.</S><S sid ="117" ssid = "2">Our goal is to define a class of formal systems  and show that any member of this class will possess certain attractive properties.</S><S sid ="92" ssid = "77">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S><S sid ="48" ssid = "33">There has been recent interest in the application of Indexed Grammars (IG's) to natural languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'", "'37'", "'117'", "'92'", "'48'"]
'155'
'37'
'117'
'92'
'48'
['155', '37', '117', '92', '48']
parsed_discourse_facet ['method_citation']
<S sid ="20" ssid = "5">As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.</S><S sid ="216" ssid = "22">Having defined LCFRS's  in Section 4.2 we established the semilinearity (and hence constant growth property) of the languages generated.</S><S sid ="59" ssid = "44">Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.</S><S sid ="179" ssid = "64">It can be seen that M performs a top-down recognition of the input al ... nin logspace.</S><S sid ="131" ssid = "16">The composition operations in the case of CFG's are parameterized by the productions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'216'", "'59'", "'179'", "'131'"]
'20'
'216'
'59'
'179'
'131'
['20', '216', '59', '179', '131']
parsed_discourse_facet ['method_citation']
<S sid ="162" ssid = "47">Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S><S sid ="219" ssid = "25">The composition operations are mapped onto operations that use concatenation to define the substrings spanned by the resulting structures.</S><S sid ="0" ssid = "0">CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*</S><S sid ="193" ssid = "78">Since the work tapes store integers (which can be written in binary) that never exceed the size of the input  no configuration has space exceeding 0(log n).</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'", "'195'", "'219'", "'0'", "'193'"]
'162'
'195'
'219'
'0'
'193'
['162', '195', '219', '0', '193']
parsed_discourse_facet ['method_citation']
<S sid ="11" ssid = "9">By considering derivation trees  and thus abstracting away from the details of the composition operation and the structures being manipulated  we are able to state the similarities and differences between the 'This work was partially supported by NSF grants MCS42-19116-CER  MCS82-07294 and DCR-84-10413  ARO grant DAA 29-84-9-0027  and DARPA grant N00014-85-K0018.</S><S sid ="115" ssid = "21">From the point of view of recognition  independent paths in the derivation structures suggests that a top-down parser (for example) can work on each branch independently  which may lead to efficient parsing using an algorithm based on the Divide and Conquer technique.</S><S sid ="153" ssid = "38">In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.</S><S sid ="1" ssid = "1">We consider the structural descriptions produced by various grammatical formalisms in terms of the complexity of the paths and the relationship between paths in the sets of structural descriptions that each system can generate.</S><S sid ="195" ssid = "1">We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'115'", "'153'", "'1'", "'195'"]
'11'
'115'
'153'
'1'
'195'
['11', '115', '153', '1', '195']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*</S><S sid ="219" ssid = "25">The composition operations are mapped onto operations that use concatenation to define the substrings spanned by the resulting structures.</S><S sid ="125" ssid = "10">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S sid ="58" ssid = "43">Unification is used in LFG's to produce structures having two dependent spines of unbounded length as in Figure 5.</S><S sid ="192" ssid = "77">Thus  the ATM has no more than 6km&quot; + 1 work tapes  where km&quot; is the maximum number of substrings spanned by a derived structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'219'", "'125'", "'58'", "'192'"]
'0'
'219'
'125'
'58'
'192'
['0', '219', '125', '58', '192']
parsed_discourse_facet ['method_citation']
<S sid ="193" ssid = "78">Since the work tapes store integers (which can be written in binary) that never exceed the size of the input  no configuration has space exceeding 0(log n).</S><S sid ="129" ssid = "14">Frontier nodes are annotated by zero arty functions corresponding to elementary structures.</S><S sid ="127" ssid = "12">Nodes are annotated by the name of the composition operation used at that step in the derivation.</S><S sid ="172" ssid = "57">A step of an ATM consists of reading a symbol from each tape and optionally moving each head to the left or right one tape cell.</S><S sid ="173" ssid = "58">A configuration of M consists of a state of the finite control  the nonblank contents of the input tape and k work tapes  and the position of each head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'193'", "'129'", "'127'", "'172'", "'173'"]
'193'
'129'
'127'
'172'
'173'
['193', '129', '127', '172', '173']
parsed_discourse_facet ['method_citation']
<S sid ="189" ssid = "74">For rules p : A fpo such that fp is constant function  giving an elementary structure  fp is defined such that fp() = (Si ... xi() where each z is a constant string.</S><S sid ="183" ssid = "68">We assume that M is in an existential state qA  with integers i1 and i2 representing zi in the (2i — 1)th and 22th work tape  for 1 < i < k. For each rule p : A fp(B  C) such that fp is mapped onto the function fp defined by the following rule. jp((xi .. •  rnt)  (1ii  • • • • Yn3))= (Zi   • • •   Zk) M breaks xi   zk into substrings xi    xn  and yi ... y&quot; conforming to the definition of fp.</S><S sid ="160" ssid = "45">A derived structure will be mapped onto a sequence zi of substrings (not necessarily contiguous in the input)  and the composition operations will be mapped onto functions that can defined as follows3. f((zi • • •   zni)  (m. • • •  Yn3)) = (Z1  • • •   Zn3) where each z  is the concatenation of strings from z 's and yk's.</S><S sid ="148" ssid = "33">If 0(A) gives the number of occurrences of each terminal in the structure named by A  then  given the constraints imposed on the formalism  for each rule A --. fp(Ai    An) we have the equality where c„ is some constant.</S><S sid ="201" ssid = "7">It is interesting to note  however  that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG  FUG and IG's.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'183'", "'160'", "'148'", "'201'"]
'189'
'183'
'160'
'148'
'201'
['189', '183', '160', '148', '201']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*</S><S sid ="162" ssid = "47">Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="196" ssid = "2">We contrasted formalisms such as CFG's  HG's  TAG's and MCTAG's  with formalisms such as IG's and unificational systems such as LFG's and FUG's.</S><S sid ="204" ssid = "10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'162'", "'227'", "'196'", "'204'"]
'0'
'162'
'227'
'196'
'204'
['0', '162', '227', '196', '204']
parsed_discourse_facet ['method_citation']
<S sid ="58" ssid = "43">Unification is used in LFG's to produce structures having two dependent spines of unbounded length as in Figure 5.</S><S sid ="214" ssid = "20">LCFRS's share several properties possessed by the class of mildly context-sensitive formalisms discussed by Joshi (1983/85).</S><S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="196" ssid = "2">We contrasted formalisms such as CFG's  HG's  TAG's and MCTAG's  with formalisms such as IG's and unificational systems such as LFG's and FUG's.</S><S sid ="163" ssid = "48">This representation of structures by substrings and the composition operation by its effect on substrings is related to the work of Rounds (1985).</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'214'", "'227'", "'196'", "'163'"]
'58'
'214'
'227'
'196'
'163'
['58', '214', '227', '196', '163']
parsed_discourse_facet ['results_citation']
<S sid ="50" ssid = "35">The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.</S><S sid ="202" ssid = "8">As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.</S><S sid ="7" ssid = "5">The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG's and IG's.</S><S sid ="230" ssid = "36">LCFRS's have only been loosely defined in this paper; we have yet to provide a complete set of formal properties associated with members of this class.</S><S sid ="110" ssid = "16">The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'202'", "'7'", "'230'", "'110'"]
'50'
'202'
'7'
'230'
'110'
['50', '202', '7', '230', '110']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "42">For example  in TAG's a derived auxiliary tree spans two substrings (to the left and right of the foot node)  and the adjunction operation inserts another substring (spanned by the subtree under the node where adjunction takes place) between them (see Figure 3).</S><S sid ="162" ssid = "47">Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .</S><S sid ="83" ssid = "68">The following context-free production captures the derivation step of the grammar shown in Figure 7  in which the trees in the auxiliary tree set are adjoined into themselves at the root node (address c).</S><S sid ="67" ssid = "52">On the one hand  the definition of composition in Steedman (1985)  which technically permits composition of functions with unbounded number of arguments  generates tree sets with dependent paths such as those shown in Figure 6.</S><S sid ="154" ssid = "39">Since each composition operation is linear and nonerasing  a bounded sequences of substrings associated with the resulting structure is obtained by combining the substrings in each of its arguments using only the concatenation operation  including each substring exactly once.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'162'", "'83'", "'67'", "'154'"]
'157'
'162'
'83'
'67'
'154'
['157', '162', '83', '67', '154']
parsed_discourse_facet ['method_citation']
<S sid ="227" ssid = "33">Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.</S><S sid ="156" ssid = "41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S><S sid ="204" ssid = "10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.</S><S sid ="213" ssid = "19">By sharing stacks (in IG's) or by using nonlinear equations over f-structures (in FUG's and LFG's)  structures with unbounded dependencies between paths can be generated.</S><S sid ="125" ssid = "10">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'227'", "'156'", "'204'", "'213'", "'125'"]
'227'
'156'
'204'
'213'
'125'
['227', '156', '204', '213', '125']
parsed_discourse_facet ['method_citation']
['In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.']
['We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.', 'CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*', 'Since the work tapes store integers (which can be written in binary) that never exceed the size of the input  no configuration has space exceeding 0(log n).', 'Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .', 'The composition operations are mapped onto operations that use concatenation to define the substrings spanned by the resulting structures.']
['system', 'ROUGE-S*', 'Average_R:', '0.01512', '(95%-conf.int.', '0.01512', '-', '0.01512)']
['system', 'ROUGE-S*', 'Average_P:', '0.02643', '(95%-conf.int.', '0.02643', '-', '0.02643)']
['system', 'ROUGE-S*', 'Average_F:', '0.01924', '(95%-conf.int.', '0.01924', '-', '0.01924)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:946', 'F:25']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
['From the point of view of recognition  independent paths in the derivation structures suggests that a top-down parser (for example) can work on each branch independently  which may lead to efficient parsing using an algorithm based on the Divide and Conquer technique.', 'We have studied the structural descriptions (tree sets) that can be assigned by various grammatical systems  and classified these formalisms on the basis of two features: path complexity; and path independence.', 'In this section for the purposes of showing that polynomial time recognition is possible  we make the additional restriction that the contribution of a derived structure to the input string can be specified by a bounded sequence of substrings of the input.', 'However  in order to capture the properties of various grammatical systems under consideration  our notation is more restrictive that ILFP  which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.', 'On the one hand  the definition of composition in Steedman (1985)  which technically permits composition of functions with unbounded number of arguments  generates tree sets with dependent paths such as those shown in Figure 6.']
['system', 'ROUGE-S*', 'Average_R:', '0.00157', '(95%-conf.int.', '0.00157', '-', '0.00157)']
['system', 'ROUGE-S*', 'Average_P:', '0.05833', '(95%-conf.int.', '0.05833', '-', '0.05833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00305', '(95%-conf.int.', '0.00305', '-', '0.00305)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4465', 'P:120', 'F:7']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
['We address the question of whether or not a formalism can generate only structural descriptions with independent paths.', 'From the discussion so far it is clear that a number of formalisms involve some type of context-free rewriting (they have derivation trees that are local sets).', "Like CFG's  the choice is predetermined by a finite number of rules encapsulated in the grammar.", "In this paper  our goal has been to use the notion of LCFRS's to classify grammatical systems on the basis of their strong generative capacity.", 'We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.']
['system', 'ROUGE-S*', 'Average_R:', '0.02551', '(95%-conf.int.', '0.02551', '-', '0.02551)']
['system', 'ROUGE-S*', 'Average_P:', '0.25000', '(95%-conf.int.', '0.25000', '-', '0.25000)']
['system', 'ROUGE-S*', 'Average_F:', '0.04630', '(95%-conf.int.', '0.04630', '-', '0.04630)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:120', 'F:30']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
['Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.', "By sharing stacks (in IG's) or by using nonlinear equations over f-structures (in FUG's and LFG's)  structures with unbounded dependencies between paths can be generated.", "The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.", 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', "Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings."]
['system', 'ROUGE-S*', 'Average_R:', '0.00298', '(95%-conf.int.', '0.00298', '-', '0.00298)']
['system', 'ROUGE-S*', 'Average_P:', '0.05833', '(95%-conf.int.', '0.05833', '-', '0.05833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00568', '(95%-conf.int.', '0.00568', '-', '0.00568)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:120', 'F:7']
["In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs."]
['Since each composition operation is linear and nonerasing  a bounded sequences of substrings associated with the resulting structure is obtained by combining the substrings in each of its arguments using only the concatenation operation  including each substring exactly once.', 'The following context-free production captures the derivation step of the grammar shown in Figure 7  in which the trees in the auxiliary tree set are adjoined into themselves at the root node (address c).', 'Some of the operations will be constant functions  corresponding to elementary structures  and will be written as f () = zi)  where each z  is a constant  the string of terminal symbols al an   .', "For example  in TAG's a derived auxiliary tree spans two substrings (to the left and right of the foot node)  and the adjunction operation inserts another substring (spanned by the subtree under the node where adjunction takes place) between them (see Figure 3).", 'On the one hand  the definition of composition in Steedman (1985)  which technically permits composition of functions with unbounded number of arguments  generates tree sets with dependent paths such as those shown in Figure 6.']
['system', 'ROUGE-S*', 'Average_R:', '0.00131', '(95%-conf.int.', '0.00131', '-', '0.00131)']
['system', 'ROUGE-S*', 'Average_P:', '0.07576', '(95%-conf.int.', '0.07576', '-', '0.07576)']
['system', 'ROUGE-S*', 'Average_F:', '0.00257', '(95%-conf.int.', '0.00257', '-', '0.00257)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3828', 'P:66', 'F:5']
["It can be easily shown from Thatcher's result that the path set of every local set is a regular set."]
["As illustrated by MCTAG's  it is possible for a formalism to give tree sets with bounded dependent paths while still sharing the constrained rewriting properties of CFG's  HG's  and TAG's.", "LCFRS's have only been loosely defined in this paper; we have yet to provide a complete set of formal properties associated with members of this class.", "The work of Rounds (1969) shows that the path sets of trees derived by IG's (like those of TAG's) are context-free languages.", "The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG's and IG's.", 'The independence of paths in the tree sets of the k tI grammatical formalism in this hierarchy can be shown by means of tree pumping lemma of the form t1ti3t .']
['system', 'ROUGE-S*', 'Average_R:', '0.00384', '(95%-conf.int.', '0.00384', '-', '0.00384)']
['system', 'ROUGE-S*', 'Average_P:', '0.20000', '(95%-conf.int.', '0.20000', '-', '0.20000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00753', '(95%-conf.int.', '0.00753', '-', '0.00753)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:45', 'F:9']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
["Having defined LCFRS's  in Section 4.2 we established the semilinearity (and hence constant growth property) of the languages generated.", 'Bresnan  Kaplan  Peters  and Zaenen (1982) argue that these structures are needed to describe crossed-serial dependencies in Dutch subordinate clauses.', "The composition operations in the case of CFG's are parameterized by the productions.", "As a result  CFG's can not provide the structural descriptions in which there are nested dependencies between symbols labelling a path.", 'It can be seen that M performs a top-down recognition of the input al ... nin logspace.']
['system', 'ROUGE-S*', 'Average_R:', '0.00408', '(95%-conf.int.', '0.00408', '-', '0.00408)']
['system', 'ROUGE-S*', 'Average_P:', '0.04167', '(95%-conf.int.', '0.04167', '-', '0.04167)']
['system', 'ROUGE-S*', 'Average_F:', '0.00743', '(95%-conf.int.', '0.00743', '-', '0.00743)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1225', 'P:120', 'F:5']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
[u'We assume that M is in an existential state qA  with integers i1 and i2 representing zi in the (2i \u2014 1)th and 22th work tape  for 1 ', 'For rules p : A fpo such that fp is constant function  giving an elementary structure  fp is defined such that fp() = (Si ... xi() where each z is a constant string.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:120', 'F:0']
['We can obtain a letter equivalent CFL defined by a CFG in which the for each rule as above, we have the production A &#8212;* A1 Anup where tk (up) = cp.']
['Formalisms such as the restricted indexed grammars (Gazdar  1985) and members of the hierarchy of grammatical systems given by Weir (1987) have independent paths  but more complex path sets.', "The composition operations in the case of CFG's are parameterized by the productions.", "The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's  HG's  TAG's  and MCTAG's are all local sets.", 'It can be seen that M performs a top-down recognition of the input al ... nin logspace.', "While the generative power of CG's is greater that of CFG's  it appears to be highly constrained."]
['system', 'ROUGE-S*', 'Average_R:', '0.00073', '(95%-conf.int.', '0.00073', '-', '0.00073)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00137', '(95%-conf.int.', '0.00137', '-', '0.00137)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:78', 'F:1']
['However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.']
["Unification is used in LFG's to produce structures having two dependent spines of unbounded length as in Figure 5.", 'CHARACTERIZING STRUCTURAL DESCRIPTIONS PRODUCED BY VARIOUS GRAMMATICAL FORMALISMS*', 'Thus  the ATM has no more than 6km&quot; + 1 work tapes  where km&quot; is the maximum number of substrings spanned by a derived structure.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'The composition operations are mapped onto operations that use concatenation to define the substrings spanned by the resulting structures.']
['system', 'ROUGE-S*', 'Average_R:', '0.00089', '(95%-conf.int.', '0.00089', '-', '0.00089)']
['system', 'ROUGE-S*', 'Average_P:', '0.00526', '(95%-conf.int.', '0.00526', '-', '0.00526)']
['system', 'ROUGE-S*', 'Average_F:', '0.00152', '(95%-conf.int.', '0.00152', '-', '0.00152)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:190', 'F:1']
0.0728599992714 0.00560299994397 0.00946899990531





input/ref/Task1/P04-1036_swastika.csv
input/res/Task1/P04-1036.csv
parsing: input/ref/Task1/P04-1036_swastika.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="24">We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.</S>
original cit marker offset is 0
new cit marker offset is 0



['68']
68
['68']
parsed_discourse_facet ['result_citation']
<S sid="83" ssid="12">The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.</S>
original cit marker offset is 0
new cit marker offset is 0



['83']
83
['83']
parsed_discourse_facet ['result_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
<S sid="101" ssid="30">Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.</S>
original cit marker offset is 0
new cit marker offset is 0



['101']
101
['101']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="83" ssid="12">The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.</S>
original cit marker offset is 0
new cit marker offset is 0



['83']
83
['83']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="46" ssid="2">This provides the nearest neighbours to each target word, along with the distributional similarity score between the target word and its neighbour.</S>
original cit marker offset is 0
new cit marker offset is 0



['46']
46
['46']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents, rather than a lexical sample task, where the target words are manually determined and the results will depend on the skew of the words in the sample.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
<S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['method_citation']
<S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="115" ssid = "13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor  which is very encouraging given that our method only uses raw text  with no manual labelling.</S><S sid ="12" ssid = "5">The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor  from those that do not (without HTD).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'123'", "'14'", "'115'", "'12'"]
'8'
'123'
'14'
'115'
'12'
['8', '123', '14', '115', '12']
parsed_discourse_facet ['results_citation']
<S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="105" ssid = "3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents  rather than a lexical sample task  where the target words are manually determined and the results will depend on the skew of the words in the sample.</S><S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="145" ssid = "22">It is not always intuitively clear which of the senses to expect as predominant sense for either a particular domain or for the BNC  but the first senses of words like division and goal shift towards the more specific senses (league and score respectively).</S><S sid ="155" ssid = "3">A major benefit of our work  rather than reliance on hand-tagged training data such as SemCor  is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'105'", "'165'", "'145'", "'155'"]
'15'
'105'
'165'
'145'
'155'
['15', '105', '165', '145', '155']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="174" ssid = "22">We have restricted ourselves to nouns in this work  since this PoS is perhaps most affected by domain.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of “nearest neighbours”.</S><S sid ="133" ssid = "10">We acquired thesauruses for these corpora using the procedure described in section 2.1.</S><S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'174'", "'34'", "'133'", "'7'"]
'100'
'174'
'34'
'133'
'7'
['100', '174', '34', '133', '7']
parsed_discourse_facet ['method_citation']
<S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="70" ssid = "26">Jiang and Conrath specify a distance measure:   where the third class ( ) is the most informative  or most specific  superordinate synset of the two senses and .</S><S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="71" ssid = "27">This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal:</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'50'", "'70'", "'61'", "'71'"]
'44'
'50'
'70'
'61'
'71'
['44', '50', '70', '61', '71']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S><S sid ="72" ssid = "1">In order to evaluate our method we use the data in SemCor as a gold-standard.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'72'", "'91'", "'10'", "'45'"]
'30'
'72'
'91'
'10'
'45'
['30', '72', '91', '10', '45']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S><S sid ="81" ssid = "10">4 We calculate the accuracy of finding the predominant sense  when there is indeed one sense with a higher frequency than the others for this word in SemCor ( ).</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'50'", "'82'", "'81'", "'44'"]
'69'
'50'
'82'
'81'
'44'
['69', '50', '82', '81', '44']
parsed_discourse_facet ['method_citation']
<S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S><S sid ="58" ssid = "14">A noun    is thus described by a set of co-occurrence triples and associated frequencies  where is a grammatical relation and is a possible cooccurrence with in that relation.</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S><S sid ="56" ssid = "12">For each noun we considered the co-occurring verbs in the direct object and subject relation  the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations.</S><S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'58'", "'45'", "'56'", "'7'"]
'55'
'58'
'45'
'56'
'7'
['55', '58', '45', '56', '7']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S><S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="53" ssid = "9">This weight is the WordNet similarity score ( ) between the target sense ( ) and the sense of ( ) that maximises this score  divided by the sum of all such WordNet similarity scores for and .</S><S sid ="52" ssid = "8">For each sense of ( ) we obtain a ranking score by summing over the of each neighbour ( ) multiplied by a weight.</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'136'", "'53'", "'52'", "'82'"]
'30'
'136'
'53'
'52'
'82'
['30', '136', '53', '52', '82']
parsed_discourse_facet ['method_citation']
<S sid ="135" ssid = "12">We therefore decided to select a limited number of words and to evaluate these words qualitatively.</S><S sid ="179" ssid = "2">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S sid ="111" ssid = "9">We give the results for this WSD task in table 2.</S><S sid ="54" ssid = "10">Thus we rank each sense using: parser (Briscoe and Carroll  2002).</S><S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'135'", "'179'", "'111'", "'54'", "'6'"]
'135'
'179'
'111'
'54'
'6'
['135', '179', '111', '54', '6']
parsed_discourse_facet ['method_citation']
<S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="156" ssid = "4">Buitelaar and Sacaleanu (2001) have previously explored ranking and selection of synsets in GermaNet for specific domains using the words in a given synset  and those related by hyponymy  and a term relevance measure taken from information retrieval.</S><S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'156'", "'61'", "'65'", "'142'"]
'137'
'156'
'61'
'65'
'142'
['137', '156', '61', '65', '142']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="71" ssid = "27">This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal:</S><S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'69'", "'44'", "'71'", "'30'"]
'50'
'69'
'44'
'71'
'30'
['50', '69', '44', '71', '30']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="70" ssid = "26">Jiang and Conrath specify a distance measure:   where the third class ( ) is the most informative  or most specific  superordinate synset of the two senses and .</S><S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'70'", "'137'", "'69'", "'44'"]
'50'
'70'
'137'
'69'
'44'
['50', '70', '137', '69', '44']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">SemCor comprises a relatively small sample of 250 000 words.</S><S sid ="161" ssid = "9">Our approach is complementary to this.</S><S sid ="29" ssid = "22">We discuss our method in the following section.</S><S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of “nearest neighbours”.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'161'", "'29'", "'6'", "'34'"]
'16'
'161'
'29'
'6'
'34'
['16', '161', '29', '6', '34']
parsed_discourse_facet ['results_citation']
<S sid ="157" ssid = "5">Buitelaar and Sacaleanu have evaluated their method on identifying domain specific concepts using human judgements on 100 items.</S><S sid ="185" ssid = "8">In the future  we will perform a large scale evaluation on domain specific corpora.</S><S sid ="193" ssid = "2">This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.</S><S sid ="39" ssid = "32">We expect that the quantity and similarity of the neighbours pertaining to different senses will reflect the dominance of the sense to which they pertain.</S><S sid ="128" ssid = "5">The Reuters corpus (Rose et al.  2002) is a collection of about 810 000 Reuters  English Language News stories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'185'", "'193'", "'39'", "'128'"]
'157'
'185'
'193'
'39'
'128'
['157', '185', '193', '39', '128']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "24">We are of course able to apply the method to other versions of WordNet. synset  is incremented with the frequency counts from the corpus of all words belonging to that synset  directly or via the hyponymy relation.</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'50'", "'65'", "'44'", "'82'"]
'68'
'50'
'65'
'44'
'82'
['68', '50', '65', '44', '82']
parsed_discourse_facet ['method_citation']
<S sid ="90" ssid = "19">From manual analysis  there are cases where the acquired first sense disagrees with SemCor  yet is intuitively plausible.</S><S sid ="175" ssid = "23">We are currently investigating the performance of the first sense heuristic  and this method  for other PoS on SENSEVAL-3 data (McCarthy et al.  2004)  although not yet with rankings from domain specific corpora.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="2" ssid = "2">The problem with using the predominant  or first sense heuristic  aside from the fact that it does not take surrounding context into account  is that it assumes some quantity of handtagged data.</S><S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'175'", "'8'", "'2'", "'1'"]
'90'
'175'
'8'
'2'
'1'
['90', '175', '8', '2', '1']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="0" ssid = "0">Finding Predominant Word Senses in Untagged Text</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'10'", "'0'", "'50'", "'30'"]
'69'
'10'
'0'
'50'
'30'
['69', '10', '0', '50', '30']
parsed_discourse_facet ['method_citation']
['We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.']
['We acquired thesauruses for these corpora using the procedure described in section 2.1.', 'In the English all-words SENSEVAL-2  25% of the noun data was monosemous.', 'We have restricted ourselves to nouns in this work  since this PoS is perhaps most affected by domain.', 'Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.', u'In these each target word is entered with an ordered list of \u201cnearest neighbours\u201d.']
['system', 'ROUGE-S*', 'Average_R:', '0.00159', '(95%-conf.int.', '0.00159', '-', '0.00159)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00272', '(95%-conf.int.', '0.00272', '-', '0.00272)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:105', 'F:1']
['The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.']
['word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.', 'From manual analysis  there are cases where the acquired first sense disagrees with SemCor  yet is intuitively plausible.', 'We are currently investigating the performance of the first sense heuristic  and this method  for other PoS on SENSEVAL-3 data (McCarthy et al.  2004)  although not yet with rankings from domain specific corpora.', 'The problem with using the predominant  or first sense heuristic  aside from the fact that it does not take surrounding context into account  is that it assumes some quantity of handtagged data.', 'The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['system', 'ROUGE-S*', 'Average_R:', '0.00769', '(95%-conf.int.', '0.00769', '-', '0.00769)']
['system', 'ROUGE-S*', 'Average_P:', '0.29091', '(95%-conf.int.', '0.29091', '-', '0.29091)']
['system', 'ROUGE-S*', 'Average_F:', '0.01499', '(95%-conf.int.', '0.01499', '-', '0.01499)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:55', 'F:16']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).', 'Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.', 'The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.', 'This weight is the WordNet similarity score ( ) between the target sense ( ) and the sense of ( ) that maximises this score  divided by the sum of all such WordNet similarity scores for and .', 'For each sense of ( ) we obtain a ranking score by summing over the of each neighbour ( ) multiplied by a weight.']
['system', 'ROUGE-S*', 'Average_R:', '0.00242', '(95%-conf.int.', '0.00242', '-', '0.00242)']
['system', 'ROUGE-S*', 'Average_P:', '0.06061', '(95%-conf.int.', '0.06061', '-', '0.06061)']
['system', 'ROUGE-S*', 'Average_F:', '0.00465', '(95%-conf.int.', '0.00465', '-', '0.00465)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:66', 'F:4']
['We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents, rather than a lexical sample task, where the target words are manually determined and the results will depend on the skew of the words in the sample.']
['The Reuters corpus (Rose et al.  2002) is a collection of about 810 000 Reuters  English Language News stories.', 'We expect that the quantity and similarity of the neighbours pertaining to different senses will reflect the dominance of the sense to which they pertain.', 'This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.', 'Buitelaar and Sacaleanu have evaluated their method on identifying domain specific concepts using human judgements on 100 items.', 'In the future  we will perform a large scale evaluation on domain specific corpora.']
['system', 'ROUGE-S*', 'Average_R:', '0.00149', '(95%-conf.int.', '0.00149', '-', '0.00149)']
['system', 'ROUGE-S*', 'Average_P:', '0.01429', '(95%-conf.int.', '0.01429', '-', '0.01429)']
['system', 'ROUGE-S*', 'Average_F:', '0.00270', '(95%-conf.int.', '0.00270', '-', '0.00270)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:210', 'F:3']
['The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.']
['We are of course able to apply the method to other versions of WordNet. synset  is incremented with the frequency counts from the corpus of all words belonging to that synset  directly or via the hyponymy relation.', 'We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).', 'We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within', 'For input we used grammatical relation data extracted using an automatic where: .', 'The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.']
['system', 'ROUGE-S*', 'Average_R:', '0.00452', '(95%-conf.int.', '0.00452', '-', '0.00452)']
['system', 'ROUGE-S*', 'Average_P:', '0.14545', '(95%-conf.int.', '0.14545', '-', '0.14545)']
['system', 'ROUGE-S*', 'Average_F:', '0.00877', '(95%-conf.int.', '0.00877', '-', '0.00877)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:55', 'F:8']
['Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.']
['We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).', u'The frequency data is used to calculate the \u201cinformation content\u201d (IC) of a class .', 'We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within', '4 We calculate the accuracy of finding the predominant sense  when there is indeed one sense with a higher frequency than the others for this word in SemCor ( ).', 'For input we used grammatical relation data extracted using an automatic where: .']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:55', 'F:0']
['The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.']
['Thus we rank each sense using: parser (Briscoe and Carroll  2002).', 'We give the results for this WSD task in table 2.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.', 'We therefore decided to select a limited number of words and to evaluate these words qualitatively.', 'This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.']
['system', 'ROUGE-S*', 'Average_R:', '0.03030', '(95%-conf.int.', '0.03030', '-', '0.03030)']
['system', 'ROUGE-S*', 'Average_P:', '0.09942', '(95%-conf.int.', '0.09942', '-', '0.09942)']
['system', 'ROUGE-S*', 'Average_F:', '0.04645', '(95%-conf.int.', '0.04645', '-', '0.04645)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:171', 'F:17']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['For input we used grammatical relation data extracted using an automatic where: .', u'The frequency data is used to calculate the \u201cinformation content\u201d (IC) of a class .', 'Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.', 'Finding Predominant Word Senses in Untagged Text', 'The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:66', 'F:0']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
[u'In these each target word is entered with an ordered list of \u201cnearest neighbours\u201d.', 'This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.', 'Our approach is complementary to this.', 'We discuss our method in the following section.', 'SemCor comprises a relatively small sample of 250 000 words.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:66', 'F:0']
['We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.']
[u'The frequency data is used to calculate the \u201cinformation content\u201d (IC) of a class .', 'We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within', 'Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.', 'For input we used grammatical relation data extracted using an automatic where: .', 'Jiang and Conrath specify a distance measure:   where the third class ( ) is the most informative  or most specific  superordinate synset of the two senses and .']
['system', 'ROUGE-S*', 'Average_R:', '0.00078', '(95%-conf.int.', '0.00078', '-', '0.00078)']
['system', 'ROUGE-S*', 'Average_P:', '0.03571', '(95%-conf.int.', '0.03571', '-', '0.03571)']
['system', 'ROUGE-S*', 'Average_F:', '0.00153', '(95%-conf.int.', '0.00153', '-', '0.00153)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:28', 'F:1']
0.0655909993441 0.00487899995121 0.00818099991819





input/ref/Task1/W06-2932_vardha.csv
input/res/Task1/W06-2932.csv
parsing: input/ref/Task1/W06-2932_vardha.csv
    <S sid="19" ssid="1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="5">However, in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="76" ssid="14">For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'"]
'76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="54" ssid="2">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'"]
'54'
['54']
parsed_discourse_facet ['method_citation']
    <S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="58" ssid="6">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'"]
'58'
['58']
parsed_discourse_facet ['method_citation']
    <S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'"]
'22'
['22']
parsed_discourse_facet ['method_citation']
    <S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
    <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'"]
'64'
['64']
parsed_discourse_facet ['method_citation']
    <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'"]
'43'
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S sid ="3" ssid = "3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S><S sid ="4" ssid = "4">We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.</S><S sid ="108" ssid = "5">Second  we plan on integrating any available morphological features in a more principled manner.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="106" ssid = "3">First  we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is nan
new cit marker offset is 0



["'3'", "'4'", "'108'", "'10'", "'106'"]
'3'
'4'
'108'
'10'
'106'
['3', '4', '108', '10', '106']
parsed_discourse_facet ['results_citation']
<S sid ="101" ssid = "23">For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.</S><S sid ="17" ssid = "13">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S><S sid ="16" ssid = "12">A dependency graph is represented by a set of ordered pairs (i  j) E y in which xj is a dependent and xi is the corresponding head.</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'17'", "'16'", "'2'", "'20'"]
'101'
'17'
'16'
'2'
'20'
['101', '17', '16', '2', '20']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S><S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="83" ssid = "5">In a preliminary test of this hypothesis  we looked at all of the sentences from a development set in which a main verb is incorrectly attached.</S><S sid ="50" ssid = "19">Various conjunctions of these were included based on performance on held-out data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'66'", "'88'", "'83'", "'50'"]
'61'
'66'
'88'
'83'
'50'
['61', '66', '88', '83', '50']
parsed_discourse_facet ['method_citation']
<S sid ="76" ssid = "14">For instance  sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.</S><S sid ="96" ssid = "18">Each stage by itself is relatively accurate (unlabeled accuracy is 79% and labeling accuracy3 is also 79%)  but since there is very little overlap in the kinds of errors each makes  overall labeled accuracy drops to 67%.</S><S sid ="57" ssid = "5">Performance is measured through unlabeled accuracy  which is the percentage of words that modify the correct head in the dependency graph  and labeled accuracy  which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'", "'41'", "'96'", "'57'", "'64'"]
'76'
'41'
'96'
'57'
'64'
['76', '41', '96', '57', '64']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S><S sid ="47" ssid = "16">We used the following: dependent have identical values?</S><S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="17" ssid = "13">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'47'", "'32'", "'36'", "'17'"]
'19'
'47'
'32'
'36'
'17'
['19', '47', '32', '36', '17']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "2">Based on performance from a held-out section of the training data  we used non-projective parsing algorithms for Czech  Danish  Dutch  German  Japanese  Portuguese and Slovene  and projective parsing algorithms for Arabic  Bulgarian  Chinese  Spanish  Swedish and Turkish.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="104" ssid = "1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al.  2005b; McDonald and Pereira  2006) generalizes well to languages other than English.</S><S sid ="21" ssid = "3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al.  2005a; McDonald et al.  2005b) to incorporate features over multiple edges in the dependency graph.</S><S sid ="63" ssid = "1">Our system has several components  including the ability to produce non-projective edges  sequential Japanese  Portuguese  Slovene  Spanish  Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'11'", "'104'", "'21'", "'63'"]
'54'
'11'
'104'
'21'
'63'
['54', '11', '104', '21', '63']
parsed_discourse_facet ['method_citation']
<S sid ="1" ssid = "1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S><S sid ="58" ssid = "6">These results show that the discriminative spanning tree parsing framework (McDonald et al.  2005b; McDonald and Pereira  2006) is easily adapted across all these languages.</S><S sid ="0" ssid = "0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S sid ="63" ssid = "1">Our system has several components  including the ability to produce non-projective edges  sequential Japanese  Portuguese  Slovene  Spanish  Swedish and Turkish.</S><S sid ="108" ssid = "5">Second  we plan on integrating any available morphological features in a more principled manner.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'58'", "'0'", "'63'", "'108'"]
'1'
'58'
'0'
'63'
'108'
['1', '58', '0', '63', '108']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="17" ssid = "13">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S><S sid ="53" ssid = "1">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).</S><S sid ="1" ssid = "1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'20'", "'17'", "'53'", "'1'"]
'0'
'20'
'17'
'53'
'1'
['0', '20', '17', '53', '1']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "16">We used the following: dependent have identical values?</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="19" ssid = "1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'2'", "'19'", "'20'", "'32'"]
'47'
'2'
'19'
'20'
'32'
['47', '2', '19', '20', '32']
parsed_discourse_facet ['method_citation']
<S sid ="81" ssid = "3">Other high-frequency word classes with relatively low attachment accuracy are prepositions (80%)  adverbs (82%) and subordinating conjunctions (80%)  for a total of another 23% of the test corpus.</S><S sid ="23" ssid = "5">That system uses MIRA  an online large-margin learning algorithm  to compute model parameters.</S><S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="43" ssid = "12">For score functions  we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation  we can find the highest scoring label sequence with Viterbi’s algorithm.</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'81'", "'23'", "'86'", "'43'", "'36'"]
'81'
'23'
'86'
'43'
'36'
['81', '23', '86', '43', '36']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216–220  New York City  June 2006. c�2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.</S><S sid ="76" ssid = "14">For instance  sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S><S sid ="79" ssid = "1">Although overall unlabeled accuracy is 86%  most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs  75% for the verb ser  and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'41'", "'76'", "'64'", "'79'"]
'18'
'41'
'76'
'64'
'79'
['18', '41', '76', '64', '79']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216–220  New York City  June 2006. c�2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="3" ssid = "3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S><S sid ="96" ssid = "18">Each stage by itself is relatively accurate (unlabeled accuracy is 79% and labeling accuracy3 is also 79%)  but since there is very little overlap in the kinds of errors each makes  overall labeled accuracy drops to 67%.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="95" ssid = "17">Note the difference in error between the unlabeled parser and the edge labeler: the former makes mistakes on edges into prepositions  conjunctions and verbs  and the latter makes mistakes on edges into nouns (subject/objects).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'3'", "'96'", "'11'", "'95'"]
'18'
'3'
'96'
'11'
'95'
['18', '3', '96', '11', '95']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="16" ssid = "12">A dependency graph is represented by a set of ordered pairs (i  j) E y in which xj is a dependent and xi is the corresponding head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'11'", "'41'", "'36'", "'16'"]
'32'
'11'
'41'
'36'
'16'
['32', '11', '41', '36', '16']
parsed_discourse_facet ['results_citation']
['Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.']
['The first stage of our system creates an unlabeled parse y for an input sentence x.', 'We used the following: dependent have identical values?', 'However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.', 'Each edge can be assigned a label l(ij) from a finite set L of predefined labels.', 'The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).']
['system', 'ROUGE-S*', 'Average_R:', '0.00142', '(95%-conf.int.', '0.00142', '-', '0.00142)']
['system', 'ROUGE-S*', 'Average_P:', '0.00333', '(95%-conf.int.', '0.00333', '-', '0.00333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00199', '(95%-conf.int.', '0.00199', '-', '0.00199)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:300', 'F:1']
['An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.']
['present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.', 'Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'This system is primarily based on the parsing models described by McDonald and Pereira (2006).', 'We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).', 'Each edge can be assigned a label l(ij) from a finite set L of predefined labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00732', '(95%-conf.int.', '0.00732', '-', '0.00732)']
['system', 'ROUGE-S*', 'Average_P:', '0.04412', '(95%-conf.int.', '0.04412', '-', '0.04412)']
['system', 'ROUGE-S*', 'Average_F:', '0.01255', '(95%-conf.int.', '0.01255', '-', '0.01255)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:136', 'F:6']
['Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.']
['In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).', 'Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.', 'Various conjunctions of these were included based on performance on held-out data.', 'These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.', 'In a preliminary test of this hypothesis  we looked at all of the sentences from a development set in which a main verb is incorrectly attached.']
['system', 'ROUGE-S*', 'Average_R:', '0.00235', '(95%-conf.int.', '0.00235', '-', '0.00235)']
['system', 'ROUGE-S*', 'Average_P:', '0.01186', '(95%-conf.int.', '0.01186', '-', '0.01186)']
['system', 'ROUGE-S*', 'Average_F:', '0.00393', '(95%-conf.int.', '0.00393', '-', '0.00393)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:253', 'F:3']
['N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.']
['Other high-frequency word classes with relatively low attachment accuracy are prepositions (80%)  adverbs (82%) and subordinating conjunctions (80%)  for a total of another 23% of the test corpus.', 'Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.', u'For score functions  we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation  we can find the highest scoring label sequence with Viterbi\xe2\u20ac\u2122s algorithm.', 'However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.', 'That system uses MIRA  an online large-margin learning algorithm  to compute model parameters.']
['system', 'ROUGE-S*', 'Average_R:', '0.00384', '(95%-conf.int.', '0.00384', '-', '0.00384)']
['system', 'ROUGE-S*', 'Average_P:', '0.03557', '(95%-conf.int.', '0.03557', '-', '0.03557)']
['system', 'ROUGE-S*', 'Average_F:', '0.00693', '(95%-conf.int.', '0.00693', '-', '0.00693)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:253', 'F:9']
['For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.']
['This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).', 'The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).', 'A dependency graph is represented by a set of ordered pairs (i  j) E y in which xj is a dependent and xi is the corresponding head.', 'However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.', u'To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm\ufffd1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm\u22121) in the tree y.']
['system', 'ROUGE-S*', 'Average_R:', '0.00513', '(95%-conf.int.', '0.00513', '-', '0.00513)']
['system', 'ROUGE-S*', 'Average_P:', '0.06494', '(95%-conf.int.', '0.06494', '-', '0.06494)']
['system', 'ROUGE-S*', 'Average_F:', '0.00950', '(95%-conf.int.', '0.00950', '-', '0.00950)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:231', 'F:15']
['In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.', 'Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'Second  we plan on integrating any available morphological features in a more principled manner.', 'Our system has several components  including the ability to produce non-projective edges  sequential Japanese  Portuguese  Slovene  Spanish  Swedish and Turkish.', 'These results show that the discriminative spanning tree parsing framework (McDonald et al.  2005b; McDonald and Pereira  2006) is easily adapted across all these languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00627', '(95%-conf.int.', '0.00627', '-', '0.00627)']
['system', 'ROUGE-S*', 'Average_P:', '0.12121', '(95%-conf.int.', '0.12121', '-', '0.12121)']
['system', 'ROUGE-S*', 'Average_F:', '0.01193', '(95%-conf.int.', '0.01193', '-', '0.01193)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:66', 'F:8']
['We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.']
['This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).', 'Based on performance from a held-out section of the training data  we used non-projective parsing algorithms for Czech  Danish  Dutch  German  Japanese  Portuguese and Slovene  and projective parsing algorithms for Arabic  Bulgarian  Chinese  Spanish  Swedish and Turkish.', 'Our system has several components  including the ability to produce non-projective edges  sequential Japanese  Portuguese  Slovene  Spanish  Swedish and Turkish.', 'That work extends the maximum spanning tree dependency parsing framework (McDonald et al.  2005a; McDonald et al.  2005b) to incorporate features over multiple edges in the dependency graph.', 'We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al.  2005b; McDonald and Pereira  2006) generalizes well to languages other than English.']
['system', 'ROUGE-S*', 'Average_R:', '0.03455', '(95%-conf.int.', '0.03455', '-', '0.03455)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.06678', '(95%-conf.int.', '0.06678', '-', '0.06678)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4950', 'P:171', 'F:171']
['To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.']
['The first stage of our system creates an unlabeled parse y for an input sentence x.', 'We used the following: dependent have identical values?', 'The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.', 'This system is primarily based on the parsing models described by McDonald and Pereira (2006).', 'The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).']
['system', 'ROUGE-S*', 'Average_R:', '0.00813', '(95%-conf.int.', '0.00813', '-', '0.00813)']
['system', 'ROUGE-S*', 'Average_P:', '0.01852', '(95%-conf.int.', '0.01852', '-', '0.01852)']
['system', 'ROUGE-S*', 'Average_F:', '0.01130', '(95%-conf.int.', '0.01130', '-', '0.01130)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:378', 'F:7']
['For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.']
['For instance  sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.', 'Performance is measured through unlabeled accuracy  which is the percentage of words that modify the correct head in the dependency graph  and labeled accuracy  which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.', 'Each stage by itself is relatively accurate (unlabeled accuracy is 79% and labeling accuracy3 is also 79%)  but since there is very little overlap in the kinds of errors each makes  overall labeled accuracy drops to 67%.', 'N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.', u'To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm\ufffd1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm\u22121) in the tree y.']
['system', 'ROUGE-S*', 'Average_R:', '0.08261', '(95%-conf.int.', '0.08261', '-', '0.08261)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.15262', '(95%-conf.int.', '0.15262', '-', '0.15262)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:7626', 'P:630', 'F:630']
0.255505552717 0.0168466664795 0.030836666324





input/ref/Task1/W06-2932_sweta.csv
input/res/Task1/W06-2932.csv
parsing: input/ref/Task1/W06-2932_sweta.csv
 <S sid="5" ssid="1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="5">However, in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["36'"]
36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="9">In fact, for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="76" ssid="14">For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["76'"]
76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="45" ssid="14">Furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["45'"]
45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="3">First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["106'"]
106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="58" ssid="6">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["58'"]
58'
['58']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
 <S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="2">Ideally one would like to make all parsing and labeling decisions jointly so that the shared knowledge of both decisions will help resolve any ambiguities.</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
 <S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["43'"]
43'
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S sid ="3" ssid = "3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S><S sid ="4" ssid = "4">We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.</S><S sid ="108" ssid = "5">Second  we plan on integrating any available morphological features in a more principled manner.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="106" ssid = "3">First  we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is nan
new cit marker offset is 0



["'3'", "'4'", "'108'", "'10'", "'106'"]
'3'
'4'
'108'
'10'
'106'
['3', '4', '108', '10', '106']
parsed_discourse_facet ['results_citation']
<S sid ="101" ssid = "23">For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.</S><S sid ="17" ssid = "13">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S><S sid ="16" ssid = "12">A dependency graph is represented by a set of ordered pairs (i  j) E y in which xj is a dependent and xi is the corresponding head.</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'17'", "'16'", "'2'", "'20'"]
'101'
'17'
'16'
'2'
'20'
['101', '17', '16', '2', '20']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S><S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="83" ssid = "5">In a preliminary test of this hypothesis  we looked at all of the sentences from a development set in which a main verb is incorrectly attached.</S><S sid ="50" ssid = "19">Various conjunctions of these were included based on performance on held-out data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'66'", "'88'", "'83'", "'50'"]
'61'
'66'
'88'
'83'
'50'
['61', '66', '88', '83', '50']
parsed_discourse_facet ['method_citation']
<S sid ="76" ssid = "14">For instance  sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.</S><S sid ="96" ssid = "18">Each stage by itself is relatively accurate (unlabeled accuracy is 79% and labeling accuracy3 is also 79%)  but since there is very little overlap in the kinds of errors each makes  overall labeled accuracy drops to 67%.</S><S sid ="57" ssid = "5">Performance is measured through unlabeled accuracy  which is the percentage of words that modify the correct head in the dependency graph  and labeled accuracy  which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'", "'41'", "'96'", "'57'", "'64'"]
'76'
'41'
'96'
'57'
'64'
['76', '41', '96', '57', '64']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S><S sid ="47" ssid = "16">We used the following: dependent have identical values?</S><S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="17" ssid = "13">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'47'", "'32'", "'36'", "'17'"]
'19'
'47'
'32'
'36'
'17'
['19', '47', '32', '36', '17']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "2">Based on performance from a held-out section of the training data  we used non-projective parsing algorithms for Czech  Danish  Dutch  German  Japanese  Portuguese and Slovene  and projective parsing algorithms for Arabic  Bulgarian  Chinese  Spanish  Swedish and Turkish.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="104" ssid = "1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al.  2005b; McDonald and Pereira  2006) generalizes well to languages other than English.</S><S sid ="21" ssid = "3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al.  2005a; McDonald et al.  2005b) to incorporate features over multiple edges in the dependency graph.</S><S sid ="63" ssid = "1">Our system has several components  including the ability to produce non-projective edges  sequential Japanese  Portuguese  Slovene  Spanish  Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'11'", "'104'", "'21'", "'63'"]
'54'
'11'
'104'
'21'
'63'
['54', '11', '104', '21', '63']
parsed_discourse_facet ['method_citation']
<S sid ="1" ssid = "1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S><S sid ="58" ssid = "6">These results show that the discriminative spanning tree parsing framework (McDonald et al.  2005b; McDonald and Pereira  2006) is easily adapted across all these languages.</S><S sid ="0" ssid = "0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S sid ="63" ssid = "1">Our system has several components  including the ability to produce non-projective edges  sequential Japanese  Portuguese  Slovene  Spanish  Swedish and Turkish.</S><S sid ="108" ssid = "5">Second  we plan on integrating any available morphological features in a more principled manner.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'58'", "'0'", "'63'", "'108'"]
'1'
'58'
'0'
'63'
'108'
['1', '58', '0', '63', '108']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="17" ssid = "13">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S><S sid ="53" ssid = "1">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).</S><S sid ="1" ssid = "1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'20'", "'17'", "'53'", "'1'"]
'0'
'20'
'17'
'53'
'1'
['0', '20', '17', '53', '1']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "16">We used the following: dependent have identical values?</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="19" ssid = "1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'2'", "'19'", "'20'", "'32'"]
'47'
'2'
'19'
'20'
'32'
['47', '2', '19', '20', '32']
parsed_discourse_facet ['method_citation']
<S sid ="81" ssid = "3">Other high-frequency word classes with relatively low attachment accuracy are prepositions (80%)  adverbs (82%) and subordinating conjunctions (80%)  for a total of another 23% of the test corpus.</S><S sid ="23" ssid = "5">That system uses MIRA  an online large-margin learning algorithm  to compute model parameters.</S><S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="43" ssid = "12">For score functions  we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation  we can find the highest scoring label sequence with Viterbi’s algorithm.</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'81'", "'23'", "'86'", "'43'", "'36'"]
'81'
'23'
'86'
'43'
'36'
['81', '23', '86', '43', '36']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216–220  New York City  June 2006. c�2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.</S><S sid ="76" ssid = "14">For instance  sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S><S sid ="79" ssid = "1">Although overall unlabeled accuracy is 86%  most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs  75% for the verb ser  and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'41'", "'76'", "'64'", "'79'"]
'18'
'41'
'76'
'64'
'79'
['18', '41', '76', '64', '79']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216–220  New York City  June 2006. c�2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="3" ssid = "3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S><S sid ="96" ssid = "18">Each stage by itself is relatively accurate (unlabeled accuracy is 79% and labeling accuracy3 is also 79%)  but since there is very little overlap in the kinds of errors each makes  overall labeled accuracy drops to 67%.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="95" ssid = "17">Note the difference in error between the unlabeled parser and the edge labeler: the former makes mistakes on edges into prepositions  conjunctions and verbs  and the latter makes mistakes on edges into nouns (subject/objects).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'3'", "'96'", "'11'", "'95'"]
'18'
'3'
'96'
'11'
'95'
['18', '3', '96', '11', '95']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="16" ssid = "12">A dependency graph is represented by a set of ordered pairs (i  j) E y in which xj is a dependent and xi is the corresponding head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'11'", "'41'", "'36'", "'16'"]
'32'
'11'
'41'
'36'
'16'
['32', '11', '41', '36', '16']
parsed_discourse_facet ['results_citation']
['Furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira, 2006).']
['The first stage of our system creates an unlabeled parse y for an input sentence x.', 'We used the following: dependent have identical values?', 'However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.', 'Each edge can be assigned a label l(ij) from a finite set L of predefined labels.', 'The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).']
['system', 'ROUGE-S*', 'Average_R:', '0.00427', '(95%-conf.int.', '0.00427', '-', '0.00427)']
['system', 'ROUGE-S*', 'Average_P:', '0.04545', '(95%-conf.int.', '0.04545', '-', '0.04545)']
['system', 'ROUGE-S*', 'Average_F:', '0.00780', '(95%-conf.int.', '0.00780', '-', '0.00780)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:66', 'F:3']
['N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.']
['present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.', 'Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'This system is primarily based on the parsing models described by McDonald and Pereira (2006).', 'We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).', 'Each edge can be assigned a label l(ij) from a finite set L of predefined labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00854', '(95%-conf.int.', '0.00854', '-', '0.00854)']
['system', 'ROUGE-S*', 'Average_P:', '0.02767', '(95%-conf.int.', '0.02767', '-', '0.02767)']
['system', 'ROUGE-S*', 'Average_F:', '0.01305', '(95%-conf.int.', '0.01305', '-', '0.01305)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:253', 'F:7']
['In fact, for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).']
['In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).', 'Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.', 'Various conjunctions of these were included based on performance on held-out data.', 'These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.', 'In a preliminary test of this hypothesis  we looked at all of the sentences from a development set in which a main verb is incorrectly attached.']
['system', 'ROUGE-S*', 'Average_R:', '0.06118', '(95%-conf.int.', '0.06118', '-', '0.06118)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.11530', '(95%-conf.int.', '0.11530', '-', '0.11530)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:78', 'F:78']
['N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.']
['Other high-frequency word classes with relatively low attachment accuracy are prepositions (80%)  adverbs (82%) and subordinating conjunctions (80%)  for a total of another 23% of the test corpus.', 'Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.', u'For score functions  we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation  we can find the highest scoring label sequence with Viterbi\xe2\u20ac\u2122s algorithm.', 'However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.', 'That system uses MIRA  an online large-margin learning algorithm  to compute model parameters.']
['system', 'ROUGE-S*', 'Average_R:', '0.00384', '(95%-conf.int.', '0.00384', '-', '0.00384)']
['system', 'ROUGE-S*', 'Average_P:', '0.03557', '(95%-conf.int.', '0.03557', '-', '0.03557)']
['system', 'ROUGE-S*', 'Average_F:', '0.00693', '(95%-conf.int.', '0.00693', '-', '0.00693)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:253', 'F:9']
['For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.']
['This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).', 'The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).', 'A dependency graph is represented by a set of ordered pairs (i  j) E y in which xj is a dependent and xi is the corresponding head.', 'However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.', u'To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm\ufffd1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm\u22121) in the tree y.']
['system', 'ROUGE-S*', 'Average_R:', '0.00513', '(95%-conf.int.', '0.00513', '-', '0.00513)']
['system', 'ROUGE-S*', 'Average_P:', '0.06494', '(95%-conf.int.', '0.06494', '-', '0.06494)']
['system', 'ROUGE-S*', 'Average_F:', '0.00950', '(95%-conf.int.', '0.00950', '-', '0.00950)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:231', 'F:15']
['In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.', 'Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'Second  we plan on integrating any available morphological features in a more principled manner.', 'Our system has several components  including the ability to produce non-projective edges  sequential Japanese  Portuguese  Slovene  Spanish  Swedish and Turkish.', 'These results show that the discriminative spanning tree parsing framework (McDonald et al.  2005b; McDonald and Pereira  2006) is easily adapted across all these languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00627', '(95%-conf.int.', '0.00627', '-', '0.00627)']
['system', 'ROUGE-S*', 'Average_P:', '0.12121', '(95%-conf.int.', '0.12121', '-', '0.12121)']
['system', 'ROUGE-S*', 'Average_F:', '0.01193', '(95%-conf.int.', '0.01193', '-', '0.01193)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:66', 'F:8']
['First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.']
['This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).', 'Based on performance from a held-out section of the training data  we used non-projective parsing algorithms for Czech  Danish  Dutch  German  Japanese  Portuguese and Slovene  and projective parsing algorithms for Arabic  Bulgarian  Chinese  Spanish  Swedish and Turkish.', 'Our system has several components  including the ability to produce non-projective edges  sequential Japanese  Portuguese  Slovene  Spanish  Swedish and Turkish.', 'That work extends the maximum spanning tree dependency parsing framework (McDonald et al.  2005a; McDonald et al.  2005b) to incorporate features over multiple edges in the dependency graph.', 'We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al.  2005b; McDonald and Pereira  2006) generalizes well to languages other than English.']
['system', 'ROUGE-S*', 'Average_R:', '0.00202', '(95%-conf.int.', '0.00202', '-', '0.00202)']
['system', 'ROUGE-S*', 'Average_P:', '0.15152', '(95%-conf.int.', '0.15152', '-', '0.15152)']
['system', 'ROUGE-S*', 'Average_F:', '0.00399', '(95%-conf.int.', '0.00399', '-', '0.00399)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4950', 'P:66', 'F:10']
['To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.']
['The first stage of our system creates an unlabeled parse y for an input sentence x.', 'We used the following: dependent have identical values?', 'The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.', 'This system is primarily based on the parsing models described by McDonald and Pereira (2006).', 'The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).']
['system', 'ROUGE-S*', 'Average_R:', '0.00813', '(95%-conf.int.', '0.00813', '-', '0.00813)']
['system', 'ROUGE-S*', 'Average_P:', '0.01852', '(95%-conf.int.', '0.01852', '-', '0.01852)']
['system', 'ROUGE-S*', 'Average_F:', '0.01130', '(95%-conf.int.', '0.01130', '-', '0.01130)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:378', 'F:7']
['For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.']
['For instance  sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.', 'Performance is measured through unlabeled accuracy  which is the percentage of words that modify the correct head in the dependency graph  and labeled accuracy  which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.', 'Each stage by itself is relatively accurate (unlabeled accuracy is 79% and labeling accuracy3 is also 79%)  but since there is very little overlap in the kinds of errors each makes  overall labeled accuracy drops to 67%.', 'N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.', u'To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm\ufffd1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm\u22121) in the tree y.']
['system', 'ROUGE-S*', 'Average_R:', '0.08261', '(95%-conf.int.', '0.08261', '-', '0.08261)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.15262', '(95%-conf.int.', '0.15262', '-', '0.15262)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:7626', 'P:630', 'F:630']
['Ideally one would like to make all parsing and labeling decisions jointly so that the shared knowledge of both decisions will help resolve any ambiguities.']
[u'We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216\u2013220  New York City  June 2006. c\ufffd2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.', 'For instance  sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.', 'N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.', 'Although overall unlabeled accuracy is 86%  most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs  75% for the verb ser  and 65% for coordinating conjunctions.', u'To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm\ufffd1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm\u22121) in the tree y.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:8515', 'P:55', 'F:0']
0.246487997535 0.018198999818 0.0332419996676





input/ref/Task1/P04-1036_aakansha.csv
input/res/Task1/P04-1036.csv
parsing: input/ref/Task1/P04-1036_aakansha.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="34">In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
    <S sid="181" ssid="4">This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'181'"]
'180'
'181'
['180', '181']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="4">To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'"]
'48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="169" ssid="17">This method obtains precision of 61% and recall 51%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'"]
'169'
['169']
parsed_discourse_facet ['result_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="115" ssid="13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
    <S sid="181" ssid="4">This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'181'"]
'180'
'181'
['180', '181']
parsed_discourse_facet ['result_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'"]
'180'
['180']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="34">In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="115" ssid = "13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor  which is very encouraging given that our method only uses raw text  with no manual labelling.</S><S sid ="12" ssid = "5">The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor  from those that do not (without HTD).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'123'", "'14'", "'115'", "'12'"]
'8'
'123'
'14'
'115'
'12'
['8', '123', '14', '115', '12']
parsed_discourse_facet ['results_citation']
<S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="105" ssid = "3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents  rather than a lexical sample task  where the target words are manually determined and the results will depend on the skew of the words in the sample.</S><S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="145" ssid = "22">It is not always intuitively clear which of the senses to expect as predominant sense for either a particular domain or for the BNC  but the first senses of words like division and goal shift towards the more specific senses (league and score respectively).</S><S sid ="155" ssid = "3">A major benefit of our work  rather than reliance on hand-tagged training data such as SemCor  is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'105'", "'165'", "'145'", "'155'"]
'15'
'105'
'165'
'145'
'155'
['15', '105', '165', '145', '155']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="174" ssid = "22">We have restricted ourselves to nouns in this work  since this PoS is perhaps most affected by domain.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of “nearest neighbours”.</S><S sid ="133" ssid = "10">We acquired thesauruses for these corpora using the procedure described in section 2.1.</S><S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'174'", "'34'", "'133'", "'7'"]
'100'
'174'
'34'
'133'
'7'
['100', '174', '34', '133', '7']
parsed_discourse_facet ['method_citation']
<S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="70" ssid = "26">Jiang and Conrath specify a distance measure:   where the third class ( ) is the most informative  or most specific  superordinate synset of the two senses and .</S><S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="71" ssid = "27">This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal:</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'50'", "'70'", "'61'", "'71'"]
'44'
'50'
'70'
'61'
'71'
['44', '50', '70', '61', '71']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S><S sid ="72" ssid = "1">In order to evaluate our method we use the data in SemCor as a gold-standard.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'72'", "'91'", "'10'", "'45'"]
'30'
'72'
'91'
'10'
'45'
['30', '72', '91', '10', '45']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S><S sid ="81" ssid = "10">4 We calculate the accuracy of finding the predominant sense  when there is indeed one sense with a higher frequency than the others for this word in SemCor ( ).</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'50'", "'82'", "'81'", "'44'"]
'69'
'50'
'82'
'81'
'44'
['69', '50', '82', '81', '44']
parsed_discourse_facet ['method_citation']
<S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S><S sid ="58" ssid = "14">A noun    is thus described by a set of co-occurrence triples and associated frequencies  where is a grammatical relation and is a possible cooccurrence with in that relation.</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S><S sid ="56" ssid = "12">For each noun we considered the co-occurring verbs in the direct object and subject relation  the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations.</S><S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'58'", "'45'", "'56'", "'7'"]
'55'
'58'
'45'
'56'
'7'
['55', '58', '45', '56', '7']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S><S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="53" ssid = "9">This weight is the WordNet similarity score ( ) between the target sense ( ) and the sense of ( ) that maximises this score  divided by the sum of all such WordNet similarity scores for and .</S><S sid ="52" ssid = "8">For each sense of ( ) we obtain a ranking score by summing over the of each neighbour ( ) multiplied by a weight.</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'136'", "'53'", "'52'", "'82'"]
'30'
'136'
'53'
'52'
'82'
['30', '136', '53', '52', '82']
parsed_discourse_facet ['method_citation']
<S sid ="135" ssid = "12">We therefore decided to select a limited number of words and to evaluate these words qualitatively.</S><S sid ="179" ssid = "2">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S sid ="111" ssid = "9">We give the results for this WSD task in table 2.</S><S sid ="54" ssid = "10">Thus we rank each sense using: parser (Briscoe and Carroll  2002).</S><S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'135'", "'179'", "'111'", "'54'", "'6'"]
'135'
'179'
'111'
'54'
'6'
['135', '179', '111', '54', '6']
parsed_discourse_facet ['method_citation']
<S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="156" ssid = "4">Buitelaar and Sacaleanu (2001) have previously explored ranking and selection of synsets in GermaNet for specific domains using the words in a given synset  and those related by hyponymy  and a term relevance measure taken from information retrieval.</S><S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'156'", "'61'", "'65'", "'142'"]
'137'
'156'
'61'
'65'
'142'
['137', '156', '61', '65', '142']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="71" ssid = "27">This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal:</S><S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'69'", "'44'", "'71'", "'30'"]
'50'
'69'
'44'
'71'
'30'
['50', '69', '44', '71', '30']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="70" ssid = "26">Jiang and Conrath specify a distance measure:   where the third class ( ) is the most informative  or most specific  superordinate synset of the two senses and .</S><S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'70'", "'137'", "'69'", "'44'"]
'50'
'70'
'137'
'69'
'44'
['50', '70', '137', '69', '44']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">SemCor comprises a relatively small sample of 250 000 words.</S><S sid ="161" ssid = "9">Our approach is complementary to this.</S><S sid ="29" ssid = "22">We discuss our method in the following section.</S><S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of “nearest neighbours”.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'161'", "'29'", "'6'", "'34'"]
'16'
'161'
'29'
'6'
'34'
['16', '161', '29', '6', '34']
parsed_discourse_facet ['results_citation']
<S sid ="157" ssid = "5">Buitelaar and Sacaleanu have evaluated their method on identifying domain specific concepts using human judgements on 100 items.</S><S sid ="185" ssid = "8">In the future  we will perform a large scale evaluation on domain specific corpora.</S><S sid ="193" ssid = "2">This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.</S><S sid ="39" ssid = "32">We expect that the quantity and similarity of the neighbours pertaining to different senses will reflect the dominance of the sense to which they pertain.</S><S sid ="128" ssid = "5">The Reuters corpus (Rose et al.  2002) is a collection of about 810 000 Reuters  English Language News stories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'185'", "'193'", "'39'", "'128'"]
'157'
'185'
'193'
'39'
'128'
['157', '185', '193', '39', '128']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "24">We are of course able to apply the method to other versions of WordNet. synset  is incremented with the frequency counts from the corpus of all words belonging to that synset  directly or via the hyponymy relation.</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'50'", "'65'", "'44'", "'82'"]
'68'
'50'
'65'
'44'
'82'
['68', '50', '65', '44', '82']
parsed_discourse_facet ['method_citation']
<S sid ="90" ssid = "19">From manual analysis  there are cases where the acquired first sense disagrees with SemCor  yet is intuitively plausible.</S><S sid ="175" ssid = "23">We are currently investigating the performance of the first sense heuristic  and this method  for other PoS on SENSEVAL-3 data (McCarthy et al.  2004)  although not yet with rankings from domain specific corpora.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="2" ssid = "2">The problem with using the predominant  or first sense heuristic  aside from the fact that it does not take surrounding context into account  is that it assumes some quantity of handtagged data.</S><S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'175'", "'8'", "'2'", "'1'"]
'90'
'175'
'8'
'2'
'1'
['90', '175', '8', '2', '1']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="0" ssid = "0">Finding Predominant Word Senses in Untagged Text</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'10'", "'0'", "'50'", "'30'"]
'69'
'10'
'0'
'50'
'30'
['69', '10', '0', '50', '30']
parsed_discourse_facet ['method_citation']
['In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.']
['We acquired thesauruses for these corpora using the procedure described in section 2.1.', 'In the English all-words SENSEVAL-2  25% of the noun data was monosemous.', 'We have restricted ourselves to nouns in this work  since this PoS is perhaps most affected by domain.', 'Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.', u'In these each target word is entered with an ordered list of \u201cnearest neighbours\u201d.']
['system', 'ROUGE-S*', 'Average_R:', '0.02540', '(95%-conf.int.', '0.02540', '-', '0.02540)']
['system', 'ROUGE-S*', 'Average_P:', '0.15238', '(95%-conf.int.', '0.15238', '-', '0.15238)']
['system', 'ROUGE-S*', 'Average_F:', '0.04354', '(95%-conf.int.', '0.04354', '-', '0.04354)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:105', 'F:16']
['The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.']
['word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.', 'From manual analysis  there are cases where the acquired first sense disagrees with SemCor  yet is intuitively plausible.', 'We are currently investigating the performance of the first sense heuristic  and this method  for other PoS on SENSEVAL-3 data (McCarthy et al.  2004)  although not yet with rankings from domain specific corpora.', 'The problem with using the predominant  or first sense heuristic  aside from the fact that it does not take surrounding context into account  is that it assumes some quantity of handtagged data.', 'The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['system', 'ROUGE-S*', 'Average_R:', '0.00721', '(95%-conf.int.', '0.00721', '-', '0.00721)']
['system', 'ROUGE-S*', 'Average_P:', '0.07895', '(95%-conf.int.', '0.07895', '-', '0.07895)']
['system', 'ROUGE-S*', 'Average_F:', '0.01322', '(95%-conf.int.', '0.01322', '-', '0.01322)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:190', 'F:15']
['To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.']
['We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).', 'Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.', 'The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.', 'This weight is the WordNet similarity score ( ) between the target sense ( ) and the sense of ( ) that maximises this score  divided by the sum of all such WordNet similarity scores for and .', 'For each sense of ( ) we obtain a ranking score by summing over the of each neighbour ( ) multiplied by a weight.']
['system', 'ROUGE-S*', 'Average_R:', '0.01270', '(95%-conf.int.', '0.01270', '-', '0.01270)']
['system', 'ROUGE-S*', 'Average_P:', '0.38182', '(95%-conf.int.', '0.38182', '-', '0.38182)']
['system', 'ROUGE-S*', 'Average_F:', '0.02459', '(95%-conf.int.', '0.02459', '-', '0.02459)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:55', 'F:21']
['We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.']
['The Reuters corpus (Rose et al.  2002) is a collection of about 810 000 Reuters  English Language News stories.', 'We expect that the quantity and similarity of the neighbours pertaining to different senses will reflect the dominance of the sense to which they pertain.', 'This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.', 'Buitelaar and Sacaleanu have evaluated their method on identifying domain specific concepts using human judgements on 100 items.', 'In the future  we will perform a large scale evaluation on domain specific corpora.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:28', 'F:0']
['The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.', 'This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.']
['We are of course able to apply the method to other versions of WordNet. synset  is incremented with the frequency counts from the corpus of all words belonging to that synset  directly or via the hyponymy relation.', 'We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).', 'We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within', 'For input we used grammatical relation data extracted using an automatic where: .', 'The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.']
['system', 'ROUGE-S*', 'Average_R:', '0.01073', '(95%-conf.int.', '0.01073', '-', '0.01073)']
['system', 'ROUGE-S*', 'Average_P:', '0.03387', '(95%-conf.int.', '0.03387', '-', '0.03387)']
['system', 'ROUGE-S*', 'Average_F:', '0.01630', '(95%-conf.int.', '0.01630', '-', '0.01630)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:561', 'F:19']
['The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.', 'This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.']
['We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).', u'The frequency data is used to calculate the \u201cinformation content\u201d (IC) of a class .', 'We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within', '4 We calculate the accuracy of finding the predominant sense  when there is indeed one sense with a higher frequency than the others for this word in SemCor ( ).', 'For input we used grammatical relation data extracted using an automatic where: .']
['system', 'ROUGE-S*', 'Average_R:', '0.02436', '(95%-conf.int.', '0.02436', '-', '0.02436)']
['system', 'ROUGE-S*', 'Average_P:', '0.03922', '(95%-conf.int.', '0.03922', '-', '0.03922)']
['system', 'ROUGE-S*', 'Average_F:', '0.03005', '(95%-conf.int.', '0.03005', '-', '0.03005)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:561', 'F:22']
['This method obtains precision of 61% and recall 51%.']
['Thus we rank each sense using: parser (Briscoe and Carroll  2002).', 'We give the results for this WSD task in table 2.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.', 'We therefore decided to select a limited number of words and to evaluate these words qualitatively.', 'This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:15', 'F:0']
['In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.']
['For input we used grammatical relation data extracted using an automatic where: .', u'The frequency data is used to calculate the \u201cinformation content\u201d (IC) of a class .', 'Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.', 'Finding Predominant Word Senses in Untagged Text', 'The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).']
['system', 'ROUGE-S*', 'Average_R:', '0.00925', '(95%-conf.int.', '0.00925', '-', '0.00925)']
['system', 'ROUGE-S*', 'Average_P:', '0.09524', '(95%-conf.int.', '0.09524', '-', '0.09524)']
['system', 'ROUGE-S*', 'Average_F:', '0.01686', '(95%-conf.int.', '0.01686', '-', '0.01686)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:105', 'F:10']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
[u'In these each target word is entered with an ordered list of \u201cnearest neighbours\u201d.', 'This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.', 'Our approach is complementary to this.', 'We discuss our method in the following section.', 'SemCor comprises a relatively small sample of 250 000 words.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:66', 'F:0']
['Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.']
[u'The frequency data is used to calculate the \u201cinformation content\u201d (IC) of a class .', 'We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within', 'Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.', 'For input we used grammatical relation data extracted using an automatic where: .', 'Jiang and Conrath specify a distance measure:   where the third class ( ) is the most informative  or most specific  superordinate synset of the two senses and .']
['system', 'ROUGE-S*', 'Average_R:', '0.00157', '(95%-conf.int.', '0.00157', '-', '0.00157)']
['system', 'ROUGE-S*', 'Average_P:', '0.01905', '(95%-conf.int.', '0.01905', '-', '0.01905)']
['system', 'ROUGE-S*', 'Average_F:', '0.00290', '(95%-conf.int.', '0.00290', '-', '0.00290)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:105', 'F:2']
0.0800529991995 0.00912199990878 0.0147459998525





input/ref/Task1/P04-1036_vardha.csv
input/res/Task1/P04-1036.csv
parsing: input/ref/Task1/P04-1036_vardha.csv
    <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
    <S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="66" ssid="22">It uses the glosses of semantically related (according to WordNet) senses too. jcn (Jiang and Conrath, 1997) This score uses corpus data to populate classes (synsets) in the WordNet hierarchy with frequency counts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'66'"]
'66'
['66']
parsed_discourse_facet ['method_citation']
 <S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="101" ssid="30">Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'"]
'101'
['101']
parsed_discourse_facet ['method_citation']
 <S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
 <S sid="115" ssid="13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
  <S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'"]
'89'
['89']
parsed_discourse_facet ['method_citation']
 <S sid="137" ssid="14">Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'"]
'137'
['137']
parsed_discourse_facet ['method_citation']
75 ssid="4">We generated a thesaurus entry for all polysemous nouns which occurred in SemCor with a frequency 2, and in the BNC with a frequency 10 in the grammatical relations listed in section 2.1 above.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'"]
'75'
['75']
Error in Reference Offset
 <S sid="155" ssid="3">A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
  <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
 <S sid="155" ssid="3">A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="24">We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'"]
'68'
['68']
parsed_discourse_facet ['method_citation']
    <S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'"]
'13'
['13']
parsed_discourse_facet ['method_citation']
  <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="123" ssid = "21">This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available.</S><S sid ="14" ssid = "7">Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient (Hoste et al.  2001).</S><S sid ="115" ssid = "13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor  which is very encouraging given that our method only uses raw text  with no manual labelling.</S><S sid ="12" ssid = "5">The figure distinguishes systems which make use of hand-tagged data (using HTD) such as SemCor  from those that do not (without HTD).</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'123'", "'14'", "'115'", "'12'"]
'8'
'123'
'14'
'115'
'12'
['8', '123', '14', '115', '12']
parsed_discourse_facet ['results_citation']
<S sid ="15" ssid = "8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful  there is a strong case for obtaining a first  or predominant  sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S><S sid ="105" ssid = "3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents  rather than a lexical sample task  where the target words are manually determined and the results will depend on the skew of the words in the sample.</S><S sid ="165" ssid = "13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus  whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S><S sid ="145" ssid = "22">It is not always intuitively clear which of the senses to expect as predominant sense for either a particular domain or for the BNC  but the first senses of words like division and goal shift towards the more specific senses (league and score respectively).</S><S sid ="155" ssid = "3">A major benefit of our work  rather than reliance on hand-tagged training data such as SemCor  is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'", "'105'", "'165'", "'145'", "'155'"]
'15'
'105'
'165'
'145'
'155'
['15', '105', '165', '145', '155']
parsed_discourse_facet ['method_citation']
<S sid ="100" ssid = "29">In the English all-words SENSEVAL-2  25% of the noun data was monosemous.</S><S sid ="174" ssid = "22">We have restricted ourselves to nouns in this work  since this PoS is perhaps most affected by domain.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of “nearest neighbours”.</S><S sid ="133" ssid = "10">We acquired thesauruses for these corpora using the procedure described in section 2.1.</S><S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'174'", "'34'", "'133'", "'7'"]
'100'
'174'
'34'
'133'
'7'
['100', '174', '34', '133', '7']
parsed_discourse_facet ['method_citation']
<S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="70" ssid = "26">Jiang and Conrath specify a distance measure:   where the third class ( ) is the most informative  or most specific  superordinate synset of the two senses and .</S><S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="71" ssid = "27">This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal:</S>
original cit marker offset is 0
new cit marker offset is 0



["'44'", "'50'", "'70'", "'61'", "'71'"]
'44'
'50'
'70'
'61'
'71'
['44', '50', '70', '61', '71']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S><S sid ="72" ssid = "1">In order to evaluate our method we use the data in SemCor as a gold-standard.</S><S sid ="91" ssid = "20">This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC.</S><S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'72'", "'91'", "'10'", "'45'"]
'30'
'72'
'91'
'10'
'45'
['30', '72', '91', '10', '45']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S><S sid ="81" ssid = "10">4 We calculate the accuracy of finding the predominant sense  when there is indeed one sense with a higher frequency than the others for this word in SemCor ( ).</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'50'", "'82'", "'81'", "'44'"]
'69'
'50'
'82'
'81'
'44'
['69', '50', '82', '81', '44']
parsed_discourse_facet ['method_citation']
<S sid ="55" ssid = "11">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S><S sid ="58" ssid = "14">A noun    is thus described by a set of co-occurrence triples and associated frequencies  where is a grammatical relation and is a possible cooccurrence with in that relation.</S><S sid ="45" ssid = "1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S><S sid ="56" ssid = "12">For each noun we considered the co-occurring verbs in the direct object and subject relation  the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations.</S><S sid ="7" ssid = "7">Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'58'", "'45'", "'56'", "'7'"]
'55'
'58'
'45'
'56'
'7'
['55', '58', '45', '56', '7']
parsed_discourse_facet ['method_citation']
<S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S><S sid ="136" ssid = "13">The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.</S><S sid ="53" ssid = "9">This weight is the WordNet similarity score ( ) between the target sense ( ) and the sense of ( ) that maximises this score  divided by the sum of all such WordNet similarity scores for and .</S><S sid ="52" ssid = "8">For each sense of ( ) we obtain a ranking score by summing over the of each neighbour ( ) multiplied by a weight.</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'", "'136'", "'53'", "'52'", "'82'"]
'30'
'136'
'53'
'52'
'82'
['30', '136', '53', '52', '82']
parsed_discourse_facet ['method_citation']
<S sid ="135" ssid = "12">We therefore decided to select a limited number of words and to evaluate these words qualitatively.</S><S sid ="179" ssid = "2">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S sid ="111" ssid = "9">We give the results for this WSD task in table 2.</S><S sid ="54" ssid = "10">Thus we rank each sense using: parser (Briscoe and Carroll  2002).</S><S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'135'", "'179'", "'111'", "'54'", "'6'"]
'135'
'179'
'111'
'54'
'6'
['135', '179', '111', '54', '6']
parsed_discourse_facet ['method_citation']
<S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="156" ssid = "4">Buitelaar and Sacaleanu (2001) have previously explored ranking and selection of synsets in GermaNet for specific domains using the words in a given synset  and those related by hyponymy  and a term relevance measure taken from information retrieval.</S><S sid ="61" ssid = "17">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S><S sid ="142" ssid = "19">The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'156'", "'61'", "'65'", "'142'"]
'137'
'156'
'61'
'65'
'142'
['137', '156', '61', '65', '142']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="71" ssid = "27">This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal:</S><S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'69'", "'44'", "'71'", "'30'"]
'50'
'69'
'44'
'71'
'30'
['50', '69', '44', '71', '30']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="70" ssid = "26">Jiang and Conrath specify a distance measure:   where the third class ( ) is the most informative  or most specific  superordinate synset of the two senses and .</S><S sid ="137" ssid = "14">Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.</S><S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'70'", "'137'", "'69'", "'44'"]
'50'
'70'
'137'
'69'
'44'
['50', '70', '137', '69', '44']
parsed_discourse_facet ['method_citation']
<S sid ="16" ssid = "9">SemCor comprises a relatively small sample of 250 000 words.</S><S sid ="161" ssid = "9">Our approach is complementary to this.</S><S sid ="29" ssid = "22">We discuss our method in the following section.</S><S sid ="6" ssid = "6">This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.</S><S sid ="34" ssid = "27">In these each target word is entered with an ordered list of “nearest neighbours”.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'161'", "'29'", "'6'", "'34'"]
'16'
'161'
'29'
'6'
'34'
['16', '161', '29', '6', '34']
parsed_discourse_facet ['results_citation']
<S sid ="157" ssid = "5">Buitelaar and Sacaleanu have evaluated their method on identifying domain specific concepts using human judgements on 100 items.</S><S sid ="185" ssid = "8">In the future  we will perform a large scale evaluation on domain specific corpora.</S><S sid ="193" ssid = "2">This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.</S><S sid ="39" ssid = "32">We expect that the quantity and similarity of the neighbours pertaining to different senses will reflect the dominance of the sense to which they pertain.</S><S sid ="128" ssid = "5">The Reuters corpus (Rose et al.  2002) is a collection of about 810 000 Reuters  English Language News stories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'185'", "'193'", "'39'", "'128'"]
'157'
'185'
'193'
'39'
'128'
['157', '185', '193', '39', '128']
parsed_discourse_facet ['method_citation']
<S sid ="68" ssid = "24">We are of course able to apply the method to other versions of WordNet. synset  is incremented with the frequency counts from the corpus of all words belonging to that synset  directly or via the hyponymy relation.</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="65" ssid = "21">The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.</S><S sid ="44" ssid = "37">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within</S><S sid ="82" ssid = "11">We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'50'", "'65'", "'44'", "'82'"]
'68'
'50'
'65'
'44'
'82'
['68', '50', '65', '44', '82']
parsed_discourse_facet ['method_citation']
<S sid ="90" ssid = "19">From manual analysis  there are cases where the acquired first sense disagrees with SemCor  yet is intuitively plausible.</S><S sid ="175" ssid = "23">We are currently investigating the performance of the first sense heuristic  and this method  for other PoS on SENSEVAL-3 data (McCarthy et al.  2004)  although not yet with rankings from domain specific corpora.</S><S sid ="8" ssid = "1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S><S sid ="2" ssid = "2">The problem with using the predominant  or first sense heuristic  aside from the fact that it does not take surrounding context into account  is that it assumes some quantity of handtagged data.</S><S sid ="1" ssid = "1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'175'", "'8'", "'2'", "'1'"]
'90'
'175'
'8'
'2'
'1'
['90', '175', '8', '2', '1']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "25">The frequency data is used to calculate the “information content” (IC) of a class .</S><S sid ="10" ssid = "3">The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).</S><S sid ="0" ssid = "0">Finding Predominant Word Senses in Untagged Text</S><S sid ="50" ssid = "6">For input we used grammatical relation data extracted using an automatic where: .</S><S sid ="30" ssid = "23">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'10'", "'0'", "'50'", "'30'"]
'69'
'10'
'0'
'50'
'30'
['69', '10', '0', '50', '30']
parsed_discourse_facet ['method_citation']
['In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).']
['We acquired thesauruses for these corpora using the procedure described in section 2.1.', 'In the English all-words SENSEVAL-2  25% of the noun data was monosemous.', 'We have restricted ourselves to nouns in this work  since this PoS is perhaps most affected by domain.', 'Furthermore  we demonstrate that our method discovers appropriate predominant senses for words from two domainspecific corpora.', u'In these each target word is entered with an ordered list of \u201cnearest neighbours\u201d.']
['system', 'ROUGE-S*', 'Average_R:', '0.01270', '(95%-conf.int.', '0.01270', '-', '0.01270)']
['system', 'ROUGE-S*', 'Average_P:', '0.07619', '(95%-conf.int.', '0.07619', '-', '0.07619)']
['system', 'ROUGE-S*', 'Average_F:', '0.02177', '(95%-conf.int.', '0.02177', '-', '0.02177)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:105', 'F:8']
['The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.']
['word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.', 'From manual analysis  there are cases where the acquired first sense disagrees with SemCor  yet is intuitively plausible.', 'We are currently investigating the performance of the first sense heuristic  and this method  for other PoS on SENSEVAL-3 data (McCarthy et al.  2004)  although not yet with rankings from domain specific corpora.', 'The problem with using the predominant  or first sense heuristic  aside from the fact that it does not take surrounding context into account  is that it assumes some quantity of handtagged data.', 'The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['system', 'ROUGE-S*', 'Average_R:', '0.00769', '(95%-conf.int.', '0.00769', '-', '0.00769)']
['system', 'ROUGE-S*', 'Average_P:', '0.29091', '(95%-conf.int.', '0.29091', '-', '0.29091)']
['system', 'ROUGE-S*', 'Average_F:', '0.01499', '(95%-conf.int.', '0.01499', '-', '0.01499)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:55', 'F:16']
['Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.']
['We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).', 'Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.', 'The words included in this experiment are not a random sample  since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words.', 'This weight is the WordNet similarity score ( ) between the target sense ( ) and the sense of ( ) that maximises this score  divided by the sum of all such WordNet similarity scores for and .', 'For each sense of ( ) we obtain a ranking score by summing over the of each neighbour ( ) multiplied by a weight.']
['system', 'ROUGE-S*', 'Average_R:', '0.00484', '(95%-conf.int.', '0.00484', '-', '0.00484)']
['system', 'ROUGE-S*', 'Average_P:', '0.07619', '(95%-conf.int.', '0.07619', '-', '0.07619)']
['system', 'ROUGE-S*', 'Average_F:', '0.00910', '(95%-conf.int.', '0.00910', '-', '0.00910)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:105', 'F:8']
['A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.']
['The Reuters corpus (Rose et al.  2002) is a collection of about 810 000 Reuters  English Language News stories.', 'We expect that the quantity and similarity of the neighbours pertaining to different senses will reflect the dominance of the sense to which they pertain.', 'This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies  UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.', 'Buitelaar and Sacaleanu have evaluated their method on identifying domain specific concepts using human judgements on 100 items.', 'In the future  we will perform a large scale evaluation on domain specific corpora.']
['system', 'ROUGE-S*', 'Average_R:', '0.00198', '(95%-conf.int.', '0.00198', '-', '0.00198)']
['system', 'ROUGE-S*', 'Average_P:', '0.02614', '(95%-conf.int.', '0.02614', '-', '0.02614)']
['system', 'ROUGE-S*', 'Average_F:', '0.00369', '(95%-conf.int.', '0.00369', '-', '0.00369)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:153', 'F:4']
['We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.']
['We are of course able to apply the method to other versions of WordNet. synset  is incremented with the frequency counts from the corpus of all words belonging to that synset  directly or via the hyponymy relation.', 'We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).', 'We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within', 'For input we used grammatical relation data extracted using an automatic where: .', 'The measures provide a similarity score between two WordNet senses ( and )  these being synsets within WordNet. lesk (Banerjee and Pedersen  2002) This score maximises the number of overlapping words in the gloss  or definition  of the senses.']
['system', 'ROUGE-S*', 'Average_R:', '0.05932', '(95%-conf.int.', '0.05932', '-', '0.05932)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.11200', '(95%-conf.int.', '0.11200', '-', '0.11200)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1770', 'P:105', 'F:105']
['Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.']
['We also calculate the WSD accuracy that would be obtained on SemCor  when using our first sense in all contexts ( ).', u'The frequency data is used to calculate the \u201cinformation content\u201d (IC) of a class .', 'We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within', '4 We calculate the accuracy of finding the predominant sense  when there is indeed one sense with a higher frequency than the others for this word in SemCor ( ).', 'For input we used grammatical relation data extracted using an automatic where: .']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:55', 'F:0']
['Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.']
['Thus we rank each sense using: parser (Briscoe and Carroll  2002).', 'We give the results for this WSD task in table 2.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.', 'We therefore decided to select a limited number of words and to evaluate these words qualitatively.', 'This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.']
['system', 'ROUGE-S*', 'Average_R:', '0.00357', '(95%-conf.int.', '0.00357', '-', '0.00357)']
['system', 'ROUGE-S*', 'Average_P:', '0.01170', '(95%-conf.int.', '0.01170', '-', '0.01170)']
['system', 'ROUGE-S*', 'Average_F:', '0.00546', '(95%-conf.int.', '0.00546', '-', '0.00546)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:171', 'F:2']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['For input we used grammatical relation data extracted using an automatic where: .', u'The frequency data is used to calculate the \u201cinformation content\u201d (IC) of a class .', 'Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.', 'Finding Predominant Word Senses in Untagged Text', 'The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al.  1993).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:66', 'F:0']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
[u'In these each target word is entered with an ordered list of \u201cnearest neighbours\u201d.', 'This is a very promising result given that our method does not require any hand-tagged text  such as SemCor.', 'Our approach is complementary to this.', 'We discuss our method in the following section.', 'SemCor comprises a relatively small sample of 250 000 words.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:66', 'F:0']
['A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.']
[u'The frequency data is used to calculate the \u201cinformation content\u201d (IC) of a class .', 'We experiment with several WordNet Similarity measures (Patwardhan and Pedersen  2003) which aim to capture semantic relatedness within', 'Additionally  we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a  2000) which annotates WordNet synsets with domain labels.', 'For input we used grammatical relation data extracted using an automatic where: .', 'Jiang and Conrath specify a distance measure:   where the third class ( ) is the most informative  or most specific  superordinate synset of the two senses and .']
['system', 'ROUGE-S*', 'Average_R:', '0.00392', '(95%-conf.int.', '0.00392', '-', '0.00392)']
['system', 'ROUGE-S*', 'Average_P:', '0.03268', '(95%-conf.int.', '0.03268', '-', '0.03268)']
['system', 'ROUGE-S*', 'Average_F:', '0.00700', '(95%-conf.int.', '0.00700', '-', '0.00700)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:153', 'F:5']
0.151380998486 0.00940199990598 0.017400999826





input/ref/Task1/W99-0623_swastika.csv
input/res/Task1/W99-0623.csv
parsing: input/ref/Task1/W99-0623_swastika.csv
<S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['aim_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['result_citation']
<S sid="91" ssid="20">Features and context were initially introduced into the models, but they refused to offer any gains in performance.</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



['139']
139
['139']
parsed_discourse_facet ['result_citation']
<S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['method_citation']
<S sid="103" ssid="32">The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.</S>
original cit marker offset is 0
new cit marker offset is 0



['103']
103
['103']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



['139']
139
['139']
parsed_discourse_facet ['result_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
<S sid="140" ssid="2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
<S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['result_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/W99-0623.csv
<S sid ="142" ssid = "4">Both of the switching techniques  as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.</S><S sid ="141" ssid = "3">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'141'", "'18'", "'81'", "'57'"]
'142'
'141'
'18'
'81'
'57'
['142', '141', '18', '81', '57']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'61'", "'81'", "'78'", "'139'"]
'112'
'61'
'81'
'78'
'139'
['112', '61', '81', '78', '139']
Error in Discourse Facet
<S sid ="62" ssid = "48">This is the parse that is closest to the centroid of the observed parses under the similarity metric.</S><S sid ="88" ssid = "17">For example  one parser could be more accurate at predicting noun phrases than the other parsers.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid ="115" ssid = "44">It is the performance we could achieve if an omniscient observer told us which parser to pick for each of the sentences.</S><S sid ="22" ssid = "8">If enough parsers suggest that a particular constituent belongs in the parse  we include it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'", "'88'", "'27'", "'115'", "'22'"]
'62'
'88'
'27'
'115'
'22'
['62', '88', '27', '115', '22']
Error in Discourse Facet
<S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="125" ssid = "54">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S><S sid ="41" ssid = "27">IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.</S><S sid ="44" ssid = "30">Each of the constituents must have received at least 1 votes from the k parsers  so a > I1 and 2 — 2k±-1 b > ri-5-111.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'21'", "'125'", "'41'", "'44'"]
'38'
'21'
'125'
'41'
'44'
['38', '21', '125', '41', '44']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="139" ssid = "1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'81'", "'15'", "'78'", "'139'"]
'112'
'81'
'15'
'78'
'139'
['112', '81', '15', '78', '139']
Error in Discourse Facet
<S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="136" ssid = "65">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S><S sid ="131" ssid = "60">It was then tested on section 22 of the Treebank in conjunction with the other parsers.</S><S sid ="103" ssid = "32">The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.</S><S sid ="132" ssid = "61">The results of this experiment can be seen in Table 5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'136'", "'131'", "'103'", "'132'"]
'21'
'136'
'131'
'103'
'132'
['21', '136', '131', '103', '132']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'78'", "'81'", "'61'", "'130'"]
'112'
'78'
'81'
'61'
'130'
['112', '78', '81', '61', '130']
Error in Discourse Facet
<S sid ="125" ssid = "54">The constituent voting and naïve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S><S sid ="27" ssid = "13">Another technique for parse hybridization is to use a naïve Bayes classifier to determine which constituents to include in the parse.</S><S sid ="21" ssid = "7">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S><S sid ="1" ssid = "1">Three state-of-the-art statistical parsers are combined to produce more accurate parses  as well as new bounds on achievable Treebank parsing accuracy.</S><S sid ="38" ssid = "24">Under certain conditions the constituent voting and naïve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'27'", "'21'", "'1'", "'38'"]
'125'
'27'
'21'
'1'
'38'
['125', '27', '21', '1', '38']
Error in Discourse Facet
<S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="51" ssid = "37">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes  a completely flat structure.</S><S sid ="18" ssid = "4">These two principles guide experimentation in this framework  and together with the evaluation measures help us decide which specific type of substructure to combine.</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'12'", "'51'", "'18'", "'57'"]
'55'
'12'
'51'
'18'
'57'
['55', '12', '51', '18', '57']
Error in Discourse Facet
<S sid ="142" ssid = "4">Both of the switching techniques  as well as the parametric hybridization technique were also shown to be robust when a poor parser was introduced into the experiments.</S><S sid ="127" ssid = "56">Parser 3  the most accurate parser  was chosen 71% of the time  and Parser 1  the least accurate parser was chosen 16% of the time.</S><S sid ="141" ssid = "3">All four of the techniques studied result in parsing systems that perform better than any previously reported.</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="140" ssid = "2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'127'", "'141'", "'12'", "'140'"]
'142'
'127'
'141'
'12'
'140'
['142', '127', '141', '12', '140']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'78'", "'61'", "'130'", "'15'"]
'112'
'78'
'61'
'130'
'15'
['112', '78', '61', '130', '15']
Error in Discourse Facet
<S sid ="55" ssid = "41">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid ="51" ssid = "37">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes  a completely flat structure.</S><S sid ="52" ssid = "38">This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.</S><S sid ="50" ssid = "36">There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.</S><S sid ="26" ssid = "12">This technique has the advantage of requiring no training  but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'", "'51'", "'52'", "'50'", "'26'"]
'55'
'51'
'52'
'50'
'26'
['55', '51', '52', '50', '26']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="114" ssid = "43">The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'81'", "'78'", "'61'", "'114'"]
'112'
'81'
'78'
'61'
'114'
['112', '81', '78', '61', '114']
Error in Discourse Facet
<S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="12" ssid = "8">The corpus-based statistical parsing community has many fast and accurate automated parsing systems  including systems produced by Collins (1997)  Charniak (1997) and Ratnaparkhi (1997).</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'81'", "'12'", "'15'", "'130'"]
'112'
'81'
'12'
'15'
'130'
['112', '81', '12', '15', '130']
Error in Discourse Facet
<S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid ="61" ssid = "47">We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.</S><S sid ="114" ssid = "43">The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.</S><S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S>
original cit marker offset is 0
new cit marker offset is 0



["'78'", "'61'", "'114'", "'112'", "'81'"]
'78'
'61'
'114'
'112'
'81'
['78', '61', '114', '112', '81']
Error in Discourse Facet
<S sid ="129" ssid = "58">In the interest of testing the robustness of these combining techniques  we added a fourth  simple nonlexicalized PCFG parser.</S><S sid ="11" ssid = "7">Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).</S><S sid ="130" ssid = "59">The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.</S><S sid ="15" ssid = "1">We are interested in combining the substructures of the input parses to produce a better parse.</S><S sid ="9" ssid = "5">Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'129'", "'11'", "'130'", "'15'", "'9'"]
'129'
'11'
'130'
'15'
'9'
['129', '11', '130', '15', '9']
Error in Discourse Facet
<S sid ="81" ssid = "10">F-measure is the harmonic mean of precision and recall  2PR/(P + R).</S><S sid ="57" ssid = "43">The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.</S><S sid ="114" ssid = "43">The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.</S><S sid ="112" ssid = "41">The second row is the accuracy of the best of the three parsers.'</S><S sid ="78" ssid = "7">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["'81'", "'57'", "'114'", "'112'", "'78'"]
'81'
'57'
'114'
'112'
'78'
['81', '57', '114', '112', '78']
Error in Discourse Facet
["In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers."]
['The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.', 'The combining technique must act as a multi-position switch indicating which parser should be trusted for the particular sentence.', 'F-measure is the harmonic mean of precision and recall  2PR/(P + R).', 'The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.', "The second row is the accuracy of the best of the three parsers.'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00945', '(95%-conf.int.', '0.00945', '-', '0.00945)']
['system', 'ROUGE-S*', 'Average_P:', '0.03333', '(95%-conf.int.', '0.03333', '-', '0.03333)']
['system', 'ROUGE-S*', 'Average_F:', '0.01472', '(95%-conf.int.', '0.01472', '-', '0.01472)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:210', 'F:7']
["In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers."]
['The parser switching oracle is the upper bound on the accuracy that can be achieved on this set in the parser switching framework.', 'We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.', "The second row is the accuracy of the best of the three parsers.'", 'F-measure is the harmonic mean of precision and recall  2PR/(P + R).', 'The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.']
['system', 'ROUGE-S*', 'Average_R:', '0.01707', '(95%-conf.int.', '0.01707', '-', '0.01707)']
['system', 'ROUGE-S*', 'Average_P:', '0.05714', '(95%-conf.int.', '0.05714', '-', '0.05714)']
['system', 'ROUGE-S*', 'Average_F:', '0.02629', '(95%-conf.int.', '0.02629', '-', '0.02629)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:210', 'F:12']
['We have presented two general approaches to studying parser combination: parser switching and parse hybridization.']
['We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.', 'This technique has the advantage of requiring no training  but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.', 'One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes  a completely flat structure.', 'There is a guarantee of no crossing brackets but there is no guarantee that a constituent in the tree has the same children as it had in any of the three original parses.', 'This drastic tree manipulation is not appropriate for situations in which we want to assign particular structures to sentences.']
['system', 'ROUGE-S*', 'Average_R:', '0.00940', '(95%-conf.int.', '0.00940', '-', '0.00940)']
['system', 'ROUGE-S*', 'Average_P:', '0.33333', '(95%-conf.int.', '0.33333', '-', '0.33333)']
['system', 'ROUGE-S*', 'Average_F:', '0.01828', '(95%-conf.int.', '0.01828', '-', '0.01828)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:45', 'F:15']
['We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.']
['Similar advances have been made in machine translation (Frederking and Nirenburg  1994)  speech recognition (Fiscus  1997) and named entity recognition (Borthwick et al.  1998).', 'In the interest of testing the robustness of these combining techniques  we added a fourth  simple nonlexicalized PCFG parser.', 'The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.', 'We are interested in combining the substructures of the input parses to produce a better parse.', 'Recently  combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al.  1998; Brill and Wu  1998).']
['system', 'ROUGE-S*', 'Average_R:', '0.00302', '(95%-conf.int.', '0.00302', '-', '0.00302)']
['system', 'ROUGE-S*', 'Average_P:', '0.13889', '(95%-conf.int.', '0.13889', '-', '0.13889)']
['system', 'ROUGE-S*', 'Average_F:', '0.00592', '(95%-conf.int.', '0.00592', '-', '0.00592)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:36', 'F:5']
['The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.']
['We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.', 'We have presented two general approaches to studying parser combination: parser switching and parse hybridization.', 'F-measure is the harmonic mean of precision and recall  2PR/(P + R).', 'The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.', "The second row is the accuracy of the best of the three parsers.'"]
['system', 'ROUGE-S*', 'Average_R:', '0.08108', '(95%-conf.int.', '0.08108', '-', '0.08108)']
['system', 'ROUGE-S*', 'Average_P:', '0.18000', '(95%-conf.int.', '0.18000', '-', '0.18000)']
['system', 'ROUGE-S*', 'Average_F:', '0.11180', '(95%-conf.int.', '0.11180', '-', '0.11180)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:300', 'F:54']
['The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.']
['The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.', 'We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.', 'The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.', 'F-measure is the harmonic mean of precision and recall  2PR/(P + R).', "The second row is the accuracy of the best of the three parsers.'"]
['system', 'ROUGE-S*', 'Average_R:', '0.06439', '(95%-conf.int.', '0.06439', '-', '0.06439)']
['system', 'ROUGE-S*', 'Average_P:', '0.11333', '(95%-conf.int.', '0.11333', '-', '0.11333)']
['system', 'ROUGE-S*', 'Average_F:', '0.08213', '(95%-conf.int.', '0.08213', '-', '0.08213)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:528', 'P:300', 'F:34']
['The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.']
['Three state-of-the-art statistical parsers are combined to produce more accurate parses  as well as new bounds on achievable Treebank parsing accuracy.', u'Under certain conditions the constituent voting and na\xefve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.', u'Another technique for parse hybridization is to use a na\xefve Bayes classifier to determine which constituents to include in the parse.', u'The constituent voting and na\xefve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.', "One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set."]
['system', 'ROUGE-S*', 'Average_R:', '0.02131', '(95%-conf.int.', '0.02131', '-', '0.02131)']
['system', 'ROUGE-S*', 'Average_P:', '0.13000', '(95%-conf.int.', '0.13000', '-', '0.13000)']
['system', 'ROUGE-S*', 'Average_F:', '0.03662', '(95%-conf.int.', '0.03662', '-', '0.03662)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:300', 'F:39']
['In our particular case the majority requires the agreement of only two parsers because we have only three.']
['It is the performance we could achieve if an omniscient observer told us which parser to pick for each of the sentences.', 'This is the parse that is closest to the centroid of the observed parses under the similarity metric.', u'Another technique for parse hybridization is to use a na\xc3\xafve Bayes classifier to determine which constituents to include in the parse.', 'For example  one parser could be more accurate at predicting noun phrases than the other parsers.', 'If enough parsers suggest that a particular constituent belongs in the parse  we include it.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:10', 'F:0']
['The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.']
['The PCFG was trained from the same sections of the Penn Treebank as the other three parsers.', 'We pick the parse that is most similar to the other parses by choosing the one with the highest sum of pairwise similarities.', 'The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.', 'We are interested in combining the substructures of the input parses to produce a better parse.', "The second row is the accuracy of the best of the three parsers.'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00168', '(95%-conf.int.', '0.00168', '-', '0.00168)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00312', '(95%-conf.int.', '0.00313', '-', '0.00313)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:45', 'F:1']
0.112026665422 0.0230444441884 0.0332088885199





input/ref/Task1/W06-2932_swastika.csv
input/res/Task1/W06-2932.csv
parsing: input/ref/Task1/W06-2932_swastika.csv
<S sid="86" ssid="8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S>
original cit marker offset is 
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="54" ssid="2">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



['54']
54
['54']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



['12']
12
['12']
parsed_discourse_facet ['method_citation']
    <S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



['41']
41
['41']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="6">Its power lies in the ability to define a rich set of features over parsing decisions, as well as surface level features relative to these decisions.</S>
original cit marker offset is 0
new cit marker offset is 0



['24']
24
['24']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



['12']
12
['12']
parsed_discourse_facet ['method_citation']
<S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



['43']
43
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S sid ="3" ssid = "3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S><S sid ="4" ssid = "4">We report results on the CoNLL-X shared task (Buchholz et al.  2006) data sets and present an error analysis.</S><S sid ="108" ssid = "5">Second  we plan on integrating any available morphological features in a more principled manner.</S><S sid ="10" ssid = "6">Dependency graphs also encode much of the deep syntactic information needed for further processing.</S><S sid ="106" ssid = "3">First  we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is nan
new cit marker offset is 0



["'3'", "'4'", "'108'", "'10'", "'106'"]
'3'
'4'
'108'
'10'
'106'
['3', '4', '108', '10', '106']
parsed_discourse_facet ['results_citation']
<S sid ="101" ssid = "23">For example if we train on 200  400  800 and the full training set  labeled accuracies are 54%  60%  62% and 67%.</S><S sid ="17" ssid = "13">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S><S sid ="16" ssid = "12">A dependency graph is represented by a set of ordered pairs (i  j) E y in which xj is a dependent and xi is the corresponding head.</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'17'", "'16'", "'2'", "'20'"]
'101'
'17'
'16'
'2'
'20'
['101', '17', '16', '2', '20']
parsed_discourse_facet ['method_citation']
<S sid ="61" ssid = "9">In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S><S sid ="66" ssid = "4">These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.</S><S sid ="88" ssid = "10">Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.</S><S sid ="83" ssid = "5">In a preliminary test of this hypothesis  we looked at all of the sentences from a development set in which a main verb is incorrectly attached.</S><S sid ="50" ssid = "19">Various conjunctions of these were included based on performance on held-out data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'61'", "'66'", "'88'", "'83'", "'50'"]
'61'
'66'
'88'
'83'
'50'
['61', '66', '88', '83', '50']
parsed_discourse_facet ['method_citation']
<S sid ="76" ssid = "14">For instance  sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.</S><S sid ="96" ssid = "18">Each stage by itself is relatively accurate (unlabeled accuracy is 79% and labeling accuracy3 is also 79%)  but since there is very little overlap in the kinds of errors each makes  overall labeled accuracy drops to 67%.</S><S sid ="57" ssid = "5">Performance is measured through unlabeled accuracy  which is the percentage of words that modify the correct head in the dependency graph  and labeled accuracy  which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'", "'41'", "'96'", "'57'", "'64'"]
'76'
'41'
'96'
'57'
'64'
['76', '41', '96', '57', '64']
parsed_discourse_facet ['method_citation']
<S sid ="19" ssid = "1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S><S sid ="47" ssid = "16">We used the following: dependent have identical values?</S><S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="17" ssid = "13">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'47'", "'32'", "'36'", "'17'"]
'19'
'47'
'32'
'36'
'17'
['19', '47', '32', '36', '17']
parsed_discourse_facet ['method_citation']
<S sid ="54" ssid = "2">Based on performance from a held-out section of the training data  we used non-projective parsing algorithms for Czech  Danish  Dutch  German  Japanese  Portuguese and Slovene  and projective parsing algorithms for Arabic  Bulgarian  Chinese  Spanish  Swedish and Turkish.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="104" ssid = "1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al.  2005b; McDonald and Pereira  2006) generalizes well to languages other than English.</S><S sid ="21" ssid = "3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al.  2005a; McDonald et al.  2005b) to incorporate features over multiple edges in the dependency graph.</S><S sid ="63" ssid = "1">Our system has several components  including the ability to produce non-projective edges  sequential Japanese  Portuguese  Slovene  Spanish  Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'", "'11'", "'104'", "'21'", "'63'"]
'54'
'11'
'104'
'21'
'63'
['54', '11', '104', '21', '63']
parsed_discourse_facet ['method_citation']
<S sid ="1" ssid = "1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S><S sid ="58" ssid = "6">These results show that the discriminative spanning tree parsing framework (McDonald et al.  2005b; McDonald and Pereira  2006) is easily adapted across all these languages.</S><S sid ="0" ssid = "0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S sid ="63" ssid = "1">Our system has several components  including the ability to produce non-projective edges  sequential Japanese  Portuguese  Slovene  Spanish  Swedish and Turkish.</S><S sid ="108" ssid = "5">Second  we plan on integrating any available morphological features in a more principled manner.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'58'", "'0'", "'63'", "'108'"]
'1'
'58'
'0'
'63'
'108'
['1', '58', '0', '63', '108']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="17" ssid = "13">Each edge can be assigned a label l(ij) from a finite set L of predefined labels.</S><S sid ="53" ssid = "1">We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).</S><S sid ="1" ssid = "1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'20'", "'17'", "'53'", "'1'"]
'0'
'20'
'17'
'53'
'1'
['0', '20', '17', '53', '1']
parsed_discourse_facet ['method_citation']
<S sid ="47" ssid = "16">We used the following: dependent have identical values?</S><S sid ="2" ssid = "2">The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.</S><S sid ="19" ssid = "1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S><S sid ="20" ssid = "2">This system is primarily based on the parsing models described by McDonald and Pereira (2006).</S><S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'2'", "'19'", "'20'", "'32'"]
'47'
'2'
'19'
'20'
'32'
['47', '2', '19', '20', '32']
parsed_discourse_facet ['method_citation']
<S sid ="81" ssid = "3">Other high-frequency word classes with relatively low attachment accuracy are prepositions (80%)  adverbs (82%) and subordinating conjunctions (80%)  for a total of another 23% of the test corpus.</S><S sid ="23" ssid = "5">That system uses MIRA  an online large-margin learning algorithm  to compute model parameters.</S><S sid ="86" ssid = "8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S><S sid ="43" ssid = "12">For score functions  we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation  we can find the highest scoring label sequence with Viterbi’s algorithm.</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'81'", "'23'", "'86'", "'43'", "'36'"]
'81'
'23'
'86'
'43'
'36'
['81', '23', '86', '43', '36']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216–220  New York City  June 2006. c�2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.</S><S sid ="76" ssid = "14">For instance  sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S><S sid ="64" ssid = "2">N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.</S><S sid ="79" ssid = "1">Although overall unlabeled accuracy is 86%  most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs  75% for the verb ser  and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'41'", "'76'", "'64'", "'79'"]
'18'
'41'
'76'
'64'
'79'
['18', '41', '76', '64', '79']
parsed_discourse_facet ['method_citation']
<S sid ="18" ssid = "14">We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216–220  New York City  June 2006. c�2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.</S><S sid ="3" ssid = "3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S><S sid ="96" ssid = "18">Each stage by itself is relatively accurate (unlabeled accuracy is 79% and labeling accuracy3 is also 79%)  but since there is very little overlap in the kinds of errors each makes  overall labeled accuracy drops to 67%.</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="95" ssid = "17">Note the difference in error between the unlabeled parser and the edge labeler: the former makes mistakes on edges into prepositions  conjunctions and verbs  and the latter makes mistakes on edges into nouns (subject/objects).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'", "'3'", "'96'", "'11'", "'95'"]
'18'
'3'
'96'
'11'
'95'
['18', '3', '96', '11', '95']
parsed_discourse_facet ['method_citation']
<S sid ="32" ssid = "1">The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).</S><S sid ="11" ssid = "7">This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).</S><S sid ="41" ssid = "10">To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm�1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm−1) in the tree y.</S><S sid ="36" ssid = "5">However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S><S sid ="16" ssid = "12">A dependency graph is represented by a set of ordered pairs (i  j) E y in which xj is a dependent and xi is the corresponding head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'", "'11'", "'41'", "'36'", "'16'"]
'32'
'11'
'41'
'36'
'16'
['32', '11', '41', '36', '16']
parsed_discourse_facet ['results_citation']
['An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.']
['The first stage of our system creates an unlabeled parse y for an input sentence x.', 'We used the following: dependent have identical values?', 'However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.', 'Each edge can be assigned a label l(ij) from a finite set L of predefined labels.', 'The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).']
['system', 'ROUGE-S*', 'Average_R:', '0.01280', '(95%-conf.int.', '0.01280', '-', '0.01280)']
['system', 'ROUGE-S*', 'Average_P:', '0.06618', '(95%-conf.int.', '0.06618', '-', '0.06618)']
['system', 'ROUGE-S*', 'Average_F:', '0.02145', '(95%-conf.int.', '0.02145', '-', '0.02145)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:703', 'P:136', 'F:9']
['An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.']
['present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.', 'Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'This system is primarily based on the parsing models described by McDonald and Pereira (2006).', 'We trained models for all 13 languages provided by the CoNLL organizers (Buchholz et al.  2006).', 'Each edge can be assigned a label l(ij) from a finite set L of predefined labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00732', '(95%-conf.int.', '0.00732', '-', '0.00732)']
['system', 'ROUGE-S*', 'Average_P:', '0.04412', '(95%-conf.int.', '0.04412', '-', '0.04412)']
['system', 'ROUGE-S*', 'Average_F:', '0.01255', '(95%-conf.int.', '0.01255', '-', '0.01255)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:136', 'F:6']
['Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.']
['In fact  for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).', 'Nevertheless  this very preliminary experiment suggests that wider-range features may be useful in improving the recognition of overall sentence structure.', 'Various conjunctions of these were included based on performance on held-out data.', 'These results report the average labeled and unlabeled precision for the 10 languages with the smallest training sets.', 'In a preliminary test of this hypothesis  we looked at all of the sentences from a development set in which a main verb is incorrectly attached.']
['system', 'ROUGE-S*', 'Average_R:', '0.00627', '(95%-conf.int.', '0.00627', '-', '0.00627)']
['system', 'ROUGE-S*', 'Average_P:', '0.04678', '(95%-conf.int.', '0.04678', '-', '0.04678)']
['system', 'ROUGE-S*', 'Average_F:', '0.01107', '(95%-conf.int.', '0.01107', '-', '0.01107)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:171', 'F:8']
['Its power lies in the ability to define a rich set of features over parsing decisions, as well as surface level features relative to these decisions.']
['Other high-frequency word classes with relatively low attachment accuracy are prepositions (80%)  adverbs (82%) and subordinating conjunctions (80%)  for a total of another 23% of the test corpus.', 'Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.', u'For score functions  we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation  we can find the highest scoring label sequence with Viterbi\xe2\u20ac\u2122s algorithm.', 'However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.', 'That system uses MIRA  an online large-margin learning algorithm  to compute model parameters.']
['system', 'ROUGE-S*', 'Average_R:', '0.00043', '(95%-conf.int.', '0.00043', '-', '0.00043)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00082', '(95%-conf.int.', '0.00082', '-', '0.00082)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2346', 'P:91', 'F:1']
['For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.']
['This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).', 'The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).', 'A dependency graph is represented by a set of ordered pairs (i  j) E y in which xj is a dependent and xi is the corresponding head.', 'However  in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.', u'To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm\ufffd1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm\u22121) in the tree y.']
['system', 'ROUGE-S*', 'Average_R:', '0.00513', '(95%-conf.int.', '0.00513', '-', '0.00513)']
['system', 'ROUGE-S*', 'Average_P:', '0.06494', '(95%-conf.int.', '0.06494', '-', '0.06494)']
['system', 'ROUGE-S*', 'Average_F:', '0.00950', '(95%-conf.int.', '0.00950', '-', '0.00950)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:231', 'F:15']
['In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.', 'Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'Second  we plan on integrating any available morphological features in a more principled manner.', 'Our system has several components  including the ability to produce non-projective edges  sequential Japanese  Portuguese  Slovene  Spanish  Swedish and Turkish.', 'These results show that the discriminative spanning tree parsing framework (McDonald et al.  2005b; McDonald and Pereira  2006) is easily adapted across all these languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00627', '(95%-conf.int.', '0.00627', '-', '0.00627)']
['system', 'ROUGE-S*', 'Average_P:', '0.12121', '(95%-conf.int.', '0.12121', '-', '0.12121)']
['system', 'ROUGE-S*', 'Average_F:', '0.01193', '(95%-conf.int.', '0.01193', '-', '0.01193)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:66', 'F:8']
['Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.']
['This has been shown through their successful use in many standard natural language processing tasks  including machine translation (Ding and Palmer  2005)  sentence compression (McDonald  2006)  and textual inference (Haghighi et al.  2005).', 'Based on performance from a held-out section of the training data  we used non-projective parsing algorithms for Czech  Danish  Dutch  German  Japanese  Portuguese and Slovene  and projective parsing algorithms for Arabic  Bulgarian  Chinese  Spanish  Swedish and Turkish.', 'Our system has several components  including the ability to produce non-projective edges  sequential Japanese  Portuguese  Slovene  Spanish  Swedish and Turkish.', 'That work extends the maximum spanning tree dependency parsing framework (McDonald et al.  2005a; McDonald et al.  2005b) to incorporate features over multiple edges in the dependency graph.', 'We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al.  2005b; McDonald and Pereira  2006) generalizes well to languages other than English.']
['system', 'ROUGE-S*', 'Average_R:', '0.06061', '(95%-conf.int.', '0.06061', '-', '0.06061)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.11429', '(95%-conf.int.', '0.11429', '-', '0.11429)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4950', 'P:300', 'F:300']
['To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.']
['The first stage of our system creates an unlabeled parse y for an input sentence x.', 'We used the following: dependent have identical values?', 'The first stage based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages.', 'This system is primarily based on the parsing models described by McDonald and Pereira (2006).', 'The second stage takes the output parse y for sentence x and classifies each edge (i  j) E y with a particular label l(i j).']
['system', 'ROUGE-S*', 'Average_R:', '0.00813', '(95%-conf.int.', '0.00813', '-', '0.00813)']
['system', 'ROUGE-S*', 'Average_P:', '0.01852', '(95%-conf.int.', '0.01852', '-', '0.01852)']
['system', 'ROUGE-S*', 'Average_F:', '0.01130', '(95%-conf.int.', '0.01130', '-', '0.01130)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:378', 'F:7']
['Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.']
['For instance  sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.', 'Performance is measured through unlabeled accuracy  which is the percentage of words that modify the correct head in the dependency graph  and labeled accuracy  which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.', 'Each stage by itself is relatively accurate (unlabeled accuracy is 79% and labeling accuracy3 is also 79%)  but since there is very little overlap in the kinds of errors each makes  overall labeled accuracy drops to 67%.', 'N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.', u'To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm\ufffd1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm\u22121) in the tree y.']
['system', 'ROUGE-S*', 'Average_R:', '0.00197', '(95%-conf.int.', '0.00197', '-', '0.00197)']
['system', 'ROUGE-S*', 'Average_P:', '0.08772', '(95%-conf.int.', '0.08772', '-', '0.08772)']
['system', 'ROUGE-S*', 'Average_F:', '0.00385', '(95%-conf.int.', '0.00385', '-', '0.00385)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:7626', 'P:171', 'F:15']
['Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.']
[u'We Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X)  pages 216\u2013220  New York City  June 2006. c\ufffd2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective  both of which are true in the data sets we use.', 'For instance  sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.', 'N/P: Allow non-projective/Force projective  S/A: Sequential labeling/Atomic labeling  M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment  and a rich feature set that incorporates morphological properties when available.', 'Although overall unlabeled accuracy is 86%  most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs  75% for the verb ser  and 65% for coordinating conjunctions.', u'To model this we treat the labeling of the edges (i  j1)  ...   (i  jM) as a sequence labeling problem  We use a first-order Markov factorization of the score s(l(i jm)  l(i jm\ufffd1)  i  y  x) in which each factor is the score of labeling the adjacent edges (i  jm) and (i  jm\u22121) in the tree y.']
['system', 'ROUGE-S*', 'Average_R:', '0.02008', '(95%-conf.int.', '0.02008', '-', '0.02008)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.03937', '(95%-conf.int.', '0.03937', '-', '0.03937)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:8515', 'P:171', 'F:171']
0.24604599754 0.012900999871 0.0236129997639





input/ref/Task1/J01-2004_sweta.csv
input/res/Task1/J01-2004.csv
parsing: input/ref/Task1/J01-2004_sweta.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["372'"]
372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="28">The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.</S>
    <S sid="41" ssid="29">There will also be a brief review of previous work using syntactic information for language modeling, before we introduce our model in Section 4.</S>
    <S sid="42" ssid="30">Three parse trees: (a) a complete parse tree; (b) a complete parse tree with an explicit stop symbol; and (c) a partial parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["40'", "'41'", "'42'"]
40'
'41'
'42'
['40', '41', '42']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="13">A parser that is not left to right, but which has rooted derivations, e.g., a headfirst parser, will be able to calculate generative joint probabilities for entire strings; however, it will not be able to calculate probabilities for each word conditioned on previously generated words, unless each derivation generates the words in the string in exactly the same order.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="364" ssid="120">We follow Chelba (2000) in dealing with this problem: for parsing purposes, we use the Penn Treebank tokenization; for interpolation with the provided trigram model, and for evaluation, the lattice tokenization is used.</S>
original cit marker offset is 0
new cit marker offset is 0



["364'"]
364'
['364']
parsed_discourse_facet ['method_citation']
<S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



["302'"]
302'
['302']
parsed_discourse_facet ['method_citation']
 <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="231" ssid="135">Since we do not know the POS for the word, we must sum the LAP for all POS For a PCFG G, a stack S = Ao An$ (which we will write AN and a look-ahead terminal item wi, we define the look-ahead probability as follows: We recursively estimate this with two empirically observed conditional probabilities for every nonterminal A,: 13(A, w,a) and P(A, c).</S>
original cit marker offset is 0
new cit marker offset is 0



["231'"]
231'
['231']
parsed_discourse_facet ['method_citation']
<S sid="297" ssid="53">The differences between a k-best and a beam-search parser (not to mention the use of dynamic programming) make a running time difference unsurprising.</S>
original cit marker offset is 0
new cit marker offset is 0



["297'"]
297'
['297']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="37">Statistically based heuristic best-first or beam-search strategies (Caraballo and Charniak 1998; Charniak, Goldwater, and Johnson 1998; Goodman 1997) have yielded an enormous improvement in the quality and speed of parsers, even without any guarantee that the parse returned is, in fact, that with the maximum likelihood for the probability model.</S>
original cit marker offset is 0
new cit marker offset is 0



["133'"]
133'
['133']
parsed_discourse_facet ['method_citation']
<S sid="291" ssid="47">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["291'"]
291'
['291']
parsed_discourse_facet ['method_citation']
<S sid="355" ssid="111">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.</S>
original cit marker offset is 0
new cit marker offset is 0



["355'"]
355'
['355']
parsed_discourse_facet ['method_citation']
<S sid="59" ssid="17">A PCFG is a CFG with a probability assigned to each rule; specifically, each righthand side has a probability given the left-hand side of the rule.</S>
original cit marker offset is 0
new cit marker offset is 0



["59'"]
59'
['59']
parsed_discourse_facet ['method_citation']
<S sid="100" ssid="4">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="12">Another approach that uses syntactic structure for language modeling has been to use a shift-reduce parser to &amp;quot;surface&amp;quot; c-commanding phrasal headwords or part-of-speech (POS) tags from arbitrarily far back in the prefix string, for use in a trigram-like model.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="168" ssid = "72">For example  in Figure 2  constituent (NP-DT-NN) is simply NP.</S><S sid ="356" ssid = "112">The DARPA '93 HUB1 test setup consists of 213 utterances read from the Wall Street Journal  a total of 3 446 words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'224'", "'322'", "'358'", "'168'", "'356'"]
'224'
'322'
'358'
'168'
'356'
['224', '322', '358', '168', '356']
parsed_discourse_facet ['results_citation']
<S sid ="17" ssid = "5">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S sid ="102" ssid = "6">One approach to syntactic language modeling is to use this distribution directly as a language model.</S><S sid ="340" ssid = "96">The trigram model was also trained on Sections 00-20 of the C&J corpus.</S><S sid ="113" ssid = "17">In empirical trials  Goddeau used the top two stack entries to condition the word probability.</S><S sid ="12" ssid = "6">A small recognition experiment also demonstrates the utility of the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'102'", "'340'", "'113'", "'12'"]
'17'
'102'
'340'
'113'
'12'
['17', '102', '340', '113', '12']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="95" ssid = "53">For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'95'", "'302'", "'224'", "'104'"]
'134'
'95'
'302'
'224'
'104'
['134', '95', '302', '224', '104']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="167" ssid = "71">In order to avoid any confusions in identifying the nonterminal label of a particular rule production in either its factored or rionfactored version  we introduce the function constituent (A) for every nonterminal in the factored grammar Gf  which is simply the label of the constituent whose factorization results in A.</S><S sid ="112" ssid = "16">Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state  and conditioning on those entries in a way similar to an n-gram.</S><S sid ="85" ssid = "43">The simple definition of c-command that we will be using in this paper is the following: a node A c-commands a node B if and only if (i) A does not dominate B; and (ii) the lowest branching node (i.e.  non-unary node) that dominates A also dominates B.'</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'167'", "'112'", "'85'", "'390'"]
'134'
'167'
'112'
'85'
'390'
['134', '167', '112', '85', '390']
parsed_discourse_facet ['method_citation']
<S sid ="321" ssid = "77">In our trials  we used the unigram  with a very small mixing coefficient: following words since the denominator is zero.</S><S sid ="95" ssid = "53">For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].</S><S sid ="199" ssid = "103">Table 1 gives a breakdown of the different levels of conditioning information used in the empirical trials  with a mnemonic label that will be used when presenting results.</S><S sid ="5" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S><S sid ="11" ssid = "5">Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models  demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'321'", "'95'", "'199'", "'5'", "'11'"]
'321'
'95'
'199'
'5'
'11'
['321', '95', '199', '5', '11']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="87" ssid = "45">Notice that the subject NP c-commands the object NP  but not vice versa  since the lowest branching node that dominates the object NP is the VP  which does not dominate the subject NP.</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S><S sid ="85" ssid = "43">The simple definition of c-command that we will be using in this paper is the following: a node A c-commands a node B if and only if (i) A does not dominate B; and (ii) the lowest branching node (i.e.  non-unary node) that dominates A also dominates B.'</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'87'", "'302'", "'104'", "'85'"]
'134'
'87'
'302'
'104'
'85'
['134', '87', '302', '104', '85']
parsed_discourse_facet ['method_citation']
<S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="52" ssid = "10">Consider  for example  the parse tree shown in (a) in Figure 1: the start symbol is St  which expands into an S. The S node expands into an NP followed by a VP.</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S>
original cit marker offset is 0
new cit marker offset is 0



["'224'", "'134'", "'52'", "'322'", "'306'"]
'224'
'134'
'52'
'322'
'306'
['224', '134', '52', '322', '306']
parsed_discourse_facet ['method_citation']
<S sid ="112" ssid = "16">Goddeau (1992) used a robust deterministic shift-reduce parser to condition word probabilities by extracting a specified number of stack entries from the top of the current state  and conditioning on those entries in a way similar to an n-gram.</S><S sid ="129" ssid = "33">The shift-reduce parser will have  perhaps  built the structure shown  and the stack state will have an NP entry with the head &quot;cat&quot; at the top of the stack  and a VBD entry with the head &quot;chased&quot; second on the stack.</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="87" ssid = "45">Notice that the subject NP c-commands the object NP  but not vice versa  since the lowest branching node that dominates the object NP is the VP  which does not dominate the subject NP.</S><S sid ="109" ssid = "13">A shift-reduce parser operates from left to right using a stack and a pointer to the next word in the input string.9 Each stack entry consists minimally of a nonterminal label.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'", "'129'", "'302'", "'87'", "'109'"]
'112'
'129'
'302'
'87'
'109'
['112', '129', '302', '87', '109']
parsed_discourse_facet ['method_citation']
<S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="213" ssid = "117">The parser takes an input string 4  a PCFG G  and a priority queue of candidate analyses.</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'358'", "'306'", "'224'", "'213'", "'322'"]
'358'
'306'
'224'
'213'
'322'
['358', '306', '224', '213', '322']
parsed_discourse_facet ['method_citation']
<S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="59" ssid = "17">A PCFG is a CFG with a probability assigned to each rule; specifically  each righthand side has a probability given the left-hand side of the rule.</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="52" ssid = "10">Consider  for example  the parse tree shown in (a) in Figure 1: the start symbol is St  which expands into an S. The S node expands into an NP followed by a VP.</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'302'", "'59'", "'134'", "'52'", "'104'"]
'302'
'59'
'134'
'52'
'104'
['302', '59', '134', '52', '104']
parsed_discourse_facet ['method_citation']
<S sid ="210" ssid = "114">It uses a PCFG with a conditional probability model of the sort defined in the previous section.</S><S sid ="142" ssid = "46">A simple PCFG conditions rule probabilities on the left-hand side of the rule.</S><S sid ="323" ssid = "79">They trained and tested the SLM on a modified  more &quot;speech-like&quot; version of the Penn Treebank.</S><S sid ="0" ssid = "0">Probabilistic Top-Down Parsing and Language Modeling</S><S sid ="370" ssid = "126">We followed Chelba (2000) in using an LM weight of 16 for the lattice trigram.</S>
original cit marker offset is 0
new cit marker offset is 0



["'210'", "'142'", "'323'", "'0'", "'370'"]
'210'
'142'
'323'
'0'
'370'
['210', '142', '323', '0', '370']
parsed_discourse_facet ['method_citation']
<S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="104" ssid = "8">The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.</S><S sid ="154" ssid = "58">We use the relative frequency estimator  and smooth our production probabilities by interpolating the relative frequency estimates with those obtained by &quot;annotating&quot; less contextual information.</S><S sid ="0" ssid = "0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'", "'224'", "'104'", "'154'", "'0'"]
'134'
'224'
'104'
'154'
'0'
['134', '224', '104', '154', '0']
parsed_discourse_facet ['method_citation']
<S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="95" ssid = "53">For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].</S><S sid ="124" ssid = "28">Each distinct derivation path within the beam has a probability and a stack state associated with it.</S><S sid ="147" ssid = "51">One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).</S><S sid ="168" ssid = "72">For example  in Figure 2  constituent (NP-DT-NN) is simply NP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'302'", "'95'", "'124'", "'147'", "'168'"]
'302'
'95'
'124'
'147'
'168'
['302', '95', '124', '147', '168']
parsed_discourse_facet ['results_citation']
<S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S><S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S>
original cit marker offset is 0
new cit marker offset is 0



["'358'", "'306'", "'322'", "'224'", "'134'"]
'358'
'306'
'322'
'224'
'134'
['358', '306', '322', '224', '134']
parsed_discourse_facet ['method_citation']
<S sid ="289" ssid = "45">These results  achieved using very straightforward conditioning events and considering only the left context  are within one to four points of the best published Observed running time on Section 23 of the Penn Treebank  with the full conditional probability model and beam of 10-11  using one 300 MHz UltraSPARC processor and 256MB of RAM of a Sun Enterprise 450. accuracies cited above.'</S><S sid ="25" ssid = "13">A parser that is not left to right  but which has rooted derivations  e.g.  a headfirst parser  will be able to calculate generative joint probabilities for entire strings; however  it will not be able to calculate probabilities for each word conditioned on previously generated words  unless each derivation generates the words in the string in exactly the same order.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S sid ="141" ssid = "45">We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.</S><S sid ="22" ssid = "10">A left-toright parser whose derivations are not rooted  i.e.  with derivations that can consist of disconnected tree fragments  such as an LR or shift-reduce parser  cannot incrementally calculate the probability of each prefix string being generated by the probabilistic grammar  because their derivations include probability mass from unrooted structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'289'", "'25'", "'390'", "'141'", "'22'"]
'289'
'25'
'390'
'141'
'22'
['289', '25', '390', '141', '22']
parsed_discourse_facet ['method_citation']
<S sid ="224" ssid = "128">For each word position i  we have a separate priority queue H  of analyses with look-ahead w .</S><S sid ="306" ssid = "62">Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).</S><S sid ="358" ssid = "114">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'</S><S sid ="134" ssid = "38">The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'</S><S sid ="322" ssid = "78">Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'224'", "'306'", "'358'", "'134'", "'322'"]
'224'
'306'
'358'
'134'
'322'
['224', '306', '358', '134', '322']
parsed_discourse_facet ['method_citation']
<S sid ="147" ssid = "51">One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).</S><S sid ="302" ssid = "58">In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.</S><S sid ="278" ssid = "34">Section 22 (41 817 words  1 700 sentences) served as the development corpus  on which the parser was tested until stable versions were ready to run on the test data  to avoid developing the parser to fit the specific test data.</S><S sid ="141" ssid = "45">We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.</S><S sid ="390" ssid = "3">With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'147'", "'302'", "'278'", "'141'", "'390'"]
'147'
'302'
'278'
'141'
'390'
['147', '302', '278', '141', '390']
parsed_discourse_facet ['method_citation']
['Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).']
['Section 22 (41 817 words  1 700 sentences) served as the development corpus  on which the parser was tested until stable versions were ready to run on the test data  to avoid developing the parser to fit the specific test data.', 'One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).', "In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.", 'With a simple conditional probability model  and simple statistical search heuristics  we were able to find very accurate parses efficiently  and  as a side effect  were able to assign word probabilities that yield a perplexity improvement over previous results.', 'We will then present empirical results in two domains: one to compare with previous work in the parsing literature  and the other to compare with previous work using parsing for language modeling for speech recognition  in particular with the Chelba and Jelinek results mentioned above.']
['system', 'ROUGE-S*', 'Average_R:', '0.00182', '(95%-conf.int.', '0.00182', '-', '0.00182)']
['system', 'ROUGE-S*', 'Average_P:', '0.03000', '(95%-conf.int.', '0.03000', '-', '0.03000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00343', '(95%-conf.int.', '0.00343', '-', '0.00343)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4950', 'P:300', 'F:9']
['The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.']
['For example  in Figure 2  constituent (NP-DT-NN) is simply NP.', 'Each distinct derivation path within the beam has a probability and a stack state associated with it.', "In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.", 'For an interpolated (n + 1)-gram: Here P is the empirically observed relative frequency  and An is a function from Vn to [0  1].', 'One way to estimate the probabilities of these rules is to annotate the heads onto the constituent labels in the training corpus and simply count the number of times particular productions occur (relative frequency estimation).']
['system', 'ROUGE-S*', 'Average_R:', '0.02005', '(95%-conf.int.', '0.02005', '-', '0.02005)']
['system', 'ROUGE-S*', 'Average_P:', '0.09117', '(95%-conf.int.', '0.09117', '-', '0.09117)']
['system', 'ROUGE-S*', 'Average_F:', '0.03287', '(95%-conf.int.', '0.03287', '-', '0.03287)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:351', 'F:32']
['Since we do not know the POS for the word, we must sum the LAP for all POS For a PCFG G, a stack S = Ao An$ (which we will write AN and a look-ahead terminal item wi, we define the look-ahead probability as follows: We recursively estimate this with two empirically observed conditional probabilities for every nonterminal A,: 13(A, w,a) and P(A, c).']
['Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.', 'Consider  for example  the parse tree shown in (a) in Figure 1: the start symbol is St  which expands into an S. The S node expands into an NP followed by a VP.', 'For each word position i  we have a separate priority queue H  of analyses with look-ahead w .', 'Recall the discussion of the grammar models above  and our definition of the set of partial derivations Dw  with respect to a prefix string wil) (see Equations 2 and 7).', "The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00100', '(95%-conf.int.', '0.00100', '-', '0.00100)']
['system', 'ROUGE-S*', 'Average_P:', '0.01087', '(95%-conf.int.', '0.01087', '-', '0.01087)']
['system', 'ROUGE-S*', 'Average_F:', '0.00183', '(95%-conf.int.', '0.00183', '-', '0.00183)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:276', 'F:3']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['Thus  Chelba and Jelinek (1998a  1998b) also used a parser to help assign word probabilities  via the structured language model outlined in Section 3.2.', "The DARPA '93 HUB1 test setup consists of 213 utterances read from the Wall Street Journal  a total of 3 446 words.", 'For each word position i  we have a separate priority queue H  of analyses with look-ahead w .', 'For example  in Figure 2  constituent (NP-DT-NN) is simply NP.', "We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice  along with the acoustic and trigram scores.'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:91', 'F:0']
['In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.']
['They trained and tested the SLM on a modified  more &quot;speech-like&quot; version of the Penn Treebank.', 'We followed Chelba (2000) in using an LM weight of 16 for the lattice trigram.', 'A simple PCFG conditions rule probabilities on the left-hand side of the rule.', 'It uses a PCFG with a conditional probability model of the sort defined in the previous section.', 'Probabilistic Top-Down Parsing and Language Modeling']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:105', 'F:0']
['Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.']
["In the beam search approach outlined above  we can estimate the string's probability in the same manner  by summing the probabilities of the parses that the algorithm finds.", 'Consider  for example  the parse tree shown in (a) in Figure 1: the start symbol is St  which expands into an S. The S node expands into an NP followed by a VP.', 'A PCFG is a CFG with a probability assigned to each rule; specifically  each righthand side has a probability given the left-hand side of the rule.', 'The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.', "The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00456', '(95%-conf.int.', '0.00456', '-', '0.00456)']
['system', 'ROUGE-S*', 'Average_P:', '0.02989', '(95%-conf.int.', '0.02989', '-', '0.02989)']
['system', 'ROUGE-S*', 'Average_F:', '0.00791', '(95%-conf.int.', '0.00791', '-', '0.00791)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2850', 'P:435', 'F:13']
['A PCFG is a CFG with a probability assigned to each rule; specifically, each righthand side has a probability given the left-hand side of the rule.']
['Probabilistic Top-Down Parsing and Language Modeling', 'For each word position i  we have a separate priority queue H  of analyses with look-ahead w .', 'We use the relative frequency estimator  and smooth our production probabilities by interpolating the relative frequency estimates with those obtained by &quot;annotating&quot; less contextual information.', 'The algorithms both utilize a left-corner matrix  which can be calculated in closed form through matrix inversion.', "The parsers with the highest published broad-coverage parsing accuracy  which include Charniak (1997  2000)  Collins (1997  1999)  and Ratnaparkhi (1997)  all utilize simple and straightforward statistically based search heuristics  pruning the search-space quite dramatically!'"]
['system', 'ROUGE-S*', 'Average_R:', '0.00048', '(95%-conf.int.', '0.00048', '-', '0.00048)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00093', '(95%-conf.int.', '0.00093', '-', '0.00093)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:78', 'F:1']
['Three parse trees: (a) a complete parse tree; (b) a complete parse tree with an explicit stop symbol; and (c) a partial parse tree.', 'The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.', 'There will also be a brief review of previous work using syntactic information for language modeling, before we introduce our model in Section 4.']
['One approach to syntactic language modeling is to use this distribution directly as a language model.', 'A small recognition experiment also demonstrates the utility of the model.', 'The trigram model was also trained on Sections 00-20 of the C&J corpus.', 'In empirical trials  Goddeau used the top two stack entries to condition the word probability.', 'This paper will examine language modeling for speech recognition from a natural language processing point of view.']
['system', 'ROUGE-S*', 'Average_R:', '0.04146', '(95%-conf.int.', '0.04146', '-', '0.04146)']
['system', 'ROUGE-S*', 'Average_P:', '0.05397', '(95%-conf.int.', '0.05397', '-', '0.05397)']
['system', 'ROUGE-S*', 'Average_F:', '0.04690', '(95%-conf.int.', '0.04690', '-', '0.04690)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:630', 'F:34']
0.0285899996426 0.00867124989161 0.0117337498533





input/ref/Task1/P05-1013_vardha.csv
input/res/Task1/P05-1013.csv
parsing: input/ref/Task1/P05-1013_vardha.csv
 <S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
  <S sid="9" ssid="5">This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
  <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
    <S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
 <S sid="62" ssid="1">In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al., 2004) and English (Nivre and Scholz, 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="23" ssid="19">By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
    <S sid="96" ssid="7">With respect to exact match, the improvement is even more noticeable, which shows quite clearly that even if non-projective dependencies are rare on the token level, they are nevertheless important for getting the global syntactic structure correct.</S>
original cit marker offset is 0
new cit marker offset is 0



["'96'"]
'96'
['96']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
  <S sid="40" ssid="11">Even this may be nondeterministic, in case the graph contains several non-projective arcs whose lifts interact, but we use the following algorithm to construct a minimal projective transformation D0 = (W, A0) of a (nonprojective) dependency graph D = (W, A): The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right).</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
  <S sid="7" ssid="3">From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="14" ssid="10">While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
 <S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'"]
'49'
['49']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P05-1013.csv
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'48'", "'21'", "'36'"]
'50'
'75'
'48'
'21'
'36'
['50', '75', '48', '21', '36']
parsed_discourse_facet ['results_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="62" ssid = "1">In the experiments below  we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs 3 previously tested on Swedish (Nivre et al.  2004) and English (Nivre and Scholz  2004).</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="91" ssid = "2">Table 5 shows the overall parsing accuracy attained with the three different encoding schemes  compared to the baseline (no special arc labels) and to training directly on non-projective dependency graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'75'", "'62'", "'21'", "'91'"]
'48'
'75'
'62'
'21'
'91'
['48', '75', '62', '21', '91']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="77" ssid = "4">The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'77'", "'48'", "'21'"]
'50'
'75'
'77'
'48'
'21'
['50', '75', '77', '48', '21']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'71'", "'21'", "'45'"]
'50'
'75'
'71'
'21'
'45'
['50', '75', '71', '21', '45']
parsed_discourse_facet ['method_citation']
<S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'50'", "'48'", "'45'", "'21'"]
'75'
'50'
'48'
'45'
'21'
['75', '50', '48', '45', '21']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p↓.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'51'", "'36'", "'21'", "'48'"]
'50'
'51'
'36'
'21'
'48'
['50', '51', '36', '21', '48']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'48'", "'21'", "'36'", "'75'"]
'50'
'48'
'21'
'36'
'75'
['50', '48', '21', '36', '75']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p↓.</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'21'", "'51'", "'48'", "'75'"]
'50'
'21'
'51'
'48'
'75'
['50', '21', '51', '48', '75']
parsed_discourse_facet ['method_citation']
<S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="51" ssid = "22">In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p↓.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="23" ssid = "19">By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'", "'51'", "'21'", "'48'", "'23'"]
'36'
'51'
'21'
'48'
'23'
['36', '51', '21', '48', '23']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'48'", "'21'", "'45'"]
'50'
'75'
'48'
'21'
'45'
['50', '75', '48', '21', '45']
parsed_discourse_facet ['method_citation']
<S sid ="2" ssid = "2">We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.</S><S sid ="83" ssid = "10">It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.</S><S sid ="7" ssid = "3">From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S><S sid ="6" ssid = "2">However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.</S><S sid ="35" ssid = "6">The dependency graph in Figure 1 satisfies all the defining conditions above  but it fails to satisfy the condition ofprojectivity (Kahane et al.  1998): The arc connecting the head jedna (one) to the dependent Z (out-of) spans the token je (is)  which is not dominated by jedna.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'83'", "'7'", "'6'", "'35'"]
'2'
'83'
'7'
'6'
'35'
['2', '83', '7', '6', '35']
parsed_discourse_facet ['method_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'75'", "'45'", "'21'", "'20'"]
'48'
'75'
'45'
'21'
'20'
['48', '75', '45', '21', '20']
parsed_discourse_facet ['method_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S><S sid ="73" ssid = "12">More details on the memory-based prediction can be found in Nivre et al. (2004) and Nivre and Scholz (2004).</S><S sid ="20" ssid = "16">In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S><S sid ="106" ssid = "17">However  the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech  with a best performance of 73% UAS (Holan  2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'45'", "'73'", "'20'", "'106'"]
'48'
'45'
'73'
'20'
'106'
['48', '45', '73', '20', '106']
parsed_discourse_facet ['results_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S><S sid ="52" ssid = "23">Thus  the arc from je to jedna will be labeled 5b↓ (to indicate that there is a syntactic head below it).</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'48'", "'75'", "'36'", "'52'"]
'50'
'48'
'75'
'36'
'52'
['50', '48', '75', '36', '52']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="84" ssid = "11">In the second part of the experiment  we applied the inverse transformation based on breadth-first search under the three different encoding schemes.</S><S sid ="79" ssid = "6">In the first part of the experiment  dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'48'", "'21'", "'84'", "'79'"]
'50'
'48'
'21'
'84'
'79'
['50', '48', '21', '84', '79']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "8">For each token  three types of features may be taken into account: the word form; the part-of-speech assigned by an automatic tagger; and labels on previously assigned dependency arcs involving the token – the arc from its head and the arcs to its leftmost and rightmost dependent  respectively.</S><S sid ="23" ssid = "19">By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S><S sid ="64" ssid = "3">At each point during the derivation  the parser has a choice between pushing the next input token onto the stack – with or without adding an arc from the token on top of the stack to the token pushed – and popping a token from the stack – with or without adding an arc from the next input token to the token popped.</S><S sid ="59" ssid = "30">The details of the transformation procedure are slightly different depending on the encoding schemes: d↑h let the linear head be the syntactic head). target arc must have the form wl −→ wm; if no target arc is found  Head is used as backoff. must have the form wl −→ wm and no outgoing arcs of the form wm p'↓ −→ wo; no backoff.</S><S sid ="46" ssid = "17">In principle  it would be possible to encode the exact position of the syntactic head in the label of the arc from the linear head  but this would give a potentially infinite set of arc labels and would make the training of the parser very hard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'23'", "'64'", "'59'", "'46'"]
'69'
'23'
'64'
'59'
'46'
['69', '23', '64', '59', '46']
parsed_discourse_facet ['method_citation']
<S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="71" ssid = "10">For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="45" ssid = "16">In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'", "'75'", "'71'", "'21'", "'45'"]
'50'
'75'
'71'
'21'
'45'
['50', '75', '71', '21', '45']
parsed_discourse_facet ['method_citation']
<S sid ="107" ssid = "18">Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 15–25%  the total proportion of non-projective arcs is normally only 1–2%.</S><S sid ="101" ssid = "12">However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.</S><S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S><S sid ="87" ssid = "14">However  it can be noted that the results for the least informative encoding  Path  are almost comparable  while the third encoding  Head  gives substantially worse results for both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'14'", "'101'", "'89'", "'87'"]
'107'
'14'
'101'
'89'
'87'
['107', '14', '101', '89', '87']
parsed_discourse_facet ['method_citation']
<S sid ="89" ssid = "16">The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.</S><S sid ="14" ssid = "10">While the proportion of sentences containing non-projective dependencies is often 15–25%  the total proportion of non-projective arcs is normally only 1–2%.</S><S sid ="88" ssid = "15">We also see that the increase in the size of the label sets for Head and Head+Path is far below the theoretical upper bounds given in Table 1.</S><S sid ="28" ssid = "24">First  in section 4  we evaluate the graph transformation techniques in themselves  with data from the Prague Dependency Treebank and the Danish Dependency Treebank.</S><S sid ="103" ssid = "14">On the other hand  given that all schemes have similar parsing accuracy overall  this means that the Path scheme is the least likely to introduce errors on projective arcs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'", "'14'", "'88'", "'28'", "'103'"]
'89'
'14'
'88'
'28'
'103'
['89', '14', '88', '28', '103']
parsed_discourse_facet ['aim_citation', 'results_citation']
<S sid ="48" ssid = "19">To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.</S><S sid ="50" ssid = "21">Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP↑Sb (signifying an AuxP that has been lifted from a Sb).</S><S sid ="75" ssid = "2">The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Hajiˇc  1998).</S><S sid ="21" ssid = "17">First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.</S><S sid ="36" ssid = "7">As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi —* wk such that wi —*∗ wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'50'", "'75'", "'21'", "'36'"]
'48'
'50'
'75'
'21'
'36'
['48', '50', '75', '21', '36']
parsed_discourse_facet ['method_citation']
['The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\xcb\u2021c  1998).', 'In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.', 'In this paper  we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.']
['system', 'ROUGE-S*', 'Average_R:', '0.00207', '(95%-conf.int.', '0.00207', '-', '0.00207)']
['system', 'ROUGE-S*', 'Average_P:', '0.13889', '(95%-conf.int.', '0.13889', '-', '0.13889)']
['system', 'ROUGE-S*', 'Average_F:', '0.00408', '(95%-conf.int.', '0.00408', '-', '0.00408)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:36', 'F:5']
['In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al., 2004) and English (Nivre and Scholz, 2004).']
['By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.', u'In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p\u2193.', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00566', '(95%-conf.int.', '0.00566', '-', '0.00566)']
['system', 'ROUGE-S*', 'Average_P:', '0.11067', '(95%-conf.int.', '0.11067', '-', '0.11067)']
['system', 'ROUGE-S*', 'Average_F:', '0.01076', '(95%-conf.int.', '0.01076', '-', '0.01076)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4950', 'P:253', 'F:28']
['The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:36', 'F:0']
['As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', u'In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p\u2193.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00889', '(95%-conf.int.', '0.00889', '-', '0.00889)']
['system', 'ROUGE-S*', 'Average_P:', '0.06349', '(95%-conf.int.', '0.06349', '-', '0.06349)']
['system', 'ROUGE-S*', 'Average_F:', '0.01559', '(95%-conf.int.', '0.01559', '-', '0.01559)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2701', 'P:378', 'F:24']
['While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.']
['However  it can be noted that the results for the least informative encoding  Path  are almost comparable  while the third encoding  Head  gives substantially worse results for both data sets.', 'However  if we consider precision  recall and Fmeasure on non-projective dependencies only  as shown in Table 6  some differences begin to emerge.', 'The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.', 'Compared to related work on the recovery of long-distance dependencies in constituency-based parsing  our approach is similar to that of Dienes and Dubey (2003) in that the processing of non-local dependencies is partly integrated in the parsing process  via an extension of the set of syntactic categories  whereas most other approaches rely on postprocessing only.', u'While the proportion of sentences containing non-projective dependencies is often 15\u201325%  the total proportion of non-projective arcs is normally only 1\u20132%.']
['system', 'ROUGE-S*', 'Average_R:', '0.02444', '(95%-conf.int.', '0.02444', '-', '0.02444)']
['system', 'ROUGE-S*', 'Average_P:', '0.72527', '(95%-conf.int.', '0.72527', '-', '0.72527)']
['system', 'ROUGE-S*', 'Average_F:', '0.04728', '(95%-conf.int.', '0.04728', '-', '0.04728)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2701', 'P:91', 'F:66']
['The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.']
['On the other hand  given that all schemes have similar parsing accuracy overall  this means that the Path scheme is the least likely to introduce errors on projective arcs.', 'We also see that the increase in the size of the label sets for Head and Head+Path is far below the theoretical upper bounds given in Table 1.', 'The increase is generally higher for PDT than for DDT  which indicates a greater diversity in non-projective constructions.', 'First  in section 4  we evaluate the graph transformation techniques in themselves  with data from the Prague Dependency Treebank and the Danish Dependency Treebank.', u'While the proportion of sentences containing non-projective dependencies is often 15\u201325%  the total proportion of non-projective arcs is normally only 1\u20132%.']
['system', 'ROUGE-S*', 'Average_R:', '0.03331', '(95%-conf.int.', '0.03331', '-', '0.03331)']
['system', 'ROUGE-S*', 'Average_P:', '0.07692', '(95%-conf.int.', '0.07692', '-', '0.07692)']
['system', 'ROUGE-S*', 'Average_F:', '0.04649', '(95%-conf.int.', '0.04649', '-', '0.04649)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:741', 'F:57']
['The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:36', 'F:0']
['The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00086', '(95%-conf.int.', '0.00086', '-', '0.00086)']
['system', 'ROUGE-S*', 'Average_P:', '0.08333', '(95%-conf.int.', '0.08333', '-', '0.08333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00170', '(95%-conf.int.', '0.00170', '-', '0.00170)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:36', 'F:3']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00041', '(95%-conf.int.', '0.00041', '-', '0.00041)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00079', '(95%-conf.int.', '0.00079', '-', '0.00079)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:105', 'F:1']
['The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
['The Danish Dependency Treebank (DDT) comprises about 100K words of text selected from the Danish PAROLE corpus  with annotation of primary and secondary dependencies (Kromann  2003).', u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2926', 'P:36', 'F:0']
['This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004).']
['Table 5 shows the overall parsing accuracy attained with the three different encoding schemes  compared to the baseline (no special arc labels) and to training directly on non-projective dependency graphs.', u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\xcb\u2021c  1998).', 'In the experiments below  we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs 3 previously tested on Swedish (Nivre et al.  2004) and English (Nivre and Scholz  2004).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.01122', '(95%-conf.int.', '0.01122', '-', '0.01122)']
['system', 'ROUGE-S*', 'Average_P:', '0.10847', '(95%-conf.int.', '0.10847', '-', '0.10847)']
['system', 'ROUGE-S*', 'Average_F:', '0.02033', '(95%-conf.int.', '0.02033', '-', '0.02033)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3655', 'P:378', 'F:41']
['Even this may be nondeterministic, in case the graph contains several non-projective arcs whose lifts interact, but we use the following algorithm to construct a minimal projective transformation D0 = (W, A0) of a (nonprojective) dependency graph D = (W, A): The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right).']
['In principle  it would be possible to encode the exact position of the syntactic head in the label of the arc from the linear head  but this would give a potentially infinite set of arc labels and would make the training of the parser very hard.', u'For each token  three types of features may be taken into account: the word form; the part-of-speech assigned by an automatic tagger; and labels on previously assigned dependency arcs involving the token \u2013 the arc from its head and the arcs to its leftmost and rightmost dependent  respectively.', u"The details of the transformation procedure are slightly different depending on the encoding schemes: d\u2191h let the linear head be the syntactic head). target arc must have the form wl \u2212\u2192 wm; if no target arc is found  Head is used as backoff. must have the form wl \u2212\u2192 wm and no outgoing arcs of the form wm p'\u2193 \u2212\u2192 wo; no backoff.", u'At each point during the derivation  the parser has a choice between pushing the next input token onto the stack \u2013 with or without adding an arc from the token on top of the stack to the token pushed \u2013 and popping a token from the stack \u2013 with or without adding an arc from the next input token to the token popped.', 'By applying an inverse transformation to the output of the parser  arcs with non-standard labels can be lowered to their proper place in the dependency graph  giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.']
['system', 'ROUGE-S*', 'Average_R:', '0.00417', '(95%-conf.int.', '0.00417', '-', '0.00417)']
['system', 'ROUGE-S*', 'Average_P:', '0.07527', '(95%-conf.int.', '0.07527', '-', '0.07527)']
['system', 'ROUGE-S*', 'Average_F:', '0.00791', '(95%-conf.int.', '0.00791', '-', '0.00791)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:8385', 'P:465', 'F:35']
['In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00287', '(95%-conf.int.', '0.00287', '-', '0.00287)']
['system', 'ROUGE-S*', 'Average_P:', '0.10989', '(95%-conf.int.', '0.10989', '-', '0.10989)']
['system', 'ROUGE-S*', 'Average_F:', '0.00559', '(95%-conf.int.', '0.00559', '-', '0.00559)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:91', 'F:10']
['In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00287', '(95%-conf.int.', '0.00287', '-', '0.00287)']
['system', 'ROUGE-S*', 'Average_P:', '0.10989', '(95%-conf.int.', '0.10989', '-', '0.10989)']
['system', 'ROUGE-S*', 'Average_F:', '0.00559', '(95%-conf.int.', '0.00559', '-', '0.00559)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3486', 'P:91', 'F:10']
['From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.']
[u'The Prague Dependency Treebank (PDT) consists of more than 1M words of newspaper text  annotated on three levels  the morphological  analytical and tectogrammatical levels (Haji\u02c7c  1998).', 'In order to facilitate this task  we extend the set of arc labels to encode information about lifting operations.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'For robustness reasons  the parser may output a set of dependency trees instead of a single tree. most dependent of the next input token  dependency type features are limited to tokens on the stack.', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3003', 'P:136', 'F:0']
['The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.']
['To explore this tradeoff  we have performed experiments with three different encoding schemes (plus a baseline)  which are described schematically in Table 1.', u'In the second scheme  Head+Path  we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p\u2193.', u'Using this encoding scheme  the arc from je to Z in Figure 2 would be assigned the label AuxP\u2191Sb (signifying an AuxP that has been lifted from a Sb).', 'First  the training data for the parser is projectivized by applying a minimal number of lifting operations (Kahane et al.  1998) and encoding information about these lifts in arc labels.', u'As observed by Kahane et al. (1998)  any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation  which replaces each non-projective arc wj wk by a projective arc wi \u2014* wk such that wi \u2014*\u2217 wj holds in the original graph.']
['system', 'ROUGE-S*', 'Average_R:', '0.00120', '(95%-conf.int.', '0.00120', '-', '0.00120)']
['system', 'ROUGE-S*', 'Average_P:', '0.02941', '(95%-conf.int.', '0.02941', '-', '0.02941)']
['system', 'ROUGE-S*', 'Average_F:', '0.00231', '(95%-conf.int.', '0.00231', '-', '0.00231)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3321', 'P:136', 'F:4']
['By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.']
['It is worth noting that  although nonprojective constructions are less frequent in DDT than in PDT  they seem to be more deeply nested  since only about 80% can be projectivized with a single lift  while almost 95% of the non-projective arcs in PDT only require a single lift.', 'We show how a datadriven deterministic dependency parser  in itself restricted to projective structures  can be combined with graph transformation techniques to produce non-projective structures.', 'The dependency graph in Figure 1 satisfies all the defining conditions above  but it fails to satisfy the condition ofprojectivity (Kahane et al.  1998): The arc connecting the head jedna (one) to the dependent Z (out-of) spans the token je (is)  which is not dominated by jedna.', 'From the point of view of computational implementation this can be problematic  since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.', 'However  this argument is only plausible if the formal framework allows non-projective dependency structures  i.e. structures where a head and its dependents may correspond to a discontinuous constituent.']
['system', 'ROUGE-S*', 'Average_R:', '0.01634', '(95%-conf.int.', '0.01634', '-', '0.01634)']
['system', 'ROUGE-S*', 'Average_P:', '0.12903', '(95%-conf.int.', '0.12903', '-', '0.12903)']
['system', 'ROUGE-S*', 'Average_F:', '0.02901', '(95%-conf.int.', '0.02901', '-', '0.02901)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3916', 'P:496', 'F:64']
0.104120587623 0.00672411760751 0.0116135293434





input/ref/Task1/W99-0613_aakansha.csv
input/res/Task1/W99-0613.csv
parsing: input/ref/Task1/W99-0613_aakansha.csv
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="30">Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'"]
'137'
['137']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="12">2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="4">The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="12">But we will show that the use of unlabeled data can drastically reduce the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
    <S sid="237" ssid="4">The numbers falling into the location, person, organization categories were 186, 289 and 402 respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'", "'237'"]
'236'
'237'
['236', '237']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
    <S sid="10" ssid="4">The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'10'"]
'9'
'10'
['9', '10']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S sid="139" ssid="6">This section describes AdaBoost, which is the basis for the CoBoost algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'39'"]
'137'
'39'
['137', '39']
parsed_discourse_facet ['method_citation']
<S sid="26" ssid="20">We present two algorithms.</S>
    <S sid="27" ssid="21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'27'"]
'26'
'27'
['26', '27']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="12">But we will show that the use of unlabeled data can drastically reduce the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="2">Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S>
    <S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'9'"]
'8'
'9'
['8', '9']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'228'", "'0'", "'75'", "'77'"]
'142'
'228'
'0'
'75'
'77'
['142', '228', '0', '75', '77']
parsed_discourse_facet ['results_citation']
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="102" ssid = "35">(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'77'", "'102'", "'52'", "'75'"]
'142'
'77'
'102'
'52'
'75'
['142', '77', '102', '52', '75']
parsed_discourse_facet ['method_citation']
<S sid ="225" ssid = "4">We again assume a training set of n examples {x1 . xri} where the first m examples have labels {y1 ... yin}  and the last (n — m) examples are unlabeled.</S><S sid ="203" ssid = "70">Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).</S><S sid ="140" ssid = "7">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S><S sid ="79" ssid = "12">2 We now introduce a new algorithm for learning from unlabeled examples  which we will call DLCoTrain (DL stands for decision list  the term Cotrain is taken from (Blum and Mitchell 98)).</S><S sid ="31" ssid = "25">Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'225'", "'203'", "'140'", "'79'", "'31'"]
'225'
'203'
'140'
'79'
'31'
['225', '203', '140', '79', '31']
parsed_discourse_facet ['method_citation']
<S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="102" ssid = "35">(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S>
original cit marker offset is 0
new cit marker offset is 0



["'142'", "'102'", "'77'", "'52'", "'228'"]
'142'
'102'
'77'
'52'
'228'
['142', '102', '77', '52', '228']
parsed_discourse_facet ['method_citation']
<S sid ="236" ssid = "3">We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.</S><S sid ="207" ssid = "74">Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S><S sid ="166" ssid = "33">The first m pairs have labels yi  whereas for i = m + 1    n the pairs are unlabeled.</S><S sid ="84" ssid = "17">For each label (Per s on  organization and Location)  take the n contextual rules with the highest value of Count' (x) whose unsmoothed3 strength is above some threshold pmin.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'", "'207'", "'248'", "'166'", "'84'"]
'236'
'207'
'248'
'166'
'84'
['236', '207', '248', '166', '84']
parsed_discourse_facet ['method_citation']
<S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'228'", "'142'", "'75'", "'77'", "'0'"]
'228'
'142'
'75'
'77'
'0'
['228', '142', '75', '77', '0']
parsed_discourse_facet ['method_citation']
<S sid ="207" ssid = "74">Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.</S><S sid ="248" ssid = "15">With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.</S><S sid ="86" ssid = "19">Thus at each iteration the method induces at most n x k rules  where k is the number of possible labels (k = 3 in the experiments in this paper). step 3.</S><S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="10" ssid = "4">The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'248'", "'86'", "'204'", "'10'"]
'207'
'248'
'86'
'204'
'10'
['207', '248', '86', '204', '10']
parsed_discourse_facet ['method_citation']
<S sid ="202" ssid = "69">The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels  and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.</S><S sid ="124" ssid = "57">In particular  it may not be possible to learn functions fi (x f2(x2 t) for i = m + 1...n: either because there is some noise in the data  or because it is just not realistic to expect to learn perfect classifiers given the features used for representation.</S><S sid ="121" ssid = "54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).</S><S sid ="50" ssid = "4">It is a sequence of proper nouns within an NP; its last word Cooper is the head of the NP; and the NP has an appositive modifier (a vice president at S.&P.) whose head is a singular noun (president).</S><S sid ="25" ssid = "19">The unlabeled data gives many such &quot;hints&quot; that two features should predict the same label  and these hints turn out to be surprisingly useful when building a classifier.</S>
original cit marker offset is 0
new cit marker offset is 0



["'202'", "'124'", "'121'", "'50'", "'25'"]
'202'
'124'
'121'
'50'
'25'
['202', '124', '121', '50', '25']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="12" ssid = "6">The approach uses both spelling and contextual rules.</S><S sid ="150" ssid = "17">Pseudo-code describing the generalized boosting algorithm of Schapire and Singer is given in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'52'", "'75'", "'12'", "'150'"]
'157'
'52'
'75'
'12'
'150'
['157', '52', '75', '12', '150']
parsed_discourse_facet ['method_citation']
<S sid ="63" ssid = "17">In this case nonalpha is the string formed by removing all upper/lower case letters from the spelling (e.g.  for Thomas E. Petry nonalpha= .</S><S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="47" ssid = "1">971 746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'52'", "'157'", "'77'", "'47'"]
'63'
'52'
'157'
'77'
'47'
['63', '52', '157', '77', '47']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="239" ssid = "6">Of these cases  38 were temporal expressions (either a day of the week or month of the year).</S><S sid ="93" ssid = "26">To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.</S><S sid ="249" ssid = "16">The test accuracy more or less asymptotes.</S><S sid ="197" ssid = "64">Note that in our formalism a weakhypothesis can abstain.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'239'", "'93'", "'249'", "'197'"]
'204'
'239'
'93'
'249'
'197'
['204', '239', '93', '249', '197']
parsed_discourse_facet ['method_citation']
<S sid ="204" ssid = "71">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid ="75" ssid = "8">Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).</S><S sid ="228" ssid = "7">Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).</S><S sid ="0" ssid = "0">Unsupervised Models for Named Entity Classification Collins</S><S sid ="93" ssid = "26">To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'", "'75'", "'228'", "'0'", "'93'"]
'204'
'75'
'228'
'0'
'93'
['204', '75', '228', '0', '93']
parsed_discourse_facet ['method_citation']
<S sid ="52" ssid = "6">The NP is a complement to a preposition  which is the head of a PP.</S><S sid ="157" ssid = "24">Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.</S><S sid ="77" ssid = "10">In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.</S><S sid ="193" ssid = "60">In Input: {(x1 i  Initialize: Vi  j : e(xi) = 0.</S><S sid ="142" ssid = "9">The input to AdaBoost is a set of training examples ((xi   yi)    (x„.„ yrn)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'", "'157'", "'77'", "'193'", "'142'"]
'52'
'157'
'77'
'193'
'142'
['52', '157', '77', '193', '142']
parsed_discourse_facet ['results_citation']
['The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.']
['2 We now introduce a new algorithm for learning from unlabeled examples  which we will call DLCoTrain (DL stands for decision list  the term Cotrain is taken from (Blum and Mitchell 98)).', "Our first algorithm is similar to Yarowsky's  but with some important modifications motivated by (Blum and Mitchell 98).", u'We again assume a training set of n examples {x1 . xri} where the first m examples have labels {y1 ... yin}  and the last (n \xe2\u20ac\u201d m) examples are unlabeled.', 'Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).', 'AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.']
['system', 'ROUGE-S*', 'Average_R:', '0.00288', '(95%-conf.int.', '0.00288', '-', '0.00288)']
['system', 'ROUGE-S*', 'Average_P:', '0.16667', '(95%-conf.int.', '0.16667', '-', '0.16667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00567', '(95%-conf.int.', '0.00567', '-', '0.00567)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2080', 'P:36', 'F:6']
['This section describes AdaBoost, which is the basis for the CoBoost algorithm.', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.']
['Pseudo-code describing the generalized boosting algorithm of Schapire and Singer is given in Figure 1.', 'Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).', 'The approach uses both spelling and contextual rules.', 'Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.', 'The NP is a complement to a preposition  which is the head of a PP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00159', '(95%-conf.int.', '0.00159', '-', '0.00159)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00272', '(95%-conf.int.', '0.00272', '-', '0.00272)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:105', 'F:1']
['The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.']
["For each label (Per s on  organization and Location)  take the n contextual rules with the highest value of Count' (x) whose unsmoothed3 strength is above some threshold pmin.", 'We chose one of four labels for each example: location  person  organization  or noise where the noise category was used for items that were outside the three categories.', 'The first m pairs have labels yi  whereas for i = m + 1    n the pairs are unlabeled.', 'With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.', 'Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.']
['system', 'ROUGE-S*', 'Average_R:', '0.00157', '(95%-conf.int.', '0.00157', '-', '0.00157)']
['system', 'ROUGE-S*', 'Average_P:', '0.02564', '(95%-conf.int.', '0.02564', '-', '0.02564)']
['system', 'ROUGE-S*', 'Average_F:', '0.00296', '(95%-conf.int.', '0.00296', '-', '0.00296)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:78', 'F:2']
['This paper discusses the use of unlabeled examples for the problem of named entity classification.', 'Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.']
['In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.', u'The input to AdaBoost is a set of training examples ((xi   yi)    (x\u201e.\u201e yrn)).', 'In Input: {(x1 i  Initialize: Vi  j : e(xi) = 0.', 'Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.', 'The NP is a complement to a preposition  which is the head of a PP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00168', '(95%-conf.int.', '0.00168', '-', '0.00168)']
['system', 'ROUGE-S*', 'Average_P:', '0.00395', '(95%-conf.int.', '0.00395', '-', '0.00395)']
['system', 'ROUGE-S*', 'Average_F:', '0.00236', '(95%-conf.int.', '0.00236', '-', '0.00236)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:253', 'F:1']
['We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.', 'The numbers falling into the location, person, organization categories were 186, 289 and 402 respectively.']
['The task is to learn a function from an input string (proper name) to its type  which we will assume to be one of the categories Person  Organization  or Location.', 'Thus at each iteration the method induces at most n x k rules  where k is the number of possible labels (k = 3 in the experiments in this paper). step 3.', 'With each iteration more examples are assigned labels by both classifiers  while a high level of agreement (> 94%) is maintained between them.', 'Pseudo-labels are formed by taking seed labels on the labeled examples  and the output of the fixed classifier on the unlabeled examples.', 'In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.']
['system', 'ROUGE-S*', 'Average_R:', '0.00627', '(95%-conf.int.', '0.00627', '-', '0.00627)']
['system', 'ROUGE-S*', 'Average_P:', '0.05848', '(95%-conf.int.', '0.05848', '-', '0.05848)']
['system', 'ROUGE-S*', 'Average_F:', '0.01132', '(95%-conf.int.', '0.01132', '-', '0.01132)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:171', 'F:10']
['We present two algorithms.', 'The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).']
['In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.', '971 746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).', 'In this case nonalpha is the string formed by removing all upper/lower case letters from the spelling (e.g.  for Thomas E. Petry nonalpha= .', 'Zt can be written as follows Following the derivation of Schapire and Singer  providing that W+ > W_  Equ.', 'The NP is a complement to a preposition  which is the head of a PP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:55', 'F:0']
['This paper discusses the use of unlabeled examples for the problem of named entity classification.', 'The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.']
['They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page  and other pages pointing to the page).', 'The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels  and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.', 'In particular  it may not be possible to learn functions fi (x f2(x2 t) for i = m + 1...n: either because there is some noise in the data  or because it is just not realistic to expect to learn perfect classifiers given the features used for representation.', 'It is a sequence of proper nouns within an NP; its last word Cooper is the head of the NP; and the NP has an appositive modifier (a vice president at S.&P.) whose head is a singular noun (president).', 'The unlabeled data gives many such &quot;hints&quot; that two features should predict the same label  and these hints turn out to be surprisingly useful when building a classifier.']
['system', 'ROUGE-S*', 'Average_R:', '0.00571', '(95%-conf.int.', '0.00571', '-', '0.00571)']
['system', 'ROUGE-S*', 'Average_P:', '0.07143', '(95%-conf.int.', '0.07143', '-', '0.07143)']
['system', 'ROUGE-S*', 'Average_F:', '0.01057', '(95%-conf.int.', '0.01057', '-', '0.01057)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:210', 'F:15']
['Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.']
['In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.', '(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different &quot;views&quot; of an example.', 'Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).', u'The input to AdaBoost is a set of training examples ((xi   yi)    (x\u201e.\u201e yrn)).', 'The NP is a complement to a preposition  which is the head of a PP.']
['system', 'ROUGE-S*', 'Average_R:', '0.00097', '(95%-conf.int.', '0.00097', '-', '0.00097)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00175', '(95%-conf.int.', '0.00175', '-', '0.00175)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:105', 'F:1']
['(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.']
['Unsupervised Models for Named Entity Classification Collins', 'Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).', 'To measure the contribution of each modification  a third  intermediate algorithm  Yarowsky-cautious was also tested.', 'In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.', 'Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:630', 'F:0']
['But we will show that the use of unlabeled data can drastically reduce the need for supervision.']
['In this paper k = 3 (the three labels are person  organization  location)  and we set a = 0.1.', 'Unsupervised Models for Named Entity Classification Collins', 'Alternatively  h can be thought of as defining a decision list of rules x y ranked by their &quot;strength&quot; h(x  y).', u'The input to AdaBoost is a set of training examples ((xi   yi)    (x\u201e.\u201e yrn)).', 'Training under this model involves estimation of parameter values for P(y)  P(m) and P(x I y).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:15', 'F:0']
0.0345209996548 0.00206699997933 0.00373499996265





input/ref/Task1/P08-1043_swastika.csv
input/res/Task1/P08-1043.csv
parsing: input/ref/Task1/P08-1043_swastika.csv
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['result_citation']
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="37">3An English sentence with ambiguous PoS assignment can be trivially represented as a lattice similar to our own, where every pair of consecutive nodes correspond to a word, and every possible PoS assignment for this word is a connecting arc.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="163" ssid="1">The accuracy results for segmentation, tagging and parsing using our different models and our standard data split are summarized in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



['163']
163
['163']
parsed_discourse_facet ['result_citation']
    <S sid="100" ssid="32">This means that the rules in our grammar are of two kinds: (a) syntactic rules relating nonterminals to a sequence of non-terminals and/or PoS tags, and (b) lexical rules relating PoS tags to lattice arcs (lexemes).</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['method_citation']
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['result_citation']
<S sid="188" ssid="2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



['188']
188
['188']
parsed_discourse_facet ['result_citation']
<S sid="86" ssid="18">A morphological analyzer M : W&#8212;* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S>
original cit marker offset is 0
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['result_citation']
<S sid="97" ssid="29">Thus our proposed model is a proper model assigning probability mass to all (7r, L) pairs, where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S>
original cit marker offset is 0
new cit marker offset is 0



['97']
97
['97']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="31" ssid = "10">Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'92'", "'86'", "'192'", "'31'"]
'183'
'92'
'86'
'192'
'31'
['183', '92', '86', '192', '31']
parsed_discourse_facet ['results_citation']
<S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="173" ssid = "11">The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter (α) which is tuned separately for each of the tasks.</S><S sid ="77" ssid = "9">Segments with the same surface form but different PoS tags are treated as different lexemes  and are represented as separate arcs (e.g. the two arcs labeled neim from node 6 to 7).</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'192'", "'173'", "'77'", "'133'"]
'183'
'192'
'173'
'77'
'133'
['183', '192', '173', '77', '133']
parsed_discourse_facet ['method_citation']
<S sid ="141" ssid = "19">This analyzer setting is similar to that of (Cohen and Smith  2007)  and models using it are denoted nohsp  Parser and Grammar We used BitPar (Schmid  2004)  an efficient general purpose parser 10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.</S><S sid ="89" ssid = "21">Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree π s.t.</S><S sid ="51" ssid = "9">Tsarfaty (2006) used a morphological analyzer (Segal  2000)  a PoS tagger (Bar-Haim et al.  2005)  and a general purpose parser (Schmid  2000) in an integrated framework in which morphological and syntactic components interact to share information  leading to improved performance on the joint task.</S><S sid ="156" ssid = "34">To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S><S sid ="82" ssid = "14">In sequential tagging models such as (Adler and Elhadad  2006; Bar-Haim et al.  2007; Smith et al.  2005) weights are assigned according to a language model The input for the joint task is a sequence W = w1  ...   wn of space-delimited tokens.</S>
original cit marker offset is 0
new cit marker offset is 0



["'141'", "'89'", "'51'", "'156'", "'82'"]
'141'
'89'
'51'
'156'
'82'
['141', '89', '51', '156', '82']
parsed_discourse_facet ['method_citation']
<S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'183'", "'86'", "'0'", "'92'"]
'192'
'183'
'86'
'0'
'92'
['192', '183', '86', '0', '92']
parsed_discourse_facet ['method_citation']
<S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'183'", "'0'", "'86'", "'92'"]
'192'
'183'
'0'
'86'
'92'
['192', '183', '0', '86', '92']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="89" ssid = "21">Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree π s.t.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'86'", "'192'", "'89'", "'92'"]
'183'
'86'
'192'
'89'
'92'
['183', '86', '192', '89', '92']
parsed_discourse_facet ['method_citation']
<S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'86'", "'57'", "'192'", "'183'"]
'92'
'86'
'57'
'192'
'183'
['92', '86', '57', '192', '183']
parsed_discourse_facet ['method_citation']
<S sid ="31" ssid = "10">Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="186" ssid = "24">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'", "'92'", "'133'", "'0'", "'186'"]
'31'
'92'
'133'
'0'
'186'
['31', '92', '133', '0', '186']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'92'", "'192'", "'183'", "'133'"]
'86'
'92'
'192'
'183'
'133'
['86', '92', '192', '183', '133']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'192'", "'183'", "'0'", "'92'"]
'86'
'192'
'183'
'0'
'92'
['86', '192', '183', '0', '92']
parsed_discourse_facet ['method_citation']
<S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="123" ssid = "1">Previous work on morphological and syntactic disambiguation in Hebrew used different sets of data  different splits  differing annotation schemes  and different evaluation measures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'57'", "'192'", "'86'", "'123'"]
'92'
'57'
'192'
'86'
'123'
['92', '57', '192', '86', '123']
parsed_discourse_facet ['method_citation']
<S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S><S sid ="69" ssid = "1">We represent all morphological analyses of a given utterance using a lattice structure.</S><S sid ="14" ssid = "10">The input for the segmentation task is however highly ambiguous for Semitic languages  and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al.  2007; Adler and Elhadad  2006).</S><S sid ="113" ssid = "45">The remaining arcs are marked OOV.</S><S sid ="50" ssid = "8">The joint morphological and syntactic hypothesis was first discussed in (Tsarfaty  2006; Tsarfaty and Sima’an  2004) and empirically explored in (Tsarfaty  2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'", "'69'", "'14'", "'113'", "'50'"]
'57'
'69'
'14'
'113'
'50'
['57', '69', '14', '113', '50']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="57" ssid = "4">For brevity we omit the segments from the analysis  and so analysis of the form “fmnh” as f/REL mnh/VB is represented simply as REL VB.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'92'", "'192'", "'183'", "'57'"]
'86'
'92'
'192'
'183'
'57'
['86', '92', '192', '183', '57']
parsed_discourse_facet ['results_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'192'", "'0'", "'92'", "'183'"]
'86'
'192'
'0'
'92'
'183'
['86', '192', '0', '92', '183']
parsed_discourse_facet ['method_citation']
<S sid ="86" ssid = "18">A morphological analyzer M : W—* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="0" ssid = "0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'", "'183'", "'192'", "'92'", "'0'"]
'86'
'183'
'192'
'92'
'0'
['86', '183', '192', '92', '0']
parsed_discourse_facet ['method_citation']
<S sid ="192" ssid = "6">Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.</S><S sid ="92" ssid = "24">This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.</S><S sid ="183" ssid = "21">Cohen and Smith approach this by introducing the α hyperparameter  which performs best when optimized independently for each sentence (cf.</S><S sid ="133" ssid = "11">Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S><S sid ="18" ssid = "14">Cohen and Smith (2007) followed up on these results and proposed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'92'", "'183'", "'133'", "'18'"]
'192'
'92'
'183'
'133'
'18'
['192', '92', '183', '133', '18']
parsed_discourse_facet ['method_citation']
['Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.']
[u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.', 'Inflectional features marking pronominal elements may be attached to different kinds of categories marking their pronominal complements.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', u'A morphological analyzer M : W\u2014* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:153', 'F:0']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
[u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', u'A morphological analyzer M : W\u2014* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.01234', '(95%-conf.int.', '0.01234', '-', '0.01234)']
['system', 'ROUGE-S*', 'Average_P:', '0.14167', '(95%-conf.int.', '0.14167', '-', '0.14167)']
['system', 'ROUGE-S*', 'Average_F:', '0.02270', '(95%-conf.int.', '0.02270', '-', '0.02270)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:120', 'F:17']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
[u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', u'A morphological analyzer M : W\u2014* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.01234', '(95%-conf.int.', '0.01234', '-', '0.01234)']
['system', 'ROUGE-S*', 'Average_P:', '0.14167', '(95%-conf.int.', '0.14167', '-', '0.14167)']
['system', 'ROUGE-S*', 'Average_F:', '0.02270', '(95%-conf.int.', '0.02270', '-', '0.02270)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:120', 'F:17']
['Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.']
['In sequential tagging models such as (Adler and Elhadad  2006; Bar-Haim et al.  2007; Smith et al.  2005) weights are assigned according to a language model The input for the joint task is a sequence W = w1  ...   wn of space-delimited tokens.', u'Each connected path (l1 ... lk) E L corresponds to one morphological segmentation possibility of W. The Parser Given a sequence of input tokens W = w1 ... wn and a morphological analyzer  we look for the most probable parse tree \xcf\u20ac s.t.', 'Tsarfaty (2006) used a morphological analyzer (Segal  2000)  a PoS tagger (Bar-Haim et al.  2005)  and a general purpose parser (Schmid  2000) in an integrated framework in which morphological and syntactic components interact to share information  leading to improved performance on the joint task.', 'To evaluate the performance on the segmentation task  we report SEG  the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).', 'This analyzer setting is similar to that of (Cohen and Smith  2007)  and models using it are denoted nohsp  Parser and Grammar We used BitPar (Schmid  2004)  an efficient general purpose parser 10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.']
['system', 'ROUGE-S*', 'Average_R:', '0.00142', '(95%-conf.int.', '0.00142', '-', '0.00142)']
['system', 'ROUGE-S*', 'Average_P:', '0.09150', '(95%-conf.int.', '0.09150', '-', '0.09150)']
['system', 'ROUGE-S*', 'Average_F:', '0.00279', '(95%-conf.int.', '0.00279', '-', '0.00279)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:9870', 'P:153', 'F:14']
['Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['Segments with the same surface form but different PoS tags are treated as different lexemes  and are represented as separate arcs (e.g. the two arcs labeled neim from node 6 to 7).', u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.', u'The addition of vertical markovization enables non-pruned models to outperform all previously reported re12Cohen and Smith (2007) make use of a parameter (\u03b1) which is tuned separately for each of the tasks.', 'Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00161', '(95%-conf.int.', '0.00161', '-', '0.00161)']
['system', 'ROUGE-S*', 'Average_P:', '0.01058', '(95%-conf.int.', '0.01058', '-', '0.01058)']
['system', 'ROUGE-S*', 'Average_F:', '0.00279', '(95%-conf.int.', '0.00279', '-', '0.00279)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2485', 'P:378', 'F:4']
['3An English sentence with ambiguous PoS assignment can be trivially represented as a lattice similar to our own, where every pair of consecutive nodes correspond to a word, and every possible PoS assignment for this word is a connecting arc.']
[u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.', u'For brevity we omit the segments from the analysis  and so analysis of the form \u201cfmnh\u201d as f/REL mnh/VB is represented simply as REL VB.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', u'A morphological analyzer M : W\u2014* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00468', '(95%-conf.int.', '0.00468', '-', '0.00468)']
['system', 'ROUGE-S*', 'Average_P:', '0.04211', '(95%-conf.int.', '0.04211', '-', '0.04211)']
['system', 'ROUGE-S*', 'Average_F:', '0.00842', '(95%-conf.int.', '0.00842', '-', '0.00842)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:190', 'F:8']
['Thus our proposed model is a proper model assigning probability mass to all (7r, L) pairs, where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.']
['Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.', u'Cohen and Smith approach this by introducing the \xce\xb1 hyperparameter  which performs best when optimized independently for each sentence (cf.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', 'Cohen and Smith (2007) followed up on these results and proposed a system for joint inference of morphological and syntactic structures using factored models each designed and trained on its own.', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00154', '(95%-conf.int.', '0.00154', '-', '0.00154)']
['system', 'ROUGE-S*', 'Average_P:', '0.01754', '(95%-conf.int.', '0.01754', '-', '0.01754)']
['system', 'ROUGE-S*', 'Average_F:', '0.00282', '(95%-conf.int.', '0.00282', '-', '0.00282)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1953', 'P:171', 'F:3']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
[u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', u'A morphological analyzer M : W\u2014* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.01234', '(95%-conf.int.', '0.01234', '-', '0.01234)']
['system', 'ROUGE-S*', 'Average_P:', '0.14167', '(95%-conf.int.', '0.14167', '-', '0.14167)']
['system', 'ROUGE-S*', 'Average_F:', '0.02270', '(95%-conf.int.', '0.02270', '-', '0.02270)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:120', 'F:17']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['Morphological Analyzer Ideally  we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.', u'Cohen and Smith approach this by introducing the \u03b1 hyperparameter  which performs best when optimized independently for each sentence (cf.', 'This is akin to PoS tags sequences induced by different parses in the setup familiar from English and explored in e.g.', u'A morphological analyzer M : W\u2014* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).', 'Using a wide-coverage morphological analyzer based on (Itai et al.  2006) should cater for a better coverage  and incorporating lexical probabilities learned from a big (unannotated) corpus (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00067', '(95%-conf.int.', '0.00067', '-', '0.00067)']
['system', 'ROUGE-S*', 'Average_P:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00125', '(95%-conf.int.', '0.00125', '-', '0.00125)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1485', 'P:120', 'F:1']
0.0661188881542 0.0052155554976 0.00957444433806





input/ref/Task1/W11-2123_vardha.csv
input/res/Task1/W11-2123.csv
parsing: input/ref/Task1/W11-2123_vardha.csv
    <S sid="12" ssid="7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
 <S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
 <S sid="131" ssid="3">Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N &#8722; 1 preceding words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'"]
'131'
['131']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="16">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="16">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="12">Compared with SRILM, IRSTLM adds several features: lower memory consumption, a binary file format with memory mapping, caching to increase speed, and quantization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'"]
'108'
['108']
parsed_discourse_facet ['method_citation']
    <S sid="129" ssid="1">In addition to the optimizations specific to each datastructure described in Section 2, we implement several general optimizations for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'129'"]
'129'
['129']
parsed_discourse_facet ['method_citation']
    <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
    <S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
    <S sid="263" ssid="5">Quantization can be improved by jointly encoding probability and backoff.</S>
original cit marker offset is 0
new cit marker offset is 0



["'263'"]
'263'
['263']
parsed_discourse_facet ['method_citation']
    <S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
    <S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="17">If the context wnf will never extend to the right (i.e. wnf v is not present in the model for all words v) then no subsequent query will match the full context.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'"]
'145'
['145']
parsed_discourse_facet ['method_citation']
 <S sid="182" ssid="1">This section measures performance on shared tasks in order of increasing complexity: sparse lookups, evaluating perplexity of a large file, and translation with Moses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'182'"]
'182'
['182']
parsed_discourse_facet ['method_citation']
    <S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
    <S sid="12" ssid="7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
    <S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="12">We have modified Moses (Koehn et al., 2007) to keep our state with hypotheses; to conserve memory, phrases do not keep state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
 <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
 <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W11-2123.csv
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="27" ssid = "5">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'183'", "'69'", "'27'", "'231'"]
'0'
'183'
'69'
'27'
'231'
['0', '183', '69', '27', '231']
parsed_discourse_facet ['results_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="149" ssid = "21">RandLM and SRILM also remove context that will not extend  but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.</S><S sid ="193" ssid = "12">It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'183'", "'69'", "'149'", "'193'"]
'0'
'183'
'69'
'149'
'193'
['0', '183', '69', '149', '193']
parsed_discourse_facet ['method_citation']
<S sid ="62" ssid = "40">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S><S sid ="142" ssid = "14">Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn−1 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.</S><S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="216" ssid = "35">Though we are not able to calculate their memory usage on our model  results reported in their paper suggest lower memory consumption than TRIE on large-scale models  at the expense of CPU time.</S><S sid ="50" ssid = "28">Given counts cn1 where e.g. c1 is the vocabulary size  total memory consumption  in bits  is Our PROBING data structure places all n-grams of the same order into a single giant hash table.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'", "'142'", "'3'", "'216'", "'50'"]
'62'
'142'
'3'
'216'
'50'
['62', '142', '3', '216', '50']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="137" ssid = "9">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="157" ssid = "29">Each visited entry wni stores backoff b(wni ).</S><S sid ="27" ssid = "5">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'137'", "'183'", "'157'", "'27'"]
'0'
'137'
'183'
'157'
'27'
['0', '137', '183', '157', '27']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="233" ssid = "52">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid ="193" ssid = "12">It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'3'", "'233'", "'193'", "'69'"]
'183'
'3'
'233'
'193'
'69'
['183', '3', '233', '193', '69']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="38" ssid = "16">The ratio of buckets to entries is controlled by space multiplier m > 1.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="71" ssid = "49">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'0'", "'38'", "'88'", "'71'"]
'183'
'0'
'38'
'88'
'71'
['183', '0', '38', '88', '71']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S><S sid ="71" ssid = "49">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'0'", "'14'", "'71'", "'88'"]
'69'
'0'
'14'
'71'
'88'
['69', '0', '14', '71', '88']
parsed_discourse_facet ['method_citation']
<S sid ="124" ssid = "28">Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="227" ssid = "46">The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.</S><S sid ="21" ssid = "16">Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.</S><S sid ="13" ssid = "8">IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.</S>
original cit marker offset is 0
new cit marker offset is 0



["'124'", "'69'", "'227'", "'21'", "'13'"]
'124'
'69'
'227'
'21'
'13'
['124', '69', '227', '21', '13']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="137" ssid = "9">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid ="116" ssid = "20">Both implementations employ a state object  opaque to the application  that carries information from one query to the next; we discuss both further in Section 4.2.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'0'", "'137'", "'116'", "'231'"]
'183'
'0'
'137'
'116'
'231'
['183', '0', '137', '116', '231']
parsed_discourse_facet ['method_citation']
<S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="118" ssid = "22">The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.</S><S sid ="14" ssid = "9">MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'", "'183'", "'69'", "'118'", "'14'"]
'88'
'183'
'69'
'118'
'14'
['88', '183', '69', '118', '14']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="227" ssid = "46">The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.</S><S sid ="124" ssid = "28">Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'231'", "'183'", "'227'", "'124'"]
'69'
'231'
'183'
'227'
'124'
['69', '231', '183', '227', '124']
parsed_discourse_facet ['method_citation']
<S sid ="157" ssid = "29">Each visited entry wni stores backoff b(wni ).</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="53" ssid = "31">Sorted arrays store key-value pairs in an array sorted by key  incurring no space overhead.</S><S sid ="27" ssid = "5">Hash tables are a common sparse mapping technique used by SRILM’s default and BerkeleyLM’s hashed variant.</S><S sid ="118" ssid = "22">The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.</S>
original cit marker offset is 0
new cit marker offset is 0



["'157'", "'0'", "'53'", "'27'", "'118'"]
'157'
'0'
'53'
'27'
'118'
['157', '0', '53', '27', '118']
parsed_discourse_facet ['method_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="195" ssid = "14">Interpolation search has a more expensive pivot but performs less pivoting and reads  so it is slow on small data and faster on large data.</S><S sid ="116" ssid = "20">Both implementations employ a state object  opaque to the application  that carries information from one query to the next; we discuss both further in Section 4.2.</S><S sid ="137" ssid = "9">The state function is integrated into the query process so that  in lieu of the query p(wnjwn−1 1 )  the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S sid ="62" ssid = "40">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'195'", "'116'", "'137'", "'62'"]
'183'
'195'
'116'
'137'
'62'
['183', '195', '116', '137', '62']
parsed_discourse_facet ['results_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="231" ssid = "50">The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI’s speed reported in Guthrie and Hepple (2010) under different conditions.</S><S sid ="233" ssid = "52">The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.</S><S sid ="193" ssid = "12">It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'69'", "'231'", "'233'", "'193'"]
'183'
'69'
'231'
'233'
'193'
['183', '69', '231', '233', '193']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="201" ssid = "20">Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'69'", "'88'", "'183'", "'201'"]
'0'
'69'
'88'
'183'
'201'
['0', '69', '88', '183', '201']
parsed_discourse_facet ['method_citation']
<S sid ="3" ssid = "3">Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S sid ="62" ssid = "40">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S><S sid ="102" ssid = "6">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie)  allocating memory all at once (eliminating the need for full pointers)  and being easy to compile.</S><S sid ="179" ssid = "51">We do not experiment with models larger than physical memory in this paper because TPT is unreleased  factors such as disk speed are hard to replicate  and in such situations we recommend switching to a more compact representation  such as RandLM.</S><S sid ="142" ssid = "14">Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn−1 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'62'", "'102'", "'179'", "'142'"]
'3'
'62'
'102'
'179'
'142'
['3', '62', '102', '179', '142']
parsed_discourse_facet ['method_citation']
<S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="2" ssid = "2">The structure uses linear probing hash tables and is designed for speed.</S><S sid ="129" ssid = "1">In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.</S><S sid ="71" ssid = "49">Nodes in the trie are based on arrays sorted by vocabulary identifier.</S><S sid ="201" ssid = "20">Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'2'", "'129'", "'71'", "'201'"]
'69'
'2'
'129'
'71'
'201'
['69', '2', '129', '71', '201']
parsed_discourse_facet ['method_citation']
<S sid ="199" ssid = "18">For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid ="205" ssid = "24">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al.  2009).</S><S sid ="182" ssid = "1">This section measures performance on shared tasks in order of increasing complexity: sparse lookups  evaluating perplexity of a large file  and translation with Moses.</S><S sid ="67" ssid = "45">While sorted arrays could be used to implement the same data structure as PROBING  effectively making m = 1  we abandoned this implementation because it is slower and larger than a trie implementation.</S><S sid ="45" ssid = "23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'", "'205'", "'182'", "'67'", "'45'"]
'199'
'205'
'182'
'67'
'45'
['199', '205', '182', '67', '45']
parsed_discourse_facet ['method_citation']
<S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="149" ssid = "21">RandLM and SRILM also remove context that will not extend  but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.</S><S sid ="201" ssid = "20">Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'183'", "'69'", "'149'", "'201'"]
'0'
'183'
'69'
'149'
'201'
['0', '183', '69', '149', '201']
parsed_discourse_facet ['aim_citation', 'results_citation']
<S sid ="183" ssid = "2">Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.</S><S sid ="0" ssid = "0">KenLM: Faster and Smaller Language Model Queries</S><S sid ="69" ssid = "47">Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM’s inverted variant  and BerkeleyLM except for the scrolling variant.</S><S sid ="88" ssid = "66">By contrast  BerkeleyLM’s hash and compressed variants will return incorrect results based on an n −1-gram.</S><S sid ="118" ssid = "22">The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM’s default.</S>
original cit marker offset is 0
new cit marker offset is 0



["'183'", "'0'", "'69'", "'88'", "'118'"]
'183'
'0'
'69'
'88'
'118'
['183', '0', '69', '88', '118']
parsed_discourse_facet ['method_citation']
['If the context wnf will never extend to the right (i.e. wnf v is not present in the model for all words v) then no subsequent query will match the full context.']
['Interpolation search has a more expensive pivot but performs less pivoting and reads  so it is slow on small data and faster on large data.', 'Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.', u'If the key distribution\u2019s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |\u2212 1] to estimate pivots; this optimization alone led to a 24% speed improvement.', 'Both implementations employ a state object  opaque to the application  that carries information from one query to the next; we discuss both further in Section 4.2.', u'The state function is integrated into the query process so that  in lieu of the query p(wnjwn\u22121 1 )  the application issues query p(wnjs(wn\u22121 1 )) which also returns s(wn1 ).']
['system', 'ROUGE-S*', 'Average_R:', '0.00026', '(95%-conf.int.', '0.00026', '-', '0.00026)']
['system', 'ROUGE-S*', 'Average_P:', '0.01515', '(95%-conf.int.', '0.01515', '-', '0.01515)']
['system', 'ROUGE-S*', 'Average_F:', '0.00050', '(95%-conf.int.', '0.00050', '-', '0.00050)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3916', 'P:66', 'F:1']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['KenLM: Faster and Smaller Language Model Queries', u'Hash tables are a common sparse mapping technique used by SRILM\u2019s default and BerkeleyLM\u2019s hashed variant.', 'Sorted arrays store key-value pairs in an array sorted by key  incurring no space overhead.', 'Each visited entry wni stores backoff b(wni ).', u'The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM\u2019s default.']
['system', 'ROUGE-S*', 'Average_R:', '0.00707', '(95%-conf.int.', '0.00707', '-', '0.00707)']
['system', 'ROUGE-S*', 'Average_P:', '0.08974', '(95%-conf.int.', '0.08974', '-', '0.08974)']
['system', 'ROUGE-S*', 'Average_F:', '0.01311', '(95%-conf.int.', '0.01311', '-', '0.01311)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:78', 'F:7']
['We have modified Moses (Koehn et al., 2007) to keep our state with hypotheses; to conserve memory, phrases do not keep state.']
['This section measures performance on shared tasks in order of increasing complexity: sparse lookups  evaluating perplexity of a large file  and translation with Moses.', 'We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al.  2009).', 'The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.', 'For the perplexity and translation tasks  we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn  2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.', 'While sorted arrays could be used to implement the same data structure as PROBING  effectively making m = 1  we abandoned this implementation because it is slower and larger than a trie implementation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00084', '(95%-conf.int.', '0.00084', '-', '0.00084)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00166', '(95%-conf.int.', '0.00166', '-', '0.00166)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3570', 'P:55', 'F:3']
['Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.']
['Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.', 'Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\xe2\u20ac\u2122s inverted variant  and BerkeleyLM except for the scrolling variant.', 'It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.', 'The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6328', 'P:171', 'F:0']
['Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N &#8722; 1 preceding words.']
['Though we are not able to calculate their memory usage on our model  results reported in their paper suggest lower memory consumption than TRIE on large-scale models  at the expense of CPU time.', u'If the key distribution\u2019s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |\u2212 1] to estimate pivots; this optimization alone led to a 24% speed improvement.', u'Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn\u22121 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.', 'Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', 'Given counts cn1 where e.g. c1 is the vocabulary size  total memory consumption  in bits  is Our PROBING data structure places all n-grams of the same order into a single giant hash table.']
['system', 'ROUGE-S*', 'Average_R:', '0.00064', '(95%-conf.int.', '0.00064', '-', '0.00064)']
['system', 'ROUGE-S*', 'Average_P:', '0.03810', '(95%-conf.int.', '0.03810', '-', '0.03810)']
['system', 'ROUGE-S*', 'Average_F:', '0.00127', '(95%-conf.int.', '0.00127', '-', '0.00127)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6216', 'P:105', 'F:4']
['Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.']
['KenLM: Faster and Smaller Language Model Queries', u'Hash tables are a common sparse mapping technique used by SRILM\u2019s default and BerkeleyLM\u2019s hashed variant.', u'The state function is integrated into the query process so that  in lieu of the query p(wnjwn\u22121 1 )  the application issues query p(wnjs(wn\u22121 1 )) which also returns s(wn1 ).', 'Each visited entry wni stores backoff b(wni ).', 'Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:171', 'F:0']
['Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.']
['The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\u2019s inverted variant  and BerkeleyLM except for the scrolling variant.', 'Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.', u'The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI\u2019s speed reported in Guthrie and Hepple (2010) under different conditions.', 'Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.']
['system', 'ROUGE-S*', 'Average_R:', '0.00082', '(95%-conf.int.', '0.00082', '-', '0.00082)']
['system', 'ROUGE-S*', 'Average_P:', '0.04545', '(95%-conf.int.', '0.04545', '-', '0.04545)']
['system', 'ROUGE-S*', 'Average_F:', '0.00161', '(95%-conf.int.', '0.00161', '-', '0.00161)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3655', 'P:66', 'F:3']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\xe2\u20ac\u2122s inverted variant  and BerkeleyLM except for the scrolling variant.', 'The structure uses linear probing hash tables and is designed for speed.', 'Nodes in the trie are based on arrays sorted by vocabulary identifier.', 'In addition to the optimizations specific to each datastructure described in Section 2  we implement several general optimizations for language modeling.']
['system', 'ROUGE-S*', 'Average_R:', '0.00210', '(95%-conf.int.', '0.00210', '-', '0.00210)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00404', '(95%-conf.int.', '0.00404', '-', '0.00404)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:55', 'F:3']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['KenLM: Faster and Smaller Language Model Queries', 'RandLM and SRILM also remove context that will not extend  but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\xe2\u20ac\u2122s inverted variant  and BerkeleyLM except for the scrolling variant.', 'It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.', 'Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.']
['system', 'ROUGE-S*', 'Average_R:', '0.00285', '(95%-conf.int.', '0.00285', '-', '0.00285)']
['system', 'ROUGE-S*', 'Average_P:', '0.16364', '(95%-conf.int.', '0.16364', '-', '0.16364)']
['system', 'ROUGE-S*', 'Average_F:', '0.00560', '(95%-conf.int.', '0.00560', '-', '0.00560)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:55', 'F:9']
['Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.']
['The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie)  allocating memory all at once (eliminating the need for full pointers)  and being easy to compile.', 'Compared with the widely- SRILM  our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing  sorted records  interpolation search  and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', u'Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore  when the model is queried for p(wnjwn\u22121 1 ) but the longest matching suffix is wnf   it may return state s(wn1) = wnf since no longer context will be found.', u'If the key distribution\u2019s range is also known (i.e. vocabulary identifiers range from 0 to the number of words)  then interpolation search can use this information instead of reading A[0] and A[|A |\u2212 1] to estimate pivots; this optimization alone led to a 24% speed improvement.', 'We do not experiment with models larger than physical memory in this paper because TPT is unreleased  factors such as disk speed are hard to replicate  and in such situations we recommend switching to a more compact representation  such as RandLM.']
['system', 'ROUGE-S*', 'Average_R:', '0.00048', '(95%-conf.int.', '0.00048', '-', '0.00048)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00095', '(95%-conf.int.', '0.00095', '-', '0.00095)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:6216', 'P:78', 'F:3']
['Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.']
['KenLM: Faster and Smaller Language Model Queries', u'Hash tables are a common sparse mapping technique used by SRILM\u2019s default and BerkeleyLM\u2019s hashed variant.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\u2019s inverted variant  and BerkeleyLM except for the scrolling variant.', u'The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI\u2019s speed reported in Guthrie and Hepple (2010) under different conditions.', 'Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.']
['system', 'ROUGE-S*', 'Average_R:', '0.00038', '(95%-conf.int.', '0.00038', '-', '0.00038)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00074', '(95%-conf.int.', '0.00074', '-', '0.00074)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:78', 'F:1']
['In addition to the optimizations specific to each datastructure described in Section 2, we implement several general optimizations for language modeling.']
[u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\u2019s inverted variant  and BerkeleyLM except for the scrolling variant.', u'By contrast  BerkeleyLM\u2019s hash and compressed variants will return incorrect results based on an n \u22121-gram.', 'MITLM 0.4 (Hsu and Glass  2008) is mostly designed for accurate model estimation  but can also compute perplexity.', 'Nodes in the trie are based on arrays sorted by vocabulary identifier.', 'KenLM: Faster and Smaller Language Model Queries']
['system', 'ROUGE-S*', 'Average_R:', '0.00218', '(95%-conf.int.', '0.00218', '-', '0.00218)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00419', '(95%-conf.int.', '0.00419', '-', '0.00419)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:55', 'F:3']
['Compared with SRILM, IRSTLM adds several features: lower memory consumption, a binary file format with memory mapping, caching to increase speed, and quantization.']
['Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.', u'By contrast  BerkeleyLM\u2019s hash and compressed variants will return incorrect results based on an n \u22121-gram.', 'The ratio of buckets to entries is controlled by space multiplier m > 1.', 'Nodes in the trie are based on arrays sorted by vocabulary identifier.', 'KenLM: Faster and Smaller Language Model Queries']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:136', 'F:0']
['We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.']
['Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.', 'KenLM: Faster and Smaller Language Model Queries', u'By contrast  BerkeleyLM\u2019s hash and compressed variants will return incorrect results based on an n \u22121-gram.', 'Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\u2019s inverted variant  and BerkeleyLM except for the scrolling variant.']
['system', 'ROUGE-S*', 'Average_R:', '0.00088', '(95%-conf.int.', '0.00088', '-', '0.00088)']
['system', 'ROUGE-S*', 'Average_P:', '0.04444', '(95%-conf.int.', '0.04444', '-', '0.04444)']
['system', 'ROUGE-S*', 'Average_F:', '0.00172', '(95%-conf.int.', '0.00172', '-', '0.00172)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:45', 'F:2']
['For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.']
['Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.', u'By contrast  BerkeleyLM\u2019s hash and compressed variants will return incorrect results based on an n \u22121-gram.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\u2019s inverted variant  and BerkeleyLM except for the scrolling variant.', u'The hash variant is a reverse trie with hash tables  a more memory-efficient version of SRILM\u2019s default.', 'KenLM: Faster and Smaller Language Model Queries']
['system', 'ROUGE-S*', 'Average_R:', '0.00226', '(95%-conf.int.', '0.00226', '-', '0.00226)']
['system', 'ROUGE-S*', 'Average_P:', '0.01538', '(95%-conf.int.', '0.01538', '-', '0.01538)']
['system', 'ROUGE-S*', 'Average_F:', '0.00394', '(95%-conf.int.', '0.00394', '-', '0.00394)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2211', 'P:325', 'F:5']
['For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.']
['The first value reports use immediately after loading while the second reports the increase during scoring. dBerkeleyLM is written in Java which requires memory be specified in advance.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\xe2\u20ac\u2122s inverted variant  and BerkeleyLM except for the scrolling variant.', 'IRSTLM 5.60.02 (Federico et al.  2008) is a sorted trie implementation designed for lower memory consumption.', 'Lossy compressed models RandLM (Talbot and Osborne  2007) and Sheffield (Guthrie and Hepple  2010) offer better memory consumption at the expense of CPU and accuracy.', 'Performance improvements transfer to the Moses (Koehn et al.  2007)  cdec (Dyer et al.  2010)  and Joshua (Li et al.  2009) translation systems where our code has been integrated.']
['system', 'ROUGE-S*', 'Average_R:', '0.00411', '(95%-conf.int.', '0.00411', '-', '0.00411)']
['system', 'ROUGE-S*', 'Average_P:', '0.04000', '(95%-conf.int.', '0.04000', '-', '0.04000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00746', '(95%-conf.int.', '0.00746', '-', '0.00746)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:3160', 'P:325', 'F:13']
['This section measures performance on shared tasks in order of increasing complexity: sparse lookups, evaluating perplexity of a large file, and translation with Moses.']
['Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.', 'The authors provided us with a ratio between TPT and SRI under different conditions. aLossy compression with the same weights. bLossy compression with retuned weights. ditions make the value appropriate for estimating repeated run times  such as in parameter tuning.', u'The developer explained that the loading process requires extra memory that it then frees. eBased on the ratio to SRI\u2019s speed reported in Guthrie and Hepple (2010) under different conditions.', 'It also uses less memory  with 8 bytes of overhead per entry (we store 16-byte entries with m = 1.5); linked list implementations hash set and unordered require at least 8 bytes per entry for pointers.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\u2019s inverted variant  and BerkeleyLM except for the scrolling variant.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4851', 'P:120', 'F:0']
['For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.']
['Unlike Germann et al. (2009)  we chose a model size so that all benchmarks fit comfortably in main memory.', 'KenLM: Faster and Smaller Language Model Queries', 'RandLM and SRILM also remove context that will not extend  but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.', u'Our TRIE implements the popular reverse trie  in which the last word of an n-gram is looked up first  as do SRILM  IRSTLM\xe2\u20ac\u2122s inverted variant  and BerkeleyLM except for the scrolling variant.', 'Our test machine has two Intel Xeon E5410 processors totaling eight cores  32 GB RAM  and four Seagate Barracuda disks in software RAID 0 running Linux 2.6.18.']
['system', 'ROUGE-S*', 'Average_R:', '0.00373', '(95%-conf.int.', '0.00373', '-', '0.00373)']
['system', 'ROUGE-S*', 'Average_P:', '0.02769', '(95%-conf.int.', '0.02769', '-', '0.02769)']
['system', 'ROUGE-S*', 'Average_F:', '0.00657', '(95%-conf.int.', '0.00657', '-', '0.00657)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2415', 'P:325', 'F:9']
0.0385844442301 0.00158888888006 0.00296444442798
creating setting: abstract
else part task 2
eval: abstract
['system', 'ROUGE-1', 'Average_R:', '0.80296', '(95%-conf.int.', '0.74321', '-', '0.84983)']
['system', 'ROUGE-1', 'Average_P:', '0.08669', '(95%-conf.int.', '0.07649', '-', '0.09790)']
['system', 'ROUGE-1', 'Average_F:', '0.15642', '(95%-conf.int.', '0.13934', '-', '0.17457)']
['system', 'ROUGE-1', 'Eval', 'A00-2018.system', 'R:82', 'P:569', 'F:64']
['system', 'ROUGE-1', 'Eval', 'A00-2030.system', 'R:31', 'P:478', 'F:24']
['system', 'ROUGE-1', 'Eval', 'A97-1014.system', 'R:36', 'P:309', 'F:24']
['system', 'ROUGE-1', 'Eval', 'D09-1092.system', 'R:50', 'P:637', 'F:40']
['system', 'ROUGE-1', 'Eval', 'D10-1044.system', 'R:44', 'P:706', 'F:31']
['system', 'ROUGE-1', 'Eval', 'E03-1005.system', 'R:53', 'P:558', 'F:46']
['system', 'ROUGE-1', 'Eval', 'J01-2004.system', 'R:90', 'P:622', 'F:71']
['system', 'ROUGE-1', 'Eval', 'P04-1036.system', 'R:87', 'P:668', 'F:81']
['system', 'ROUGE-1', 'Eval', 'P05-1013.system', 'R:52', 'P:586', 'F:42']
['system', 'ROUGE-1', 'Eval', 'P08-1028.system', 'R:40', 'P:566', 'F:40']
['system', 'ROUGE-1', 'Eval', 'P08-1043.system', 'R:67', 'P:348', 'F:40']
['system', 'ROUGE-1', 'Eval', 'P08-1102.system', 'R:62', 'P:572', 'F:49']
['system', 'ROUGE-1', 'Eval', 'P11-1060.system', 'R:63', 'P:565', 'F:54']
['system', 'ROUGE-1', 'Eval', 'P11-1061.system', 'R:70', 'P:660', 'F:55']
['system', 'ROUGE-1', 'Eval', 'P87-1015.system', 'R:60', 'P:735', 'F:48']
['system', 'ROUGE-1', 'Eval', 'W06-2932.system', 'R:53', 'P:578', 'F:53']
['system', 'ROUGE-1', 'Eval', 'W06-3114.system', 'R:24', 'P:272', 'F:23']
['system', 'ROUGE-1', 'Eval', 'W11-2123.system', 'R:79', 'P:566', 'F:67']
['system', 'ROUGE-1', 'Eval', 'W99-0613.system', 'R:77', 'P:361', 'F:45']
['system', 'ROUGE-1', 'Eval', 'W99-0623.system', 'R:32', 'P:383', 'F:29']
['system', 'ROUGE-2', 'Average_R:', '0.47537', '(95%-conf.int.', '0.38526', '-', '0.55888)']
yay
['system', 'ROUGE-2', 'Average_P:', '0.05046', '(95%-conf.int.', '0.04066', '-', '0.05936)']
yay
['system', 'ROUGE-2', 'Average_F:', '0.09119', '(95%-conf.int.', '0.07393', '-', '0.10707)']
yay
['system', 'ROUGE-2', 'Eval', 'A00-2018.system', 'R:81', 'P:568', 'F:38']
yay
['system', 'ROUGE-2', 'Eval', 'A00-2030.system', 'R:30', 'P:477', 'F:18']
yay
['system', 'ROUGE-2', 'Eval', 'A97-1014.system', 'R:35', 'P:308', 'F:12']
yay
['system', 'ROUGE-2', 'Eval', 'D09-1092.system', 'R:49', 'P:636', 'F:21']
yay
['system', 'ROUGE-2', 'Eval', 'D10-1044.system', 'R:43', 'P:705', 'F:6']
yay
['system', 'ROUGE-2', 'Eval', 'E03-1005.system', 'R:52', 'P:557', 'F:33']
yay
['system', 'ROUGE-2', 'Eval', 'J01-2004.system', 'R:89', 'P:621', 'F:39']
yay
['system', 'ROUGE-2', 'Eval', 'P04-1036.system', 'R:86', 'P:667', 'F:52']
yay
['system', 'ROUGE-2', 'Eval', 'P05-1013.system', 'R:51', 'P:585', 'F:25']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1028.system', 'R:39', 'P:565', 'F:36']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1043.system', 'R:66', 'P:347', 'F:11']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1102.system', 'R:61', 'P:571', 'F:30']
yay
['system', 'ROUGE-2', 'Eval', 'P11-1060.system', 'R:62', 'P:564', 'F:38']
yay
['system', 'ROUGE-2', 'Eval', 'P11-1061.system', 'R:69', 'P:659', 'F:27']
yay
['system', 'ROUGE-2', 'Eval', 'P87-1015.system', 'R:59', 'P:734', 'F:24']
yay
['system', 'ROUGE-2', 'Eval', 'W06-2932.system', 'R:52', 'P:577', 'F:49']
yay
['system', 'ROUGE-2', 'Eval', 'W06-3114.system', 'R:23', 'P:271', 'F:9']
yay
['system', 'ROUGE-2', 'Eval', 'W11-2123.system', 'R:78', 'P:565', 'F:40']
yay
['system', 'ROUGE-2', 'Eval', 'W99-0613.system', 'R:76', 'P:360', 'F:16']
yay
['system', 'ROUGE-2', 'Eval', 'W99-0623.system', 'R:31', 'P:382', 'F:15']
yay
['system', 'ROUGE-SU*', 'Average_R:', '0.58811', '(95%-conf.int.', '0.50000', '-', '0.65663)']
['system', 'ROUGE-SU*', 'Average_P:', '0.00727', '(95%-conf.int.', '0.00562', '-', '0.00914)']
['system', 'ROUGE-SU*', 'Average_F:', '0.01435', '(95%-conf.int.', '0.01113', '-', '0.01801)']
['system', 'ROUGE-SU*', 'Eval', 'A00-2018.system', 'R:3402', 'P:162164', 'F:1996']
['system', 'ROUGE-SU*', 'Eval', 'A00-2030.system', 'R:495', 'P:114480', 'F:290']
['system', 'ROUGE-SU*', 'Eval', 'A97-1014.system', 'R:665', 'P:47894', 'F:271']
['system', 'ROUGE-SU*', 'Eval', 'D09-1092.system', 'R:1274', 'P:203202', 'F:691']
['system', 'ROUGE-SU*', 'Eval', 'D10-1044.system', 'R:989', 'P:249570', 'F:466']
['system', 'ROUGE-SU*', 'Eval', 'E03-1005.system', 'R:1430', 'P:155960', 'F:995']
['system', 'ROUGE-SU*', 'Eval', 'J01-2004.system', 'R:4094', 'P:193752', 'F:2465']
['system', 'ROUGE-SU*', 'Eval', 'P04-1036.system', 'R:3827', 'P:223445', 'F:2726']
['system', 'ROUGE-SU*', 'Eval', 'P05-1013.system', 'R:1377', 'P:171990', 'F:867']
['system', 'ROUGE-SU*', 'Eval', 'P08-1028.system', 'R:819', 'P:160460', 'F:762']
['system', 'ROUGE-SU*', 'Eval', 'P08-1043.system', 'R:2277', 'P:60725', 'F:665']
['system', 'ROUGE-SU*', 'Eval', 'P08-1102.system', 'R:1952', 'P:163877', 'F:1268']
['system', 'ROUGE-SU*', 'Eval', 'P11-1060.system', 'R:2015', 'P:159894', 'F:1187']
['system', 'ROUGE-SU*', 'Eval', 'P11-1061.system', 'R:2484', 'P:218129', 'F:1445']
['system', 'ROUGE-SU*', 'Eval', 'P87-1015.system', 'R:1829', 'P:270479', 'F:1121']
['system', 'ROUGE-SU*', 'Eval', 'W06-2932.system', 'R:1430', 'P:167330', 'F:1274']
['system', 'ROUGE-SU*', 'Eval', 'W06-3114.system', 'R:299', 'P:37127', 'F:265']
['system', 'ROUGE-SU*', 'Eval', 'W11-2123.system', 'R:3159', 'P:160460', 'F:1980']
['system', 'ROUGE-SU*', 'Eval', 'W99-0613.system', 'R:3002', 'P:65340', 'F:887']
['system', 'ROUGE-SU*', 'Eval', 'W99-0623.system', 'R:527', 'P:73535', 'F:379']
creating setting: community
else part task 2
eval: community
['system', 'ROUGE-1', 'Average_R:', '0.58778', '(95%-conf.int.', '0.53987', '-', '0.62842)']
['system', 'ROUGE-1', 'Average_P:', '0.30604', '(95%-conf.int.', '0.25799', '-', '0.35421)']
['system', 'ROUGE-1', 'Average_F:', '0.40188', '(95%-conf.int.', '0.35598', '-', '0.44355)']
['system', 'ROUGE-1', 'Eval', 'A00-2018.system', 'R:1269', 'P:1707', 'F:816']
['system', 'ROUGE-1', 'Eval', 'A00-2030.system', 'R:1053', 'P:1434', 'F:471']
['system', 'ROUGE-1', 'Eval', 'A97-1014.system', 'R:603', 'P:927', 'F:231']
['system', 'ROUGE-1', 'Eval', 'D09-1092.system', 'R:1296', 'P:1911', 'F:795']
['system', 'ROUGE-1', 'Eval', 'D10-1044.system', 'R:1125', 'P:2118', 'F:774']
['system', 'ROUGE-1', 'Eval', 'E03-1005.system', 'R:801', 'P:1674', 'F:585']
['system', 'ROUGE-1', 'Eval', 'J01-2004.system', 'R:558', 'P:1866', 'F:327']
['system', 'ROUGE-1', 'Eval', 'P04-1036.system', 'R:1004', 'P:2672', 'F:608']
['system', 'ROUGE-1', 'Eval', 'P05-1013.system', 'R:633', 'P:1758', 'F:429']
['system', 'ROUGE-1', 'Eval', 'P08-1028.system', 'R:612', 'P:1698', 'F:324']
['system', 'ROUGE-1', 'Eval', 'P08-1043.system', 'R:1323', 'P:1044', 'F:654']
['system', 'ROUGE-1', 'Eval', 'P08-1102.system', 'R:972', 'P:1716', 'F:618']
['system', 'ROUGE-1', 'Eval', 'P11-1060.system', 'R:762', 'P:1695', 'F:432']
['system', 'ROUGE-1', 'Eval', 'P11-1061.system', 'R:768', 'P:1980', 'F:438']
['system', 'ROUGE-1', 'Eval', 'P87-1015.system', 'R:897', 'P:2205', 'F:567']
['system', 'ROUGE-1', 'Eval', 'W06-2932.system', 'R:1014', 'P:1734', 'F:783']
['system', 'ROUGE-1', 'Eval', 'W06-3114.system', 'R:543', 'P:816', 'F:288']
['system', 'ROUGE-1', 'Eval', 'W11-2123.system', 'R:300', 'P:1698', 'F:147']
['system', 'ROUGE-1', 'Eval', 'W99-0613.system', 'R:824', 'P:1444', 'F:372']
['system', 'ROUGE-1', 'Eval', 'W99-0623.system', 'R:852', 'P:1149', 'F:468']
['system', 'ROUGE-2', 'Average_R:', '0.34049', '(95%-conf.int.', '0.29067', '-', '0.39589)']
yay
['system', 'ROUGE-2', 'Average_P:', '0.17727', '(95%-conf.int.', '0.13670', '-', '0.22162)']
yay
['system', 'ROUGE-2', 'Average_F:', '0.23280', '(95%-conf.int.', '0.18731', '-', '0.27780)']
yay
['system', 'ROUGE-2', 'Eval', 'A00-2018.system', 'R:1266', 'P:1704', 'F:567']
yay
['system', 'ROUGE-2', 'Eval', 'A00-2030.system', 'R:1050', 'P:1431', 'F:267']
yay
['system', 'ROUGE-2', 'Eval', 'A97-1014.system', 'R:600', 'P:924', 'F:87']
yay
['system', 'ROUGE-2', 'Eval', 'D09-1092.system', 'R:1293', 'P:1908', 'F:474']
yay
['system', 'ROUGE-2', 'Eval', 'D10-1044.system', 'R:1122', 'P:2115', 'F:489']
yay
['system', 'ROUGE-2', 'Eval', 'E03-1005.system', 'R:798', 'P:1671', 'F:411']
yay
['system', 'ROUGE-2', 'Eval', 'J01-2004.system', 'R:555', 'P:1863', 'F:111']
yay
['system', 'ROUGE-2', 'Eval', 'P04-1036.system', 'R:1000', 'P:2668', 'F:348']
yay
['system', 'ROUGE-2', 'Eval', 'P05-1013.system', 'R:630', 'P:1755', 'F:246']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1028.system', 'R:609', 'P:1695', 'F:90']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1043.system', 'R:1320', 'P:1041', 'F:417']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1102.system', 'R:969', 'P:1713', 'F:372']
yay
['system', 'ROUGE-2', 'Eval', 'P11-1060.system', 'R:759', 'P:1692', 'F:195']
yay
['system', 'ROUGE-2', 'Eval', 'P11-1061.system', 'R:765', 'P:1977', 'F:216']
yay
['system', 'ROUGE-2', 'Eval', 'P87-1015.system', 'R:894', 'P:2202', 'F:300']
yay
['system', 'ROUGE-2', 'Eval', 'W06-2932.system', 'R:1011', 'P:1731', 'F:666']
yay
['system', 'ROUGE-2', 'Eval', 'W06-3114.system', 'R:540', 'P:813', 'F:132']
yay
['system', 'ROUGE-2', 'Eval', 'W11-2123.system', 'R:297', 'P:1695', 'F:24']
yay
['system', 'ROUGE-2', 'Eval', 'W99-0613.system', 'R:820', 'P:1440', 'F:192']
yay
['system', 'ROUGE-2', 'Eval', 'W99-0623.system', 'R:849', 'P:1146', 'F:246']
yay
['system', 'ROUGE-SU*', 'Average_R:', '0.33127', '(95%-conf.int.', '0.28004', '-', '0.38261)']
['system', 'ROUGE-SU*', 'Average_P:', '0.09354', '(95%-conf.int.', '0.06824', '-', '0.12130)']
['system', 'ROUGE-SU*', 'Average_F:', '0.14525', '(95%-conf.int.', '0.11238', '-', '0.17792)']
['system', 'ROUGE-SU*', 'Eval', 'A00-2018.system', 'R:269025', 'P:486492', 'F:101898']
['system', 'ROUGE-SU*', 'Eval', 'A00-2030.system', 'R:185325', 'P:343440', 'F:35622']
['system', 'ROUGE-SU*', 'Eval', 'A97-1014.system', 'R:60900', 'P:143682', 'F:6942']
['system', 'ROUGE-SU*', 'Eval', 'D09-1092.system', 'R:280581', 'P:609606', 'F:86583']
['system', 'ROUGE-SU*', 'Eval', 'D10-1044.system', 'R:211497', 'P:748710', 'F:88461']
['system', 'ROUGE-SU*', 'Eval', 'E03-1005.system', 'R:107331', 'P:467880', 'F:51879']
['system', 'ROUGE-SU*', 'Eval', 'J01-2004.system', 'R:52170', 'P:581256', 'F:15975']
['system', 'ROUGE-SU*', 'Eval', 'P04-1036.system', 'R:126500', 'P:893780', 'F:45856']
['system', 'ROUGE-SU*', 'Eval', 'P05-1013.system', 'R:67095', 'P:515970', 'F:28647']
['system', 'ROUGE-SU*', 'Eval', 'P08-1028.system', 'R:62727', 'P:481380', 'F:16854']
['system', 'ROUGE-SU*', 'Eval', 'P08-1043.system', 'R:292380', 'P:182175', 'F:65124']
['system', 'ROUGE-SU*', 'Eval', 'P08-1102.system', 'R:157947', 'P:491631', 'F:57597']
['system', 'ROUGE-SU*', 'Eval', 'P11-1060.system', 'R:97152', 'P:479682', 'F:28287']
['system', 'ROUGE-SU*', 'Eval', 'P11-1061.system', 'R:98685', 'P:654387', 'F:34353']
['system', 'ROUGE-SU*', 'Eval', 'P87-1015.system', 'R:134547', 'P:811437', 'F:51075']
['system', 'ROUGE-SU*', 'Eval', 'W06-2932.system', 'R:171870', 'P:501990', 'F:95733']
['system', 'ROUGE-SU*', 'Eval', 'W06-3114.system', 'R:49410', 'P:111381', 'F:12768']
['system', 'ROUGE-SU*', 'Eval', 'W11-2123.system', 'R:15147', 'P:481380', 'F:3456']
['system', 'ROUGE-SU*', 'Eval', 'W99-0613.system', 'R:85280', 'P:261360', 'F:15444']
['system', 'ROUGE-SU*', 'Eval', 'W99-0623.system', 'R:121407', 'P:220605', 'F:34947']
creating setting: human
else part task 2
eval: human
['system', 'ROUGE-1', 'Average_R:', '0.74330', '(95%-conf.int.', '0.69721', '-', '0.78897)']
['system', 'ROUGE-1', 'Average_P:', '0.11754', '(95%-conf.int.', '0.10701', '-', '0.12815)']
['system', 'ROUGE-1', 'Average_F:', '0.20290', '(95%-conf.int.', '0.18726', '-', '0.21852)']
['system', 'ROUGE-1', 'Eval', 'A00-2018.system', 'R:336', 'P:1707', 'F:228']
['system', 'ROUGE-1', 'Eval', 'A00-2030.system', 'R:252', 'P:1434', 'F:201']
['system', 'ROUGE-1', 'Eval', 'A97-1014.system', 'R:228', 'P:927', 'F:123']
['system', 'ROUGE-1', 'Eval', 'D09-1092.system', 'R:201', 'P:1911', 'F:165']
['system', 'ROUGE-1', 'Eval', 'D10-1044.system', 'R:348', 'P:2118', 'F:270']
['system', 'ROUGE-1', 'Eval', 'E03-1005.system', 'R:309', 'P:1674', 'F:273']
['system', 'ROUGE-1', 'Eval', 'J01-2004.system', 'R:201', 'P:1866', 'F:180']
['system', 'ROUGE-1', 'Eval', 'P04-1036.system', 'R:344', 'P:2672', 'F:292']
['system', 'ROUGE-1', 'Eval', 'P05-1013.system', 'R:243', 'P:1758', 'F:174']
['system', 'ROUGE-1', 'Eval', 'P08-1028.system', 'R:333', 'P:1698', 'F:189']
['system', 'ROUGE-1', 'Eval', 'P08-1043.system', 'R:198', 'P:1044', 'F:153']
['system', 'ROUGE-1', 'Eval', 'P08-1102.system', 'R:318', 'P:1716', 'F:225']
['system', 'ROUGE-1', 'Eval', 'P11-1060.system', 'R:270', 'P:1695', 'F:207']
['system', 'ROUGE-1', 'Eval', 'P11-1061.system', 'R:306', 'P:1980', 'F:225']
['system', 'ROUGE-1', 'Eval', 'P87-1015.system', 'R:180', 'P:2205', 'F:147']
['system', 'ROUGE-1', 'Eval', 'W06-2932.system', 'R:219', 'P:1734', 'F:192']
['system', 'ROUGE-1', 'Eval', 'W06-3114.system', 'R:213', 'P:816', 'F:141']
['system', 'ROUGE-1', 'Eval', 'W11-2123.system', 'R:252', 'P:1698', 'F:204']
['system', 'ROUGE-1', 'Eval', 'W99-0613.system', 'R:256', 'P:1444', 'F:160']
['system', 'ROUGE-1', 'Eval', 'W99-0623.system', 'R:261', 'P:1149', 'F:168']
['system', 'ROUGE-2', 'Average_R:', '0.42071', '(95%-conf.int.', '0.35786', '-', '0.48521)']
yay
['system', 'ROUGE-2', 'Average_P:', '0.06583', '(95%-conf.int.', '0.05618', '-', '0.07610)']
yay
['system', 'ROUGE-2', 'Average_F:', '0.11379', '(95%-conf.int.', '0.09771', '-', '0.13136)']
yay
['system', 'ROUGE-2', 'Eval', 'A00-2018.system', 'R:333', 'P:1704', 'F:117']
yay
['system', 'ROUGE-2', 'Eval', 'A00-2030.system', 'R:249', 'P:1431', 'F:120']
yay
['system', 'ROUGE-2', 'Eval', 'A97-1014.system', 'R:225', 'P:924', 'F:54']
yay
['system', 'ROUGE-2', 'Eval', 'D09-1092.system', 'R:198', 'P:1908', 'F:99']
yay
['system', 'ROUGE-2', 'Eval', 'D10-1044.system', 'R:345', 'P:2115', 'F:90']
yay
['system', 'ROUGE-2', 'Eval', 'E03-1005.system', 'R:306', 'P:1671', 'F:204']
yay
['system', 'ROUGE-2', 'Eval', 'J01-2004.system', 'R:198', 'P:1863', 'F:138']
yay
['system', 'ROUGE-2', 'Eval', 'P04-1036.system', 'R:340', 'P:2668', 'F:168']
yay
['system', 'ROUGE-2', 'Eval', 'P05-1013.system', 'R:240', 'P:1755', 'F:66']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1028.system', 'R:330', 'P:1695', 'F:90']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1043.system', 'R:195', 'P:1041', 'F:75']
yay
['system', 'ROUGE-2', 'Eval', 'P08-1102.system', 'R:315', 'P:1713', 'F:153']
yay
['system', 'ROUGE-2', 'Eval', 'P11-1060.system', 'R:267', 'P:1692', 'F:126']
yay
['system', 'ROUGE-2', 'Eval', 'P11-1061.system', 'R:303', 'P:1977', 'F:123']
yay
['system', 'ROUGE-2', 'Eval', 'P87-1015.system', 'R:177', 'P:2202', 'F:72']
yay
['system', 'ROUGE-2', 'Eval', 'W06-2932.system', 'R:216', 'P:1731', 'F:150']
yay
['system', 'ROUGE-2', 'Eval', 'W06-3114.system', 'R:210', 'P:813', 'F:48']
yay
['system', 'ROUGE-2', 'Eval', 'W11-2123.system', 'R:249', 'P:1695', 'F:126']
yay
['system', 'ROUGE-2', 'Eval', 'W99-0613.system', 'R:252', 'P:1440', 'F:52']
yay
['system', 'ROUGE-2', 'Eval', 'W99-0623.system', 'R:258', 'P:1146', 'F:114']
yay
['system', 'ROUGE-SU*', 'Average_R:', '0.51926', '(95%-conf.int.', '0.45545', '-', '0.58224)']
['system', 'ROUGE-SU*', 'Average_P:', '0.01288', '(95%-conf.int.', '0.01052', '-', '0.01521)']
['system', 'ROUGE-SU*', 'Average_F:', '0.02512', '(95%-conf.int.', '0.02065', '-', '0.02952)']
['system', 'ROUGE-SU*', 'Eval', 'A00-2018.system', 'R:18981', 'P:486492', 'F:8208']
['system', 'ROUGE-SU*', 'Eval', 'A00-2030.system', 'R:10707', 'P:343440', 'F:6492']
['system', 'ROUGE-SU*', 'Eval', 'A97-1014.system', 'R:8775', 'P:143682', 'F:2136']
['system', 'ROUGE-SU*', 'Eval', 'D09-1092.system', 'R:6831', 'P:609606', 'F:4404']
['system', 'ROUGE-SU*', 'Eval', 'D10-1044.system', 'R:20355', 'P:748710', 'F:11055']
['system', 'ROUGE-SU*', 'Eval', 'E03-1005.system', 'R:16065', 'P:467880', 'F:11544']
['system', 'ROUGE-SU*', 'Eval', 'J01-2004.system', 'R:6831', 'P:581256', 'F:5340']
['system', 'ROUGE-SU*', 'Eval', 'P04-1036.system', 'R:14960', 'P:893780', 'F:9884']
['system', 'ROUGE-SU*', 'Eval', 'P05-1013.system', 'R:9960', 'P:515970', 'F:4716']
['system', 'ROUGE-SU*', 'Eval', 'P08-1028.system', 'R:18645', 'P:481380', 'F:5718']
['system', 'ROUGE-SU*', 'Eval', 'P08-1043.system', 'R:6630', 'P:182175', 'F:3456']
['system', 'ROUGE-SU*', 'Eval', 'P08-1102.system', 'R:17010', 'P:491631', 'F:8574']
['system', 'ROUGE-SU*', 'Eval', 'P11-1060.system', 'R:12282', 'P:479682', 'F:6702']
['system', 'ROUGE-SU*', 'Eval', 'P11-1061.system', 'R:15756', 'P:654387', 'F:8757']
['system', 'ROUGE-SU*', 'Eval', 'P87-1015.system', 'R:5487', 'P:811437', 'F:3390']
['system', 'ROUGE-SU*', 'Eval', 'W06-2932.system', 'R:8100', 'P:501990', 'F:5532']
['system', 'ROUGE-SU*', 'Eval', 'W06-3114.system', 'R:7665', 'P:111381', 'F:3297']
['system', 'ROUGE-SU*', 'Eval', 'W11-2123.system', 'R:10707', 'P:481380', 'F:6384']
['system', 'ROUGE-SU*', 'Eval', 'W99-0613.system', 'R:8316', 'P:261360', 'F:2560']
['system', 'ROUGE-SU*', 'Eval', 'W99-0623.system', 'R:11481', 'P:220605', 'F:4236']
