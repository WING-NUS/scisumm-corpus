Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P11-1061,P11-1144,0,2011,0,Subramanya et al? s model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers,Subramanya et al's model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers,"['11', '37', '19', '123', '39']","<S sid =""11"" ssid = ""7"">This scenario is applicable to a large set of languages and has been considered by a number of authors in the past (Alshawi et al.  2000; Xi and Hwa  2005; Ganchev et al.  2009).</S><S sid =""37"" ssid = ""3"">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid =""19"" ssid = ""15"">Syntactic universals are a well studied concept in linguistics (Carnie  2002; Newmeyer  2005)  and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S><S sid =""123"" ssid = ""23"">This can be seen as a rough approximation of Yarowsky and Ngai (2001).</S><S sid =""39"" ssid = ""5"">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S>",['Method_Citation']
3,P11-1061,P14-1126,0,2011,0,"Fortunately, some recently proposed POS taggers, such as the POStagger of Das and Petrov (2011), rely only on labeled training data for English and the same kind of parallel text in our approach","Fortunately, some recently proposed POS taggers, such as the POS tagger of Das and Petrov (2011), rely only on labeled training data for English and the same kind of parallel text in our approach","['24', '16', '113', '104', '144']","<S sid =""24"" ssid = ""1"">The focus of this work is on building POS taggers for foreign languages  assuming that we have an English POS tagger and some parallel text between the two languages.</S><S sid =""16"" ssid = ""12"">To this end  we construct a bilingual graph over word types to establish a connection between the two languages (§3)  and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S sid =""113"" ssid = ""13"">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid =""104"" ssid = ""4"">For monolingual treebank data we relied on the CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi  2006; Nivre et al.  2007).</S><S sid =""144"" ssid = ""7"">For comparison  the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%  and goes up to 88.7% with a treebank dictionary.</S>",['Method_Citation']
4,P11-1061,N12-1086,0,"Das and Petrov, 2011",0,"Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al, 2010), unsupervised learning ofPOS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith,2011)","Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al, 2010), unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith,2011)","['93', '57', '23', '147', '157']","<S sid =""93"" ssid = ""24"">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S><S sid =""57"" ssid = ""23"">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid =""23"" ssid = ""19"">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%)  and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S sid =""147"" ssid = ""10"">As indicated by bolding  for seven out of eight languages the improvements of the “With LP” setting are statistically significant with respect to the other models  including the “No LP” setting.11 Overall  it performs 10.4% better than the hitherto state-of-the-art feature-HMM baseline  and 4.6% better than direct projection  when we macro-average the accuracy over all languages.</S><S sid =""157"" ssid = ""20"">As a result  its POS tag needs to be induced in the “No LP” case  while the 11A word level paired-t-test is significant at p < 0.01 for Danish  Greek  Italian  Portuguese  Spanish and Swedish  and p < 0.05 for Dutch. correct tag is available as a constraint feature in the “With LP” case.</S>",['Method_Citation']
5,P11-1061,N12-1086,0,2011,0,"Following Das and Petrov (2011) and Subramanya et al (2010), a similarity score between two trigram types was computed by measuring the cosine similarity between their empirical sentential context statistics","Following Das and Petrov (2011) and Subramanya et al (2010), a similarity score between two trigram types was computed by measuring the cosine similarity between their empirical sentential context statistics","['88', '37', '123', '38', '39']","<S sid =""88"" ssid = ""19"">In our experiments  we used the same set of features as BergKirkpatrick et al. (2010): an indicator feature based In a traditional Markov model  the emission distribution PΘ(Xi = xi  |Zi = zi) is a set of multinomials.</S><S sid =""37"" ssid = ""3"">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid =""123"" ssid = ""23"">This can be seen as a rough approximation of Yarowsky and Ngai (2001).</S><S sid =""38"" ssid = ""4"">More recently  Subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model.</S><S sid =""39"" ssid = ""5"">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S>",['Method_Citation']
6,P11-1061,N12-1086,0,"Das and Petrov, 2011",0,"Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type is unavailable (e.g., Das and Petrov, 2011)","Sparsity is desirable in settings where labeled development data for tuning thresholds that select the most probable labels for a given type is unavailable (e.g., Das and Petrov, 2011)","['36', '4', '113', '39', '99']","<S sid =""36"" ssid = ""2"">Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand  using individual words as the vertices throws away the context necessary for disambiguation; on the other hand  it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences.</S><S sid =""4"" ssid = ""4"">Across eight European languages  our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline  and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.</S><S sid =""113"" ssid = ""13"">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid =""39"" ssid = ""5"">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid =""99"" ssid = ""30"">It would have therefore also been possible to use the integer programming (IP) based approach of Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side.</S>",['Method_Citation']
7,P11-1061,N12-1052,0,2011,0,"Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. Below, we extend this approach to universal parsing by adding cross-lingual word cluster features","Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. Below, we extend this approach to universal parsing by adding cross-lingual word cluster features","['57', '74', '51', '93', '157']","<S sid =""57"" ssid = ""23"">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid =""74"" ssid = ""5"">This stage of label propagation results in a tag distribution ri over labels y  which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph  optimizing the following objective: 5 POS Induction After running label propagation (LP)  we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1  ...   |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S><S sid =""51"" ssid = ""17"">For each trigram type x2 x3 x4 in a sequence x1 x2 x3 x4 x5  we count how many times that trigram type co-occurs with the different instantiations of each concept  and compute the point-wise mutual information (PMI) between the two.5 The similarity between two trigram types is given by summing over the PMI values over feature instantiations that they have in common.</S><S sid =""93"" ssid = ""24"">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S><S sid =""157"" ssid = ""20"">As a result  its POS tag needs to be induced in the “No LP” case  while the 11A word level paired-t-test is significant at p < 0.01 for Danish  Greek  Italian  Portuguese  Spanish and Swedish  and p < 0.05 for Dutch. correct tag is available as a constraint feature in the “With LP” case.</S>",['Method_Citation']
8,P11-1061,N12-1052,0,2011,0,"We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters","We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of McDonald et al (2011), which only has features derived from universal part-of-speech tags, projected from English with the method of Das and Petrov (2011), to the same model when adding features derived from cross-lingual clusters","['93', '147', '36', '23', '29']","<S sid =""93"" ssid = ""24"">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S><S sid =""147"" ssid = ""10"">As indicated by bolding  for seven out of eight languages the improvements of the “With LP” setting are statistically significant with respect to the other models  including the “No LP” setting.11 Overall  it performs 10.4% better than the hitherto state-of-the-art feature-HMM baseline  and 4.6% better than direct projection  when we macro-average the accuracy over all languages.</S><S sid =""36"" ssid = ""2"">Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand  using individual words as the vertices throws away the context necessary for disambiguation; on the other hand  it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences.</S><S sid =""23"" ssid = ""19"">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%)  and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S sid =""29"" ssid = ""6"">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S>",['Method_Citation']
9,P11-1061,N12-1090,0,"Das and Petrov, (2011)",0,"MT-based projection has been applied to various NLP tasks, such as part of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al (2007)) .There have been two initial attempts to apply projection to create co reference-annotated data for aresource-poor language, both of which involve projecting hand-annotated co reference data from English to Romanian via a parallel corpus","MT-based projection has been applied to various NLP tasks, such as part of-speech tagging (e.g., Das and Petrov (2011)), mention detection (e.g., Zitouni and Florian (2008)), and sentiment analysis (e.g., Mihalcea et al (2007))","['93', '57', '157', '23', '111']","<S sid =""93"" ssid = ""24"">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S><S sid =""57"" ssid = ""23"">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid =""157"" ssid = ""20"">As a result  its POS tag needs to be induced in the “No LP” case  while the 11A word level paired-t-test is significant at p < 0.01 for Danish  Greek  Italian  Portuguese  Spanish and Swedish  and p < 0.05 for Dutch. correct tag is available as a constraint feature in the “With LP” case.</S><S sid =""23"" ssid = ""19"">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%)  and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S sid =""111"" ssid = ""11"">We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns)  VERB (verbs)  ADJ (adjectives)  ADV (adverbs)  PRON (pronouns)  DET (determiners)  ADP (prepositions or postpositions)  NUM (numerals)  CONJ (conjunctions)  PRT (particles)  PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).</S>",['Method_Citation']
10,P11-1061,W11-2205,0,2011,0,"For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being 35 induced, but it assumes access to a labeled dataset of a different language. We begin by surveying recent work on unsupervised PoS tagging, focusing on the issue of evaluation (Section 2)","For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being 35 induced, but it assumes access to a labeled dataset of a different language. We begin by surveying recent work on unsupervised PoS tagging, focusing on the issue of evaluation (Section 2)","['93', '57', '23', '147', '60']","<S sid =""93"" ssid = ""24"">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S><S sid =""57"" ssid = ""23"">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid =""23"" ssid = ""19"">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%)  and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S sid =""147"" ssid = ""10"">As indicated by bolding  for seven out of eight languages the improvements of the “With LP” setting are statistically significant with respect to the other models  including the “No LP” setting.11 Overall  it performs 10.4% better than the hitherto state-of-the-art feature-HMM baseline  and 4.6% better than direct projection  when we macro-average the accuracy over all languages.</S><S sid =""60"" ssid = ""26"">To initialize the graph for label propagation we use a supervised English tagger to label the English side of the bitext.7 We then simply count the individual labels of the English tokens and normalize the counts to produce tag distributions over English word types.</S>",['Results_Citation']
11,P11-1061,P13-1155,0,"Das and Petrov, 2011",0,"(Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfers to induce POS tags between two languages","(Das and Petrov, 2011) used graph-based label propagation for cross-lingual knowledge transfers to induce POS tags between two languages","['37', '113', '38', '85', '3']","<S sid =""37"" ssid = ""3"">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid =""113"" ssid = ""13"">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid =""38"" ssid = ""4"">More recently  Subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model.</S><S sid =""85"" ssid = ""16"">We develop our POS induction model based on the feature-based HMM of Berg-Kirkpatrick et al. (2010).</S><S sid =""3"" ssid = ""3"">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al.  2010).</S>",['Method_Citation']
12,P11-1061,D12-1127,0,2011,0,Recent work by Das and Petrov (2011 )buildsa dictionary for a particular language by transfer ring annotated data from a resource-rich language through the use of word alignments in parallel text,Recent work by Das and Petrov (2011 ) builds a dictionary for a particular language by transferring annotated data from a resource-rich language through the use of word alignments in parallel text,"['105', '143', '102', '25', '63']","<S sid =""105"" ssid = ""5"">The parallel data came from the Europarl corpus (Koehn  2005) and the ODS United Nations dataset (UN  2006).</S><S sid =""143"" ssid = ""6"">Overall  it gives improvements ranging from 1.1% for German to 14.7% for Italian  for an average improvement of 8.3% over the unsupervised feature-HMM model.</S><S sid =""102"" ssid = ""2"">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid =""25"" ssid = ""2"">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S sid =""63"" ssid = ""29"">A very small excerpt from an Italian-English graph is shown in Figure 1.</S>",['Method_Citation']
13,P11-1061,D12-1127,0,"Das and Petrov, 2011",0,"Theseapproaches build a dictionary by transferring labeled data from a resource rich language (English) to a re source poor language (Das and Petrov, 2011)","These approaches build a dictionary by transferring labeled data from a resource rich language (English) to a resource poor language (Das and Petrov, 2011)","['102', '113', '119', '144', '156']","<S sid =""102"" ssid = ""2"">We utilized two kinds of datasets in our experiments: (i) monolingual treebanks9 and (ii) large amounts of parallel text with English on one side.</S><S sid =""113"" ssid = ""13"">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid =""119"" ssid = ""19"">To provide a thorough analysis  we evaluated three baselines and two oracles in addition to two variants of our graph-based approach.</S><S sid =""144"" ssid = ""7"">For comparison  the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%  and goes up to 88.7% with a treebank dictionary.</S><S sid =""156"" ssid = ""19"">As Figure 1 shows  this word has no high-confidence alignment in the Italian-English bitext.</S>",['Method_Citation']
14,P11-1061,P12-3012,0,"Das and Petrov, 2011",0,"In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages ,infact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntactico semantic (Peirsman and Pado?, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al, 2011)","In recent years research in Natural Language Processing (NLP) has been steadily moving towards multilingual processing: the availability of ever growing amounts of text in different languages, in fact, has been a major driving force behind research on multilingual approaches, from morphosyntactic (Das and Petrov, 2011) and syntactico semantic (Peirsman and Pado?, 2010) phenomena to high-end tasks like textual entailment (Mehdad et al., 2011) and sentiment analysis (Lu et al, 2011)","['57', '74', '111', '157', '93']","<S sid =""57"" ssid = ""23"">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid =""74"" ssid = ""5"">This stage of label propagation results in a tag distribution ri over labels y  which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph  optimizing the following objective: 5 POS Induction After running label propagation (LP)  we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1  ...   |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S><S sid =""111"" ssid = ""11"">We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns)  VERB (verbs)  ADJ (adjectives)  ADV (adverbs)  PRON (pronouns)  DET (determiners)  ADP (prepositions or postpositions)  NUM (numerals)  CONJ (conjunctions)  PRT (particles)  PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).</S><S sid =""157"" ssid = ""20"">As a result  its POS tag needs to be induced in the “No LP” case  while the 11A word level paired-t-test is significant at p < 0.01 for Danish  Greek  Italian  Portuguese  Spanish and Swedish  and p < 0.05 for Dutch. correct tag is available as a constraint feature in the “With LP” case.</S><S sid =""93"" ssid = ""24"">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S>",['Method_Citation']
15,P11-1061,D11-1006,0,2011,0,"Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-of speech tags from the projected part-of-speech tagger of Das and Petrov (2011) .2 This tagger relies only onlabeled training data for English, and achieves accuracies around 85% on the languages that we con sider","Furthermore, we evaluate with both gold-standard part-of-speech tags, as well as predicted part-of speech tags from the projected part-of-speech tagger of Das and Petrov (2011). This tagger relies only on labeled training data for English, and achieves accuracies around 85% on the languages that we consider","['23', '147', '93', '57', '29']","<S sid =""23"" ssid = ""19"">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%)  and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S sid =""147"" ssid = ""10"">As indicated by bolding  for seven out of eight languages the improvements of the “With LP” setting are statistically significant with respect to the other models  including the “No LP” setting.11 Overall  it performs 10.4% better than the hitherto state-of-the-art feature-HMM baseline  and 4.6% better than direct projection  when we macro-average the accuracy over all languages.</S><S sid =""93"" ssid = ""24"">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S><S sid =""57"" ssid = ""23"">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid =""29"" ssid = ""6"">To establish a soft correspondence between the two languages  we use a second similarity function  which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data  our goal is to project syntactic information from the English side to the foreign side.</S>",['Method_Citation']
16,P11-1061,D11-1006,0,2011,0,"In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language","In the first, we assumed that the test set for each target language had gold part-of-speech tags, and in the second we used predicted part-of-speech tags from the projection tagger of Das and Petrov (2011), which also uses English as the source language","['93', '57', '23', '10', '113']","<S sid =""93"" ssid = ""24"">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S><S sid =""57"" ssid = ""23"">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid =""23"" ssid = ""19"">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%)  and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S sid =""10"" ssid = ""6"">To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.</S><S sid =""113"" ssid = ""13"">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>",['Method_Citation']
17,P11-1061,P13-2112,0,2011,0,"This parallel data can be exploited to bridge languages, and in particular, transfer information from a highly-resourced language to a lesser-resourced language, to build unsupervised POS taggers. In this paper, we propose an unsupervised approach to POS tagging in a similar vein to the work of Das and Petrov (2011)","This parallel data can be exploited to bridge languages, and in particular, transfer information from a highly-resourced language to a lesser-resourced language, to build unsupervised POS taggers. In this paper, we propose an unsupervised approach to POS tagging in a similar vein to the work of Das and Petrov (2011)","['57', '93', '23', '10', '74']","<S sid =""57"" ssid = ""23"">Since our graph is built from a parallel corpus  we can use standard word alignment techniques to align the English sentences De 5Note that many combinations are impossible giving a PMI value of 0; e.g.  when the trigram type and the feature instantiation don’t have words in common. and their foreign language translations Df.6 Label propagation in the graph will provide coverage and high recall  and we therefore extract only intersected high-confidence (> 0.9) alignments De�f.</S><S sid =""93"" ssid = ""24"">For English POS tagging  BergKirkpatrick et al. (2010) found that this direct gradient method performed better (>7% absolute accuracy) than using a feature-enhanced modification of the Expectation-Maximization (EM) algorithm (Dempster et al.  1977).8 Moreover  this route of optimization outperformed a vanilla HMM trained with EM by 12%.</S><S sid =""23"" ssid = ""19"">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%)  and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S sid =""10"" ssid = ""6"">To bridge this gap  we consider a practically motivated scenario  in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest  but that we have access to parallel data with a resource-rich language.</S><S sid =""74"" ssid = ""5"">This stage of label propagation results in a tag distribution ri over labels y  which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph  optimizing the following objective: 5 POS Induction After running label propagation (LP)  we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1  ...   |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S>",['Method_Citation']
18,P11-1061,P13-2112,0,2011,0,Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high confidence alignments to copy tags from the source language to the target language,Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high confidence alignments to copy tags from the source language to the target language,"['37', '113', '38', '39', '88']","<S sid =""37"" ssid = ""3"">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid =""113"" ssid = ""13"">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S><S sid =""38"" ssid = ""4"">More recently  Subramanya et al. (2010) defined a graph over the cliques in an underlying structured prediction model.</S><S sid =""39"" ssid = ""5"">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid =""88"" ssid = ""19"">In our experiments  we used the same set of features as BergKirkpatrick et al. (2010): an indicator feature based In a traditional Markov model  the emission distribution PΘ(Xi = xi  |Zi = zi) is a set of multinomials.</S>",['Method_Citation']
19,P11-1061,P13-2112,0,"Das and Petrov, 2011",0,"We have proposed a method for unsupervised POStagging that performs on par with the current state of-the-art (Das and Petrov, 2011), but is subs tan tially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM)","We have proposed a method for unsupervised POS tagging that performs on par with the current state of-the-art (Das and Petrov, 2011), but is substantially less-sophisticated (specifically not requiring convex optimization or a feature-based HMM)","['39', '37', '99', '36', '113']","<S sid =""39"" ssid = ""5"">They considered a semi-supervised POS tagging scenario and showed that one can use a graph over trigram types  and edge weights based on distributional similarity  to improve a supervised conditional random field tagger.</S><S sid =""37"" ssid = ""3"">Altun et al. (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning.</S><S sid =""99"" ssid = ""30"">It would have therefore also been possible to use the integer programming (IP) based approach of Ravi and Knight (2009) instead of the feature-HMM for POS induction on the foreign side.</S><S sid =""36"" ssid = ""2"">Graph construction for structured prediction problems such as POS tagging is non-trivial: on the one hand  using individual words as the vertices throws away the context necessary for disambiguation; on the other hand  it is unclear how to define (sequence) similarity if the vertices correspond to entire sentences.</S><S sid =""113"" ssid = ""13"">For each language under consideration  Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>",['Method_Citation']
