Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
2,A00-2018,N10-1002,0,"Charniak, 2000",0,"As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)","As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)","['91', '88', '90', '16', '110']","<S sid =""91"" ssid = ""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid =""88"" ssid = ""57"">While we could have smoothed in the same fashion  we choose instead to use standard deleted interpolation.</S><S sid =""90"" ssid = ""1"">We created a parser based upon the maximumentropy-inspired model of the last section  smoothed using standard deleted interpolation.</S><S sid =""16"" ssid = ""5"">In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.</S><S sid =""110"" ssid = ""1"">In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach  the aspect of the parser that is most novel.</S>",['Method_Citation']
3,A00-2018,W11-0610,0,"Charniak, 2000",0,"Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniakparser (Charniak, 2000), also trained on the Switch board tree bank","Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniak parser (Charniak, 2000), also trained on the Switch board tree bank","['110', '133', '176', '32', '31']","<S sid =""110"" ssid = ""1"">In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach  the aspect of the parser that is most novel.</S><S sid =""133"" ssid = ""24"">One of the first and without doubt the most significant change we made in the current parser is to move from two stages of probabilistic decisions at each node to three.</S><S sid =""176"" ssid = ""3"">That the previous three best parsers on this test [5 9 17] all perform within a percentage point of each other  despite quite different basic mechanisms  led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training  and to conjecture that perhaps we were at it.</S><S sid =""32"" ssid = ""1"">The major problem confronting the author of a generative parser is what information to use to condition the probabilities required in the model  and how to smooth the empirically obtained probabilities to take the sting out of the sparse data problems that are inevitable with even the most modest conditioning.</S><S sid =""31"" ssid = ""20"">Note that the As on both ends of the expansion in Expression 2 are conditioned just like any other label in the expansion.</S>",['Method_Citation']
4,A00-2018,W06-3119,0,"Charniak, 2000",0,"We then use Charniak? s parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus","We then use Charniak's parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus","['133', '9', '32', '176', '110']","<S sid =""133"" ssid = ""24"">One of the first and without doubt the most significant change we made in the current parser is to move from two stages of probabilistic decisions at each node to three.</S><S sid =""9"" ssid = ""5"">The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.</S><S sid =""32"" ssid = ""1"">The major problem confronting the author of a generative parser is what information to use to condition the probabilities required in the model  and how to smooth the empirically obtained probabilities to take the sting out of the sparse data problems that are inevitable with even the most modest conditioning.</S><S sid =""176"" ssid = ""3"">That the previous three best parsers on this test [5 9 17] all perform within a percentage point of each other  despite quite different basic mechanisms  led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training  and to conjecture that perhaps we were at it.</S><S sid =""110"" ssid = ""1"">In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach  the aspect of the parser that is most novel.</S>",['Method_Citation']
5,A00-2018,N03-2024,0,"Charniak, 2000",0,"We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak? s statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)","We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak's statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)","['105', '63', '44', '126', '1']","<S sid =""105"" ssid = ""16"">Note that the definitions of labeled precision and recall are those given in [9] and used in all of the previous work.</S><S sid =""63"" ssid = ""32"">As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).</S><S sid =""44"" ssid = ""13"">Now consider computing a conditional probability p(a H) with a set of features h that connect a to the history H. In a log-linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability  the higher the absolute value of the associated A.</S><S sid =""126"" ssid = ""17"">This is indicated in Figure 2  where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%  0.4% lower than the results on the official test corpus.</S><S sid =""1"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S>",['Implication_Citation']
6,A00-2018,N06-1039,0,"Charniak, 2000",0,"After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARFstructure for each sentence in every article","After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARF structure for each sentence in every article","['189', '144', '32', '91', '38']","<S sid =""189"" ssid = ""16"">Indeed  we initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail.</S><S sid =""144"" ssid = ""35"">This quantity is a relatively intuitive one (as  for example  it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it  in effect  as the unsmoothed probability upon which all smoothing of p(h) is based.</S><S sid =""32"" ssid = ""1"">The major problem confronting the author of a generative parser is what information to use to condition the probabilities required in the model  and how to smooth the empirically obtained probabilities to take the sting out of the sparse data problems that are inevitable with even the most modest conditioning.</S><S sid =""91"" ssid = ""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid =""38"" ssid = ""7"">To compute a probability in a log-linear model one first defines a set of &quot;features&quot;  functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.</S>",['Method_Citation']
7,A00-2018,C04-1180,0,2000,0,"The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)","The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)","['1', '12', '105', '54', '76']","<S sid =""1"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid =""12"" ssid = ""1"">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c  t(c) (t for &quot;tag&quot;)  then the lexical head of c  h(c)  and then the expansion of c into further constituents e(c).</S><S sid =""105"" ssid = ""16"">Note that the definitions of labeled precision and recall are those given in [9] and used in all of the previous work.</S><S sid =""54"" ssid = ""23"">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid =""76"" ssid = ""45"">This requires finding the appropriate Ais for Equation 3  which is accomplished using an algorithm such as iterative scaling [II] in which values for the Ai are initially &quot;guessed&quot; and then modified until they converge on stable values.</S>",['Method_Citation']
8,A00-2018,W05-0638,0,"Charniak, 2000",0,"In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)","In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)","['107', '117', '110', '133', '135']","<S sid =""107"" ssid = ""18"">The results for the new parser as well as for the previous top-three individual parsers on this corpus are given in Figure 1.</S><S sid =""117"" ssid = ""8"">Also  the earlier parser uses two techniques not employed in the current parser.</S><S sid =""110"" ssid = ""1"">In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach  the aspect of the parser that is most novel.</S><S sid =""133"" ssid = ""24"">One of the first and without doubt the most significant change we made in the current parser is to move from two stages of probabilistic decisions at each node to three.</S><S sid =""135"" ssid = ""26"">In contrast  the current parser first guesses the head's pre-terminal  then the head  and then the expansion.</S>",['Method_Citation']
9,A00-2018,P05-1065,0,"Charniak, 2000",0,"We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis","We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis","['189', '91', '88', '48', '81']","<S sid =""189"" ssid = ""16"">Indeed  we initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail.</S><S sid =""91"" ssid = ""2"">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2 7]  we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S sid =""88"" ssid = ""57"">While we could have smoothed in the same fashion  we choose instead to use standard deleted interpolation.</S><S sid =""48"" ssid = ""17"">Maximum-entropy models have two benefits for a parser builder.</S><S sid =""81"" ssid = ""50"">Now we observe that if we were to use a maximum-entropy approach but run iterative scaling zero times  we would  in fact  just have Equation 7.</S>",['Method_Citation']
10,A00-2018,P05-1065,0,"Charniak, 2000",0,"For each article, we calculated the per cent age of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus","For each article, we calculated the percentage of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus","['176', '31', '71', '133', '105']","<S sid =""176"" ssid = ""3"">That the previous three best parsers on this test [5 9 17] all perform within a percentage point of each other  despite quite different basic mechanisms  led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training  and to conjecture that perhaps we were at it.</S><S sid =""31"" ssid = ""20"">Note that the As on both ends of the expansion in Expression 2 are conditioned just like any other label in the expansion.</S><S sid =""71"" ssid = ""40"">Rather than conditioning each term on the previous ones  they are now conditioned only on those aspects of the history that seem most relevant.</S><S sid =""133"" ssid = ""24"">One of the first and without doubt the most significant change we made in the current parser is to move from two stages of probabilistic decisions at each node to three.</S><S sid =""105"" ssid = ""16"">Note that the definitions of labeled precision and recall are those given in [9] and used in all of the previous work.</S>",['Method_Citation']
11,A00-2018,P04-1040,0,2000,0,"The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 %unlabelled and 84 %labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank.The paper is organized as follows","The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 % unlabelled and 84 % labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank. The paper is organized as follows","['105', '126', '131', '63', '31']","<S sid =""105"" ssid = ""16"">Note that the definitions of labeled precision and recall are those given in [9] and used in all of the previous work.</S><S sid =""126"" ssid = ""17"">This is indicated in Figure 2  where the model labeled &quot;Best&quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%  0.4% lower than the results on the official test corpus.</S><S sid =""131"" ssid = ""22"">This is consistent with the average precision/recall of 86.6% for [5] mentioned above  as the latter was on the test corpus and the former on the development corpus.</S><S sid =""63"" ssid = ""32"">As we discuss in more detail in Section 5  several different features in the context surrounding c are useful to include in H: the label  head pre-terminal and head of the parent of c (denoted as lp  tp  hp)  the label of c's left sibling (lb for &quot;before&quot;)  and the label of the grandparent of c (la).</S><S sid =""31"" ssid = ""20"">Note that the As on both ends of the expansion in Expression 2 are conditioned just like any other label in the expansion.</S>",['Implication_Citation']
12,A00-2018,P04-1040,0,2000,0,"Blaheta and Charniak (2000) presented the first method for assigning Pennfunctional tags to constituents identified by a parser. Pattern-matching approaches were used in (John son, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees","Blaheta and Charniak (2000) presented the first method for assigning Penn functional tags to constituents identified by a parser. Pattern-matching approaches were used in (Johnson, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees","['155', '32', '1', '70', '134']","<S sid =""155"" ssid = ""46"">For example  in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S><S sid =""32"" ssid = ""1"">The major problem confronting the author of a generative parser is what information to use to condition the probabilities required in the model  and how to smooth the empirically obtained probabilities to take the sting out of the sparse data problems that are inevitable with even the most modest conditioning.</S><S sid =""1"" ssid = ""1"">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less  and for of length 100 and less when trained and tested on the previously established [5 9 10 15 17] &quot;standard&quot; sections of the Wall Street Journal treebank.</S><S sid =""70"" ssid = ""39"">One way to do this is to modify the simple version shown in Equation 6 to allow this: Note the changes to the last three terms in Equation 7.</S><S sid =""134"" ssid = ""25"">As already noted  Char97 first guesses the lexical head of a constituent and then  given the head  guesses the PCFG rule used to expand the constituent in question.</S>",['Method_Citation']
13,A00-2018,P04-1040,0,2000,0,"As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically","As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically","['70', '189', '32', '155', '144']","<S sid =""70"" ssid = ""39"">One way to do this is to modify the simple version shown in Equation 6 to allow this: Note the changes to the last three terms in Equation 7.</S><S sid =""189"" ssid = ""16"">Indeed  we initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail.</S><S sid =""32"" ssid = ""1"">The major problem confronting the author of a generative parser is what information to use to condition the probabilities required in the model  and how to smooth the empirically obtained probabilities to take the sting out of the sparse data problems that are inevitable with even the most modest conditioning.</S><S sid =""155"" ssid = ""46"">For example  in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S><S sid =""144"" ssid = ""35"">This quantity is a relatively intuitive one (as  for example  it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it  in effect  as the unsmoothed probability upon which all smoothing of p(h) is based.</S>",['Implication_Citation']
17,A00-2018,N06-1022,0,2000,0,"The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents","The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents","['27', '20', '144', '58', '32']","<S sid =""27"" ssid = ""16"">In the simplest of such models  a zeroorder Markov grammar  each label on the righthand side is generated conditioned only on / â€” that is  according to the distributions p(Li j1)  p(M I 1)  and p(Ri I 1).</S><S sid =""20"" ssid = ""9"">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar  and a right-hand side that is a sequence of one or more such symbols.</S><S sid =""144"" ssid = ""35"">This quantity is a relatively intuitive one (as  for example  it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it  in effect  as the unsmoothed probability upon which all smoothing of p(h) is based.</S><S sid =""58"" ssid = ""27"">Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.</S><S sid =""32"" ssid = ""1"">The major problem confronting the author of a generative parser is what information to use to condition the probabilities required in the model  and how to smooth the empirically obtained probabilities to take the sting out of the sparse data problems that are inevitable with even the most modest conditioning.</S>",['Method_Citation']
18,A00-2018,N06-1022,0,2000,0,"Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))","Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))","['187', '158', '54', '144', '80']","<S sid =""187"" ssid = ""14"">Though in some respects not quite as flexible as true maximum entropy  it is much simpler and  in our estimation  has benefits when it comes to smoothing.</S><S sid =""158"" ssid = ""49"">A vp coordinate structure is defined here as a constituent with two or more vp children  one or more of the constituents comma  cc  conjp (conjunctive phrase)  and nothing else; coordinate np phrases are defined similarly.</S><S sid =""54"" ssid = ""23"">The traditional way to handle this is also to compute P(a b)  and perhaps P(a c) as well  and take some combination of these values as one's best estimate for p(a I b  c).</S><S sid =""144"" ssid = ""35"">This quantity is a relatively intuitive one (as  for example  it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it  in effect  as the unsmoothed probability upon which all smoothing of p(h) is based.</S><S sid =""80"" ssid = ""49"">(Our experience is that rather than requiring 50 or so iterations  three suffice.)</S>",['Method_Citation']
19,A00-2018,H05-1035,0,"Charniak, 2000",0,"The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions","The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions","['84', '72', '40', '178', '176']","<S sid =""84"" ssid = ""53"">In the more interesting version  Equation 7  this is not true in general  but one would not expect it to differ much from one  and we assume that as long as we are not publishing the raw probabilities (as we would be doing  for example  in publishing perplexity results) the difference from one should be unimportant.</S><S sid =""72"" ssid = ""41"">The hope is that by doing this we will have less difficulty with the splitting of conditioning events  and thus somewhat less difficulty with sparse data.</S><S sid =""40"" ssid = ""9"">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S><S sid =""178"" ssid = ""5"">The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought.</S><S sid =""176"" ssid = ""3"">That the previous three best parsers on this test [5 9 17] all perform within a percentage point of each other  despite quite different basic mechanisms  led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training  and to conjecture that perhaps we were at it.</S>",['Implication_Citation']
20,A00-2018,P04-1042,0,2000,0,"Note that the dependency figures of Dienes lag behind even the parsed results for Johnson? s model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5% .Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size","Note that the dependency figures of Dienes lag behind even the parsed results for Johnson's model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5%. Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size","['180', '32', '131', '31', '148']","<S sid =""180"" ssid = ""7"">From our perspective  perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.</S><S sid =""32"" ssid = ""1"">The major problem confronting the author of a generative parser is what information to use to condition the probabilities required in the model  and how to smooth the empirically obtained probabilities to take the sting out of the sparse data problems that are inevitable with even the most modest conditioning.</S><S sid =""131"" ssid = ""22"">This is consistent with the average precision/recall of 86.6% for [5] mentioned above  as the latter was on the test corpus and the former on the development corpus.</S><S sid =""31"" ssid = ""20"">Note that the As on both ends of the expansion in Expression 2 are conditioned just like any other label in the expansion.</S><S sid =""148"" ssid = ""39"">So  e.g.  even if the word &quot;conflating&quot; does not appear in the training corpus (and it does not)  the &quot;ng&quot; ending allows our program to guess with relative security that the word has the vbg pre-terminal  and thus the probability of various rule expansions can be considerable sharpened.</S>",['Method_Citation']
