<S sid ="11" ssid = "11">We focus here on extending the applicability of unsupervised methods, as in (Schulte im Walde and Brew, 2002; Stevenson and Merlo, 1999), to the lexical semantic classification of verbs.</S>
<S sid ="15" ssid = "15">Development of minimally supervised methods is of particular importance if we are to automatically classify verbs for languages other than English, where substantial amounts of labelled data are not available for training classifiers.</S>
<S sid ="17" ssid = "17">We have previously shown that a broad set of 220 noisy features performs well in supervised verb classification (Joanis and Stevenson, 2003).</S>
<S sid ="23" ssid = "23">In an unsupervised (clustering) scenario of verb class discovery, can we maintain the benefit of only needing noisy features, without the generality of the feature space leading to “the curse of dimensionality”?</S>
<S sid ="27" ssid = "27">In this paper, we report results on several feature selection approaches to the problem: manual selection (based on linguistic knowledge), unsupervised selection (based on an entropy measure among the features, Dash et al., 1997), and a semi- supervised approach (in which seed verbs are used to train a supervised learner, from which we extract the useful features).</S>
<S sid ="28" ssid = "28">Although our motivation is verb class discovery, we perform our experiments on English, for which we have an accepted classification to serve as a gold standard (Levin, 1993).</S>
<S sid ="34" ssid = "1">Like others, we have assumed lexical semantic classes of verbs as defined in Levin (1993) (hereafter Levin), which have served as a gold standard in computational linguistics research</S>
<S sid ="35" ssid = "2">Levin’s classes form a hierarchy of verb groupings with shared meaning and syntax.</S>
<S sid ="114" ssid = "64">7 5 0 Table 1: Verb classes (see Section 3.1), their Levin class numbers, and the number of experimental verbs in each (see Section 3.2).</S>
<S sid ="117" ssid = "67">We started with a list of all the verbs in the given classes from Levin, removing any verb that did not occur at least 100 times in our corpus (the BNC, described below).</S>
<S sid ="123" ssid = "73">All experiments reported here were run on this same final set of 20 verbs per class (including a replication of our earlier supervised experiments).</S>
<S sid ="130" ssid = "1">4.1 Clustering Parameters.</S>
<S sid ="131" ssid = "2">We used the hierarchical clustering command in Matlab, which implements bottom-up agglomerative clustering, for all our unsupervised experiments.</S>
<S sid ="137" ssid = "8">In the experiments here, however, we report only results for , since we found no principled way of automatically determining a good cutoff.</S>
<S>mean precision of the clusters, weighted according to cluster size.</S>
<S sid ="144" ssid = "15">Then accuracy has the standard definition:2 2 is equivalent to the weighted mean precision of the clusters, weighted according to cluster size.</S>
<S sid ="150" ssid = "21">4.2.2 Adjusted Rand Measure Accuracy can be relatively high for a clustering when a few clusters are very good, and others are not good.</S><S sid ="151" ssid = "22">Our second measure, the adjusted Rand measure used by Schulte im Walde (2003), instead gives a measure of how consistent the given clustering is overall with respect to the gold standard classification.</S>
<S sid ="157" ssid = "28">4.2.3 Mean Silhouette gives an average of the individual goodness of the clusters, and a measure of the overall goodness, both with respect to the gold standard classes.</S>
<S sid ="195" ssid = "33">All 13 Classes .58 .19 .29 .31 .29 .07 .08 .09 .05 .12 .16 Mean of multiway .67 .23 .30 .36 .38 .07 .10 .11 .08 .19 .23 Table 2: Experimental Results.</S>
<S sid ="229" ssid = "67">This feature selection method is highly successful, outperforming the full feature set (Full) on and on most tasks, and performing the same or very close on the remainder.</S>
<S sid ="230" ssid = "68">Moreover, the seed set of features outperforms the manually selected set (Ling) on over half the tasks.</S>
<S sid ="254" ssid = "3">Our feature set is essentially a generalization of theirs, but in scaling up the feature space to be useful across English verb classes in general, we necessarily face a dimensionality problem that did not arise in their research.</S>
<S sid ="261" ssid = "1">We have explored manual, unsupervised, and semi- supervised methods for feature selection in a clustering approach for verb class discovery.</S>

