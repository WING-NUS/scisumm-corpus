Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W99-0623,A00-2005,0,1999,0,1 Introduct ion Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy,Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy,"'1','85','86','146'","<S sid=""1"" ssid=""1"">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S sid=""85"" ssid=""14"">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid=""86"" ssid=""15"">Finally we show the combining techniques degrade very little when a poor parser is added to the set.</S><S sid=""146"" ssid=""1"">We would like to thank Eugene Charniak, Michael Collins, and Adwait Ratnaparkhi for enabling all of this research by providing us with their parsers and helpful comments.</S>","Aim_Citation,Hypothesis_Citation,Implication_Citation"
2,W99-0623,A00-2005,0,1999,0,the collection of hypotheses ti =fi (Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999),"Given a novel sentence Stest E Ctest, combine the collection of hypotheses ti = fi(Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999)","'39','41','105','120'","<S sid=""39"" ssid=""25"">There are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.</S><S sid=""41"" ssid=""27"">IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.</S><S sid=""105"" ssid=""34"">Similarly Figures 1 and 2 show how the isolated constituent precision varies by sentence length and the size of the span of the hypothesized constituent.</S><S sid=""120"" ssid=""49"">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>",Method_Citation
4,W99-0623,N10-1091,0,"Henderson and Brill, 1999",0,"5 (Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers","(Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers","'26','27','93','94'","<S sid=""26"" ssid=""12"">This technique has the advantage of requiring no training, but it has the disadvantage of treating all parsers equally even though they may have differing accuracies or may specialize in modeling different phenomena.</S><S sid=""27"" ssid=""13"">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S><S sid=""93"" ssid=""22"">Because we are working with only three parsers, the only situation in which context will help us is when it can indicate we should choose to believe a single parser that disagrees with the majority hypothesis instead of the majority hypothesis itself.</S><S sid=""94"" ssid=""23"">This is the only important case, because otherwise the simple majority combining technique would pick the correct constituent.</S>","Method_Citation,Results_Citation"
5,W99-0623,W05-1518,0,"Henderson and Brill, 1999",0,"A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)","A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)","'23','27','39','40'","<S sid=""23"" ssid=""9"">We call this technique constituent voting.</S><S sid=""27"" ssid=""13"">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S><S sid=""39"" ssid=""25"">There are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.</S><S sid=""40"" ssid=""26"">Lemma: If the number of votes required by constituent voting is greater than half of the parsers under consideration the resulting structure has no crossing constituents.</S>",Method_Citation
6,W99-0623,W05-1518,0,"Henderson and Brill, 1999",0,"This approach roughly corresponds to (Henderson and Brill, 1999)? s Na ?ve Bayes parse hybridization","This approach roughly corresponds to (Henderson and Brill, 1999)'s Naive Bayes parse hybridization","'27','32','118','140'","<S sid=""27"" ssid=""13"">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S><S sid=""32"" ssid=""18"">In Equations 1 through 3 we develop the model for constructing our parse using na&#239;ve Bayes classification.</S><S sid=""118"" ssid=""47"">The maximum precision oracle is an upper bound on the possible gain we can achieve by parse hybridization.</S><S sid=""140"" ssid=""2"">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>",Method_Citation
7,W99-0623,W05-1518,0,1999,0,Henderson and Brill (1999) also reported that context did not help them to outperform simple voting,Henderson and Brill (1999) also reported that context did not help them to outperform simple voting,"'39','92','93','120'","<S sid=""39"" ssid=""25"">There are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.</S><S sid=""92"" ssid=""21"">While we cannot prove there are no such useful features on which one should condition trust, we can give some insight into why the features we explored offered no gain.</S><S sid=""93"" ssid=""22"">Because we are working with only three parsers, the only situation in which context will help us is when it can indicate we should choose to believe a single parser that disagrees with the majority hypothesis instead of the majority hypothesis itself.</S><S sid=""120"" ssid=""49"">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>","Method_Citation,Results_Citation"
8,W99-0623,W05-1518,0,"Henderson and Brill, 1999",0,"(Henderson and Brill, 1999) improved their best parser? s F-measure of 89.7 to 91.3, using their na ?ve Bayes voting on the Penn TreeBank constituent structures (16% error reduction)","(Henderson and Brill, 1999) improved their best parser's F-measure of 89.7 to 91.3, using their naive Bayes voting on the Penn TreeBank constituent structures (16% error reduction)","'13','40','41','120'","<S sid=""13"" ssid=""9"">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993).</S><S sid=""40"" ssid=""26"">Lemma: If the number of votes required by constituent voting is greater than half of the parsers under consideration the resulting structure has no crossing constituents.</S><S sid=""41"" ssid=""27"">IL+-1Proof: Assume a pair of crossing constituents appears in the output of the constituent voting technique using k parsers.</S><S sid=""120"" ssid=""49"">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>",Method_Citation
10,W99-0623,P01-1005,0,"Henderson and Brill, 1999",0,"Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000)","Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000)","'9','12','23','135'","<S sid=""9"" ssid=""5"">Recently, combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al., 1998; Brill and Wu, 1998).</S><S sid=""12"" ssid=""8"">The corpus-based statistical parsing community has many fast and accurate automated parsing systems, including systems produced by Collins (1997), Charniak (1997) and Ratnaparkhi (1997).</S><S sid=""23"" ssid=""9"">We call this technique constituent voting.</S><S sid=""135"" ssid=""64"">The average individual parser accuracy was reduced by more than 5% when we added this new parser, but the precision of the constituent voting technique was the only result that decreased significantly.</S>","Hypothesis_Citation,Method_Citation"
11,W99-0623,D09-1161,0,1999,0,"Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees","Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees","'21','51','95','96'","<S sid=""21"" ssid=""7"">One hybridization strategy is to let the parsers vote on constituents\ membership in the hypothesized set.</S><S sid=""51"" ssid=""37"">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure.</S><S sid=""95"" ssid=""24"">One side of the decision making process is when we choose to believe a constituent should be in the parse, even though only one parser suggests it.</S><S sid=""96"" ssid=""25"">We call such a constituent an isolated constituent.</S>",Method_Citation
12,W99-0623,D09-1161,0,1999,0,"Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper","Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper","'1','13','85','120'","<S sid=""1"" ssid=""1"">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S sid=""13"" ssid=""9"">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993).</S><S sid=""85"" ssid=""14"">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S><S sid=""120"" ssid=""49"">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>",Method_Citation
13,W99-0623,D09-1161,0,"Henderson and Brill, 1999",0,"Besides the two model scores, we also adopt constituent count as an additional feature in spired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)","Besides the two model scores, we also adopt constituent count as an additional feature in spired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)","'87','88','89','108'","<S sid=""87"" ssid=""16"">It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.</S><S sid=""88"" ssid=""17"">For example, one parser could be more accurate at predicting noun phrases than the other parsers.</S><S sid=""89"" ssid=""18"">None of the models we have presented utilize features associated with a particular constituent (i.e. the label, span, parent label, etc.) to influence parser preference.</S><S sid=""108"" ssid=""37"">From this we see that a finer-grained model for parser combination, at least for the features we have examined, will not give us any additional power.</S>",Method_Citation
14,W99-0623,N06-2033,0,"Henderson and Brill, 1999",0,"Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees","Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees","'21','51','55','87'","<S sid=""21"" ssid=""7"">One hybridization strategy is to let the parsers vote on constituents\ membership in the hypothesized set.</S><S sid=""51"" ssid=""37"">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure.</S><S sid=""55"" ssid=""41"">We have developed a general approach for combining parsers when preserving the entire structure of a parse tree is important.</S><S sid=""87"" ssid=""16"">It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.</S>",Method_Citation
15,W99-0623,N09-2064,0,"Henderson and Brill, 1999",0,"(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined","(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined","'1','77','78','98'","<S sid=""1"" ssid=""1"">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S sid=""77"" ssid=""6"">Each parse is converted into a set of constituents represented as a tuples: (label, start, end).</S><S sid=""78"" ssid=""7"">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid=""98"" ssid=""27"">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S>",Method_Citation
16,W99-0623,N09-2064,0,"Henderson and Brill, 1999",0,"(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents","(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents","'20','23','27','145'","<S sid=""20"" ssid=""6"">Since our goal is to perform well under these measures we will similarly treat constituents as the minimal substructures for combination.</S><S sid=""23"" ssid=""9"">We call this technique constituent voting.</S><S sid=""27"" ssid=""13"">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S><S sid=""145"" ssid=""7"">We plan to explore more powerful techniques for exploiting the diversity of parsing methods.</S>",Method_Citation
17,W99-0623,N09-2064,0,"Henderson and Brill, 1999",0,"output (Figure 3) .Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Mini mum Bayes Risk (MBR) framework. Third, we extend these parser combination methods from 1-best outputs to n-best outputs","Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Mini mum Bayes Risk (MBR) framework","'70','98','99','118'","<S sid=""70"" ssid=""56"">In this case we are interested in finding\ the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S><S sid=""98"" ssid=""27"">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S><S sid=""99"" ssid=""28"">Consider for a set of constituents the isolated constituent precision parser metric, the portion of isolated constituents that are correctly hypothesized.</S><S sid=""118"" ssid=""47"">The maximum precision oracle is an upper bound on the possible gain we can achieve by parse hybridization.</S>",Method_Citation
18,W99-0623,P09-1065,0,"Henderson and Brill, 1999",0,"System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999))","System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999))","'6','9','12','54'","<S sid=""6"" ssid=""2"">The machine learning community has been in a similar situation and has studied the combination of multiple classifiers (Wolpert, 1992; Heath et al., 1996).</S><S sid=""9"" ssid=""5"">Recently, combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al., 1998; Brill and Wu, 1998).</S><S sid=""12"" ssid=""8"">The corpus-based statistical parsing community has many fast and accurate automated parsing systems, including systems produced by Collins (1997), Charniak (1997) and Ratnaparkhi (1997).</S><S sid=""54"" ssid=""40"">If the parse contains productions from outside our grammar the machine has no direct method for handling them (e.g. the resulting database query may be syntactically malformed).</S>",Method_Citation
20,W99-0623,C10-1151,0,1999,0,Henderson and Brill (1999) performs parse selection by maximizing the expected precision of selected parse with respect to the set of parses to be combined,Henderson and Brill (1999) performs parse selection by maximizing the expected precision of selected parse with respect to the set of parses to be combined,"'1','77','78','98'","<S sid=""1"" ssid=""1"">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S sid=""77"" ssid=""6"">Each parse is converted into a set of constituents represented as a tuples: (label, start, end).</S><S sid=""78"" ssid=""7"">The set is then compared with the set generated from the Penn Treebank parse to determine the precision and recall.</S><S sid=""98"" ssid=""27"">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S>","Method_Citation,Results_Citation"
