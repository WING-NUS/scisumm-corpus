Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W99-0613,N01-1023,0,"Collins and Singer, 1999",0,"Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)","Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)","'1','9','28','29'","<S sid=""1"" ssid=""1"">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid=""9"" ssid=""3"">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid=""28"" ssid=""22"">(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance.</S><S sid=""29"" ssid=""23"">Unfortunately, Yarowsky\s method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective function.</S>","Method_Citation,Results_Citation"
2,W99-0613,N01-1023,0,"Collins and Singer, 1999",0,"They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co Boosting)","(Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called Co Boosting)","'33','140','173','174'","<S sid=""33"" ssid=""27"">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S><S sid=""140"" ssid=""7"">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S><S sid=""173"" ssid=""40"">(3)), with one term for each classifier.</S><S sid=""174"" ssid=""41"">The two new terms force the two classifiers to agree, as much as possible, on the unlabeled examples.</S>",Method_Citation
3,W99-0613,W03-1509,0,Collins and Singer 1999,0,"Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on","Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on","'91','127','138','139'","<S sid=""91"" ssid=""24"">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S><S sid=""127"" ssid=""60"">The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.</S><S sid=""138"" ssid=""5"">(We would like to note though that unlike previous boosting algorithms, the CoBoost algorithm presented here is not a boosting algorithm under Valiant\s (Valiant 84) Probably Approximately Correct (PAC) model.)</S><S sid=""139"" ssid=""6"">This section describes AdaBoost, which is the basis for the CoBoost algorithm.</S>",Method_Citation
4,W99-0613,C02-1154,0,"Collins and Singer, 1999",0,"DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syn tactically analyzed corpus","DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus","'0','2','10','254'","<S sid=""0"">Unsupervised Models for Named Entity Classification Collins</S><S sid=""2"" ssid=""2"">A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classi- However, we show that the use of data can reduce the requirements for supervision to just 7 simple &amp;quot;seed&amp;quot; rules.</S><S sid=""10"" ssid=""4"">The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.</S><S sid=""254"" ssid=""5"">Future work should also extend the approach to build a complete named entity extractor - a method that pulls proper names from text and then classifies them.</S>",Method_Citation
5,W99-0613,C02-1154,0,"Collins and Singer, 1999",0,"(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances itset out to classify","(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances it set out to classify","'9','11','77','236'","<S sid=""9"" ssid=""3"">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid=""11"" ssid=""5"">For example, a good classifier would identify Mrs. Frank as a person, Steptoe &amp; Johnson as a company, and Honduras as a location.</S><S sid=""77"" ssid=""10"">In this paper k = 3 (the three labels are person, organization, location), and we set a = 0.1.</S><S sid=""236"" ssid=""3"">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>",Implication_Citation
6,W99-0613,W06-2204,0,"Collins and Singer, 1999",0,"In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification","In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification","'0','1','2','18'","<S sid=""0"">Unsupervised Models for Named Entity Classification Collins</S><S sid=""1"" ssid=""1"">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid=""2"" ssid=""2"">A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classi- However, we show that the use of data can reduce the requirements for supervision to just 7 simple &amp;quot;seed&amp;quot; rules.</S><S sid=""18"" ssid=""12"">But we will show that the use of unlabeled data can drastically reduce the need for supervision.</S>",Method_Citation
8,W99-0613,W03-1022,0,"Collins and Singer, 1999",0,"Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)","Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)","'0','1','8','9'","<S sid=""0"">Unsupervised Models for Named Entity Classification Collins</S><S sid=""1"" ssid=""1"">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid=""8"" ssid=""2"">Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S><S sid=""9"" ssid=""3"">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>",Method_Citation
9,W99-0613,E09-1018,0,"Collinsand Singer, 1999",0,"While EM has worked quite well for a few tasks, notably ma chine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success inmost others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)","While EM has worked quite well for a few tasks, notably machine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success in most others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)","'7','9','16','204'","<S sid=""7"" ssid=""1"">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S><S sid=""9"" ssid=""3"">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid=""16"" ssid=""10"">Supervised methods have been applied quite successfully to the full MUC named-entity task (Bikel et al. 97).</S><S sid=""204"" ssid=""71"">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S>",Method_Citation
11,W99-0613,W07-1712,0,"Collins and Singer, 1999",0,"In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)","In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)","'33','55','204','219'","<S sid=""33"" ssid=""27"">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S><S sid=""55"" ssid=""9"">In addition to the named-entity string (Maury Cooper or Georgia), a contextual predictor was also extracted.</S><S sid=""204"" ssid=""71"">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S><S sid=""219"" ssid=""86"">Finally, we would like to note that it is possible to devise similar algorithms based with other objective functions than the one given in Equ.</S>",Method_Citation
12,W99-0613,W09-2208,0,"Collins and Singer, 1999",0,"Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky ?smethod (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)","Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky's method (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)","'5','27','29','30'","<S sid=""5"" ssid=""5"">The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98).</S><S sid=""27"" ssid=""21"">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S><S sid=""29"" ssid=""23"">Unfortunately, Yarowsky\s method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective function.</S><S sid=""30"" ssid=""24"">(Blum and Mitchell 98) offer a promising formulation of redundancy, also prove some results about how the use of unlabeled examples can help classification, and suggest an objective function when training with unlabeled examples.</S>","Results_Citation,Method_Citation"
13,W99-0613,W06-2207,0,"Collins and Singer, 1999",0,"This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)","This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)","'2','7','130','203'","<S sid=""2"" ssid=""2"">A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classi- However, we show that the use of data can reduce the requirements for supervision to just 7 simple &amp;quot;seed&amp;quot; rules.</S><S sid=""7"" ssid=""1"">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S><S sid=""130"" ssid=""63"">So the success of the algorithm may well be due to its success in maximizing the number of unlabeled examples on which the two decision lists agree.</S><S sid=""203"" ssid=""70"">Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).</S>",Results_Citation
15,W99-0613,W06-2207,0,"Collins and Singer, 1999",0,"(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here","(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here","'0','12','60','140'","<S sid=""0"">Unsupervised Models for Named Entity Classification Collins</S><S sid=""12"" ssid=""6"">The approach uses both spelling and contextual rules.</S><S sid=""60"" ssid=""14"">In principle a feature could be an arbitrary predicate of the (spelling, context) pair; for reasons that will become clear, features are limited to querying either the spelling or context alone.</S><S sid=""140"" ssid=""7"">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S>",Method_Citation
16,W99-0613,P12-1065,0,1999,0,We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores ?fj?| ?fj |+ |? f |+ L (1) where  is a smoothing constant,We use Collins and Singer (1999) for our exact specification of Yarowsky,"'0','5','12','80'","<S sid=""0"">Unsupervised Models for Named Entity Classification Collins</S><S sid=""5"" ssid=""5"">The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98).</S><S sid=""12"" ssid=""6"">The approach uses both spelling and contextual rules.</S><S sid=""80"" ssid=""13"">The 2(Yarowsky 95) describes the use of more sophisticated smoothing methods.</S>",Method_Citation
