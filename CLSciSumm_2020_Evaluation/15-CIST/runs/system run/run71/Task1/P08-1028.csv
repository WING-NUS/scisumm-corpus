Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1028,D08-1094,0,2008,0,"Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and Kadditional knowledge","Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and K additional knowledge","[51, 52, 57, 194]","['<S sid=""51"" ssid=""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>', '<S sid=""52"" ssid=""25"">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S>', '<S sid=""57"" ssid=""5"">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>', '<S sid=""194"" ssid=""6"">Importantly, additive models capture composition by considering all vector components representing the meaning of the verb and its subject, whereas multiplicative models consider a subset, namely non-zero components.</S>']",Method_Citation
4,P08-1028,P14-1060,0,2008,0,"While works such asthe SDSM model suffer from the problem of sparsity in composing structures beyond bi grams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstetteand Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","[10, 23, 42, 87]","['<S sid=""10"" ssid=""6"">Moreover, the vector similarities within such semantic spaces have been shown to substantially correlate with human similarity judgments (McDonald, 2000) and word association norms (Denhire and Lemaire, 2004).</S>', '<S sid=""23"" ssid=""19"">The downside of this approach is that differences in meaning are qualitative rather than quantitative, and degrees of similarity cannot be expressed easily.</S>', '<S sid=""42"" ssid=""15"">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>', '<S sid=""87"" ssid=""35"">Combining the multiplicative model with an additive model, which does not suffer from this problem, could mitigate this problem: pi = &#945;ui +&#946;vi +&#947;uivi (11) where &#945;, &#946;, and &#947; are weighting constants.</S>']",Method_Citation
6,P08-1028,P10-1097,0,2008,0,"Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","[21, 44, 51, 138]","['<S sid=""21"" ssid=""17"">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S>', '<S sid=""44"" ssid=""17"">For example, assuming that individual words are represented by vectors, we can compute the meaning of a sentence by taking their mean (Foltz et al., 1998; Landauer and Dumais, 1997).</S>', '<S sid=""51"" ssid=""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>', '<S sid=""138"" ssid=""51"">Model Parameters Irrespectively of their form, all composition models discussed here are based on a semantic space for representing the meanings of individual words.</S>']","Results_Citation,Method_Citation"
7,P08-1028,P10-1097,0,2008,0,"Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting","Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting","[26, 27, 90, 183]","['<S sid=""26"" ssid=""22"">Specifically, we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S>', '<S sid=""27"" ssid=""23"">Our results show that the multiplicative models are superior and correlate significantly with behavioral data.</S>', '<S sid=""90"" ssid=""3"">He argues that the subjects of ran in The color ran and The horse ran select different senses of ran.</S>', '<S sid=""183"" ssid=""17"">The weighted additive model (p = 0.09) is not significantly different from the baseline either or Kintsch (2001) (p = 0.09).</S>']",Method_Citation
8,P08-1028,D11-1094,0,2008,0,"And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","[50, 51, 52, 75]","['<S sid=""50"" ssid=""23"">The merits of different approaches are illustrated with a few hand picked examples and parameter values and large scale evaluations are uniformly absent (see Frank et al. (2007) for a criticism of Kintsch&#8217;s (2001) evaluation standards).</S>', '<S sid=""51"" ssid=""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>', '<S sid=""52"" ssid=""25"">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S>', '<S sid=""75"" ssid=""23"">An extreme form of this differential in the contribution of constituents is where one of the vectors, say u, contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic, however it can serve as a simple baseline against which to compare more sophisticated models.</S>']",Method_Citation
9,P08-1028,W11-0131,0,2008,0,"Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","[9, 35, 52, 57]","['<S sid=""9"" ssid=""5"">In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997) and text comprehension (Landauer and Dumais, 1997; Foltz et al., 1998).</S>', '<S sid=""35"" ssid=""8"">The tensor product u &#174; v is a matrix whose components are all the possible products uivj of the components of vectors u and v. A major difficulty with tensor products is their dimensionality which is higher than the dimensionality of the original vectors (precisely, the tensor product has dimensionality m x n).</S>', '<S sid=""52"" ssid=""25"">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S>', '<S sid=""57"" ssid=""5"">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>']",Hypothesis_Citation
10,P08-1028,W11-0131,0,2008,0,"As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","[1, 42, 43, 57]","['<S sid=""1"" ssid=""1"">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S>', '<S sid=""42"" ssid=""15"">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>', '<S sid=""43"" ssid=""16"">Vector addition is by far the most common method for representing the meaning of linguistic sequences.</S>', '<S sid=""57"" ssid=""5"">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>']",Method_Citation
11,P08-1028,P13-2083,0,2008,0,"Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","[57, 58, 59, 60]","['<S sid=""57"" ssid=""5"">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>', '<S sid=""58"" ssid=""6"">We define a general class of models for this process of composition as: The expression above allows us to derive models for which p is constructed in a distinct space from u and v, as is the case for tensor products.</S>', '<S sid=""59"" ssid=""7"">It also allows us to derive models in which composition makes use of background knowledge K and models in which composition has a dependence, via the argument R, on syntax.</S>', '<S sid=""60"" ssid=""8"">To derive specific models from this general framework requires the identification of appropriate constraints to narrow the space of functions being considered.</S>']","Aim_Citation,Method_Citation"
12,P08-1028,P13-2083,0,"Mitchell and Lapata, 2008",0,"As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","[51, 52, 74, 168]","['<S sid=""51"" ssid=""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>', '<S sid=""52"" ssid=""25"">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S>', '<S sid=""74"" ssid=""22"">As an example if we set &#945; to 0.4 and &#946; to 0.6, then horse= [0 2.4 0.8 4 1.6] and run = [0.6 4.8 2.4 2.4 0], and their sum horse + run = [0.6 5.6 3.2 6.4 1.6].</S>', '<S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch&#8217;s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S>']","Aim_Citation,Method_Citation,Results_Citation"
13,P08-1028,P10-1021,0,2008,0,"Althoughthis model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003) .In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","Although this model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","[9, 51, 52, 60]","['<S sid=""9"" ssid=""5"">In cognitive science vector-based models have been successful in simulating semantic priming (Lund and Burgess, 1996; Landauer and Dumais, 1997) and text comprehension (Landauer and Dumais, 1997; Foltz et al., 1998).</S>', '<S sid=""51"" ssid=""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>', '<S sid=""52"" ssid=""25"">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S>', '<S sid=""60"" ssid=""8"">To derive specific models from this general framework requires the identification of appropriate constraints to narrow the space of functions being considered.</S>']","Hypothesis_Citation,Method_Citation"
14,P08-1028,P10-1021,0,2008,0,"Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi =ui+ vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi =ui? vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","[38, 39, 65, 194]","['<S sid=""38"" ssid=""11"">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>', '<S sid=""39"" ssid=""12"">The compression is achieved by summing along the transdiagonal elements of the tensor product.</S>', '<S sid=""65"" ssid=""13"">So, if we assume that only the ith components of u and v contribute to the ith component of p, that these components are not dependent on i, and that the function is symmetric with regard to the interchange of u and v, we obtain a simpler instantiation of an additive model: Analogously, under the same assumptions, we obtain the following simpler multiplicative model: only the ith components of u and v contribute to the ith component of p. Another class of models can be derived by relaxing this constraint.</S>', '<S sid=""194"" ssid=""6"">Importantly, additive models capture composition by considering all vector components representing the meaning of the verb and its subject, whereas multiplicative models consider a subset, namely non-zero components.</S>']",Method_Citation
15,P08-1028,W11-0115,0,2008,0,"Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","[2, 168, 174, 191]","['<S sid=""2"" ssid=""2"">Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.</S>', '<S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch&#8217;s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S>', '<S sid=""174"" ssid=""8"">The simple additive model fails to distinguish between High and Low Similarity items.</S>', '<S sid=""191"" ssid=""3"">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S>']","Method_Citation,Results_Citation"
16,P08-1028,W11-0115,0,2008,0,"The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","[64, 156, 174, 176]","['<S sid=""64"" ssid=""12"">Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>', '<S sid=""156"" ssid=""69"">This yielded a weighted sum consisting of 95% verb, 0% noun and 5% of their multiplicative combination.</S>', '<S sid=""174"" ssid=""8"">The simple additive model fails to distinguish between High and Low Similarity items.</S>', '<S sid=""176"" ssid=""10"">The multiplicative and combined models yield means closer to the human ratings.</S>']","Method_Citation,Results_Citation"
17,P08-1028,W11-0115,0,2008,0,"For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","[26, 99, 163, 172]","['<S sid=""26"" ssid=""22"">Specifically, we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S>', '<S sid=""99"" ssid=""12"">In order to establish an independent measure of sentence similarity, we assembled a set of experimental materials and elicited similarity ratings from human subjects.</S>', '<S sid=""163"" ssid=""76"">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S>', '<S sid=""172"" ssid=""6"">Here, we are interested in relative differences, since the two types of ratings correspond to different scales.</S>']","Implication_Citation,Results_Citation"
18,P08-1028,W11-1310,0,2008,0,We use other WSM settings following Mitchell and Lapata (2008),We use other WSM settings following Mitchell and Lapata (2008),"[7, 14, 143, 163]","['<S sid=""7"" ssid=""3"">A variety of NLP tasks have made good use of vector-based models.</S>', '<S sid=""14"" ssid=""10"">This is illustrated in the example below taken from Landauer et al. (1997).</S>', '<S sid=""143"" ssid=""56"">We used WordSim353, a benchmark dataset (Finkelstein et al., 2002), consisting of relatedness judgments (on a scale of 0 to 10) for 353 word pairs.</S>', '<S sid=""163"" ssid=""76"">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S>']",Method_Citation
19,P08-1028,W11-1310,0,2008,0,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,"[51, 64, 164, 182]","['<S sid=""51"" ssid=""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>', '<S sid=""64"" ssid=""12"">Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>', '<S sid=""164"" ssid=""77"">A more scrupulous evaluation requires directly correlating all the individual participants&#8217; similarity judgments with those of the models.6 We used Spearman&#8217;s p for our correlation analyses.</S>', '<S sid=""182"" ssid=""16"">The lowest correlation (p = 0.04) is observed for the simple additive model which is not significantly different from the non-compositional baseline model.</S>']","Implication_Citation,Method_Citation,Results_Citation"
20,P08-1028,W11-1310,0,"Mitchell and Lapata, 2008",0,"We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","[51, 52, 74, 168]","['<S sid=""51"" ssid=""24"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>', '<S sid=""52"" ssid=""25"">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S>', '<S sid=""74"" ssid=""22"">As an example if we set &#945; to 0.4 and &#946; to 0.6, then horse= [0 2.4 0.8 4 1.6] and run = [0.6 4.8 2.4 2.4 0], and their sum horse + run = [0.6 5.6 3.2 6.4 1.6].</S>', '<S sid=""168"" ssid=""2"">These included three additive models, i.e., simple addition (equation (5), Add), weighted addition (equation (7), WeightAdd), and Kintsch&#8217;s (2001) model (equation (10), Kintsch), a multiplicative model (equation (6), Multiply), and also a model which combines multiplication with addition (equation (11), Combined).</S>']","Aim_Citation,Hypothesis_Citation,Method_Citation"
