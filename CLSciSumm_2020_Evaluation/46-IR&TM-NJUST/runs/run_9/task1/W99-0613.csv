Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W99-0613,N01-1023,0,"Collins and Singer, 1999",0,"Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)","Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)","'55','57','102','150'","<S sid=""55"" ssid=""9"">In addition to the named-entity string (Maury Cooper or Georgia), a contextual predictor was also extracted.</S><S sid=""57"" ssid=""11"">From here on we will refer to the named-entity string itself as the spelling of the entity, and the contextual predicate as the context.</S><S sid=""102"" ssid=""35"">(Blum and Mitchell 98) describe learning in the following situation: X = X1 X X2 where X1 and X2 correspond to two different ""views"" of an example.</S><S sid=""150"" ssid=""17"">Pseudo-code describing the generalized boosting algorithm of Schapire and Singer is given in Figure 1.</S>",'Method_Citation'
2,W99-0613,N01-1023,0,"Collins and Singer, 1999",0,"They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co Boosting)","(Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called Co Boosting)","'136','174','181','251'","<S sid=""136"" ssid=""3"">We then discuss how we adapt and generalize a boosting algorithm, AdaBoost, to the problem of named entity classification.</S><S sid=""174"" ssid=""41"">The two new terms force the two classifiers to agree, as much as possible, on the unlabeled examples.</S><S sid=""181"" ssid=""48"">Each round is composed of two stages; each stage updates one of the classifiers while keeping the other classifier fixed.</S><S sid=""251"" ssid=""2"">In addition to a heuristic based on decision list learning, we also presented a boosting-like framework that builds on ideas from (Blum and Mitchell 98).</S>",'Method_Citation'
3,W99-0613,W03-1509,0,Collins and Singer 1999,0,"Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on","Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on","'33','81','139','178'","<S sid=""33"" ssid=""27"">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S><S sid=""81"" ssid=""14"">It's not clear how to apply these methods in the unsupervised case, as they required cross-validation techniques: for this reason we use the simpler smoothing method shown here. input to the unsupervised algorithm is an initial, ""seed"" set of rules.</S><S sid=""139"" ssid=""6"">This section describes AdaBoost, which is the basis for the CoBoost algorithm.</S><S sid=""178"" ssid=""45"">Then, it can be verified that We can now derive the CoBoost algorithm as a means of minimizing Zco.</S>",'Method_Citation'
4,W99-0613,C02-1154,0,"Collins and Singer, 1999",0,"DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syn tactically analyzed corpus","DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus","'0','10','47','204'","<S sid=""0"" ssid=""1"">Unsupervised Models for Named Entity Classification Collins</S><S sid=""10"" ssid=""4"">The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.</S><S sid=""47"" ssid=""1"">971,746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).</S><S sid=""204"" ssid=""71"">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S>",'Method_Citation'
5,W99-0613,C02-1154,0,"Collins and Singer, 1999",0,"(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances itset out to classify","(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances it set out to classify","'0','11','33','203'","<S sid=""0"" ssid=""1"">Unsupervised Models for Named Entity Classification Collins</S><S sid=""11"" ssid=""5"">For example, a good classifier would identify Mrs. Frank as a person, Steptoe & Johnson as a company, and Honduras as a location.</S><S sid=""33"" ssid=""27"">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S><S sid=""203"" ssid=""70"">Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).</S>",'Method_Citation'
6,W99-0613,W06-2204,0,"Collins and Singer, 1999",0,"In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification","In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification","'1','9','136','254'","<S sid=""1"" ssid=""1"">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid=""9"" ssid=""3"">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid=""136"" ssid=""3"">We then discuss how we adapt and generalize a boosting algorithm, AdaBoost, to the problem of named entity classification.</S><S sid=""254"" ssid=""5"">Future work should also extend the approach to build a complete named entity extractor - a method that pulls proper names from text and then classifies them.</S>",'Method_Citation'
8,W99-0613,W03-1022,0,"Collins and Singer, 1999",0,"Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)","Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)","'0','47'","<S sid=""0"" ssid=""1"">Unsupervised Models for Named Entity Classification Collins</S><S sid=""47"" ssid=""1"">971,746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).</S>",'Method_Citation'
9,W99-0613,E09-1018,0,"Collinsand Singer, 1999",0,"While EM has worked quite well for a few tasks, notably ma chine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success inmost others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)","While EM has worked quite well for a few tasks, notably machine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success in most others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)","'3','47','55','254'","<S sid=""3"" ssid=""3"">The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type.</S><S sid=""47"" ssid=""1"">971,746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).</S><S sid=""55"" ssid=""9"">In addition to the named-entity string (Maury Cooper or Georgia), a contextual predictor was also extracted.</S><S sid=""254"" ssid=""5"">Future work should also extend the approach to build a complete named entity extractor - a method that pulls proper names from text and then classifies them.</S>",'Method_Citation'
11,W99-0613,W07-1712,0,"Collins and Singer, 1999",0,"In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)","In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)","'33','140','142','204'","<S sid=""33"" ssid=""27"">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S><S sid=""140"" ssid=""7"">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S><S sid=""142"" ssid=""9"">The input to AdaBoost is a set of training examples ((xi , yi), , (xâ€ž.â€ž yrn)).</S><S sid=""204"" ssid=""71"">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S>",'Method_Citation'
12,W99-0613,W09-2208,0,"Collins and Singer, 1999",0,"Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky ?smethod (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)","Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky's method (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)","'27','33','135','204'","<S sid=""27"" ssid=""21"">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S><S sid=""33"" ssid=""27"">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S><S sid=""135"" ssid=""2"">We first give a brief overview of boosting algorithms.</S><S sid=""204"" ssid=""71"">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S>",'Method_Citation'
13,W99-0613,W06-2207,0,"Collins and Singer, 1999",0,"This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)","This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)",'33',"<S sid=""33"" ssid=""27"">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S>",'Method_Citation'
15,W99-0613,W06-2207,0,"Collins and Singer, 1999",0,"(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here","(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here","'33','140','203','204'","<S sid=""33"" ssid=""27"">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S><S sid=""140"" ssid=""7"">AdaBoost was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.</S><S sid=""203"" ssid=""70"">Several extensions of AdaBoost for multiclass problems have been suggested (Freund and Schapire 97; Schapire and Singer 98).</S><S sid=""204"" ssid=""71"">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S>",'Method_Citation'
16,W99-0613,P12-1065,0,1999,0,We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores ?fj?| ?fj |+ |? f |+ L (1) where  is a smoothing constant,We use Collins and Singer (1999) for our exact specification of Yarowsky,"'33','80','156','204'","<S sid=""33"" ssid=""27"">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S><S sid=""80"" ssid=""13"">The 2(Yarowsky 95) describes the use of more sophisticated smoothing methods.</S><S sid=""156"" ssid=""23"">Our derivation is slightly different from the one presented in (Schapire and Singer 98) as we restrict at to be positive.</S><S sid=""204"" ssid=""71"">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S>",'Method_Citation'
