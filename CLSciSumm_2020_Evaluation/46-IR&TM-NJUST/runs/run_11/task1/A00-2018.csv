Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
2,A00-2018,N10-1002,0,"Charniak, 2000",0,"As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)","As a benchmark VPC extraction system, we use the Charniak parser (Charniak, 2000)","'2','5','6','87'","<S sid=""2"" ssid=""2"">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid=""5"" ssid=""1"">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] ""standard"" sections of the Wall Street Journal tree-bank.</S><S sid=""6"" ssid=""2"">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid=""87"" ssid=""56"">In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17].</S>",'Method_Citation'
3,A00-2018,W11-0610,0,"Charniak, 2000",0,"Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniakparser (Charniak, 2000), also trained on the Switch board tree bank","Each of these scores can be calculated from a provided syntactic parse tree, and to generate these we made use of the Charniak parser (Charniak, 2000), also trained on the Switch board tree bank","'108','172','173','178'","<S sid=""108"" ssid=""19"">As is typical, all of the standard measures tell pretty much the same story, with the new parser outperforming the other three parsers.</S><S sid=""172"" ssid=""63"">As one can see in Figure 2, a firstorder Markov grammar (with all the aforementioned improvements) performs slightly worse than the equivalent tree-bank-grammar parser.</S><S sid=""173"" ssid=""64"">However, a second-order grammar does slightly better and a third-order grammar does significantly better than the tree-bank parser.</S><S sid=""178"" ssid=""5"">The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought.</S>",'Method_Citation'
4,A00-2018,W06-3119,0,"Charniak, 2000",0,"We then use Charniak? s parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus","We then use Charniak's parser (Charniak, 2000) to generate the most likely parse tree for each English target sentence in the training corpus","'17','107','115','148'","<S sid=""17"" ssid=""6"">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S><S sid=""107"" ssid=""18"">The results for the new parser as well as for the previous top-three individual parsers on this corpus are given in Figure 1.</S><S sid=""115"" ssid=""6"">As noted in [5], that system is based upon a ""tree-bank grammar"" - a grammar read directly off the training corpus.</S><S sid=""148"" ssid=""39"">So, e.g., even if the word ""conflating"" does not appear in the training corpus (and it does not), the ""ng"" ending allows our program to guess with relative security that the word has the vbg pre-terminal, and thus the probability of various rule expansions can be considerable sharpened.</S>",'Method_Citation'
5,A00-2018,N03-2024,0,"Charniak, 2000",0,"We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak? s statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)","We were interested in the occurrence of features such as type and number of premodifiers, presence and type of post modifiers, and form of name reference for people. We constructed a large, automatically annotated corpus by merging the output of Charniak's statistical parser (Charniak, 2000) with that of the IBM named entity recognition system Nominator (Wacholder et al,1997)","'0','107','115','129'","<S sid=""0"" ssid=""1"">A Maximum-Entropy-Inspired Parser *</S><S sid=""107"" ssid=""18"">The results for the new parser as well as for the previous top-three individual parsers on this corpus are given in Figure 1.</S><S sid=""115"" ssid=""6"">As noted in [5], that system is based upon a ""tree-bank grammar"" - a grammar read directly off the training corpus.</S><S sid=""129"" ssid=""20"">It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments), it does not guess the pre-terminal before guessing the lexical head, and it uses a tree-bank grammar rather than a Markov grammar.</S>",'Method_Citation'
6,A00-2018,N06-1039,0,"Charniak, 2000",0,"After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARFstructure for each sentence in every article","After getting a set of basic clusters, we pass them to an existing statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARF structure for each sentence in every article",'155',"<S sid=""155"" ssid=""46"">For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S>",'Method_Citation'
7,A00-2018,C04-1180,0,2000,0,"The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)","The levels of accuracy and robustness recently achieved by statistical parsers (e.g. Collins (1999), Charniak (2000)) have led to their use in a number of NLP applications, such as question-answering (Pasca and Harabagiu, 2001), machine translation (Charniak et al, 2003), sentence simplification (Carroll et al, 1999), and a linguist? s search engine (Resnik and Elkiss, 2003)","'108','176','178'","<S sid=""108"" ssid=""19"">As is typical, all of the standard measures tell pretty much the same story, with the new parser outperforming the other three parsers.</S><S sid=""176"" ssid=""3"">That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.</S><S sid=""178"" ssid=""5"">The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought.</S>",'Method_Citation'
8,A00-2018,W05-0638,0,"Charniak, 2000",0,"In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)","In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000)","'87','133','136','178'","<S sid=""87"" ssid=""56"">In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17].</S><S sid=""133"" ssid=""24"">One of the first and without doubt the most significant change we made in the current parser is to move from two stages of probabilistic decisions at each node to three.</S><S sid=""136"" ssid=""27"">It turns out that usefulness of this process had already been discovered by Collins [10], who in turn notes (personal communication) that it was previously used by Eisner [12].</S><S sid=""178"" ssid=""5"">The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought.</S>",'Method_Citation'
9,A00-2018,P05-1065,0,"Charniak, 2000",0,"We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis","We also use a standard statistical parser (Charniak, 2000) to provide syntactic analysis","'48','87','108','114'","<S sid=""48"" ssid=""17"">Maximum-entropy models have two benefits for a parser builder.</S><S sid=""87"" ssid=""56"">In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17].</S><S sid=""108"" ssid=""19"">As is typical, all of the standard measures tell pretty much the same story, with the new parser outperforming the other three parsers.</S><S sid=""114"" ssid=""5"">That parser, as stated in Figure 1, achieves an average precision/recall of 87.5.</S>",'Method_Citation'
10,A00-2018,P05-1065,0,"Charniak, 2000",0,"For each article, we calculated the per cent age of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus","For each article, we calculated the percentage of a) all word instances (tokens) and b) all unique words (types) not on these lists, resulting in three token OOV rate features and three type OOV rate features per article. The parse features are generated using the Charniak parser (Charniak, 2000) trained on the standard Wall Street Journal Treebank corpus","'0','50','104','114'","<S sid=""0"" ssid=""1"">A Maximum-Entropy-Inspired Parser *</S><S sid=""50"" ssid=""19"">This point is emphasized by Ratnaparkhi in discussing his parser [17).</S><S sid=""104"" ssid=""15"">Again as standard, we take separate measurements for all sentences of length < 40 and all sentences of length < 100.</S><S sid=""114"" ssid=""5"">That parser, as stated in Figure 1, achieves an average precision/recall of 87.5.</S>",'Method_Citation'
11,A00-2018,P04-1040,0,2000,0,"The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 %unlabelled and 84 %labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank.The paper is organized as follows","The evaluation of the transformed output of the parsers of Charniak (2000) and Collins (1999) gives 90 % unlabelled and 84 % labelled accuracy with respect to dependencies, when measured against a dependency corpus derived from the Penn Treebank. The paper is organized as follows",'108',"<S sid=""108"" ssid=""19"">As is typical, all of the standard measures tell pretty much the same story, with the new parser outperforming the other three parsers.</S>",'Method_Citation'
12,A00-2018,P04-1040,0,2000,0,"Blaheta and Charniak (2000) presented the first method for assigning Pennfunctional tags to constituents identified by a parser. Pattern-matching approaches were used in (John son, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees","Blaheta and Charniak (2000) presented the first method for assigning Penn functional tags to constituents identified by a parser. Pattern-matching approaches were used in (Johnson, 2002) and (Jijkoun, 2003) to recover non-local dependencies in phrase trees","'0','32','40','87'","<S sid=""0"" ssid=""1"">A Maximum-Entropy-Inspired Parser *</S><S sid=""32"" ssid=""1"">The major problem confronting the author of a generative parser is what information to use to condition the probabilities required in the model, and how to smooth the empirically obtained probabilities to take the sting out of the sparse data problems that are inevitable with even the most modest conditioning.</S><S sid=""40"" ssid=""9"">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S><S sid=""87"" ssid=""56"">In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17].</S>",'Method_Citation'
13,A00-2018,P04-1040,0,2000,0,"As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically","As an alternative to hard coded heuristics, Blaheta and Charniak (2000) proposed to recover the Penn functional tags automatically",'155',"<S sid=""155"" ssid=""46"">For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S>",'Method_Citation'
17,A00-2018,N06-1022,0,2000,0,"The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents","The parser of Charniak (2000) is also a two-stage ctf model, where the first stage is a smoothed Markov grammar (it uses up to three previous constituents as context), and the second stage is a lexicalized Markov grammar with extra annotations about parents and grandparents","'44','49','87','167'","<S sid=""44"" ssid=""13"">Now consider computing a conditional probability p(a H) with a set of features h that connect a to the history H. In a log-linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability, the higher the absolute value of the associated A.</S><S sid=""49"" ssid=""18"">First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various ""features"" suggests that the probability model should be easily changeable â€” just change the set of features used.</S><S sid=""87"" ssid=""56"">In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17].</S><S sid=""167"" ssid=""58"">Including this information within a standard deleted-interpolation model causes a 0.6% decrease from the results using the less conventional model.</S>",'Method_Citation'
18,A00-2018,N06-1022,0,2000,0,"Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))","Most recently, McDonald et al (2005) have implemented a dependency parser with good accuracy (it is almost as good at dependency parsing as Charniak (2000)) and very impressive speed (it is about ten times faster than Collins (1997) and four times faster than Charniak (2000))","'2','114','136','137'","<S sid=""2"" ssid=""2"">This represents a 13% decrease in error rate over the best single-parser results on this corpus [9].</S><S sid=""114"" ssid=""5"">That parser, as stated in Figure 1, achieves an average precision/recall of 87.5.</S><S sid=""136"" ssid=""27"">It turns out that usefulness of this process had already been discovered by Collins [10], who in turn notes (personal communication) that it was previously used by Eisner [12].</S><S sid=""137"" ssid=""28"">However, Collins in [10] does not stress the decision to guess the head's pre-terminal first, and it might be lost on the casual reader.</S>",'Method_Citation'
19,A00-2018,H05-1035,0,"Charniak, 2000",0,"The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions","The feature set contains complex information extracted automatically from candidate syntax trees generated by parsing (Charniak, 2000), trees that will be improved by more accurate PP-attachment decisions",'9',"<S sid=""9"" ssid=""5"">The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.</S>",'Method_Citation'
20,A00-2018,P04-1042,0,2000,0,"Note that the dependency figures of Dienes lag behind even the parsed results for Johnson? s model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5% .Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size","Note that the dependency figures of Dienes lag behind even the parsed results for Johnson's model; this may well be due to the fact that Dienes built his model as an extension of Collins (1999), which lags behind Charniak (2000) by about 1.3-1.5%. Manual investigation of errors on English gold standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size","'87','164','165','166'","<S sid=""87"" ssid=""56"">In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17].</S><S sid=""164"" ssid=""55"">When we do so using our maximum-entropy-inspired conditioning, we get another 0.45% improvement in average precision/recall, as indicated in Figure 2 on the line labeled ""MaxEnt-Inspired'.</S><S sid=""165"" ssid=""56"">Note that we also tried including this information using a standard deleted-interpolation model.</S><S sid=""166"" ssid=""57"">The results here are shown in the line ""Standard Interpolation"".</S>",'Method_Citation'
