<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
NA
"<S sid=""189"" ssid=""1"">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid=""190"" ssid=""2"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>"
"S sid=""185"" ssid=""19"">The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.</S>
    <S sid=""186"" ssid=""20"">The combined model is best overall with &#961; = 0.19.</S>
    <S sid=""187"" ssid=""21"">However, the difference between the two models is not statistically significant.</S>"
<S sid="48" ssid="21">The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.</S>
<S sid="53" ssid="1">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.</S><S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
"<S sid=""68"" ssid=""16"">Although the composition model in (5) is commonly used in the literature, from a linguistic perspective, the model in (6) is more appealing.</S>
    <S sid=""69"" ssid=""17"">Simply adding the vectors u and v lumps their contents together rather than allowing the content of one vector to pick out the relevant content of the other.</S>
    <S sid=""70"" ssid=""18"">Instead, it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v, and vice versa.</S>"
"<S sid=""24"" ssid=""20"">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
    <S sid=""25"" ssid=""21"">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>"
<S sid="64" ssid="12">Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>
"<S sid=""176"" ssid=""10"">The multiplicative and combined models yield means closer to the human ratings.</S>
    <S sid=""177"" ssid=""11"">The difference between High and Low similarity values estimated by these models are statistically significant (p &lt; 0.01 using the Wilcoxon rank sum test).</S>"
<S sid="191" ssid="3">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S
<S sid="24" ssid="20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>