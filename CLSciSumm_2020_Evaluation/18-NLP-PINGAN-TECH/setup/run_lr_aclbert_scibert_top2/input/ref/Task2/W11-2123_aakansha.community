<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
<S sid="45" ssid="23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
<S sid="136" ssid="8">We offer a state function s(wn1) = wn&#65533; where substring wn&#65533; is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.</S>
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
<S sid="8" ssid="3">Queries take the form p(wn|wn&#8722;1 1 ) where wn1 is an n-gram.</S>
S sid="205" ssid="24">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al., 2009).</S>
<S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
<S sid="204" ssid="23">For RandLM, we used the settings in the documentation: 8 bits per value and false positive probability 1 256.</S>
<S sid="229" ssid="48">Then we ran binary search to determine the least amount of memory with which it would run.</S>
<S sid="93" ssid="71">The cost of storing these averages, in bits, is Because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.</S>
S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
