This paper talks about KenLM: Faster and Smaller Language Model Queries.  The PROBING data structure uses linear probing hash tables and is designed for speed. This paper presents methods to query N-gram language models, minimizing time and space costs. Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders. For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed. The TRIE model uses less memory than the smallest lossless alternative and is still faster than SRILM. The code is open source, has minimal dependencies, and offers both C++ and Java interfaces for integration.