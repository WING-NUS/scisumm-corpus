<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
<S sid="114" ssid="63">Ideally, the &#8220;glue&#8221; documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.</S>
<S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
<S sid="18" ssid="14">In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S>
<S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
<S sid="119" ssid="68">The lower the divergence, the more similar the distributions are to each other.</S>
<S sid="148" ssid="97">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
<S sid="163" ssid="112">Performance continues to improve with longer documents, most likely due to better topic inference.</S>
<S sid="105" ssid="54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.</S>
    <S sid="10" ssid="6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S>