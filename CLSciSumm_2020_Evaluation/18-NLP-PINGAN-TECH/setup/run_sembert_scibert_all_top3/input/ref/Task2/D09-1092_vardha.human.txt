This paper talks about Polylingual Topic Models. The main keywords annotated in this paper are Polylingual Topic Model, languages and parallel documents. This paper introduces a Polylingual topic model that discovers topics aligned across multiple languages. The Polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) for modeling Polylingual document tuples. First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents. This property is useful for building machine translation systems as well as for human readers. Second, because comparable texts may not use exactly the same topics, it becomes crucially important to be able to characterize differences in topic prevalence at the document level and at the language-wide level. When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.