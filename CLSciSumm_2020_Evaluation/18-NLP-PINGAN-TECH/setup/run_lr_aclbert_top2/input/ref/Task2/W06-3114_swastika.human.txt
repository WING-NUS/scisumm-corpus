Koehn and Monz carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs. While many systems had similar performance, the results offered interesting insights, especially, about the relative performance of statistical and rule-based systems. Due to many similarly performing systems, they are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics. The bias of automatic methods in favour of statistical systems seemed to be less pronounced on out-of-domain test data. The manual evaluation of scoring translation on a graded scale from 1&#8211;5 seemed to be very hard to perform. Human judges also pointed out difficulties with the evaluation of long sentences. They found replacing it with a ranked evaluation to be more suitable.