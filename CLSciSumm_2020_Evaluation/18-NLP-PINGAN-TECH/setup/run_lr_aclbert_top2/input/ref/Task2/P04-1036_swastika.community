<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
<S sid="68" ssid="24">We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.</S>
<S sid="83" ssid="12">The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.</S>
<S sid="101" ssid="30">Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.</S>
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
<S sid="46" ssid="2">This provides the nearest neighbours to each target word, along with the distributional similarity score between the target word and its neighbour.</S>
<S sid="105" ssid="3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents, rather than a lexical sample task, where the target words are manually determined and the results will depend on the skew of the words in the sample.</S>
<S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>