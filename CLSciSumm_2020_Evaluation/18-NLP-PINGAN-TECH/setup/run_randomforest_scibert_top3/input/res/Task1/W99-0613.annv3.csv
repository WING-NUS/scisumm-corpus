Citance Number,Citation Marker,Citation Marker Offset,Citation Offset,Citation Text,Citation Text Clean,Citing Article,Discourse Facet,Reference Article,Reference Offset,Reference Text
1.0,"Collins and Singer, 1999",0,0,"Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)","Co-Training has been used before in applications like word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named entity identification (Collins and Singer, 1999)",N01-1023,"Method_Citation,Hypothesis_Citation,Implication_Citation",W99-0613,"'0','30','28'","<S sid=""0"" ssid=""0"">Unsupervised Models for Named Entity Classification Collins</S><S sid=""30"" ssid=""24"">(Blum and Mitchell 98) offer a promising formulation of redundancy, also prove some results about how the use of unlabeled examples can help classification, and suggest an objective function when training with unlabeled examples.</S><S sid=""28"" ssid=""22"">(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance.</S>"
2.0,"Collins and Singer, 1999",0,0,"They also discuss an application of classifying web pages by using their method of mutually constrained models. (Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms toAdaBoost which force the classifiers to agree (called Co Boosting)","(Collins and Singer, 1999) further extend the use of classifiers that have mutual constraints by adding terms to AdaBoost which force the classifiers to agree (called Co Boosting)",N01-1023,"Method_Citation,Hypothesis_Citation,Implication_Citation",W99-0613,"'33','248','174'","<S sid=""33"" ssid=""27"">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S><S sid=""248"" ssid=""15"">With each iteration more examples are assigned labels by both classifiers, while a high level of agreement (> 94%) is maintained between them.</S><S sid=""174"" ssid=""41"">The two new terms force the two classifiers to agree, as much as possible, on the unlabeled examples.</S>"
3.0,Collins and Singer 1999,0,0,"Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on","Recent methods for English NER focus on machine-learning algorithms such as DL-CoTrain, CoBoost [Collins and Singer 1999], HMM [Daniel M. Bikel 1997], maximum entropy model [Borthwick, et al 1999] and so on",W03-1509,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",W99-0613,"'220','0','33'","<S sid=""220"" ssid=""87"">(7), such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).</S><S sid=""0"" ssid=""0"">Unsupervised Models for Named Entity Classification Collins</S><S sid=""33"" ssid=""27"">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S>"
4.0,"Collins and Singer, 1999",0,0,"DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syn tactically analyzed corpus","DL-CoTrain, (Collins and Singer, 1999), learns capitalized proper name NEs from a syntactically analyzed corpus",C02-1154,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",W99-0613,"'41','40','0'","<S sid=""41"" ssid=""35"">(Hearst 92) describes a method for extracting hyponyms from a corpus (pairs of words in &quot;isa&quot; relations).</S><S sid=""40"" ssid=""34"">(Berland and Charniak 99) describe a method for extracting parts of objects from wholes (e.g., &quot;speedometer&quot; from &quot;car&quot;) from a large corpus using hand-crafted patterns.</S><S sid=""0"" ssid=""0"">Unsupervised Models for Named Entity Classification Collins</S>"
5.0,"Collins and Singer, 1999",0,0,"(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances itset out to classify","(Collins and Singer, 1999) also makes use of competing categories (person, organization, and location), which cover 96% of all the instances it set out to classify",C02-1154,"Method_Citation,Hypothesis_Citation,Implication_Citation",W99-0613,"'10','0','19'","<S sid=""10"" ssid=""4"">The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.</S><S sid=""0"" ssid=""0"">Unsupervised Models for Named Entity Classification Collins</S><S sid=""19"" ssid=""13"">Given around 90,000 unlabeled examples, the methods described in this paper classify names with over 91% accuracy.</S>"
6.0,"Collins and Singer, 1999",0,0,"In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification","In (Collins and Singer, 1999) Collins and Singer show that unlabeled data can be used to reduce the level of supervision required for named entity classification",W06-2204,"Method_Citation,Hypothesis_Citation,Implication_Citation",W99-0613,"'0','9','19'","<S sid=""0"" ssid=""0"">Unsupervised Models for Named Entity Classification Collins</S><S sid=""9"" ssid=""3"">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S><S sid=""19"" ssid=""13"">Given around 90,000 unlabeled examples, the methods described in this paper classify names with over 91% accuracy.</S>"
8.0,"Collins and Singer, 1999",0,0,"Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)","Collins and Singer (1999) for example report that 88% of the named entities occurring in their data set belong to these three categories (Collins and Singer, 1999)",W03-1022,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",W99-0613,"'19','0','256'","<S sid=""19"" ssid=""13"">Given around 90,000 unlabeled examples, the methods described in this paper classify names with over 91% accuracy.</S><S sid=""0"" ssid=""0"">Unsupervised Models for Named Entity Classification Collins</S><S sid=""256"" ssid=""7"">The problem of &quot;noise&quot; items that do not fall into any of the three categories also needs to be addressed.</S>"
9.0,"Collinsand Singer, 1999",0,0,"While EM has worked quite well for a few tasks, notably ma chine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success inmost others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)","While EM has worked quite well for a few tasks, notably machine translations (starting with the IBM models 1-5 (Brown et al, 1993), it has not had success in most others, such as part-of-speech tagging (Merialdo, 1991), named-entity recognition (Collinsand Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention)",E09-1018,"Method_Citation,Hypothesis_Citation,Implication_Citation",W99-0613,"'0','16','38'","<S sid=""0"" ssid=""0"">Unsupervised Models for Named Entity Classification Collins</S><S sid=""16"" ssid=""10"">Supervised methods have been applied quite successfully to the full MUC named-entity task (Bikel et al. 97).</S><S sid=""38"" ssid=""32"">There has been additional recent work on inducing lexicons or other knowledge sources from large corpora.</S>"
11.0,"Collins and Singer, 1999",0,0,"In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)","In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999)",W07-1712,"Method_Citation,Hypothesis_Citation,Implication_Citation",W99-0613,"'0','221','253'","<S sid=""0"" ssid=""0"">Unsupervised Models for Named Entity Classification Collins</S><S sid=""221"" ssid=""88"">We are currently exploring such algorithms.</S><S sid=""253"" ssid=""4"">We are currently exploring other methods that employ similar ideas and their formal properties.</S>"
12.0,"Collins and Singer, 1999",0,0,"Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky ?smethod (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)","Collins et al (Collins and Singer, 1999) proposed two algorithms for NER by modifying Yarowsky's method (Yarowsky, 1995) and the framework suggested by (Blum and Mitchell, 1998)",W09-2208,"Method_Citation,Hypothesis_Citation,Implication_Citation",W99-0613,"'26','27','4'","<S sid=""26"" ssid=""20"">We present two algorithms.</S><S sid=""27"" ssid=""21"">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S><S sid=""4"" ssid=""4"">We present two algorithms.</S>"
13.0,"Collins and Singer, 1999",0,0,"This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)","This approach was shown to perform well on real-world natural language problems (Collins and Singer, 1999)",W06-2207,"Results_Citation,Hypothesis_Citation,Implication_Citation,Method_Citation,Aim_Citation",W99-0613,"'44','39','28'","<S sid=""44"" ssid=""38"">More recently, (Riloff and Jones 99) describe a method they term &quot;mutual bootstrapping&quot; for simultaneously constructing a lexicon and contextual extraction patterns.</S><S sid=""39"" ssid=""33"">(Brin 98) ,describes a system for extracting (author, book-title) pairs from the World Wide Web using an approach that bootstraps from an initial seed set of examples.</S><S sid=""28"" ssid=""22"">(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance.</S>"
15.0,"Collins and Singer, 1999",0,0,"(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here","(6) Similarly to (Collins and Singer, 1999) we used T= 0.95 for all experiments reported here",W06-2207,"Method_Citation,Hypothesis_Citation",W99-0613,"'85','28','27'","<S sid=""85"" ssid=""18"">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S><S sid=""28"" ssid=""22"">(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance.</S><S sid=""27"" ssid=""21"">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S>"
16.0,1999,0,0,We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores ?fj?| ?fj |+ |? f |+ L (1) where  is a smoothing constant,We use Collins and Singer (1999) for our exact specification of Yarowsky,P12-1065,"Method_Citation,Hypothesis_Citation,Implication_Citation",W99-0613,"'68','0','204'","<S sid=""68"" ssid=""1"">The first unsupervised algorithm we describe is based on the decision list method from (Yarowsky 95).</S><S sid=""0"" ssid=""0"">Unsupervised Models for Named Entity Classification Collins</S><S sid=""204"" ssid=""71"">In this work we extended the AdaBoost.MH (Schapire and Singer 98) algorithm to the cotraining case.</S>"
