Citance Number,Citation Marker,Citation Marker Offset,Citation Offset,Citation Text,Citation Text Clean,Citing Article,Discourse Facet,Reference Article,Reference Offset,Reference Text
1.0,2001,0,0,"Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))","Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))",W05-0104,"Method_Citation,Hypothesis_Citation,Implication_Citation",J01-2004,"'372','355','218'","<S sid=""372"" ssid=""128"">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S><S sid=""355"" ssid=""111"">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.</S><S sid=""218"" ssid=""122"">We can define a derives relation, denoted between two candidate analyses as follows.</S>"
2.0,2001,0,0,"Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a tree bank, with the exception of Chelba and Jelinek (2000)",Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition,P08-1013,"Method_Citation,Aim_Citation,Results_Citation,Implication_Citation",J01-2004,"'17','15','93'","<S sid=""17"" ssid=""5"">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S sid=""15"" ssid=""3"">In the past few years, however, some improvements have been made over these language models through the use of statistical methods of natural language processing, and the development of innovative, linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S><S sid=""93"" ssid=""51"">The standard language model used in many speech recognition systems is the trigram model, i.e., a Markov model of order 2, which can be characterized by the following equation: To smooth the trigram models that are used in this paper, we interpolate the probability estimates of higher-order Markov models with lower-order Markov models (Jelinek and Mercer 1980).</S>"
4.0,"Roark, 2001a",0,0,"Theperceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn tree bank","The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank",P04-1015,"Results_Citation,Hypothesis_Citation,Implication_Citation,Method_Citation,Aim_Citation",J01-2004,"'10','4','0'","<S sid=""10"" ssid=""4"">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid=""4"" ssid=""4"">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid=""0"" ssid=""0"">Probabilistic Top-Down Parsing and Language Modeling</S>"
5.0,"Roark, 2001a",0,0,"We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the per ceptron model gives performance competitive to that of the generative model on parsing the Penn tree bank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search","We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search",P04-1015,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",J01-2004,"'347','10','4'","<S sid=""347"" ssid=""103"">The perplexity improvement was achieved by simply taking the existing parsing model and applying it, with no extra training beyond that done for parsing.</S><S sid=""10"" ssid=""4"">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid=""4"" ssid=""4"">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S>"
6.0,2001a,0,0,"One way around this problem is to adopt a two-pass approach, where GEN (x) is the top N analyses under some initial model, as in the re ranking approach of Collins (2000) .In the current paper we explore alternatives to rerank ing approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)","In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the argmax, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)",P04-1015,"Method_Citation,Hypothesis_Citation,Implication_Citation",J01-2004,"'223','136','137'","<S sid=""223"" ssid=""127"">We implement this as a beam search.</S><S sid=""136"" ssid=""40"">That is, search efficiency for these parsers is improved by both statistical search heuristics and DP.</S><S sid=""137"" ssid=""41"">Here we will present a parser that uses simple search heuristics of this sort without DP.</S>"
7.0,2001a,0,0,"approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights","The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights",P04-1015,"Method_Citation,Hypothesis_Citation,Implication_Citation",J01-2004,"'213','137','115'","<S sid=""213"" ssid=""117"">The parser takes an input string 4, a PCFG G, and a priority queue of candidate analyses.</S><S sid=""137"" ssid=""41"">Here we will present a parser that uses simple search heuristics of this sort without DP.</S><S sid=""115"" ssid=""19"">The structured language model (SLM) used in Chelba and Jelinek (1998a, 1998b, 1999), Jelinek and Chelba (1999), and Chelba (2000) is similar to that of Goddeau, except that (i) their shift-reduce parser follows a nondeterministic beam search, and (ii) each stack entry contains, in addition to the nonterminal node label, the headword of the constituent.</S>"
9.0,2001a,0,0,"Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word","Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word",P04-1015,"Method_Citation,Hypothesis_Citation,Implication_Citation",J01-2004,"'231','80','202'","<S sid=""231"" ssid=""135"">Since we do not know the POS for the word, we must sum the LAP for all POS For a PCFG G, a stack S = Ao An$ (which we will write AN and a look-ahead terminal item wi, we define the look-ahead probability as follows: We recursively estimate this with two empirically observed conditional probabilities for every nonterminal A,: 13(A, w,a) and P(A, c).</S><S sid=""80"" ssid=""38"">It also brings words further downstream into the look-ahead at the point of specification.</S><S sid=""202"" ssid=""106"">Then we add lexical information, first for non-POS expansions, then for leftmost POS expansions, then for all expansions.</S>"
10.0,"Roark, 2001",0,0,"A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning","A good example of this is the Roark parser (Roark, 2001) which works left-to-right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning",P05-1022,"Method_Citation,Implication_Citation",J01-2004,"'21','223','120'","<S sid=""21"" ssid=""9"">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S><S sid=""223"" ssid=""127"">We implement this as a beam search.</S><S sid=""120"" ssid=""24"">The last stage performs all possible parser operations (reducing stack entries and shifting the new word).</S>"
11.0,"Roark, 2001",0,0,"At the end one has a beam-width? s number of best parses (Roark, 2001) .The Collins parser (Collins, 1997) does use dynamic programming in its search","At the end one has a beam-width's number of best parses (Roark, 2001)",P05-1022,"Method_Citation,Hypothesis_Citation,Implication_Citation",J01-2004,"'223','213','289'","<S sid=""223"" ssid=""127"">We implement this as a beam search.</S><S sid=""213"" ssid=""117"">The parser takes an input string 4, a PCFG G, and a priority queue of candidate analyses.</S><S sid=""289"" ssid=""45"">These results, achieved using very straightforward conditioning events and considering only the left context, are within one to four points of the best published Observed running time on Section 23 of the Penn Treebank, with the full conditional probability model and beam of 10-11, using one 300 MHz UltraSPARC processor and 256MB of RAM of a Sun Enterprise 450. accuracies cited above.'</S>"
12.0,"Roark, 2001",0,0,"To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses","To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses",P05-1022,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",J01-2004,"'277','9','3'","<S sid=""277"" ssid=""33"">The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989,860 words, 39,832 sentences) of the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) served as the training data, Section 24 (34,199 words, 1,346 sentences) as the held-out data for parameter estimation, and Section 23 (59,100 words, 2,416 sentences) as the test data.</S><S sid=""9"" ssid=""3"">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S sid=""3"" ssid=""3"">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>"
13.0,"Roark, 2001",0,0,"We ran the first stage parser with 4-timesoverparsing for each string in 7The n? best lists were provided by Brian Roark (Roark, 2001) 8A local-tree is an explicit expansion of an edge and its children","The n-best lists were provided by Brian Roark (Roark, 2001)",P04-1006,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",J01-2004,'355',"<S sid=""355"" ssid=""111"">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.</S>"
14.0,"Roark, 2001a",0,0,"Incremental top down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models","Incremental top-down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models",P05-1063,"Method_Citation,Hypothesis_Citation,Implication_Citation",J01-2004,"'0','7','31'","<S sid=""0"" ssid=""0"">Probabilistic Top-Down Parsing and Language Modeling</S><S sid=""7"" ssid=""1"">This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition.</S><S sid=""31"" ssid=""19"">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>"
15.0,"Roark, 2001",0,0,"Levy, on the other hand ,argued that studies of probabilistic parsing reveal that typically a small number of analyses are as signed the majority of probability mass (Roark, 2001)","Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001)",W10-2009,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",J01-2004,"'267','277','100'","<S sid=""267"" ssid=""23"">In addition, we show the average number of rule expansions considered per word, that is, the number of rule expansions for which a probability was calculated (see Roark and Charniak [2000]), and the average number of analyses advanced to the next priority queue per word.</S><S sid=""277"" ssid=""33"">The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989,860 words, 39,832 sentences) of the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) served as the training data, Section 24 (34,199 words, 1,346 sentences) as the held-out data for parameter estimation, and Section 23 (59,100 words, 2,416 sentences) as the test data.</S><S sid=""100"" ssid=""4"">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>"
17.0,2001,0,0,"For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lex icalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)","For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)",D09-1034,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",J01-2004,"'277','245','218'","<S sid=""277"" ssid=""33"">The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989,860 words, 39,832 sentences) of the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) served as the training data, Section 24 (34,199 words, 1,346 sentences) as the held-out data for parameter estimation, and Section 23 (59,100 words, 2,416 sentences) as the test data.</S><S sid=""245"" ssid=""1"">The empirical results will be presented in three stages: (i) trials to examine the accuracy and efficiency of the parser; (ii) trials to examine its effect on test corpus perplexity and recognition performance; and (iii) trials to examine the effect of beam variation on these performance measures.</S><S sid=""218"" ssid=""122"">We can define a derives relation, denoted between two candidate analyses as follows.</S>"
18.0,2001,0,0,"We modified the Roark (2001) parser to calculate the discussed measures 1, and the empirical results in? 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time","We modified the Roark (2001) parser to calculate the discussed measures, and the empirical results in ?4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time",D09-1034,"Method_Citation,Hypothesis_Citation,Implication_Citation",J01-2004,"'329','141','10'","<S sid=""329"" ssid=""85"">Table 3 shows several things.</S><S sid=""141"" ssid=""45"">We will then present empirical results in two domains: one to compare with previous work in the parsing literature, and the other to compare with previous work using parsing for language modeling for speech recognition, in particular with the Chelba and Jelinek results mentioned above.</S><S sid=""10"" ssid=""4"">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S>"
19.0,2001,0,0,"In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here","In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here",D09-1034,"Method_Citation,Hypothesis_Citation,Implication_Citation",J01-2004,"'140','7','208'","<S sid=""140"" ssid=""44"">Next we will outline our conditional probability model over rules in the PCFG, followed by a presentation of the top-down parsing algorithm.</S><S sid=""7"" ssid=""1"">This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition.</S><S sid=""208"" ssid=""112"">We now outline the top-down parsing algorithm.</S>"
20.0,2001,0,0,"At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is re quired to calculate such measures. Let H (D) be the entropy over a set of derivations D, calculated as follows: H (D)=? X D? D? (D) P D?? D? (D?) log? (D) P D?? D? (D?) (10) If the set of derivations D= D (G, W [1, i]) is a set of partial derivations for string W [1, i], then H (D) is a measure of uncertainty over the partial derivations ,i.e., the uncertainty regarding the correct analysis of what has already been processed","At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures",D09-1034,"Method_Citation,Hypothesis_Citation,Implication_Citation",J01-2004,"'211','21','213'","<S sid=""211"" ssid=""115"">We will first define candidate analysis (i.e., a partial parse), and then a derives relation between candidate analyses.</S><S sid=""21"" ssid=""9"">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S><S sid=""213"" ssid=""117"">The parser takes an input string 4, a PCFG G, and a priority queue of candidate analyses.</S>"
