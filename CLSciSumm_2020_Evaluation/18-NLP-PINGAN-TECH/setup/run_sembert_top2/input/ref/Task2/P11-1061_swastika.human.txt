Das and Petrov, in this paper, approached inducing unsupervised part-of-speech taggers for languages that had no labeled training data, but had translated text in a resource-rich language. Their method did not assume any knowledge about the target language, making it applicable to a wide array of resource-poor languages. They used graph-based label propagation for cross-lingual knowledge transfer and used the projected labels as features in an unsupervised model. Across eight European languages, their approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm. They showed the efficacy of graph-based label propagation for projecting part-of-speech information across languages. Their results suggested that it was possible to learn accurate POS taggers for languages which did not have any annotated data, but have translations into a resource-rich language. It outperformed strong unsupervised baselines as well as approaches that relied on direct projections, and bridged the gap between purely supervised and unsupervised POS tagging models.