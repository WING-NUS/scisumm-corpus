In this paper the author describes a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not. Domain adaptation is a common concern when optimizing empirical NLP applications. For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components, which precludes a single universal approach to adaptation. The author also studies the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material. First, they aim to explicitly characterize examples from OUT as belonging to general language or not. Their second contribution is to apply instance weighting at the level of phrase pairs. Finally, they make some improvements to baseline approaches. In future work the author plan to try this approach with more competitive SMT systems, and to extend instance weighting to other standard SMT components such as the LM, lexical phrase weights, and lexicalized distortion. They will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences. Finally, they intend to explore more sophisticated instance weighting features for capturing the degree of generality of phrase pairs.