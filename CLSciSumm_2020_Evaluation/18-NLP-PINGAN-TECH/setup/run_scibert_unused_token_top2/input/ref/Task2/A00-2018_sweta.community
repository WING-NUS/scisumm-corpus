 <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
 <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
<S sid="120" ssid="11">Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &amp;quot;correct&amp;quot;, and statistics were collected on the resulting parses.</S>
<S sid="119" ssid="10">(It is &amp;quot;soft&amp;quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.)</S>
<S sid="95" ssid="6">We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.</S>
 <S sid="175" ssid="2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S>
<S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
<S sid="1" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal treebank.</S>
<S sid="126" ssid="17">This is indicated in Figure 2, where the model labeled &amp;quot;Best&amp;quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus.</S>
<S sid="12" ssid="1">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c, t(c) (t for &amp;quot;tag&amp;quot;), then the lexical head of c, h(c), and then the expansion of c into further constituents e(c).</S>
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
<S sid="63" ssid="32">As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &amp;quot;before&amp;quot;), and the label of the grandparent of c (la).</S>
<S sid="78" ssid="47">With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.</S>
 <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
<S sid="180" ssid="7">From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.</S>