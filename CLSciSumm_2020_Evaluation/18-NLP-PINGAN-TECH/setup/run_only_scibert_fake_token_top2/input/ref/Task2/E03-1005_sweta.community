<S sid="20" ssid="17">Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).</S>
 <S sid="74" ssid="26">Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.</S>
<S sid="44" ssid="41">But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
<S sid="145" ssid="10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S>
<S sid="134" ssid="15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.</S>
<S sid="22" ssid="19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S>
<S sid="133" ssid="14">It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).</S>
<S sid="25" ssid="22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier &amp; Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S>
<S sid="38" ssid="35">Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003).</S>
 <S sid="100" ssid="3">In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.</S>
<S sid="32" ssid="29">However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.</S>
 <S sid="130" ssid="11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
 <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
<S sid="27" ssid="24">Goodman (1996, 1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.</S>