<S sid="17" ssid="13">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S>
 <S sid="20" ssid="16">We also explore how the characteristics of different languages affect topic model performance.</S>
    <S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
<S sid="10" ssid="6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S>
   <S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
 sid="9" ssid="5">In this paper, we present the polylingual topic model (PLTM).</S>
<S sid="118" ssid="67">We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.</S>
<S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
 <S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
<S sid="102" ssid="51">In contrast, PLTM assigns a significant number of tokens to almost all 800 topics, in very similar proportions in both languages.</S>
 <S sid="38" ssid="4">This is unlike LDA, in which each document is assumed to have its own document-specific distribution over topics.</S>
 <S sid="156" ssid="105">We use both Jensen-Shannon divergence and cosine distance.</S>
    <S sid="36" ssid="2">Each tuple is a set of documents that are loosely equivalent to each other, but written in different languages, e.g., corresponding Wikipedia articles in French, English and German.</S>
  <S sid="170" ssid="4">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>
    <S sid="29" ssid="5">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>
<S sid="168" ssid="2">However, the growth of the web, and in particular Wikipedia, has made comparable text corpora &#8211; documents that are topically similar but are not direct translations of one another &#8211; considerably more abundant than true parallel corpora.</S>
 <S sid="110" ssid="59">Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S>