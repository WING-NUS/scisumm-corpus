This paper aims at describing the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modelling for speech recognition. With certain exceptions, computational linguists have in the past generally formed a separate research community from speech recognition researchers, despite obvious overlap of interest. While language models built around shallow local dependencies are still the standard in state-of-the-art speech recognition systems, there is reason to hope that better language models can and will be developed by computational linguists for this task. Two features of our top-down parsing approach will emerge as key to its success. First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar. A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model. There are few implications that are to be taken further in this area. First, there is reason to believe that some of the conditioning information is not uniformly useful, and we would benefit from finer distinctions. Second, there are advantages to top-down parsing that have not been examined to date.