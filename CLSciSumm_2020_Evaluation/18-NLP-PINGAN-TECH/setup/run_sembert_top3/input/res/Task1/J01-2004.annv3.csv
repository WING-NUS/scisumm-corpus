Citance Number,Citation Marker,Citation Marker Offset,Citation Offset,Citation Text,Citation Text Clean,Citing Article,Discourse Facet,Reference Article,Reference Offset,Reference Text
1.0,2001,0,0,"Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))","Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))",W05-0104,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",J01-2004,"'355','16','253'","<S sid=""355"" ssid=""111"">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.</S><S sid=""16"" ssid=""4"">While language models built around shallow local dependencies are still the standard in state-of-the-art speech recognition systems, there is reason to hope that better language models can and will be developed by computational linguists for this task.</S><S sid=""253"" ssid=""9"">Some of the trials discussed below will report results in terms of word and/or sentence error rate, which are obtained when the language model is embedded in a speech recognition system.</S>"
2.0,2001,0,0,"Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a tree bank, with the exception of Chelba and Jelinek (2000)",Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition,P08-1013,"Method_Citation,Hypothesis_Citation,Implication_Citation",J01-2004,"'93','253','40'","<S sid=""93"" ssid=""51"">The standard language model used in many speech recognition systems is the trigram model, i.e., a Markov model of order 2, which can be characterized by the following equation: To smooth the trigram models that are used in this paper, we interpolate the probability estimates of higher-order Markov models with lower-order Markov models (Jelinek and Mercer 1980).</S><S sid=""253"" ssid=""9"">Some of the trials discussed below will report results in terms of word and/or sentence error rate, which are obtained when the language model is embedded in a speech recognition system.</S><S sid=""40"" ssid=""28"">The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.</S>"
4.0,"Roark, 2001a",0,0,"Theperceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn tree bank","The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank",P04-1015,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",J01-2004,"'277','10','4'","<S sid=""277"" ssid=""33"">The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989,860 words, 39,832 sentences) of the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) served as the training data, Section 24 (34,199 words, 1,346 sentences) as the held-out data for parameter estimation, and Section 23 (59,100 words, 2,416 sentences) as the test data.</S><S sid=""10"" ssid=""4"">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S><S sid=""4"" ssid=""4"">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S>"
5.0,"Roark, 2001a",0,0,"We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the per ceptron model gives performance competitive to that of the generative model on parsing the Penn tree bank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search","We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search",P04-1015,"Method_Citation,Hypothesis_Citation,Implication_Citation",J01-2004,"'364','0','277'","<S sid=""364"" ssid=""120"">We follow Chelba (2000) in dealing with this problem: for parsing purposes, we use the Penn Treebank tokenization; for interpolation with the provided trigram model, and for evaluation, the lattice tokenization is used.</S><S sid=""0"" ssid=""0"">Probabilistic Top-Down Parsing and Language Modeling</S><S sid=""277"" ssid=""33"">The first set of results looks at the performance of the parser on the standard corpora for statistical parsing trials: Sections 2-21 (989,860 words, 39,832 sentences) of the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) served as the training data, Section 24 (34,199 words, 1,346 sentences) as the held-out data for parameter estimation, and Section 23 (59,100 words, 2,416 sentences) as the test data.</S>"
6.0,2001a,0,0,"One way around this problem is to adopt a two-pass approach, where GEN (x) is the top N analyses under some initial model, as in the re ranking approach of Collins (2000) .In the current paper we explore alternatives to rerank ing approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)","In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the argmax, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)",P04-1015,"Method_Citation,Hypothesis_Citation,Implication_Citation",J01-2004,"'134','302','133'","<S sid=""134"" ssid=""38"">The parsers with the highest published broad-coverage parsing accuracy, which include Charniak (1997, 2000), Collins (1997, 1999), and Ratnaparkhi (1997), all utilize simple and straightforward statistically based search heuristics, pruning the search-space quite dramatically!'</S><S sid=""302"" ssid=""58"">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S><S sid=""133"" ssid=""37"">Statistically based heuristic best-first or beam-search strategies (Caraballo and Charniak 1998; Charniak, Goldwater, and Johnson 1998; Goodman 1997) have yielded an enormous improvement in the quality and speed of parsers, even without any guarantee that the parse returned is, in fact, that with the maximum likelihood for the probability model.</S>"
7.0,2001a,0,0,"approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights","The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights",P04-1015,"Method_Citation,Hypothesis_Citation,Implication_Citation",J01-2004,"'137','268','297'","<S sid=""137"" ssid=""41"">Here we will present a parser that uses simple search heuristics of this sort without DP.</S><S sid=""268"" ssid=""24"">This is an incremental parser with a pruning strategy and no backtracking.</S><S sid=""297"" ssid=""53"">The differences between a k-best and a beam-search parser (not to mention the use of dynamic programming) make a running time difference unsurprising.</S>"
9.0,2001a,0,0,"Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word","Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word",P04-1015,"Method_Citation,Hypothesis_Citation,Implication_Citation",J01-2004,"'224','201','183'","<S sid=""224"" ssid=""128"">For each word position i, we have a separate priority queue H, of analyses with look-ahead w,.</S><S sid=""201"" ssid=""105"">We first include structural information from the context, namely, node labels from constituents in the left context.</S><S sid=""183"" ssid=""87"">POS nodes have lexical items on the right-hand side, and hence can bring into the model some of the head-head dependencies that have been shown to be so effective.</S>"
10.0,"Roark, 2001",0,0,"A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning","A good example of this is the Roark parser (Roark, 2001) which works left-to-right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning",P05-1022,"Hypothesis_Citation,Implication_Citation",J01-2004,"'297','109','223'","<S sid=""297"" ssid=""53"">The differences between a k-best and a beam-search parser (not to mention the use of dynamic programming) make a running time difference unsurprising.</S><S sid=""109"" ssid=""13"">A shift-reduce parser operates from left to right using a stack and a pointer to the next word in the input string.9 Each stack entry consists minimally of a nonterminal label.</S><S sid=""223"" ssid=""127"">We implement this as a beam search.</S>"
11.0,"Roark, 2001",0,0,"At the end one has a beam-width? s number of best parses (Roark, 2001) .The Collins parser (Collins, 1997) does use dynamic programming in its search","At the end one has a beam-width's number of best parses (Roark, 2001)",P05-1022,"Hypothesis_Citation,Implication_Citation",J01-2004,"'297','358','9'","<S sid=""297"" ssid=""53"">The differences between a k-best and a beam-search parser (not to mention the use of dynamic programming) make a running time difference unsurprising.</S><S sid=""358"" ssid=""114"">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice, along with the acoustic and trigram scores.'</S><S sid=""9"" ssid=""3"">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>"
12.0,"Roark, 2001",0,0,"To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses","To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses",P05-1022,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",J01-2004,"'291','9','3'","<S sid=""291"" ssid=""47"">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length < 100.</S><S sid=""9"" ssid=""3"">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S sid=""3"" ssid=""3"">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>"
13.0,"Roark, 2001",0,0,"We ran the first stage parser with 4-timesoverparsing for each string in 7The n? best lists were provided by Brian Roark (Roark, 2001) 8A local-tree is an explicit expansion of an edge and its children","The n-best lists were provided by Brian Roark (Roark, 2001)",P04-1006,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",J01-2004,"'355','372','358'","<S sid=""355"" ssid=""111"">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.</S><S sid=""372"" ssid=""128"">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S><S sid=""358"" ssid=""114"">We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice, along with the acoustic and trigram scores.'</S>"
14.0,"Roark, 2001a",0,0,"Incremental top down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models","Incremental top-down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models",P05-1063,"Method_Citation,Hypothesis_Citation,Implication_Citation",J01-2004,"'0','21','102'","<S sid=""0"" ssid=""0"">Probabilistic Top-Down Parsing and Language Modeling</S><S sid=""21"" ssid=""9"">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S><S sid=""102"" ssid=""6"">One approach to syntactic language modeling is to use this distribution directly as a language model.</S>"
15.0,"Roark, 2001",0,0,"Levy, on the other hand ,argued that studies of probabilistic parsing reveal that typically a small number of analyses are as signed the majority of probability mass (Roark, 2001)","Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001)",W10-2009,"Method_Citation,Hypothesis_Citation,Implication_Citation",J01-2004,"'304','313','320'","<S sid=""304"" ssid=""60"">The hope is that a large amount of the probability mass will be accounted for by the parses in the beam.</S><S sid=""313"" ssid=""69"">By utilizing a figure of merit to identify promising analyses, we are simply focusing our attention on those parses that are likely to have a high probability, and thus we are increasing the amount of probability mass that we do capture, of the total possible.</S><S sid=""320"" ssid=""76"">One final note on assigning probabilities to strings: because this parser does garden path on a small percentage of sentences, this must be interpolated with another estimate, to ensure that every word receives a probability estimate.</S>"
17.0,2001,0,0,"For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lex icalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)","For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)",D09-1034,"Method_Citation,Hypothesis_Citation,Implication_Citation",J01-2004,"'108','253','40'","<S sid=""108"" ssid=""12"">Another approach that uses syntactic structure for language modeling has been to use a shift-reduce parser to &quot;surface&quot; c-commanding phrasal headwords or part-of-speech (POS) tags from arbitrarily far back in the prefix string, for use in a trigram-like model.</S><S sid=""253"" ssid=""9"">Some of the trials discussed below will report results in terms of word and/or sentence error rate, which are obtained when the language model is embedded in a speech recognition system.</S><S sid=""40"" ssid=""28"">The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.</S>"
18.0,2001,0,0,"We modified the Roark (2001) parser to calculate the discussed measures 1, and the empirical results in? 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time","We modified the Roark (2001) parser to calculate the discussed measures, and the empirical results in ?4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time",D09-1034,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",J01-2004,"'9','3','19'","<S sid=""9"" ssid=""3"">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S sid=""3"" ssid=""3"">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S sid=""19"" ssid=""7"">A new language model, based on probabilistic top-down parsing, will be outlined and compared with the previous literature, and extensive empirical results will be presented which demonstrate its utility.</S>"
19.0,2001,0,0,"In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here","In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here",D09-1034,"Method_Citation,Hypothesis_Citation,Implication_Citation",J01-2004,"'208','0','21'","<S sid=""208"" ssid=""112"">We now outline the top-down parsing algorithm.</S><S sid=""0"" ssid=""0"">Probabilistic Top-Down Parsing and Language Modeling</S><S sid=""21"" ssid=""9"">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>"
20.0,2001,0,0,"At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is re quired to calculate such measures. Let H (D) be the entropy over a set of derivations D, calculated as follows: H (D)=? X D? D? (D) P D?? D? (D?) log? (D) P D?? D? (D?) (10) If the set of derivations D= D (G, W [1, i]) is a set of partial derivations for string W [1, i], then H (D) is a measure of uncertainty over the partial derivations ,i.e., the uncertainty regarding the correct analysis of what has already been processed","At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures",D09-1034,"Method_Citation,Implication_Citation",J01-2004,"'21','269','37'","<S sid=""21"" ssid=""9"">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S><S sid=""269"" ssid=""25"">In such a model, it is possible to commit to a set of partial analyses at a particular point that cannot be completed given the rest of the input string (i.e., the parser can &quot;garden path&quot;).</S><S sid=""37"" ssid=""25"">Thus, our top-down parser makes it very easy to condition the probabilistic grammar on an arbitrary number of values extracted from the rooted, fully specified derivation.</S>"
