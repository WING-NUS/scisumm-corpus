In this paper the author aims at describing a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labelled training data, but have translated text in a resource-rich language. They use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model. Supervised learning approaches have advanced the state-of-the-art on a variety of tasks in natural language processing, resulting in highly accurate systems. Unsupervised learning approaches appear to be a natural solution to this problem, as they require only unannotated text for training models. To bridge this gap, the author considers a practically motivated scenario, in which he wants to leverage existing resources from a resource-rich language when building tools for resource-poor foreign languages. Their final average POS tagging accuracy of 83.4% compares very favourably to the average accuracy of Berg-Kirkpatrick et al.â€™s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).Their results suggest that it is possible to learn accurate POS taggers for languages which do not have any annotated data, but have translations into a resource-rich language. Their results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.