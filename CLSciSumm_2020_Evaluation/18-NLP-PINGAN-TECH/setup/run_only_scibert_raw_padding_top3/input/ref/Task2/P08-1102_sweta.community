<S sid="21" ssid="17">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&amp;T.</S>
<S sid="28" ssid="24">A subsequence of boundary-POS labelling result indicates a word with POS t only if the boundary tag sequence composed of its boundary part conforms to s or bm*e style, and all POS tags in its POS part equal to t. For example, a tag sequence b NN m NN e NN represents a threecharacter word with POS tag NN.</S>
<S sid="43" ssid="15">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S>
<S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
<S sid="9" ssid="5">To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&amp;T).</S
"<S sid=""33"" ssid=""5"">In following subsections, we describe the feature templates and the perceptron training algorithm.</S>
    <S sid=""34"" ssid=""6"">The feature templates we adopted are selected from those of Ng and Low (2004).</S"
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
NA
<S sid="64" ssid="15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>
<S sid="79" ssid="4">By maintaining a stack of size N at each position i of the sequence, we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S>
<S sid="96" ssid="7">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>
<S sid="91" ssid="2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S>
 <S sid="16" ssid="12">Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.</S
<S sid="35" ssid="7">To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation.</S>
<S sid="1" ssid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S>