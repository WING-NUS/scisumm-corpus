In this paper the author aims at proposing a framework for representing the meaning of phrases and sentences in vector space. Central to the approach is vector composition which we operationalize in terms of additive and multiplicative functions. Under this framework, they introduce a wide range of composition models which we evaluate empirically on a sentence similarity task. Vector-based models of word meaning have become increasingly popular in natural language processing (NLP) and cognitive science. Despite their widespread use, vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature. In fact, the commonest method for combining the vectors is to average them. Vector averaging is unfortunately insensitive to word order, and more generally syntactic structure, giving the same representation to any constructions that happen to share the same vocabulary. Further research is needed to gain a deeper understanding of vector composition, both in terms of modelling a wider range of structures and also in terms of exploring the space of models more fully. Future directions include constraining the number of free parameters in linguistically plausible ways and scaling to larger datasets.