<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
<S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
<S sid="177" ssid="49">However, TRIE partitions storage by n-gram length, so walking the trie reads N disjoint pages.</S>
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
<S sid="278" ssid="5">We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns.</S>