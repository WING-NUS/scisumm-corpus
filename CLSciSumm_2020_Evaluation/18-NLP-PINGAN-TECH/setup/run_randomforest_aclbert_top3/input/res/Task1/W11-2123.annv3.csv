Citance Number,Citation Marker,Citation Marker Offset,Citation Offset,Citation Text,Citation Text Clean,Citing Article,Discourse Facet,Reference Article,Reference Offset,Reference Text
1.0,"Heafield, 2011",0,0,"We used common tools for phrase-based translation? Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","We used common tools for phrase-based translation Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments",W11-2138,"Method_Citation,Hypothesis_Citation,Implication_Citation",W11-2123,"'199','218','12'","<S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""218"" ssid=""37"">We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation,9 translating the 3003-sentence test set.</S><S sid=""12"" ssid=""7"">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S>"
2.0,"Heafield, 2011",0,0,"The language model was com piled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run",P14-2022,"Method_Citation,Hypothesis_Citation,Implication_Citation",W11-2123,"'218','199','180'","<S sid=""218"" ssid=""37"">We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation,9 translating the 3003-sentence test set.</S><S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""180"" ssid=""52"">In all of our experiments, the binary file (whether mapped or, in the case of most other packages, interpreted) is loaded into the disk cache in advance so that lazy mapping will never fault to disk.</S>"
3.0,"Heafield, 2011",0,0,"Thus given afragment tf consisting of a sequence of target to kens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning.2 While this increases the number ofLM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","Thus given a fragment tf consisting of a sequence of target tokens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning. While this increases the number of LM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states",W12-3145,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",W11-2123,"'278','199','0'","<S sid=""278"" ssid=""5"">We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns.</S><S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""0"" ssid=""0"">KenLM: Faster and Smaller Language Model Queries</S>"
4.0,"Heafield, 2011",0,0,"Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference",W12-3131,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",W11-2123,'21',"<S sid=""21"" ssid=""16"">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S>"
5.0,"Heafield, 2011",0,0,"The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime",W12-3154,"Method_Citation,Hypothesis_Citation,Implication_Citation",W11-2123,"'199','218','12'","<S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""218"" ssid=""37"">We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation,9 translating the 3003-sentence test set.</S><S sid=""12"" ssid=""7"">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S>"
6.0,"Heafield, 2011",0,0,"The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)",P12-2058,"Method_Citation,Hypothesis_Citation,Implication_Citation",W11-2123,"'199','0','182'","<S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""0"" ssid=""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid=""182"" ssid=""1"">This section measures performance on shared tasks in order of increasing complexity: sparse lookups, evaluating perplexity of a large file, and translation with Moses.</S>"
7.0,2011,0,0,Inference was carried out using the language modeling library described by Heafield (2011),Inference was carried out using the language modeling library described by Heafield (2011),W11-2139,"Results_Citation,Hypothesis_Citation,Implication_Citation,Method_Citation,Aim_Citation",W11-2123,"'1','103','130'","<S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S><S sid=""130"" ssid=""2"">Applications such as machine translation use language model probability as a feature to assist in choosing between hypotheses.</S>"
8.0,"Heafield, 2011",0,0,"We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)",P13-2003,"Method_Citation,Hypothesis_Citation,Implication_Citation",W11-2123,"'199','197','196'","<S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""197"" ssid=""16"">However, reads in the TRIE data structure are more expensive due to bit-level packing, so we found that it is faster to use interpolation search the entire time.</S><S sid=""196"" ssid=""15"">This suggests a strategy: run interpolation search until the range narrows to 4096 or fewer entries, then switch to binary search.</S>"
9.0,2011,0,0,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language mod els by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,W12-3134,"Method_Citation,Hypothesis_Citation,Aim_Citation,Implication_Citation",W11-2123,"'103','264','1'","<S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S><S sid=""264"" ssid=""6"">For even larger models, storing counts (Talbot and Osborne, 2007; Pauls and Klein, 2011; Guthrie and Hepple, 2010) is a possibility.</S><S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>"
10.0,2011,0,0,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,W12-3134,"Method_Citation,Hypothesis_Citation,Implication_Citation",W11-2123,"'92','26','32'","<S sid=""92"" ssid=""70"">To quantize, we use the binning method (Federico and Bertoldi, 2006) that sorts values, divides into equally sized bins, and averages within each bin.</S><S sid=""26"" ssid=""4"">We use two common techniques, hash tables and sorted arrays, describing each before the model that uses the technique.</S><S sid=""32"" ssid=""10"">Linear probing places at most one entry in each bucket.</S>"
11.0,2011,0,0,"With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)",W12-3134,"Method_Citation,Hypothesis_Citation,Results_Citation,Implication_Citation",W11-2123,"'21','97','12'","<S sid=""21"" ssid=""16"">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S><S sid=""97"" ssid=""1"">SRILM (Stolcke, 2002) is widely used within academia.</S><S sid=""12"" ssid=""7"">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S>"
12.0,"Heafield, 2011",0,0,"This was used to create a KenLM (Heafield, 2011)","This was used to create a KenLM (Heafield, 2011)",W12-3160,"Method_Citation,Hypothesis_Citation,Implication_Citation",W11-2123,"'199','1','281'","<S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S sid=""281"" ssid=""2"">Hieu Hoang named the code “KenLM” and assisted with Moses along with Barry Haddow.</S>"
13.0,"Heafield, 2011",0,0,"In the Opinum system we query the M p, M n mod els with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable fora web application","In the Opinum system we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application",W12-3706,"Method_Citation,Hypothesis_Citation,Aim_Citation,Implication_Citation",W11-2123,"'103','0','1'","<S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S><S sid=""0"" ssid=""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>"
14.0,"Heafield, 2011",0,0,"Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights",W11-2147,"Method_Citation,Hypothesis_Citation,Implication_Citation",W11-2123,'218',"<S sid=""218"" ssid=""37"">We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation,9 translating the 3003-sentence test set.</S>"
15.0,"Heafield, 2011",0,0,"For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)",E12-1083,"Method_Citation,Hypothesis_Citation,Implication_Citation",W11-2123,"'199','103','0'","<S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S><S sid=""0"" ssid=""0"">KenLM: Faster and Smaller Language Model Queries</S>"
16.0,"Heafield, 2011",0,0,"Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima ?an,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Simaan,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)",P12-1002,"Method_Citation,Hypothesis_Citation,Aim_Citation,Implication_Citation",W11-2123,"'103','199','218'","<S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S><S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""218"" ssid=""37"">We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation,9 translating the 3003-sentence test set.</S>"
17.0,"Heafield, 2011",0,0,"n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3",D12-1108,"Method_Citation,Hypothesis_Citation,Implication_Citation",W11-2123,"'199','131','103'","<S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""131"" ssid=""3"">Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N − 1 preceding words.</S><S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S>"
18.0,"Heafield, 2011",0,0,"Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)",P12-2006,"Hypothesis_Citation,Implication_Citation",W11-2123,"'0','103','1'","<S sid=""0"" ssid=""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S><S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>"
19.0,"Heafield, 2011",0,0,"For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)",P13-2073,"Method_Citation,Hypothesis_Citation,Implication_Citation",W11-2123,"'199','0','182'","<S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""0"" ssid=""0"">KenLM: Faster and Smaller Language Model Queries</S><S sid=""182"" ssid=""1"">This section measures performance on shared tasks in order of increasing complexity: sparse lookups, evaluating perplexity of a large file, and translation with Moses.</S>"
20.0,"Heafield, 2011",0,0,"For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing",P13-1109,"Method_Citation,Hypothesis_Citation,Implication_Citation",W11-2123,"'199','218','0'","<S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S><S sid=""218"" ssid=""37"">We run the baseline Moses system for the French-English track of the 2011 Workshop on Machine Translation,9 translating the 3003-sentence test set.</S><S sid=""0"" ssid=""0"">KenLM: Faster and Smaller Language Model Queries</S>"
