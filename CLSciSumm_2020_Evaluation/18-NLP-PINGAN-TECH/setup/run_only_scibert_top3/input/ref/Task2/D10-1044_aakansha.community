<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
"<S sid=""95"" ssid=""32"">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S>
    <S sid=""96"" ssid=""33"">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>"
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
NA
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
<S sid="31" ssid="28">For comparison to information-retrieval inspired baselines, eg (L&#168;u et al., 2007), we select sentences from OUT using language model perplexities from IN.</S>
<S sid="22" ssid="19">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
"<S sid=""119"" ssid=""23"">The 2nd block contains the IR system, which was tuned by selecting text in multiples of the size of the EMEA training corpus, according to dev set performance.</S>
    <S sid=""120"" ssid=""24"">This significantly underperforms log-linear combination.</S>"
"<S sid=""23"" ssid=""20"">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
    <S sid=""24"" ssid=""21"">Sentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language.</S>"
<S sid="40" ssid="4">We focus here instead on adapting the two most important features: the language model (LM), which estimates the probability p(wIh) of a target word w following an ngram h; and the translation models (TM) p(slt) and p(t1s), which give the probability of source phrase s translating to target phrase t, and vice versa.</S>