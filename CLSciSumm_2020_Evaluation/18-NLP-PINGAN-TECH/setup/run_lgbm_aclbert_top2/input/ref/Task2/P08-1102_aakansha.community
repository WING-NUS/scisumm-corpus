<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
<S sid="42" ssid="14">As predications generated from such templates depend on the current character, we name these templates lexical-target.</S>
<S sid="25" ssid="21">According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
<S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
<S sid="121" ssid="32">Among other features, the 4-gram POS LM plays the most important role, removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&amp;T.</S>
<S sid="37" ssid="9">C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).</S>
<S sid="73" ssid="24">For instance, if the word w appears N times in training corpus and is labelled as POS t for n times, the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.</S>
<S sid="46" ssid="18">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) &#8712; X &#215; Y to a feature vector 4)(x, y) &#8712; Rd, and a parameter vector &#945;&#65533; &#8712; Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S>