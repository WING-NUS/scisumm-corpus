This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its
application to the problem of language modeling for speech recognition. The paper first introduces
key notions in language modeling and probabilistic parsing, and briefly reviews some previous
approaches to using syntactic structure for language modeling. A lexicalized probabilistic topdown
parser is then presented, which performs very well, in terms of both the accuracy of returned
parses and the efficiency with which they are found, relative to the best broad-coverage statistical
parsers. A new language model that utilizes probabilistic top-down parsing is then outlined, and
empirical results show that it improves upon previous work in test corpus perplexity. Interpolation
with a trigram model yields an exceptional improvement relative to the improvement observed
by other models, demonstrating the degree to which the information captured by our parsing
model is orthogonal to that captured by a trigram model. A small recognition experiment also
demonstrates the utility of the model.