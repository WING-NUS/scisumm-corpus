    <S sid="12" ssid="7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S>
 <S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
 <S sid="131" ssid="3">Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N &#8722; 1 preceding words.</S>
 <S sid="21" ssid="16">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S>
<S sid="108" ssid="12">Compared with SRILM, IRSTLM adds several features: lower memory consumption, a binary file format with memory mapping, caching to increase speed, and quantization.</S>
    <S sid="129" ssid="1">In addition to the optimizations specific to each datastructure described in Section 2, we implement several general optimizations for language modeling.</S>
    <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
    <S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
    <S sid="263" ssid="5">Quantization can be improved by jointly encoding probability and backoff.</S>
    <S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
<S sid="145" ssid="17">If the context wnf will never extend to the right (i.e. wnf v is not present in the model for all words v) then no subsequent query will match the full context.</S>
 <S sid="182" ssid="1">This section measures performance on shared tasks in order of increasing complexity: sparse lookups, evaluating perplexity of a large file, and translation with Moses.</S>
    <S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
    <S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
    <S sid="140" ssid="12">We have modified Moses (Koehn et al., 2007) to keep our state with hypotheses; to conserve memory, phrases do not keep state.</S>
 <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>