In this paper the author aims at the major technical innovation that is the use of a "maximum-entropy-inspired" model for conditioning and smoothing that let successfully to test and combines many different conditioning events. They also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head. They also talk about the new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision or recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established  "standard" sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus. Indeed, they initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail. It is to this project that their future parsing work will be devoted.