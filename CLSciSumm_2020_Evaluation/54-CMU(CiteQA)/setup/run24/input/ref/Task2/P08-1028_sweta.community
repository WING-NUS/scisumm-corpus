<S sid="65" ssid="13">So, if we assume that only the ith components of u and v contribute to the ith component of p, that these components are not dependent on i, and that the function is symmetric with regard to the interchange of u and v, we obtain a simpler instantiation of an additive model: Analogously, under the same assumptions, we obtain the following simpler multiplicative model: only the ith components of u and v contribute to the ith component of p. Another class of models can be derived by relaxing this constraint.</S>
 <S sid="75" ssid="23">An extreme form of this differential in the contribution of constituents is where one of the vectors, say u, contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic, however it can serve as a simple baseline against which to compare more sophisticated models.</S>
<S sid="42" ssid="15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>
 <S sid="21" ssid="17">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S>
    <S sid="195" ssid="7">The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately.</S>
<S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
 <S sid="29" ssid="2">While neural networks can readily represent single distinct objects, in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>
    <S sid="38" ssid="11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>
<S sid="64" ssid="12">Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>
<S sid="163" ssid="76">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S>
<S sid="185" ssid="19">The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.</S>