Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1102,C08-1049,0,2008,0,"Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","'1','0','21'","<S ssid=""1"" sid=""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid=""1"" sid=""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid=""1"" sid=""21"">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>","['method_citation', 'result_citation']"
2,P08-1102,C08-1049,0,2008,0,"As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","As described in Ng and Low (2004) and Jiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","'0','96','60'","<S ssid=""1"" sid=""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid=""1"" sid=""96"">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S ssid=""1"" sid=""60"">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>",['method_citation']
3,P08-1102,C08-1049,0,2008,0,plates called lexical-target in the column below areintroduced by Jiang et al (2008),plates called lexical-target in the column below are introduced by Jiang et al (2008),"'0','60','21'","<S ssid=""1"" sid=""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid=""1"" sid=""60"">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S><S ssid=""1"" sid=""21"">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>",['method_citation']
4,P08-1102,P12-1110,0,2008,0,"For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","'0','1','21'","<S ssid=""1"" sid=""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid=""1"" sid=""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid=""1"" sid=""21"">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>","['method_citation', 'result_citation']"
5,P08-1102,D12-1126,0,2008,0,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,"'21','3','60'","<S ssid=""1"" sid=""21"">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid=""1"" sid=""3"">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S ssid=""1"" sid=""60"">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>",['method_citation']
6,P08-1102,C10-1135,0,2008,0,"We use the feature templates the same as Jiang et al, (2008) to extract features form E model","We use the feature templates the same as Jiang et al, (2008) to extract features form E model","'21','0','60'","<S ssid=""1"" sid=""21"">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid=""1"" sid=""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid=""1"" sid=""60"">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>",['method_citation']
8,P08-1102,P12-1025,0,"Jiangetal., 2008a",0,"approach, where basic processing units are characters which compose words (Jiangetal., 2008a)","basic processing units are characters which compose words (Jiangetal., 2008a)","'0','21','64'","<S ssid=""1"" sid=""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid=""1"" sid=""21"">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid=""1"" sid=""64"">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>",['method_citation']
9,P08-1102,C10-2096,0,2008b,0,"The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","'0','21','60'","<S ssid=""1"" sid=""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid=""1"" sid=""21"">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid=""1"" sid=""60"">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>",['method_citation']
10,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","'1','0','24'","<S ssid=""1"" sid=""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid=""1"" sid=""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid=""1"" sid=""24"">It is a better idea to perform segmentation and POS tagging jointly in a uniform framework.</S>","['method_citation', 'result_citation']"
11,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.2 Lattice-forest SystemWe first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","'0','1','96'","<S ssid=""1"" sid=""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid=""1"" sid=""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid=""1"" sid=""96"">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>",['method_citation']
12,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","'0','1','21'","<S ssid=""1"" sid=""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid=""1"" sid=""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid=""1"" sid=""21"">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>","['method_citation', 'result_citation']"
13,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","'0','96','72'","<S ssid=""1"" sid=""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid=""1"" sid=""96"">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S ssid=""1"" sid=""72"">Suppose the conditional probability Pr(t|w) describes the probability that the word w is labelled as the POS t, while Pr(w|t) describes the probability that the POS t generates the word w, then P(T|W) can be approximated by: Pr(w|t) and Pr(t|w) can be easily acquired by Maximum Likelihood Estimates (MLE) over the corpus.</S>",['method_citation']
14,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be? n?, at least in principle","As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be n, at least in principle","'0','96','60'","<S ssid=""1"" sid=""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid=""1"" sid=""96"">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S ssid=""1"" sid=""60"">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>",['method_citation']
15,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","'0','1','2'","<S ssid=""1"" sid=""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid=""1"" sid=""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid=""1"" sid=""2"">With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S>",['method_citation']
17,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","'0','60','64'","<S ssid=""1"" sid=""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid=""1"" sid=""60"">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S><S ssid=""1"" sid=""64"">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>",['method_citation']
20,P08-1102,D12-1046,0,Jiang et al2008a,0,"Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a) ,perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","'1','0','2'","<S ssid=""1"" sid=""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid=""1"" sid=""0"">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid=""1"" sid=""2"">With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S>",['method_citation']
