Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,D09-1092,P14-1004,0,"Mimno et al, 2009",0,"This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs","This configuration is similar to PolyLDA (Mimno et al, 2009) or LinkLDA (Yano et al, 2009), such that utterances from different parties are treated as different languages or blog-post and comments pairs","'185','192','145'","<S ssid=""1"" sid=""185"">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid=""1"" sid=""192"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid=""1"" sid=""145"">For T = 800, the top English and Spanish words in 448 topics were exact translations of one another.</S>",['method_citation']
2,D09-1092,P10-1044,0,"Mimno et al, 2009",0,"Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)","Our particular model, LinkLDA, has been applied to a few NLP tasks such as simultaneously modeling the words appearing in blog posts and users who will likely respond to them (Yano et al, 2009), modeling topic-aligned articles in different languages (Mimno et al, 2009), and word sense induction (Brody and Lapata, 2009)","'15','78','13'","<S ssid=""1"" sid=""15"">Previous work on bilingual topic modeling has focused on machine translation applications, which rely on sentence-aligned parallel translations.</S><S ssid=""1"" sid=""78"">Although the model does not distinguish between topic assignment variables within a given document tuple (so it is technically incorrect to speak of different posterior distributions over topics for different documents in a given tuple), we can nevertheless divide topic assignment variables between languages and use them to estimate a Dirichlet-multinomial posterior distribution for each language in each tuple.</S><S ssid=""1"" sid=""13"">To perform topic-based bibliometric analysis on these collections, it is necessary to have topic models that are aligned across languages.</S>",['method_citation']
3,D09-1092,P11-2084,0,"Mimno et al, 2009",0,"(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations","(Mimno et al, 2009) retrieve a list of potential translations simply by selecting a small number N of the most probable words in both languages and then add the Cartesian product of these sets for every topic to a set of candidate translations","'138','148','185'","<S ssid=""1"" sid=""138"">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S><S ssid=""1"" sid=""148"">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S><S ssid=""1"" sid=""185"">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S>",['method_citation']
4,D09-1092,E12-1014,0,"Mimno et al, 2009",0,"Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingualtopic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction","Our Wikipedia-based topic similarity feature, w (f, e), is similar in spirit to polylingual topic models (Mimno et al 2009), but it is scalable to full bilingual lexicon induction","'192','31','11'","<S ssid=""1"" sid=""192"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid=""1"" sid=""31"">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S ssid=""1"" sid=""11"">There are many potential applications for polylingual topic models.</S>",['aim_citation']
5,D09-1092,D11-1086,0,"Mimno et al, 2009",0,"of English document and the second half of its aligned foreign language document (Mimno et al,2009)","For Europarl data sets, we artificially make them comparable by considering the first half of English document and the second half of its aligned foreign language document (Mimno et al,2009)","'148','194','111'","<S ssid=""1"" sid=""148"">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S><S ssid=""1"" sid=""194"">We also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora.</S><S ssid=""1"" sid=""111"">In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts – i.e., we put each of these documents in a single-document tuple.</S>",['method_citation']
6,D09-1092,N12-1007,0,"Mimno et al, 2009",0,"Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details","Since the PLTM is not a contribution of this paper, we refer the interested reader to (Mimno et al, 2009) for more details","'185','180','193'","<S ssid=""1"" sid=""185"">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid=""1"" sid=""180"">We present results for a PLTM with 400 topics.</S><S ssid=""1"" sid=""193"">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>",['method_citation']
7,D09-1092,N12-1007,0,2009,0,"Evaluation Corpus The automatic evaluation of cross-lingual co reference systems requires annotated 10Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly","Mimno et al (2009) showed that so long as the proportion of topically-aligned to non-aligned documents exceeded 0.25, the topic distributions (as measured by mean Jensen-Shannon Divergence between distributions) did not degrade significantly","'185','118','79'","<S ssid=""1"" sid=""185"">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid=""1"" sid=""118"">We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.</S><S ssid=""1"" sid=""79"">For each tuple we can then calculate the JensenShannon divergence (the average of the KL divergences between each distribution and a mean distribution) between these distributions.</S>",['method_citation']
8,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages","Similarly, Polylingual Topic Models (PLTM) (Mimno et al, 2009) generalized LDA to tuples of documents from multiple languages","'9','192','169'","<S ssid=""1"" sid=""9"">In this paper, we present the polylingual topic model (PLTM).</S><S ssid=""1"" sid=""192"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid=""1"" sid=""169"">In this section, we explore two questions relating to comparable text corpora and polylingual topic modeling.</S>",['aim_citation']
9,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)","Our baseline joint PLSA model (JPLSA) is closely related to the poly-lingual LDA model of (Mimno et al, 2009)","'192','9','193'","<S ssid=""1"" sid=""192"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid=""1"" sid=""9"">In this paper, we present the polylingual topic model (PLTM).</S><S ssid=""1"" sid=""193"">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>",['method_citation']
10,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)","We describe the model for two languages, but it is straightforward to generalize to more than two languages, as in (Mimno et al, 2009)","'185','88','77'","<S ssid=""1"" sid=""185"">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid=""1"" sid=""88"">The higher the probability of the held-out document tuples, the better the generalization ability of the model.</S><S ssid=""1"" sid=""77"">Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.</S>",['method_citation']
11,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"j=1 P (z2j|?) P (w 2 j| ?z2j) The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)","The difference between the JPLSA model and the poly-lingual topic model of (Mimno et al, 2009) is that we merge the vocabularies in the two languages and learn topic-specific word distributions over these merged vocabularies, instead of having pairs of topic-specific word distributions, one for each language, like in (Mimno et al, 2009)","'185','31','9'","<S ssid=""1"" sid=""185"">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid=""1"" sid=""31"">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S ssid=""1"" sid=""9"">In this paper, we present the polylingual topic model (PLTM).</S>",['method_citation']
12,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference","Another difference between our model and the poly-lingual LDA model of (Mimno et al, 2009) is that we use maximum aposteriori (MAP) instead of Bayesian inference","'163','31','97'","<S ssid=""1"" sid=""163"">Performance continues to improve with longer documents, most likely due to better topic inference.</S><S ssid=""1"" sid=""31"">They also provide little analysis of the differences between polylingual and single-language topic models.</S><S ssid=""1"" sid=""97"">Rather, these results are intended as a quantitative analysis of the difference between the two models.</S>",['method_citation']
13,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)","For computing distance we used the L1-norm of the difference, which worked a bit better than the Jensen Shannon divergence between the topic vectors used in (Mimno et al, 2009)","'185','156','158'","<S ssid=""1"" sid=""185"">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid=""1"" sid=""156"">We use both Jensen-Shannon divergence and cosine distance.</S><S ssid=""1"" sid=""158"">Results averaged over all query/target language pairs are shown in figure 7 for Jensen-Shannon divergence.</S>",['method_citation']
15,D09-1092,D10-1025,0,"Mimno et al, 2009",0,"In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours","In previously reported work, (Mimno et al, 2009) evaluate parallel document retrieval using PLTM on Europarl speeches in English and Spanish, using training and test sets of size similar to ours","'19','148','110'","<S ssid=""1"" sid=""19"">We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S ssid=""1"" sid=""148"">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S><S ssid=""1"" sid=""110"">Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S>",['method_citation']
16,D09-1092,W12-3117,0,"Mimno et al, 2009",0,"We assume that a higher dimensionality can lead to a better repartitioning of the vocabulary over the topics. Multilingual LDA has been used before in natural language processing ,e.g .polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)","Multilingual LDA has been used before in natural language processing, e.g. polylingual topic models (Mimno et al, 2009) or multilingual topic models for unaligned text (Boyd-Graber and Blei, 2009)","'192','3','18'","<S ssid=""1"" sid=""192"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid=""1"" sid=""3"">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid=""1"" sid=""18"">In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S>",['aim_citation']
17,D09-1092,W11-2133,0,2009,0,"ji =wjk? M j i? m k=1w j k, (1) where M j is the topic distribution of document j and wk is the number of occurrences of phrase pair Xk in document j. Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipediaarticles)","Mimno et al (2009) extend the original concept of LDA to support polylingual topic models (PLTM), both on parallel (such as EuroParl) and partly comparable documents (such as Wikipedia articles)","'192','9','18'","<S ssid=""1"" sid=""192"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid=""1"" sid=""9"">In this paper, we present the polylingual topic model (PLTM).</S><S ssid=""1"" sid=""18"">In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S>",['aim_citation']
18,D09-1092,W11-2133,0,2009,0,Tuple-specific topic distributions arelearned using LDA with distinct topic-word concentration parameters? l. Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora,Mimno et al (2009) show that PLTM sufficiently aligns topics in parallel corpora,"'146','19','148'","<S ssid=""1"" sid=""146"">In addition to enhancing lexicons by aligning topic-specific vocabulary, PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S><S ssid=""1"" sid=""19"">We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S><S ssid=""1"" sid=""148"">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>",['method_citation']
19,D09-1092,P14-2110,0,"Mimno et al, 2009",0,"A good candidate for multilingual topic analyses are polylin gual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic","A good candidate for multilingual topic analyses are polylingual topic models (Mimno et al, 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic","'3','192','0'","<S ssid=""1"" sid=""3"">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid=""1"" sid=""192"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid=""1"" sid=""0"">Polylingual Topic Models</S>",['aim_citation']
20,D09-1092,P14-2110,0,2009,0,"3csLDATo train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics. First ,polylingual topic models require parallel or comparable corpora in which each document has an assigned language","To train a polylingual topic model on social media, we make two modifications to the model of Mimno et al (2009): add a token specific language variable, and a process for identifying aligned top ics","'3','192','9'","<S ssid=""1"" sid=""3"">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid=""1"" sid=""192"">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid=""1"" sid=""9"">In this paper, we present the polylingual topic model (PLTM).</S>",['aim_citation']
