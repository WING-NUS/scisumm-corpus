Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,J01-2004,W05-0104,0,2001,0,"Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))","Second, their language models were used to rescore n-best speech lists (supplied by Brian Roark, see Roark (2001))","'355','98','247'","<S ssid=""1"" sid=""355"">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.</S><S ssid=""1"" sid=""98"">We will focus our very brief review, however, on those that use grammars or parsing for their language models.</S><S ssid=""1"" sid=""247"">Perplexity is a standard measure within the speech recognition community for comparing language models.</S>",['method_citation']
2,J01-2004,P08-1013,0,2001,0,"Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition. These models have in common that they explicitly or implicitly use a context-free grammar induced from a tree bank, with the exception of Chelba and Jelinek (2000)",Other linguistically inspired language models like Chelba and Jelinek (2000) and Roark (2001) have been applied to continuous speech recognition,"'141','14','1'","<S ssid=""1"" sid=""141"">We will then present empirical results in two domains: one to compare with previous work in the parsing literature, and the other to compare with previous work using parsing for language modeling for speech recognition, in particular with the Chelba and Jelinek results mentioned above.</S><S ssid=""1"" sid=""14"">Perhaps one reason for this is that, until relatively recently, few methods have come out of the natural language processing community that were shown to improve upon the very simple language models still standardly in use in speech recognition systems.</S><S ssid=""1"" sid=""1"">This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition.</S>",['method_citation']
4,J01-2004,P04-1015,0,"Roark, 2001a",0,"Theperceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn tree bank","The perceptron approach was implemented with the same feature set as that of an existing generative model (Roark, 2001a), and experimental results show that it gives competitive performance to the generative model on parsing the Penn treebank","'347','346','343'","<S ssid=""1"" sid=""347"">The perplexity improvement was achieved by simply taking the existing parsing model and applying it, with no extra training beyond that done for parsing.</S><S ssid=""1"" sid=""346"">These results are particularly remarkable, given that we did not build our model as a language model per se, but rather as a parsing model.</S><S ssid=""1"" sid=""343"">Our parsing model's perplexity improves upon their first result fairly substantially, but is only slightly better than their second result.'</S>",['method_citation']
5,J01-2004,P04-1015,0,"Roark, 2001a",0,"We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the per ceptron model gives performance competitive to that of the generative model on parsing the Penn tree bank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search","We implemented the perceptron approach with the same feature set as that of an existing generative model (Roark, 2001a), and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank, thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search","'390','30','19'","<S ssid=""1"" sid=""390"">With a simple conditional probability model, and simple statistical search heuristics, we were able to find very accurate parses efficiently, and, as a side effect, were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S ssid=""1"" sid=""30"">Of course, the joint probability can be used as a language model, but it cannot be interpolated on a word-by-word basis with, say, a trigram model, which we will demonstrate is a useful thing to do.</S><S ssid=""1"" sid=""19"">A new language model, based on probabilistic top-down parsing, will be outlined and compared with the previous literature, and extensive empirical results will be presented which demonstrate its utility.</S>",['method_citation']
6,J01-2004,P04-1015,0,2001a,0,"One way around this problem is to adopt a two-pass approach, where GEN (x) is the top N analyses under some initial model, as in the re ranking approach of Collins (2000) .In the current paper we explore alternatives to rerank ing approaches, namely heuristic methods for finding the arg max, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)","In the current paper we explore alternatives to reranking approaches, namely heuristic methods for finding the argmax, specifically incremental beam-search strategies related to the parsers of Roark (2001a) and Ratnaparkhi (1999)","'268','390','137'","<S ssid=""1"" sid=""268"">This is an incremental parser with a pruning strategy and no backtracking.</S><S ssid=""1"" sid=""390"">With a simple conditional probability model, and simple statistical search heuristics, we were able to find very accurate parses efficiently, and, as a side effect, were able to assign word probabilities that yield a perplexity improvement over previous results.</S><S ssid=""1"" sid=""137"">Here we will present a parser that uses simple search heuristics of this sort without DP.</S>",['method_citation']
7,J01-2004,P04-1015,0,2001a,0,"approach The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights","The parser is an incremental beam-search parser very similar to the sort described in Roark (2001a; 2004), with some changes in the search strategy to accommodate the perceptron feature weights","'268','137','383'","<S ssid=""1"" sid=""268"">This is an incremental parser with a pruning strategy and no backtracking.</S><S ssid=""1"" sid=""137"">Here we will present a parser that uses simple search heuristics of this sort without DP.</S><S ssid=""1"" sid=""383"">It was selected with the goal of high parser accuracy; but in this new domain, parser accuracy is a secondary measure of performance.</S>",['method_citation']
9,J01-2004,P04-1015,0,2001a,0,"Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word","Unlike in Roark (2001a; 2004), there is no look-ahead statistic, so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word","'217','80','309'","<S ssid=""1"" sid=""217"">The probability PD is the product of the probabilities of all rules in the derivation D. F is the product of PD and a look-ahead probability, LAP(S,w,), which is a measure of the likelihood of the stack S rewriting with w, at its left corner.</S><S ssid=""1"" sid=""80"">It also brings words further downstream into the look-ahead at the point of specification.</S><S ssid=""1"" sid=""309"">Let Ht be the priority queue H, before any processing has begun with word w, in the look-ahead.</S>",['method_citation']
10,J01-2004,P05-1022,0,"Roark, 2001",0,"A good example of this is the Roark parser (Roark, 2001) which works left-to right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning","A good example of this is the Roark parser (Roark, 2001) which works left-to-right through the sentence, and abjures dynamic programming in favor of a beam search, keeping some large number of possibilities to extend by adding the next word, and then re-pruning","'268','295','297'","<S ssid=""1"" sid=""268"">This is an incremental parser with a pruning strategy and no backtracking.</S><S ssid=""1"" sid=""295"">Our observed times look polynomial, which is to be expected given our pruning strategy: the denser the competitors within a narrow probability range of the best analysis, the more time will be spent working on these competitors; and the farther along in the sentence, the more chance for ambiguities that can lead to such a situation.</S><S ssid=""1"" sid=""297"">The differences between a k-best and a beam-search parser (not to mention the use of dynamic programming) make a running time difference unsurprising.</S>",['method_citation']
11,J01-2004,P05-1022,0,"Roark, 2001",0,"At the end one has a beam-width? s number of best parses (Roark, 2001) .The Collins parser (Collins, 1997) does use dynamic programming in its search","At the end one has a beam-width's number of best parses (Roark, 2001)","'382','381','302'","<S ssid=""1"" sid=""382"">The base beam factor that we have used to this point is 10', which is quite wide.</S><S ssid=""1"" sid=""381"">The last set of results that we will present addresses the question of how wide the beam must be for adequate results.</S><S ssid=""1"" sid=""302"">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>",['method_citation']
12,J01-2004,P05-1022,0,"Roark, 2001",0,"To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses","To put this in perspective, Roark (Roark, 2001) reports oracle results of 0.941 (with the same experimental setup) using his parser to return a variable number of parses","'291','270','283'","<S ssid=""1"" sid=""291"">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length < 100.</S><S ssid=""1"" sid=""270"">In such a case, the parser fails to return a complete parse.</S><S ssid=""1"" sid=""283"">Unlike the Roark and Johnson parser, however, our coverage did not substantially drop as the amount of conditioning information increased, and in some cases, coverage improved slightly.</S>",['result_citation']
13,J01-2004,P04-1006,0,"Roark, 2001",0,"We ran the first stage parser with 4-timesoverparsing for each string in 7The n? best lists were provided by Brian Roark (Roark, 2001) 8A local-tree is an explicit expansion of an edge and its children","The n-best lists were provided by Brian Roark (Roark, 2001)","'372','355','264'","<S ssid=""1"" sid=""372"">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S><S ssid=""1"" sid=""355"">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.</S><S ssid=""1"" sid=""264"">Sometimes in figures we will plot their average, and also what can be termed the parse error, which is one minus their average.</S>",['method_citation']
14,J01-2004,P05-1063,0,"Roark, 2001a",0,"Incremental top down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models","Incremental top-down and left-corner parsing (Roark, 2001a; Roark, 2001b) and head-driven parsing (Charniak, 2001) approaches have directly used generative PCFG models as language models","'21','4','10'","<S ssid=""1"" sid=""21"">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S><S ssid=""1"" sid=""4"">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S><S ssid=""1"" sid=""10"">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S>",['method_citation']
15,J01-2004,W10-2009,0,"Roark, 2001",0,"Levy, on the other hand ,argued that studies of probabilistic parsing reveal that typically a small number of analyses are as signed the majority of probability mass (Roark, 2001)","Levy, on the other hand, argued that studies of probabilistic parsing reveal that typically a small number of analyses are assigned the majority of probability mass (Roark, 2001)","'100','19','0'","<S ssid=""1"" sid=""100"">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S ssid=""1"" sid=""19"">A new language model, based on probabilistic top-down parsing, will be outlined and compared with the previous literature, and extensive empirical results will be presented which demonstrate its utility.</S><S ssid=""1"" sid=""0"">Probabilistic Top-Down Parsing and Language Modeling</S>",['method_citation']
17,J01-2004,D09-1034,0,2001,0,"For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lex icalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)","For example, in Demberg and Keller (2008), trials were run deriving surprisal from the Roark (2001) parser under two different conditions: fully lexicalized parsing, and fully unlexicalized parsing (to pre-terminal part-of-speech tags)","'171','100','291'","<S ssid=""1"" sid=""171"">Then a subsequent function could be defined as follows: return the parent of the parent (the grandparent) of constituent (A) only if constituent (A) has no sibling to the left—in other words, if the previous function returns NULL; otherwise return the second closest sibling to the left of constituent (A), or, as always, NULL if no such node exists.</S><S ssid=""1"" sid=""100"">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S ssid=""1"" sid=""291"">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length < 100.</S>",['method_citation']
18,J01-2004,D09-1034,0,2001,0,"We modified the Roark (2001) parser to calculate the discussed measures 1, and the empirical results in? 4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time","We modified the Roark (2001) parser to calculate the discussed measures, and the empirical results in ?4 show several things, including: 1) using a fully lexicalized parser to calculate syntactic surprisal and entropy provides higher predictive utility for reading times than these measures calculated via unlexicalized parsing (as in Demberg and Keller); and 2) syntactic entropy is a useful predictor of reading time","'245','107','317'","<S ssid=""1"" sid=""245"">The empirical results will be presented in three stages: (i) trials to examine the accuracy and efficiency of the parser; (ii) trials to examine its effect on test corpus perplexity and recognition performance; and (iii) trials to examine the effect of beam variation on these performance measures.</S><S ssid=""1"" sid=""107"">Interpolating the observed bigram probabilities with these calculated bigrams led, in both cases, to improvements in word error rate over using the observed bigrams alone, demonstrating that there is some benefit to using these syntactic language models to generalize beyond observed n-grams.</S><S ssid=""1"" sid=""317"">Note, however, that this renormalization factor is necessarily less than one, and thus would uniformly increase each word's probability under the model, that is, any perplexity results reported below will be higher than the &quot;true&quot; perplexity that would be assigned with a properly normalized distribution.</S>",['method_citation']
19,J01-2004,D09-1034,0,2001,0,"In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here","In this section, we review relevant details of the Roark (2001) incremental top-down parser, as configured for use here","'19','208','4'","<S ssid=""1"" sid=""19"">A new language model, based on probabilistic top-down parsing, will be outlined and compared with the previous literature, and extensive empirical results will be presented which demonstrate its utility.</S><S ssid=""1"" sid=""208"">We now outline the top-down parsing algorithm.</S><S ssid=""1"" sid=""4"">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S>",['method_citation']
20,J01-2004,D09-1034,0,2001,0,"At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is re quired to calculate such measures. Let H (D) be the entropy over a set of derivations D, calculated as follows: H (D)=? X D? D? (D) P D?? D? (D?) log? (D) P D?? D? (D?) (10) If the set of derivations D= D (G, W [1, i]) is a set of partial derivations for string W [1, i], then H (D) is a measure of uncertainty over the partial derivations ,i.e., the uncertainty regarding the correct analysis of what has already been processed","At each word in the string, the Roark (2001) top-down parser provides access to the weighted set of partial analyses in the beam; the set of complete derivations consistent with these is not immediately accessible, hence additional work is required to calculate such measures","'383','310','21'","<S ssid=""1"" sid=""383"">It was selected with the goal of high parser accuracy; but in this new domain, parser accuracy is a secondary measure of performance.</S><S ssid=""1"" sid=""310"">This is a subset of the possible leftmost partial derivations with respect to the prefix string W. Since RV is produced by expanding only analyses on priority queue H;', the set of complete trees consistent with the partial derivations on priority queue Ht is a subset of the set of complete trees consistent with the partial derivations on priority queue HT'', that is, the total probability mass represented by the priority queues is monotonically decreasing.</S><S ssid=""1"" sid=""21"">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>",['method_citation']
