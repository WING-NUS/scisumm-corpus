Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1028,D08-1094,0,2008,0,"Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and Kadditional knowledge","Mitchell and Lapata (2008) propose a framework to represent the meaning of the combination p+ a as a function f operating on four components: c= f (p, a, R, K) (3) R is the relation holding between p and a, and K additional knowledge","'62','57','1'","<S ssid=""1"" sid=""62"">Another simplification concerns K which can be ignored so as to explore what can be achieved in the absence of additional knowledge.</S><S ssid=""1"" sid=""57"">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S><S ssid=""1"" sid=""1"">This paper proposes a framework for representing the meaning of phrases and sentences in vector space.</S>",['method_citation']
4,P08-1028,P14-1060,0,2008,0,"While works such asthe SDSM model suffer from the problem of sparsity in composing structures beyond bi grams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstetteand Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, methods such as Mitchell and Lapata (2008) and (Socher et al, 2012) and Grefenstette and Sadrzadeh (2011) are restricted by significant model biases in representing semantic com position by generic algebraic operations","'97','11','138'","<S ssid=""1"" sid=""97"">Moreover by using a minimal structure we factor out inessential degrees of freedom and are able to assess the merits of different models on an equal footing.</S><S ssid=""1"" sid=""11"">Despite their widespread use, vector-based models are typically directed at representing words in isolation and methods for constructing representations for phrases or sentences have received little attention in the literature.</S><S ssid=""1"" sid=""138"">Model Parameters Irrespectively of their form, all composition models discussed here are based on a semantic space for representing the meanings of individual words.</S>",['method_citation']
6,P08-1028,P10-1097,0,2008,0,"Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","Mitchell and Lapata (2008), henceforth M& amp; L, propose a general framework in which meaning representations for complex expressions are computed compositionally by combining the vector representations of the individual words of the complex expression","'21','53','194'","<S ssid=""1"" sid=""21"">Central in these models is the notion of compositionality — the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S><S ssid=""1"" sid=""53"">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word’s vector typically represents its co-occurrence with neighboring words.</S><S ssid=""1"" sid=""194"">Importantly, additive models capture composition by considering all vector components representing the meaning of the verb and its subject, whereas multiplicative models consider a subset, namely non-zero components.</S>",['method_citation']
7,P08-1028,P10-1097,0,2008,0,"Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting","Interestingly, Mitchell and Lapata (2008) came to the same result in a different setting","'131','109','195'","<S ssid=""1"" sid=""131"">A Wilcoxon rank sum test confirmed that the difference is statistically significant (p < 0.01).</S><S ssid=""1"" sid=""109"">These were further pretested to allow the selection of a subset of items showing clear variations in sense as we wanted to have a balanced set of similar and dissimilar sentences.</S><S ssid=""1"" sid=""195"">The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately.</S>",['method_citation']
8,P08-1028,D11-1094,0,2008,0,"And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","And Mitchell and Lapata (2008) propose a model for vector composition, focusing on the different functions that might be used to combine the constituent vectors","'25','190','51'","<S ssid=""1"" sid=""25"">We present a general framework for vector-based composition which allows us to consider different classes of models.</S><S ssid=""1"" sid=""190"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S ssid=""1"" sid=""51"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>",['method_citation']
9,P08-1028,W11-0131,0,2008,0,"Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","Mitchell and Lapata (2008) provide a general framework for semantic vector composition: p= f (u, v, R, K) (1) 295 where u and v are the vectors to be composed, R is syntactic context, K is a semantic knowledge base, and p is a resulting composed vector (or tensor)","'95','189','25'","<S ssid=""1"" sid=""95"">Focusing on a single compositional structure, namely intransitive verbs and their subjects, is a good point of departure for studying vector combination.</S><S ssid=""1"" sid=""189"">In this paper we presented a general framework for vector-based semantic composition.</S><S ssid=""1"" sid=""25"">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>",['result_citation']
10,P08-1028,W11-0131,0,2008,0,"As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","As Mitchell and Lapata (2008) did, let us temporarily suspend discussion on what semantics populate our vectors for now","'57','86','64'","<S ssid=""1"" sid=""57"">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S><S ssid=""1"" sid=""86"">Since the product of zero with any number is itself zero, the presence of zeroes in either of the vectors leads to information being essentially thrown away.</S><S ssid=""1"" sid=""64"">Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>",['method_citation']
11,P08-1028,P13-2083,0,2008,0,"Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","Mitchell and Lapata (2008) propose a framework to define the composition c= f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition","'62','57','25'","<S ssid=""1"" sid=""62"">Another simplification concerns K which can be ignored so as to explore what can be achieved in the absence of additional knowledge.</S><S ssid=""1"" sid=""57"">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S><S ssid=""1"" sid=""25"">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>",['method_citation']
12,P08-1028,P13-2083,0,"Mitchell and Lapata, 2008",0,"As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","As our final set of baselines, we extend two simple techniques proposed by (Mitchell and Lapata, 2008) that use element-wise addition and multiplication operators to perform composition","'51','167','194'","<S ssid=""1"" sid=""51"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid=""1"" sid=""167"">Our experiments assessed the performance of seven composition models.</S><S ssid=""1"" sid=""194"">Importantly, additive models capture composition by considering all vector components representing the meaning of the verb and its subject, whereas multiplicative models consider a subset, namely non-zero components.</S>",['method_citation']
13,P08-1028,P10-1021,0,2008,0,"Althoughthis model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003) .In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","Although this model has been shown to successfully simulate single and multiple-word priming (McDonald and Brew 2004), it failed to predict processing costs in the Embra eye-tracking corpus (McDonald and Shillcock 2003). In this work we model semantic constraint using the representational framework put forward in Mitchell and Lapata (2008)","'60','138','52'","<S ssid=""1"" sid=""60"">To derive specific models from this general framework requires the identification of appropriate constraints to narrow the space of functions being considered.</S><S ssid=""1"" sid=""138"">Model Parameters Irrespectively of their form, all composition models discussed here are based on a semantic space for representing the meanings of individual words.</S><S ssid=""1"" sid=""52"">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S>",['method_citation']
14,P08-1028,P10-1021,0,2008,0,"Assuming that h is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature: hi =ui+ vi (3) Alternatively, we can assume that h is a linear function of the tensor product of u and v, and thus derive models based on multiplication: hi =ui? vi (4) Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","Mitchell and Lapata (2008) show that several additive and multiplicative models can be formulated under this framework, including the well known tensor products (Smolensky 1990) and circular convolution (Plate 1995)","'194','26','4'","<S ssid=""1"" sid=""194"">Importantly, additive models capture composition by considering all vector components representing the meaning of the verb and its subject, whereas multiplicative models consider a subset, namely non-zero components.</S><S ssid=""1"" sid=""26"">Specifically, we present both additive and multiplicative models of vector combination and assess their performance on a sentence similarity rating experiment.</S><S ssid=""1"" sid=""4"">Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.</S>",['method_citation']
15,P08-1028,W11-0115,0,2008,0,"Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","Mitchell and Lapata (2008) introduce a whole family of models of compositionality based on vector addition and point wise-multiplication (and a weighted combination of both), evaluated on a sentence similarity task inspired by Kintsch (2001)","'190','3','95'","<S ssid=""1"" sid=""190"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S ssid=""1"" sid=""3"">Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S><S ssid=""1"" sid=""95"">Focusing on a single compositional structure, namely intransitive verbs and their subjects, is a good point of departure for studying vector combination.</S>",['method_citation']
16,P08-1028,W11-0115,0,2008,0,"The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","The approach proposed by Guevara (2010) is really only an extension of the full additive model of Mitchell and Lapata (2008), the only difference being that adopting a supervised learning methodology ensures that the weight parameters in the function are estimated optimally by linear regression","'187','84','177'","<S ssid=""1"" sid=""187"">However, the difference between the two models is not statistically significant.</S><S ssid=""1"" sid=""84"">The proposal is not merely notational.</S><S ssid=""1"" sid=""177"">The difference between High and Low similarity values estimated by these models are statistically significant (p < 0.01 using the Wilcoxon rank sum test).</S>",['method_citation']
17,P08-1028,W11-0115,0,2008,0,"For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","For example, Mitchell and Lapata (2008) use their models to approximate the human ratings in their sentence similarity dataset","'180','170','163'","<S ssid=""1"" sid=""180"">As can be seen, all models are significantly correlated with the human ratings.</S><S ssid=""1"" sid=""170"">Table 2 shows the average model ratings for High and Low similarity items.</S><S ssid=""1"" sid=""163"">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S>",['method_citation']
18,P08-1028,W11-1310,0,2008,0,We use other WSM settings following Mitchell and Lapata (2008),We use other WSM settings following Mitchell and Lapata (2008),"'171','69','154'","<S ssid=""1"" sid=""171"">For comparison, we also show the human ratings for these items (UpperBound).</S><S ssid=""1"" sid=""69"">Simply adding the vectors u and v lumps their contents together rather than allowing the content of one vector to pick out the relevant content of the other.</S><S ssid=""1"" sid=""154"">For the best performing model the weight for the verb was 80% and for the noun 20%.</S>",['method_citation']
19,P08-1028,W11-1310,0,2008,0,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,Mitchell and Lapata (2008) observed that a simple multiplication function modelled compositionality better than addition,"'190','51','73'","<S ssid=""1"" sid=""190"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S ssid=""1"" sid=""51"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid=""1"" sid=""73"">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware, since semantically important constituents can participate more actively in the composition.</S>",['method_citation']
20,P08-1028,W11-1310,0,"Mitchell and Lapata, 2008",0,"We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","We use the compositionality functions, simple addition and simple multiplication to build compositional vectors Vwr1+wr2 and Vwr1 ?wr2. These are as described in (Mitchell and Lapata, 2008)","'190','51','95'","<S ssid=""1"" sid=""190"">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S><S ssid=""1"" sid=""51"">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid=""1"" sid=""95"">Focusing on a single compositional structure, namely intransitive verbs and their subjects, is a good point of departure for studying vector combination.</S>",['method_citation']
