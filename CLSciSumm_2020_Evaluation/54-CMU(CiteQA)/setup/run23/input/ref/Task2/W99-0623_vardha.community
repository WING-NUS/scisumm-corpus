    <S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
<S sid="117" ssid="46">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S>
<S sid="72" ssid="1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
    <S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
  <S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
 <S sid="84" ssid="13">The first shows how constituent features and context do not help in deciding which parser to trust.</S>
    <S sid="76" ssid="5">The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.</S>
 <S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
  <S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
 <S sid="72" ssid="1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
    <S sid="87" ssid="16">It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.</S>
    <S sid="51" ssid="37">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure.</S>
    <S sid="79" ssid="8">Precision is the portion of hypothesized constituents that are correct and recall is the portion of the Treebank constituents that are hypothesized.</S>
  <S sid="27" ssid="13">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S>
 <S sid="77" ssid="6">Each parse is converted into a set of constituents represented as a tuples: (label, start, end).</S>
NA
  <S sid="11" ssid="7">Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).</S>
  <S sid="116" ssid="45">The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.</S>