Authors Mimno et all introduced a poly-lingual topic model that discovered topics aligned across multiple languages. They explored the models characteristics using two large corpora, each with over ten different languages, and demonstrated its usefulness in supporting machine translation and tracking topic trends across languages. They analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics. They also demonstrated that relatively small numbers of topically comparable document tuples are sufficient to align topics between languages in non-comparable corpora. When applied to comparable document collections such as Wikipedia, PLTM supported data-driven analysis of differences and similarities across all languages for readers who understand any one language.