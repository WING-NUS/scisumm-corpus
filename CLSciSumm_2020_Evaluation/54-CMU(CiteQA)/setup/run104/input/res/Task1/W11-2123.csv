Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W11-2123,W11-2138,0,"Heafield, 2011",0,"We used common tools for phrase-based translation? Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","We used common tools for phrase-based translation Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","'5','26','1'","<S ssid=""1"" sid=""5"">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid=""1"" sid=""26"">We use two common techniques, hash tables and sorted arrays, describing each before the model that uses the technique.</S><S ssid=""1"" sid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>",['method_citation']
2,W11-2123,P14-2022,0,"Heafield, 2011",0,"The language model was com piled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","'1','102','32'","<S ssid=""1"" sid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid=""1"" sid=""102"">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie), allocating memory all at once (eliminating the need for full pointers), and being easy to compile.</S><S ssid=""1"" sid=""32"">Linear probing places at most one entry in each bucket.</S>",['method_citation']
3,W11-2123,W12-3145,0,"Heafield, 2011",0,"Thus given afragment tf consisting of a sequence of target to kens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning.2 While this increases the number ofLM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","Thus given a fragment tf consisting of a sequence of target tokens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning. While this increases the number of LM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","'1','149','0'","<S ssid=""1"" sid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid=""1"" sid=""149"">RandLM and SRILM also remove context that will not extend, but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.</S><S ssid=""1"" sid=""0"">KenLM: Faster and Smaller Language Model Queries</S>",['method_citation']
4,W11-2123,W12-3131,0,"Heafield, 2011",0,"Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","'3','1','4'","<S ssid=""1"" sid=""3"">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S ssid=""1"" sid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid=""1"" sid=""4"">Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems.</S>",['method_citation']
5,W11-2123,W12-3154,0,"Heafield, 2011",0,"The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","'3','1','156'","<S ssid=""1"" sid=""3"">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S ssid=""1"" sid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid=""1"" sid=""156"">As noted in Section 1, our code finds the longest matching entry wnf for query p(wn|s(wn−1 f ) The probability p(wn|wn−1 f ) is stored with wnf and the backoffs are immediately accessible in the provided state s(wn−1 When our code walks the data structure to find wnf , it visits wnn, wnn−1, ... , wnf .</S>",['method_citation']
6,W11-2123,P12-2058,0,"Heafield, 2011",0,"The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","'3','32','164'","<S ssid=""1"" sid=""3"">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S ssid=""1"" sid=""32"">Linear probing places at most one entry in each bucket.</S><S ssid=""1"" sid=""164"">This makes memory usage comparable to our PROBING model.</S>",['method_citation']
7,W11-2123,W11-2139,0,2011,0,Inference was carried out using the language modeling library described by Heafield (2011),Inference was carried out using the language modeling library described by Heafield (2011),"'3','129','26'","<S ssid=""1"" sid=""3"">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S ssid=""1"" sid=""129"">In addition to the optimizations specific to each datastructure described in Section 2, we implement several general optimizations for language modeling.</S><S ssid=""1"" sid=""26"">We use two common techniques, hash tables and sorted arrays, describing each before the model that uses the technique.</S>",['method_citation']
8,W11-2123,P13-2003,0,"Heafield, 2011",0,"We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","'3','62','1'","<S ssid=""1"" sid=""3"">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S ssid=""1"" sid=""62"">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words), then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S><S ssid=""1"" sid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>",['method_citation']
9,W11-2123,W12-3134,0,2011,0,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language mod els by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,"'3','5','1'","<S ssid=""1"" sid=""3"">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S ssid=""1"" sid=""5"">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid=""1"" sid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>",['method_citation']
10,W11-2123,W12-3134,0,2011,0,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,"'3','29','149'","<S ssid=""1"" sid=""3"">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S ssid=""1"" sid=""29"">This range is collapsed to a number of buckets, typically by taking the hash modulo the number of buckets.</S><S ssid=""1"" sid=""149"">RandLM and SRILM also remove context that will not extend, but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.</S>",['method_citation']
11,W11-2123,W12-3134,0,2011,0,"With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)",,,"['implication_citation', 'result_citation']"
12,W11-2123,W12-3160,0,"Heafield, 2011",0,"This was used to create a KenLM (Heafield, 2011)","This was used to create a KenLM (Heafield, 2011)",,,"['implication_citation', 'result_citation']"
13,W11-2123,W12-3706,0,"Heafield, 2011",0,"In the Opinum system we query the M p, M n mod els with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable fora web application","In the Opinum system we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application",'156',"<S ssid=""1"" sid=""156"">As noted in Section 1, our code finds the longest matching entry wnf for query p(wn|s(wn−1 f ) The probability p(wn|wn−1 f ) is stored with wnf and the backoffs are immediately accessible in the provided state s(wn−1 When our code walks the data structure to find wnf , it visits wnn, wnn−1, ... , wnf .</S>",['method_citation']
14,W11-2123,W11-2147,0,"Heafield, 2011",0,"Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","'144','1','32'","<S ssid=""1"" sid=""144"">State will ultimately be used as context in a subsequent query.</S><S ssid=""1"" sid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid=""1"" sid=""32"">Linear probing places at most one entry in each bucket.</S>",['method_citation']
15,W11-2123,E12-1083,0,"Heafield, 2011",0,"For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)",,,"['implication_citation', 'result_citation']"
16,W11-2123,P12-1002,0,"Heafield, 2011",0,"Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima ?an,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Simaan,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","'149','1','58'","<S ssid=""1"" sid=""149"">RandLM and SRILM also remove context that will not extend, but SRILM performs a second lookup in its trie whereas our approach has minimal additional cost.</S><S ssid=""1"" sid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid=""1"" sid=""58"">Given a key k, it estimates the position If the estimate is exact (A[pivot] = k), then the algorithm terminates succesfully.</S>",['method_citation']
17,W11-2123,D12-1108,0,"Heafield, 2011",0,"n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","'1','131','52'","<S ssid=""1"" sid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid=""1"" sid=""131"">Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N − 1 preceding words.</S><S ssid=""1"" sid=""52"">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>",['method_citation']
18,W11-2123,P12-2006,0,"Heafield, 2011",0,"Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","'3','137','62'","<S ssid=""1"" sid=""3"">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S ssid=""1"" sid=""137"">The state function is integrated into the query process so that, in lieu of the query p(wnjwn−1 1 ), the application issues query p(wnjs(wn−1 1 )) which also returns s(wn1 ).</S><S ssid=""1"" sid=""62"">If the key distribution’s range is also known (i.e. vocabulary identifiers range from 0 to the number of words), then interpolation search can use this information instead of reading A[0] and A[|A |− 1] to estimate pivots; this optimization alone led to a 24% speed improvement.</S>",['method_citation']
19,W11-2123,P13-2073,0,"Heafield, 2011",0,"For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","'3','1','143'","<S ssid=""1"" sid=""3"">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S ssid=""1"" sid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid=""1"" sid=""143"">IRSTLM and BerkeleyLM use this state function (and a limit of N −1 words), but it is more strict than necessary, so decoders using these packages will miss some recombination opportunities.</S>",['method_citation']
20,W11-2123,P13-1109,0,"Heafield, 2011",0,"For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing",'156',"<S ssid=""1"" sid=""156"">As noted in Section 1, our code finds the longest matching entry wnf for query p(wn|s(wn−1 f ) The probability p(wn|wn−1 f ) is stored with wnf and the backoffs are immediately accessible in the provided state s(wn−1 When our code walks the data structure to find wnf , it visits wnn, wnn−1, ... , wnf .</S>",['method_citation']
