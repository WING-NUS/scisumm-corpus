parsing: input/ref/Task1/P04-1036_aakansha.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="34">In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
    <S sid="181" ssid="4">This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'181'"]
'180'
'181'
['180', '181']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="4">To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'"]
'48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="169" ssid="17">This method obtains precision of 61% and recall 51%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'"]
'169'
['169']
parsed_discourse_facet ['result_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="115" ssid="13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
    <S sid="181" ssid="4">This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'181'"]
'180'
'181'
['180', '181']
parsed_discourse_facet ['result_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'"]
'180'
['180']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="34">In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S ssid="1" sid="107">To disambiguate senses a system should take context into account.</S><S ssid="1" sid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'1'", "'153'"]
'107'
'1'
'153'
['107', '1', '153']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="162">It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'162'"]
'179'
'178'
'162'
['179', '178', '162']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="34">In these each target word is entered with an ordered list of “nearest neighbours”.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'34'"]
'179'
'153'
'34'
['179', '153', '34']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="0">Finding Predominant Word Senses in Untagged Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'0'"]
'179'
'4'
'0'
['179', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'153'"]
'179'
'4'
'153'
['179', '4', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="169">This method obtains precision of 61% and recall 51%.</S><S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'", "'179'", "'4'"]
'169'
'179'
'4'
['169', '179', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'153'"]
'179'
'4'
'153'
['179', '4', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="0">Finding Predominant Word Senses in Untagged Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'0'"]
'179'
'4'
'0'
['179', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="62">2 The WordNet Similarity package supports a range of WordNet similarity scores.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'62'", "'4'"]
'179'
'62'
'4'
['179', '62', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'179'", "'178'"]
'4'
'179'
'178'
['4', '179', '178']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'36'"]
'179'
'4'
'36'
['179', '4', '36']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'153'"]
'179'
'178'
'153'
['179', '178', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'36'"]
'179'
'178'
'36'
['179', '178', '36']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="162">It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'162'", "'178'"]
'4'
'162'
'178'
['4', '162', '178']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'4'"]
'179'
'153'
'4'
['179', '153', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'4'"]
'179'
'153'
'4'
['179', '153', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'36'"]
'179'
'153'
'36'
['179', '153', '36']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/A00-2030_vardha.csv
 <S sid="11" ssid="1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="10">Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'"]
'50'
['50']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
 <S sid="106" ssid="3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
 <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
 <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="2">The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
 <S sid="3" ssid="1">Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'"]
'3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="15">Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'"]
'32'
['32']
parsed_discourse_facet ['method_citation']
 <S sid="10" ssid="8">Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="13">Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
    <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
 <S sid="26" ssid="9">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'"]
'26'
['26']
parsed_discourse_facet ['method_citation']
    <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="2">Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'104'"]
'2'
'33'
'104'
['2', '33', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'6'"]
'2'
'33'
'6'
['2', '33', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="19">Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'19'"]
'2'
'33'
'19'
['2', '33', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'6'", "'2'"]
'33'
'6'
'2'
['33', '6', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'2'", "'33'"]
'6'
'2'
'33'
['6', '2', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'6'"]
'2'
'33'
'6'
['2', '33', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="92">2.</S><S ssid="1" sid="69">8.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'92'", "'69'"]
'2'
'92'
'69'
['2', '92', '69']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'104'"]
'2'
'6'
'104'
['2', '6', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'2'", "'104'"]
'6'
'2'
'104'
['6', '2', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'33'"]
'2'
'6'
'33'
['2', '6', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'104'"]
'2'
'6'
'104'
['2', '6', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'104'"]
'2'
'33'
'104'
['2', '33', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'104'"]
'2'
'6'
'104'
['2', '6', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'33'"]
'2'
'6'
'33'
['2', '6', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="0">A Novel Use of Statistical Parsing to Extract Information from Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'0'"]
'2'
'6'
'0'
['2', '6', '0']
parsed_discourse_facet ['method_citation']



A00-2030
W10-2924
0
method_citation
['method_citation']



A00-2030
W05-0636
0
method_citation
['method_citation']



A00-2030
P14-1078
0
method_citation
['method_citation']



A00-2030
P05-1061
0
method_citation
['method_citation']



A00-2030
W06-0508
0
method_citation
['method_citation']
parsing: input/ref/Task1/P11-1061_aakansha.csv
<S sid="40" ssid="6">We extend Subramanya et al.&#8217;s intuitions to our bilingual setup.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="11">First, we use a novel graph-based framework for projecting syntactic information across language boundaries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="29" ssid="6">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'"]
'158'
['158']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'"]
'158'
['158']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="1">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="17" ssid="13">Second, we treat the projected labels as features in an unsupervised model (&#167;5), rather than using them directly for supervised training.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="111" ssid="11">We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).</S>
original cit marker offset is 0
new cit marker offset is 0



["'111'"]
'111'
['111']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="29" ssid="6">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'161'"]
'161'
['161']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1061.csv
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="74">This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'74'", "'25'"]
'0'
'74'
'25'
['0', '74', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="24">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'24'", "'25'"]
'0'
'24'
'25'
['0', '24', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'0'", "'23'"]
'25'
'0'
'23'
['25', '0', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="51">For each trigram type x2 x3 x4 in a sequence x1 x2 x3 x4 x5, we count how many times that trigram type co-occurs with the different instantiations of each concept, and compute the point-wise mutual information (PMI) between the two.5 The similarity between two trigram types is given by summing over the PMI values over feature instantiations that they have in common.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'51'"]
'0'
'25'
'51'
['0', '25', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="74">This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'74'"]
'0'
'25'
'74'
['0', '25', '74']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="81">Instead, we resort to an iterative update based method.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'81'", "'25'"]
'0'
'81'
'25'
['0', '81', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'23'"]
'0'
'25'
'23'
['0', '25', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'23'", "'25'"]
'0'
'23'
'25'
['0', '23', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'23'", "'25'"]
'0'
'23'
'25'
['0', '23', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="74">This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'74'", "'16'"]
'0'
'74'
'16'
['0', '74', '16']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="29">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'29'"]
'0'
'25'
'29'
['0', '25', '29']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'16'", "'0'"]
'25'
'16'
'0'
['25', '16', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S ssid="1" sid="87">This locally normalized log-linear model can look at various aspects of the observation x, incorporating overlapping features of the observation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'16'", "'87'"]
'25'
'16'
'87'
['25', '16', '87']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.</S><S ssid="1" sid="24">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'24'"]
'0'
'1'
'24'
['0', '1', '24']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'16'", "'25'"]
'0'
'16'
'25'
['0', '16', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="24">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'24'", "'23'"]
'0'
'24'
'23'
['0', '24', '23']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="29">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'29'"]
'0'
'25'
'29'
['0', '25', '29']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="81">Instead, we resort to an iterative update based method.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'81'", "'25'"]
'0'
'81'
'25'
['0', '81', '25']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P05-1013_aakansha.csv
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="17">However, the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech, with a best performance of 73% UAS (Holan, 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="86" ssid="13">As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'"]
'86'
['86']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="7">As shown in Table 3, the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.</S>
    <S sid="81"  ssid="8">However, the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'81'"]
'80'
'81'
['80', '81']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'"]
'49'
['49']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="10">While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P05-1013.csv
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="20">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'20'"]
'0'
'2'
'20'
['0', '2', '20']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="63">The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'63'"]
'0'
'2'
'63'
['0', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'0'", "'109'"]
'2'
'0'
'109'
['2', '0', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="63">The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'63'"]
'0'
'2'
'63'
['0', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'0'", "'109'"]
'2'
'0'
'109'
['2', '0', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



P05-1013
D11-1006
0
method_citation
['method_citation']
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W99-0613_aakansha.csv
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="30">Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'"]
'137'
['137']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="12">2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="4">The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="12">But we will show that the use of unlabeled data can drastically reduce the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
    <S sid="237" ssid="4">The numbers falling into the location, person, organization categories were 186, 289 and 402 respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'", "'237'"]
'236'
'237'
['236', '237']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
    <S sid="10" ssid="4">The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'10'"]
'9'
'10'
['9', '10']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S sid="139" ssid="6">This section describes AdaBoost, which is the basis for the CoBoost algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'39'"]
'137'
'39'
['137', '39']
parsed_discourse_facet ['method_citation']
<S sid="26" ssid="20">We present two algorithms.</S>
    <S sid="27" ssid="21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'27'"]
'26'
'27'
['26', '27']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="12">But we will show that the use of unlabeled data can drastically reduce the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="2">Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S>
    <S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'9'"]
'8'
'9'
['8', '9']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'0'", "'179'"]
'137'
'0'
'179'
['137', '0', '179']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="33">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'33'", "'179'"]
'137'
'33'
'179'
['137', '33', '179']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S><S ssid="1" sid="33">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'0'", "'33'"]
'137'
'0'
'33'
['137', '0', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="79">2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'79'", "'0'"]
'137'
'79'
'0'
['137', '79', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'137'", "'0'"]
'179'
'137'
'0'
['179', '137', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="224">The model was parameterized such that the joint probability of a (label, feature-set) pair P(yi, xi) is written as The model assumes that (y, x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'224'"]
'137'
'179'
'224'
['137', '179', '224']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="222">The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'222'", "'179'"]
'137'
'222'
'179'
['137', '222', '179']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']



W99-0613
W03-1509
0
method_citation
['method_citation']



W99-0613
W07-1712
0
method_citation
['aim_citation', 'method_citation']
parsing: input/ref/Task1/P08-1102_aakansha.csv
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="14">As predications generated from such templates depend on the current character, we name these templates lexical-target.</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="121" ssid="32">Among other features, the 4-gram POS LM plays the most important role, removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'121'"]
'121'
['121']
parsed_discourse_facet ['method_citation']
<S sid="37" ssid="9">C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'"]
'37'
['37']
parsed_discourse_facet ['method_citation']
<S sid="73" ssid="24">For instance, if the word w appears N times in training corpus and is labelled as POS t for n times, the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'73'"]
'73'
['73']
parsed_discourse_facet ['method_citation']
<S sid="46" ssid="18">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) &#8712; X &#215; Y to a feature vector 4)(x, y) &#8712; Rd, and a parameter vector &#945;&#65533; &#8712; Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'"]
'46'
['46']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'21'"]
'1'
'0'
'21'
['1', '0', '21']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'96'", "'60'"]
'0'
'96'
'60'
['0', '96', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'60'", "'21'"]
'0'
'60'
'21'
['0', '60', '21']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'21'"]
'0'
'1'
'21'
['0', '1', '21']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'3'", "'60'"]
'21'
'3'
'60'
['21', '3', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'60'"]
'21'
'0'
'60'
['21', '0', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="64">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'64'"]
'0'
'21'
'64'
['0', '21', '64']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'60'"]
'0'
'21'
'60'
['0', '21', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="24">It is a better idea to perform segmentation and POS tagging jointly in a uniform framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'24'"]
'1'
'0'
'24'
['1', '0', '24']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'96'"]
'0'
'1'
'96'
['0', '1', '96']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'21'"]
'0'
'1'
'21'
['0', '1', '21']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S ssid="1" sid="72">Suppose the conditional probability Pr(t|w) describes the probability that the word w is labelled as the POS t, while Pr(w|t) describes the probability that the POS t generates the word w, then P(T|W) can be approximated by: Pr(w|t) and Pr(t|w) can be easily acquired by Maximum Likelihood Estimates (MLE) over the corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'96'", "'72'"]
'0'
'96'
'72'
['0', '96', '72']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'96'", "'60'"]
'0'
'96'
'60'
['0', '96', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="2">With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'2'"]
'0'
'1'
'2'
['0', '1', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S><S ssid="1" sid="64">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'60'", "'64'"]
'0'
'60'
'64'
['0', '60', '64']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="2">With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'2'"]
'1'
'0'
'2'
['1', '0', '2']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/J01-2004_aakansha.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'"]
'372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="17" ssid="5">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



["'302'"]
'302'
['302']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="37">This underspecification of the nonterminal predictions (e.g., VP-VBD in the example in Figure 2, as opposed to NP), allows lexical items to become part of the left context, and so be used to condition production probabilities, even the production probabilities of constituents that dominate them in the unfactored tree.</S>
    <S sid="80" ssid="38">It also brings words further downstream into the look-ahead at the point of specification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'", "'80'"]
'79'
'80'
['79', '80']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="42">Our approach is found to yield very accurate parses efficiently, and, in addition, to lend itself straightforwardly to estimating word probabilities on-line, that is, in a single pass from left to right.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="291" ssid="47">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'291'"]
'291'
['291']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['method_citation']
<S sid="209" ssid="113">This parser is essentially a stochastic version of the top-down parser described in Aho, Sethi, and Ullman (1986).</S>
    <S sid="210" ssid="114">It uses a PCFG with a conditional probability model of the sort defined in the previous section.</S>
original cit marker offset is 0
new cit marker offset is 0



["'209'", "'210'"]
'209'
'210'
['209', '210']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'"]
'372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="8">Two features of our top-down parsing approach will emerge as key to its success.</S>
    <S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S><S sid="32" ssid="20">A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'21'", "'32'"]
'20'
'21'
'32'
['20', '21', '32']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="21">Because the rooted partial derivation is fully connected, all of the conditioning information that might be extracted from the top-down left context has already been specified, and a conditional probability model built on this information will not impose any additional burden on the search.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'"]
'33'
['33']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="17">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'0'", "'3'"]
'17'
'0'
'3'
['17', '0', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="4">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S><S ssid="1" sid="10">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'10'"]
'0'
'4'
'10'
['0', '4', '10']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="137">Here we will present a parser that uses simple search heuristics of this sort without DP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'137'"]
'3'
'9'
'137'
['3', '9', '137']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="223">We implement this as a beam search.</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'223'", "'100'"]
'0'
'223'
'100'
['0', '223', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'100'"]
'3'
'9'
'100'
['3', '9', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'100'"]
'3'
'9'
'100'
['3', '9', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'100'", "'3'"]
'0'
'100'
'3'
['0', '100', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="4">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S><S ssid="1" sid="10">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'10'"]
'0'
'4'
'10'
['0', '4', '10']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']



J01-2004
P08-1013
0
method_citation
['method_citation']
parsing: input/ref/Task1/D10-1044_aakansha.csv
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S>
    <S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'96'"]
'95'
'96'
['95', '96']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'"]
'75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="28">For comparison to information-retrieval inspired baselines, eg (L&#168;u et al., 2007), we select sentences from OUT using language model perplexities from IN.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'"]
'22'
['22']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="23">The 2nd block contains the IR system, which was tuned by selecting text in multiples of the size of the EMEA training corpus, according to dev set performance.</S>
    <S sid="120" ssid="24">This significantly underperforms log-linear combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'", "'120'"]
'119'
'120'
['119', '120']
parsed_discourse_facet ['result_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
    <S sid="24" ssid="21">Sentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="4">We focus here instead on adapting the two most important features: the language model (LM), which estimates the probability p(wIh) of a target word w following an ngram h; and the translation models (TM) p(slt) and p(t1s), which give the probability of source phrase s translating to target phrase t, and vice versa.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D10-1044.csv
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="62">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'37'", "'62'"]
'1'
'37'
'62'
['1', '37', '62']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="120">This significantly underperforms log-linear combination.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="3">We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'", "'37'", "'3'"]
'120'
'37'
'3'
['120', '37', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="68">First, we learn weights on individual phrase pairs rather than sentences.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'68'", "'71'"]
'37'
'68'
'71'
['37', '68', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="3">We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'3'", "'37'"]
'1'
'3'
'37'
['1', '3', '37']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="28">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="120">This significantly underperforms log-linear combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'", "'37'", "'120'"]
'28'
'37'
'120'
['28', '37', '120']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="22">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S><S ssid="1" sid="146">The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair’s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'22'", "'146'"]
'37'
'22'
'146'
['37', '22', '146']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="146">The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair’s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'146'"]
'37'
'1'
'146'
['37', '1', '146']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'37'", "'71'"]
'1'
'37'
'71'
['1', '37', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="148">Mixing, smoothing, and instance-feature weights are learned at the same time using an efficient maximum-likelihood procedure that relies on only a small in-domain development corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'148'"]
'37'
'1'
'148'
['37', '1', '148']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="148">Mixing, smoothing, and instance-feature weights are learned at the same time using an efficient maximum-likelihood procedure that relies on only a small in-domain development corpus.</S><S ssid="1" sid="151">In future work we plan to try this approach with more competitive SMT systems, and to extend instance weighting to other standard SMT components such as the LM, lexical phrase weights, and lexicalized distortion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'148'", "'151'"]
'37'
'148'
'151'
['37', '148', '151']
parsed_discourse_facet ['method_citation']



D10-1044
P14-2093
0
method_citation
['method_citation']



D10-1044
P12-1099
0
method_citation
['method_citation']
parsing: input/ref/Task1/A97-1014_sweta.csv
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["168'"]
168'
['168']
parsed_discourse_facet ['method_citation']
<S sid="165" ssid="7">In addition the approach provides empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linea.rizations, and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.</S>
original cit marker offset is 0
new cit marker offset is 0



["165'"]
165'
['165']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="26">This amount of data suffices as training material to reliably assign the grammatical functions if the user determines the elements of a phrase and its type (step 1 of the list above).</S>
original cit marker offset is 0
new cit marker offset is 0



["145'"]
145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.<
original cit marker offset is 0
new cit marker offset is 0



["15'"]
15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="31">Apart from this rather technical problem, two further arguments speak against phrase structure as the structural pivot of the annotation scheme: Finally, the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies, which has proved tedious.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
<S sid="47" ssid="37">Argument structure can be represented in terms of unordered trees (with crossing branches).</S>
original cit marker offset is 0
new cit marker offset is 0



["47'"]
47'
['47']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
 <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["160'"]
160'
['160']
parsed_discourse_facet ['method_citation']
<S sid="166" ssid="8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
<S sid="166" ssid="8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="24">Sentences annotated in previous steps are used as training material for further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
  <S sid="71" ssid="16">However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["71'"]
71'
['71']
parsed_discourse_facet ['method_citation']
  <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A97-1014.csv
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'0'"]
'100'
'12'
'0'
['100', '12', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'0'"]
'100'
'12'
'0'
['100', '12', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="47">Argument structure can be represented in terms of unordered trees (with crossing branches).</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'47'", "'12'"]
'100'
'47'
'12'
['100', '47', '12']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="74">During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'74'"]
'100'
'12'
'74'
['100', '12', '74']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="47">Argument structure can be represented in terms of unordered trees (with crossing branches).</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'0'", "'100'"]
'47'
'0'
'100'
['47', '0', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S ssid="1" sid="74">During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'4'", "'74'"]
'100'
'4'
'74'
['100', '4', '74']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="114">Fig.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'114'", "'12'"]
'100'
'114'
'12'
['100', '114', '12']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="14">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'0'", "'14'"]
'4'
'0'
'14'
['4', '0', '14']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'0'", "'4'"]
'100'
'0'
'4'
['100', '0', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="21">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'0'", "'21'"]
'100'
'0'
'21'
['100', '0', '21']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S ssid="1" sid="14">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'4'", "'14'"]
'12'
'4'
'14'
['12', '4', '14']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="114">Fig.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'114'"]
'100'
'12'
'114'
['100', '12', '114']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="170">Our annotation tool supplies efficient manipulation and immediate visualization of argument structures.</S><S ssid="1" sid="162">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'170'", "'162'"]
'100'
'170'
'162'
['100', '170', '162']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="105">In addition, a. number of clear-cut NP cornponents can be defined outside that, juxtapositional kernel: pre- and postnominal genitives (GL, GR), relative clauses (RC), clausal and sentential complements (OC).</S><S ssid="1" sid="21">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'105'", "'21'"]
'100'
'105'
'21'
['100', '105', '21']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
parsing: input/ref/Task1/P11-1060_aakansha.csv
<S sid="11" ssid="7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'"]
'2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="5">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'"]
'112'
['112']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="11">Although a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">However, we still model the logical form (now as a latent variable) to capture the complexities of language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="12">It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="21">The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="88" ssid="64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets, a restrictor A and a nuclear scope B.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'"]
'88'
['88']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'2'"]
'100'
'21'
'2'
['100', '21', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="23">We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'21'", "'23'"]
'2'
'21'
'23'
['2', '21', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'2'"]
'0'
'21'
'2'
['0', '21', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'25'"]
'21'
'0'
'25'
['21', '0', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'106'"]
'21'
'0'
'106'
['21', '0', '106']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'100'"]
'21'
'0'
'100'
['21', '0', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'100'"]
'0'
'21'
'100'
['0', '21', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'100'", "'2'"]
'21'
'100'
'2'
['21', '100', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'25'"]
'21'
'0'
'25'
['21', '0', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="23">We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'23'"]
'0'
'21'
'23'
['0', '21', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'25'"]
'21'
'0'
'25'
['21', '0', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'25'"]
'0'
'21'
'25'
['0', '21', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="174">598</S><S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'174'", "'106'"]
'21'
'174'
'106'
['21', '174', '106']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'21'", "'100'"]
'106'
'21'
'100'
['106', '21', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="23">We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'0'", "'23'"]
'100'
'0'
'23'
['100', '0', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'106'"]
'21'
'0'
'106'
['21', '0', '106']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'0'"]
'100'
'21'
'0'
['100', '21', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'100'"]
'0'
'21'
'100'
['0', '21', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="12">We represent logical forms z as labeled trees, induced automatically from (x, y) pairs.</S><S ssid="1" sid="17">The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'12'", "'17'"]
'21'
'12'
'17'
['21', '12', '17']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



P11-1060
P13-1092
0
method_citation
['method_citation']



P11-1060
P12-1045
0
method_citation
['method_citation']
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P08-1102_sweta.csv
<S sid="21" ssid="17">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="24">A subsequence of boundary-POS labelling result indicates a word with POS t only if the boundary tag sequence composed of its boundary part conforms to s or bm*e style, and all POS tags in its POS part equal to t. For example, a tag sequence b NN m NN e NN represents a threecharacter word with POS tag NN.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="43" ssid="15">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["43'"]
43'
['43']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["92'"]
92'
['92']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="5">To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&amp;T).</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="5">In following subsections, we describe the feature templates and the perceptron training algorithm.</S>
    <S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["33'", "'34'"]
33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="4">By maintaining a stack of size N at each position i of the sequence, we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S>
original cit marker offset is 0
new cit marker offset is 0



["79'"]
79'
['79']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="7">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["96'"]
96'
['96']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="16" ssid="12">Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="7">To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'21'"]
'1'
'0'
'21'
['1', '0', '21']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'96'", "'60'"]
'0'
'96'
'60'
['0', '96', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'60'", "'21'"]
'0'
'60'
'21'
['0', '60', '21']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'21'"]
'0'
'1'
'21'
['0', '1', '21']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'3'", "'60'"]
'21'
'3'
'60'
['21', '3', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'60'"]
'21'
'0'
'60'
['21', '0', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="64">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'64'"]
'0'
'21'
'64'
['0', '21', '64']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'60'"]
'0'
'21'
'60'
['0', '21', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="24">It is a better idea to perform segmentation and POS tagging jointly in a uniform framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'24'"]
'1'
'0'
'24'
['1', '0', '24']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'96'"]
'0'
'1'
'96'
['0', '1', '96']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'21'"]
'0'
'1'
'21'
['0', '1', '21']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S ssid="1" sid="72">Suppose the conditional probability Pr(t|w) describes the probability that the word w is labelled as the POS t, while Pr(w|t) describes the probability that the POS t generates the word w, then P(T|W) can be approximated by: Pr(w|t) and Pr(t|w) can be easily acquired by Maximum Likelihood Estimates (MLE) over the corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'96'", "'72'"]
'0'
'96'
'72'
['0', '96', '72']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'96'", "'60'"]
'0'
'96'
'60'
['0', '96', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="2">With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'2'"]
'0'
'1'
'2'
['0', '1', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S><S ssid="1" sid="64">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'60'", "'64'"]
'0'
'60'
'64'
['0', '60', '64']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="2">With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'2'"]
'1'
'0'
'2'
['1', '0', '2']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/D09-1092_sweta.csv
 <S sid="32" ssid="8">Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="5">Additionally, PLTM assumes that each &#8220;topic&#8221; consists of a set of discrete distributions over words&#8212;one for each language l = 1, ... , L. In other words, rather than using a single set of topics &#934; = {&#966;1, ... , &#966;T}, as in LDA, there are L sets of language-specific topics, &#934;1, ... , &#934;L, each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter &#946;l.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



["138'"]
138'
['138']
parsed_discourse_facet ['method_citation']
 <S sid="196" ssid="5">When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>
original cit marker offset is 0
new cit marker offset is 0



["196'"]
196'
['196']
parsed_discourse_facet ['method_citation']
<S sid="111" ssid="60">In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts &#8211; i.e., we put each of these documents in a single-document tuple.</S>
original cit marker offset is 0
new cit marker offset is 0



["111'"]
111'
['111']
parsed_discourse_facet ['method_citation']
<S sid="128" ssid="77">Although the PLTM is clearly not a substitute for a machine translation system&#8212;it has no way to represent syntax or even multi-word phrases&#8212;it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.</S>
original cit marker offset is 0
new cit marker offset is 0



["128'"]
128'
['128']
parsed_discourse_facet ['method_citation']
<S sid="122" ssid="71">Divergence drops significantly when the proportion of &#8220;glue&#8221; tuples increases from 0.01 to 0.25.</S>
original cit marker offset is 0
new cit marker offset is 0



["122'"]
122'
['122']
parsed_discourse_facet ['method_citation']
 <S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
 <S sid="110" ssid="59">Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S>
original cit marker offset is 0
new cit marker offset is 0



["110'"]
110'
['110']
parsed_discourse_facet ['method_citation']
<S sid="30" ssid="6">However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["30'"]
30'
['30']
parsed_discourse_facet ['method_citation']
    <S sid="146" ssid="95">In addition to enhancing lexicons by aligning topic-specific vocabulary, PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["146'"]
146'
['146']
parsed_discourse_facet ['method_citation']
<S sid="77" ssid="26">Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.</
original cit marker offset is 0
new cit marker offset is 0



["77'"]
77'
['77']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="105">We use both Jensen-Shannon divergence and cosine distance.</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="7">They also provide little analysis of the differences between polylingual and single-language topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>
original cit marker offset is 0
new cit marker offset is 0



["19'"]
19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="131" ssid="80">We evaluate sets of high-probability words in each topic and multilingual &#8220;synsets&#8221; by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["131'"]
131'
['131']
parsed_discourse_facet ['method_citation']
<S sid="196" ssid="5">When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>
original cit marker offset is 0
new cit marker offset is 0



["196'"]
196'
['196']
parsed_discourse_facet ['method_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["192'"]
192'
['192']
parsed_discourse_facet ['method_citation']
<S sid="195" ssid="4">Additionally, PLTM can support the creation of bilingual lexica for low resource language pairs, providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["195'"]
195'
['195']
parsed_discourse_facet ['method_citation']
<S sid="6" ssid="2">Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["6'"]
6'
['6']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'0'"]
'3'
'192'
'0'
['3', '192', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'0'"]
'3'
'192'
'0'
['3', '192', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'185'"]
'3'
'192'
'185'
['3', '192', '185']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'9'"]
'0'
'192'
'9'
['0', '192', '9']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'185'", "'3'"]
'192'
'185'
'3'
['192', '185', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'185'", "'3'"]
'192'
'185'
'3'
['192', '185', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'3'"]
'0'
'192'
'3'
['0', '192', '3']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="148">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'3'", "'148'"]
'192'
'3'
'148'
['192', '3', '148']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'9'", "'192'"]
'0'
'9'
'192'
['0', '9', '192']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'3'", "'185'"]
'192'
'3'
'185'
['192', '3', '185']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'192'"]
'0'
'3'
'192'
['0', '3', '192']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'9'"]
'0'
'192'
'9'
['0', '192', '9']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'185'", "'9'", "'192'"]
'185'
'9'
'192'
['185', '9', '192']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="153">In order to make the task more difficult, we train a relatively coarse-grained PLTM with 50 topics on the training set.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="19">We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>
original cit marker offset is 0
new cit marker offset is 0



["'153'", "'192'", "'19'"]
'153'
'192'
'19'
['153', '192', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'0'"]
'3'
'192'
'0'
['3', '192', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'3'", "'0'"]
'192'
'3'
'0'
['192', '3', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'0'", "'3'"]
'192'
'0'
'3'
['192', '0', '3']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'3'"]
'0'
'192'
'3'
['0', '192', '3']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'9'", "'3'"]
'192'
'9'
'3'
['192', '9', '3']
parsed_discourse_facet ['aim_citation']



D09-1092
D10-1025
0
method_citation
['method_citation']



D09-1092
W11-2133
0
method_citation
['aim_citation']
parsing: input/ref/Task1/D09-1092_swastika.csv
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="114" ssid="63">Ideally, the &#8220;glue&#8221; documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.</S>
original cit marker offset is 0
new cit marker offset is 0



['114']
114
['114']
parsed_discourse_facet ['aim_citation']
<S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



['138']
138
['138']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



['55']
55
['55']
parsed_discourse_facet ['method_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['192']
192
['192']
parsed_discourse_facet ['result_citation']
<S sid="119" ssid="68">The lower the divergence, the more similar the distributions are to each other.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['result_citation']
<S sid="148" ssid="97">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="163" ssid="112">Performance continues to improve with longer documents, most likely due to better topic inference.</S>
original cit marker offset is 0
new cit marker offset is 0



['163']
163
['163']
parsed_discourse_facet ['result_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="148" ssid="97">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['192']
192
['192']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['method_citation']
    <S sid="10" ssid="6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'0'"]
'3'
'192'
'0'
['3', '192', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'0'"]
'3'
'192'
'0'
['3', '192', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'185'"]
'3'
'192'
'185'
['3', '192', '185']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'9'"]
'0'
'192'
'9'
['0', '192', '9']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'185'", "'3'"]
'192'
'185'
'3'
['192', '185', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'185'", "'3'"]
'192'
'185'
'3'
['192', '185', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'3'"]
'0'
'192'
'3'
['0', '192', '3']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="148">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'3'", "'148'"]
'192'
'3'
'148'
['192', '3', '148']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'9'", "'192'"]
'0'
'9'
'192'
['0', '9', '192']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'3'", "'185'"]
'192'
'3'
'185'
['192', '3', '185']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'192'"]
'0'
'3'
'192'
['0', '3', '192']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'9'"]
'0'
'192'
'9'
['0', '192', '9']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'185'", "'9'", "'192'"]
'185'
'9'
'192'
['185', '9', '192']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="153">In order to make the task more difficult, we train a relatively coarse-grained PLTM with 50 topics on the training set.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="19">We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>
original cit marker offset is 0
new cit marker offset is 0



["'153'", "'192'", "'19'"]
'153'
'192'
'19'
['153', '192', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'0'"]
'3'
'192'
'0'
['3', '192', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'3'", "'0'"]
'192'
'3'
'0'
['192', '3', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'0'", "'3'"]
'192'
'0'
'3'
['192', '0', '3']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'3'"]
'0'
'192'
'3'
['0', '192', '3']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'9'", "'3'"]
'192'
'9'
'3'
['192', '9', '3']
parsed_discourse_facet ['aim_citation']



D09-1092
W11-2133
0
result_citation
['aim_citation']
parsing: input/ref/Task1/J01-2004_sweta.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["372'"]
372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="28">The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.</S>
    <S sid="41" ssid="29">There will also be a brief review of previous work using syntactic information for language modeling, before we introduce our model in Section 4.</S>
    <S sid="42" ssid="30">Three parse trees: (a) a complete parse tree; (b) a complete parse tree with an explicit stop symbol; and (c) a partial parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["40'", "'41'", "'42'"]
40'
'41'
'42'
['40', '41', '42']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="13">A parser that is not left to right, but which has rooted derivations, e.g., a headfirst parser, will be able to calculate generative joint probabilities for entire strings; however, it will not be able to calculate probabilities for each word conditioned on previously generated words, unless each derivation generates the words in the string in exactly the same order.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="364" ssid="120">We follow Chelba (2000) in dealing with this problem: for parsing purposes, we use the Penn Treebank tokenization; for interpolation with the provided trigram model, and for evaluation, the lattice tokenization is used.</S>
original cit marker offset is 0
new cit marker offset is 0



["364'"]
364'
['364']
parsed_discourse_facet ['method_citation']
<S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



["302'"]
302'
['302']
parsed_discourse_facet ['method_citation']
 <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="231" ssid="135">Since we do not know the POS for the word, we must sum the LAP for all POS For a PCFG G, a stack S = Ao An$ (which we will write AN and a look-ahead terminal item wi, we define the look-ahead probability as follows: We recursively estimate this with two empirically observed conditional probabilities for every nonterminal A,: 13(A, w,a) and P(A, c).</S>
original cit marker offset is 0
new cit marker offset is 0



["231'"]
231'
['231']
parsed_discourse_facet ['method_citation']
<S sid="297" ssid="53">The differences between a k-best and a beam-search parser (not to mention the use of dynamic programming) make a running time difference unsurprising.</S>
original cit marker offset is 0
new cit marker offset is 0



["297'"]
297'
['297']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="37">Statistically based heuristic best-first or beam-search strategies (Caraballo and Charniak 1998; Charniak, Goldwater, and Johnson 1998; Goodman 1997) have yielded an enormous improvement in the quality and speed of parsers, even without any guarantee that the parse returned is, in fact, that with the maximum likelihood for the probability model.</S>
original cit marker offset is 0
new cit marker offset is 0



["133'"]
133'
['133']
parsed_discourse_facet ['method_citation']
<S sid="291" ssid="47">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["291'"]
291'
['291']
parsed_discourse_facet ['method_citation']
<S sid="355" ssid="111">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.</S>
original cit marker offset is 0
new cit marker offset is 0



["355'"]
355'
['355']
parsed_discourse_facet ['method_citation']
<S sid="59" ssid="17">A PCFG is a CFG with a probability assigned to each rule; specifically, each righthand side has a probability given the left-hand side of the rule.</S>
original cit marker offset is 0
new cit marker offset is 0



["59'"]
59'
['59']
parsed_discourse_facet ['method_citation']
<S sid="100" ssid="4">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="12">Another approach that uses syntactic structure for language modeling has been to use a shift-reduce parser to &amp;quot;surface&amp;quot; c-commanding phrasal headwords or part-of-speech (POS) tags from arbitrarily far back in the prefix string, for use in a trigram-like model.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="17">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'0'", "'3'"]
'17'
'0'
'3'
['17', '0', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="4">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S><S ssid="1" sid="10">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'10'"]
'0'
'4'
'10'
['0', '4', '10']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="137">Here we will present a parser that uses simple search heuristics of this sort without DP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'137'"]
'3'
'9'
'137'
['3', '9', '137']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="223">We implement this as a beam search.</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'223'", "'100'"]
'0'
'223'
'100'
['0', '223', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'100'"]
'3'
'9'
'100'
['3', '9', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'100'"]
'3'
'9'
'100'
['3', '9', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'100'", "'3'"]
'0'
'100'
'3'
['0', '100', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="4">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S><S ssid="1" sid="10">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'10'"]
'0'
'4'
'10'
['0', '4', '10']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']



J01-2004
W10-2009
0
method_citation
['method_citation']
parsing: input/ref/Task1/W11-2123_swastika.csv
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



['52']
52
['52']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="177" ssid="49">However, TRIE partitions storage by n-gram length, so walking the trie reads N disjoint pages.</S>
original cit marker offset is 0
new cit marker offset is 0



['177']
177
['177']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="278" ssid="5">We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns.</S>
original cit marker offset is 0
new cit marker offset is 0



['278']
278
['278']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/W11-2123.csv
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="32">Linear probing places at most one entry in each bucket.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'32'"]
'5'
'0'
'32'
['5', '0', '32']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="187">The data structure was populated with 64-bit integers sampled uniformly without replacement.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'187'"]
'5'
'0'
'187'
['5', '0', '187']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'109'"]
'0'
'25'
'109'
['0', '25', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'5'"]
'1'
'0'
'5'
['1', '0', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'109'"]
'5'
'0'
'109'
['5', '0', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S><S ssid="1" sid="7">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'25'", "'7'"]
'5'
'25'
'7'
['5', '25', '7']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'25'"]
'5'
'1'
'25'
['5', '1', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="7">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'0'", "'7'"]
'3'
'0'
'7'
['3', '0', '7']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="7">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'7'"]
'5'
'1'
'7'
['5', '1', '7']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="32">Linear probing places at most one entry in each bucket.</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'32'", "'109'"]
'5'
'32'
'109'
['5', '32', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'5'", "'1'"]
'0'
'5'
'1'
['0', '5', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'25'"]
'5'
'1'
'25'
['5', '1', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'109'", "'5'"]
'1'
'109'
'5'
['1', '109', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="3">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'5'", "'3'"]
'0'
'5'
'3'
['0', '5', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'0'", "'5'"]
'109'
'0'
'5'
['109', '0', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="102">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie), allocating memory all at once (eliminating the need for full pointers), and being easy to compile.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="200">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'", "'0'", "'200'"]
'102'
'0'
'200'
['102', '0', '200']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="9">Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf , returning where the probability p(wn|wn−1 f ) and backoff penalties b(wn−1 i ) are given by an already-estimated model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'9'"]
'5'
'0'
'9'
['5', '0', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S><S ssid="1" sid="3">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'3'", "'5'"]
'109'
'3'
'5'
['109', '3', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'1'"]
'5'
'0'
'1'
['5', '0', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'109'"]
'5'
'0'
'109'
['5', '0', '109']
parsed_discourse_facet ['method_citation']



W11-2123
W12-3160
0
aim_citation
['method_citation']



W11-2123
W12-3131
0
aim_citation
['method_citation']



W11-2123
P12-1002
0
method_citation
['method_citation']



W11-2123
W11-2139
0
aim_citation
['method_citation']
parsing: input/ref/Task1/P08-1043_aakansha.csv
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="69" ssid="1">We represent all morphological analyses of a given utterance using a lattice structure.</S>
    <S sid="70" ssid="2">Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'70'"]
'69'
'70'
['69', '70']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="17">The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty, 2006) and (Cohen and Smith, 2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="11">Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
    <S sid="134" ssid="12">Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training 	similar to (Cohen and Smith, 2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'134'"]
'133'
'134'
['133', '134']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="17">The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="155" ssid="33">Evaluation We use 8 different measures to evaluate the performance of our system on the joint disambiguation task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'4'", "'0'"]
'19'
'4'
'0'
['19', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="186">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'186'", "'19'", "'4'"]
'186'
'19'
'4'
['186', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'0'", "'19'"]
'4'
'0'
'19'
['4', '0', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="186">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'186'"]
'0'
'19'
'186'
['0', '19', '186']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'19'"]
'0'
'4'
'19'
['0', '4', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'4'"]
'0'
'19'
'4'
['0', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="116">We simulate lexical constraints by using an external lexical resource against which we verify whether OOV segments are in fact valid Hebrew lexemes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'116'"]
'0'
'19'
'116'
['0', '19', '116']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'19'"]
'0'
'4'
'19'
['0', '4', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'4'"]
'0'
'19'
'4'
['0', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="117">This heuristics is used to prune all segmentation possibilities involving “lexically improper” segments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'117'"]
'0'
'4'
'117'
['0', '4', '117']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'4'"]
'0'
'19'
'4'
['0', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="188">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'188'"]
'0'
'4'
'188'
['0', '4', '188']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="186">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'186'"]
'0'
'19'
'186'
['0', '19', '186']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="136">Lexicon and OOV Handling Our data-driven morphological-analyzer proposes analyses for unknown tokens as described in Section 5.</S><S ssid="1" sid="133">Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'136'", "'133'"]
'0'
'136'
'133'
['0', '136', '133']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="69">We represent all morphological analyses of a given utterance using a lattice structure.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'69'", "'19'"]
'0'
'69'
'19'
['0', '69', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="135">We construct a mapping from all the space-delimited tokens seen in the training sentences to their corresponding analyses.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'135'", "'19'"]
'4'
'135'
'19'
['4', '135', '19']
parsed_discourse_facet ['method_citation']



P08-1043
P11-1089
0
method_citation
['method_citation']



P08-1043
P10-1074
0
method_citation
['method_citation']



P08-1043
P11-1141
0
method_citation
['method_citation']



P08-1043
W10-1404
0
method_citation
['method_citation']



P08-1043
D12-1046
0
method_citation
['method_citation']
parsing: input/ref/Task1/D10-1044_swastika.csv
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
<S sid="71" ssid="8">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c&#955;(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



['71']
71
['71']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
<S sid="71" ssid="8">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c&#955;(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



['71']
71
['71']
parsed_discourse_facet ['result_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="9">An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (Och, 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



['45']
45
['45']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



['75']
75
['75']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
  <S sid="42" ssid="6">The natural baseline approach is to concatenate data from IN and OUT.</S>
original cit marker offset is 0
new cit marker offset is 0



['42']
42
['42']
parsed_discourse_facet ['aim_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/D10-1044.csv
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="62">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'37'", "'62'"]
'1'
'37'
'62'
['1', '37', '62']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="120">This significantly underperforms log-linear combination.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="3">We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'", "'37'", "'3'"]
'120'
'37'
'3'
['120', '37', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="68">First, we learn weights on individual phrase pairs rather than sentences.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'68'", "'71'"]
'37'
'68'
'71'
['37', '68', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="3">We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'3'", "'37'"]
'1'
'3'
'37'
['1', '3', '37']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="28">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="120">This significantly underperforms log-linear combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'", "'37'", "'120'"]
'28'
'37'
'120'
['28', '37', '120']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="22">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S><S ssid="1" sid="146">The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair’s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'22'", "'146'"]
'37'
'22'
'146'
['37', '22', '146']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="146">The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair’s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'146'"]
'37'
'1'
'146'
['37', '1', '146']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'37'", "'71'"]
'1'
'37'
'71'
['1', '37', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="148">Mixing, smoothing, and instance-feature weights are learned at the same time using an efficient maximum-likelihood procedure that relies on only a small in-domain development corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'148'"]
'37'
'1'
'148'
['37', '1', '148']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="148">Mixing, smoothing, and instance-feature weights are learned at the same time using an efficient maximum-likelihood procedure that relies on only a small in-domain development corpus.</S><S ssid="1" sid="151">In future work we plan to try this approach with more competitive SMT systems, and to extend instance weighting to other standard SMT components such as the LM, lexical phrase weights, and lexicalized distortion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'148'", "'151'"]
'37'
'148'
'151'
['37', '148', '151']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



D10-1044
P14-2093
0
method_citation
['method_citation']



D10-1044
P12-1099
0
method_citation
['method_citation']
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W99-0613_swastika.csv
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
<S sid="159" ssid="26">To prevent this we &amp;quot;smooth&amp;quot; the confidence by adding a small value, e, to both W+ and W_, giving at = Plugging the value of at from Equ.</S>
original cit marker offset is 0
new cit marker offset is 0



['159']
159
['159']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S>
original cit marker offset is 0
new cit marker offset is 0



['137']
137
['137']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['method_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
 <S sid="250" ssid="1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S>
original cit marker offset is 0
new cit marker offset is 0



['250']
250
['250']
parsed_discourse_facet ['result_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
    <S sid="36" ssid="30">Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.</S>
original cit marker offset is 0
new cit marker offset is 0



['36']
36
['36']
parsed_discourse_facet ['result_citation']
<S sid="29" ssid="23">Unfortunately, Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective function.</S>
original cit marker offset is 0
new cit marker offset is 0



['29']
29
['29']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
    <S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['method_citation']
    <S sid="95" ssid="28">(Specifically, the limit n starts at 5 and increases by 5 at each iteration.)</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['method_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'0'", "'179'"]
'137'
'0'
'179'
['137', '0', '179']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="33">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'33'", "'179'"]
'137'
'33'
'179'
['137', '33', '179']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S><S ssid="1" sid="33">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'0'", "'33'"]
'137'
'0'
'33'
['137', '0', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="79">2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'79'", "'0'"]
'137'
'79'
'0'
['137', '79', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'137'", "'0'"]
'179'
'137'
'0'
['179', '137', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="224">The model was parameterized such that the joint probability of a (label, feature-set) pair P(yi, xi) is written as The model assumes that (y, x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'224'"]
'137'
'179'
'224'
['137', '179', '224']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="222">The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'222'", "'179'"]
'137'
'222'
'179'
['137', '222', '179']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



W99-0613
W03-1509
0
method_citation
['method_citation']
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W06-2932_swastika.csv
<S sid="86" ssid="8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S>
original cit marker offset is 
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="54" ssid="2">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



['54']
54
['54']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



['12']
12
['12']
parsed_discourse_facet ['method_citation']
    <S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



['41']
41
['41']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="6">Its power lies in the ability to define a rich set of features over parsing decisions, as well as surface level features relative to these decisions.</S>
original cit marker offset is 0
new cit marker offset is 0



['24']
24
['24']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



['12']
12
['12']
parsed_discourse_facet ['method_citation']
<S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



['43']
43
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="41">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm�1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm−1) in the tree y.</S>
original cit marker offset is 
new cit marker offset is 0



["'0'", "'12'", "'41'"]
'0'
'12'
'41'
['0', '12', '41']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="38">Such a model could easily be trained using the provided training data for each language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'38'"]
'0'
'12'
'38'
['0', '12', '38']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="106">First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'106'"]
'0'
'12'
'106'
['0', '12', '106']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="106">First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'106'"]
'0'
'12'
'106'
['0', '12', '106']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="41">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm�1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm−1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'41'"]
'0'
'12'
'41'
['0', '12', '41']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'1'"]
'0'
'12'
'1'
['0', '12', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'1'"]
'0'
'12'
'1'
['0', '12', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="41">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm�1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm−1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'0'", "'41'"]
'12'
'0'
'41'
['12', '0', '41']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="16">A dependency graph is represented by a set of ordered pairs (i, j) E y in which xj is a dependent and xi is the corresponding head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'16'"]
'0'
'12'
'16'
['0', '12', '16']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="16">A dependency graph is represented by a set of ordered pairs (i, j) E y in which xj is a dependent and xi is the corresponding head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'16'"]
'0'
'12'
'16'
['0', '12', '16']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'1'"]
'0'
'12'
'1'
['0', '12', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="54">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'54'"]
'0'
'12'
'54'
['0', '12', '54']
parsed_discourse_facet ['aim_citation', 'result_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'3'"]
'0'
'12'
'3'
['0', '12', '3']
parsed_discourse_facet ['method_citation']



W06-2932
I08-1012
0
method_citation
['method_citation']
parsing: input/ref/Task1/P08-1102_swastika.csv
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="10">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="8">All feature templates and their instances are shown in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



['36']
36
['36']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
<S sid="97" ssid="8">Figure 3 shows their learning curves depicting the F-measure on the development set after 1 to 10 training iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



['97']
97
['97']
parsed_discourse_facet ['result_citation']
<S sid="91" ssid="2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['aim_citation']
<S sid="16" ssid="12">Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



['34']
34
['34']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'21'"]
'1'
'0'
'21'
['1', '0', '21']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'96'", "'60'"]
'0'
'96'
'60'
['0', '96', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'60'", "'21'"]
'0'
'60'
'21'
['0', '60', '21']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'21'"]
'0'
'1'
'21'
['0', '1', '21']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'3'", "'60'"]
'21'
'3'
'60'
['21', '3', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'60'"]
'21'
'0'
'60'
['21', '0', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="64">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'64'"]
'0'
'21'
'64'
['0', '21', '64']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'60'"]
'0'
'21'
'60'
['0', '21', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="24">It is a better idea to perform segmentation and POS tagging jointly in a uniform framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'24'"]
'1'
'0'
'24'
['1', '0', '24']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'96'"]
'0'
'1'
'96'
['0', '1', '96']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'21'"]
'0'
'1'
'21'
['0', '1', '21']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S ssid="1" sid="72">Suppose the conditional probability Pr(t|w) describes the probability that the word w is labelled as the POS t, while Pr(w|t) describes the probability that the POS t generates the word w, then P(T|W) can be approximated by: Pr(w|t) and Pr(t|w) can be easily acquired by Maximum Likelihood Estimates (MLE) over the corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'96'", "'72'"]
'0'
'96'
'72'
['0', '96', '72']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'96'", "'60'"]
'0'
'96'
'60'
['0', '96', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="2">With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'2'"]
'0'
'1'
'2'
['0', '1', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S><S ssid="1" sid="64">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'60'", "'64'"]
'0'
'60'
'64'
['0', '60', '64']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="2">With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'2'"]
'1'
'0'
'2'
['1', '0', '2']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/J01-2004_swastika.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="3">In the past few years, however, some improvements have been made over these language models through the use of statistical methods of natural language processing, and the development of innovative, linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
<S sid="215" ssid="119">The first word in the string remaining to be parsed, w1, we will call the look-ahead word.</S>
original cit marker offset is 0
new cit marker offset is 0



['215']
215
['215']
parsed_discourse_facet ['method_citation']
    <S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



['302']
302
['302']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="100" ssid="4">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['method_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="17">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'0'", "'3'"]
'17'
'0'
'3'
['17', '0', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="4">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S><S ssid="1" sid="10">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'10'"]
'0'
'4'
'10'
['0', '4', '10']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="137">Here we will present a parser that uses simple search heuristics of this sort without DP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'137'"]
'3'
'9'
'137'
['3', '9', '137']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="223">We implement this as a beam search.</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'223'", "'100'"]
'0'
'223'
'100'
['0', '223', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'100'"]
'3'
'9'
'100'
['3', '9', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'100'"]
'3'
'9'
'100'
['3', '9', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'100'", "'3'"]
'0'
'100'
'3'
['0', '100', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="4">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S><S ssid="1" sid="10">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'10'"]
'0'
'4'
'10'
['0', '4', '10']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']



J01-2004
W10-2009
0
method_citation
['method_citation']
parsing: input/ref/Task1/D09-1092_vardha.csv
<S sid="17" ssid="13">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
 <S sid="20" ssid="16">We also explore how the characteristics of different languages affect topic model performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
    <S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
   <S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'"]
'55'
['55']
parsed_discourse_facet ['method_citation']
 sid="9" ssid="5">In this paper, we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
Error in Reference Offset
<S sid="118" ssid="67">We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
 <S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'"]
'55'
['55']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="51">In contrast, PLTM assigns a significant number of tokens to almost all 800 topics, in very similar proportions in both languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'"]
'102'
['102']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="4">This is unlike LDA, in which each document is assumed to have its own document-specific distribution over topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'"]
'38'
['38']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="105">We use both Jensen-Shannon divergence and cosine distance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'"]
'156'
['156']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="2">Each tuple is a set of documents that are loosely equivalent to each other, but written in different languages, e.g., corresponding Wikipedia articles in French, English and German.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
  <S sid="170" ssid="4">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
    <S sid="29" ssid="5">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
  <S sid="170" ssid="4">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="168" ssid="2">However, the growth of the web, and in particular Wikipedia, has made comparable text corpora &#8211; documents that are topically similar but are not direct translations of one another &#8211; considerably more abundant than true parallel corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'168'"]
'168'
['168']
parsed_discourse_facet ['method_citation']
    <S sid="29" ssid="5">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
 <S sid="110" ssid="59">Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'"]
'110'
['110']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'0'"]
'3'
'192'
'0'
['3', '192', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'0'"]
'3'
'192'
'0'
['3', '192', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'185'"]
'3'
'192'
'185'
['3', '192', '185']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'9'"]
'0'
'192'
'9'
['0', '192', '9']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'185'", "'3'"]
'192'
'185'
'3'
['192', '185', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'185'", "'3'"]
'192'
'185'
'3'
['192', '185', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'3'"]
'0'
'192'
'3'
['0', '192', '3']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="148">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'3'", "'148'"]
'192'
'3'
'148'
['192', '3', '148']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'9'", "'192'"]
'0'
'9'
'192'
['0', '9', '192']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'3'", "'185'"]
'192'
'3'
'185'
['192', '3', '185']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'192'"]
'0'
'3'
'192'
['0', '3', '192']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'9'"]
'0'
'192'
'9'
['0', '192', '9']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'185'", "'9'", "'192'"]
'185'
'9'
'192'
['185', '9', '192']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="153">In order to make the task more difficult, we train a relatively coarse-grained PLTM with 50 topics on the training set.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="19">We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>
original cit marker offset is 0
new cit marker offset is 0



["'153'", "'192'", "'19'"]
'153'
'192'
'19'
['153', '192', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'0'"]
'3'
'192'
'0'
['3', '192', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'3'", "'0'"]
'192'
'3'
'0'
['192', '3', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'0'", "'3'"]
'192'
'0'
'3'
['192', '0', '3']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'3'"]
'0'
'192'
'3'
['0', '192', '3']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'9'", "'3'"]
'192'
'9'
'3'
['192', '9', '3']
parsed_discourse_facet ['aim_citation']
parsing: input/ref/Task1/W11-2123_aakansha.csv
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="136" ssid="8">We offer a state function s(wn1) = wn&#65533; where substring wn&#65533; is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'"]
'136'
['136']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="3">Queries take the form p(wn|wn&#8722;1 1 ) where wn1 is an n-gram.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="205" ssid="24">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al., 2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'205'"]
'205'
['205']
parsed_discourse_facet ['method_citation']
<S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
<S sid="204" ssid="23">For RandLM, we used the settings in the documentation: 8 bits per value and false positive probability 1 256.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'"]
'204'
['204']
parsed_discourse_facet ['method_citation']
<S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="229" ssid="48">Then we ran binary search to determine the least amount of memory with which it would run.</S>
original cit marker offset is 0
new cit marker offset is 0



["'229'"]
'229'
['229']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="93" ssid="71">The cost of storing these averages, in bits, is Because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.</S>
original cit marker offset is 0
new cit marker offset is 0



["'93'"]
'93'
['93']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W11-2123.csv
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="32">Linear probing places at most one entry in each bucket.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'32'"]
'5'
'0'
'32'
['5', '0', '32']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="187">The data structure was populated with 64-bit integers sampled uniformly without replacement.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'187'"]
'5'
'0'
'187'
['5', '0', '187']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'109'"]
'0'
'25'
'109'
['0', '25', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'5'"]
'1'
'0'
'5'
['1', '0', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'109'"]
'5'
'0'
'109'
['5', '0', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S><S ssid="1" sid="7">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'25'", "'7'"]
'5'
'25'
'7'
['5', '25', '7']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'25'"]
'5'
'1'
'25'
['5', '1', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="7">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'0'", "'7'"]
'3'
'0'
'7'
['3', '0', '7']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="7">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'7'"]
'5'
'1'
'7'
['5', '1', '7']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="32">Linear probing places at most one entry in each bucket.</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'32'", "'109'"]
'5'
'32'
'109'
['5', '32', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'5'", "'1'"]
'0'
'5'
'1'
['0', '5', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'25'"]
'5'
'1'
'25'
['5', '1', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'109'", "'5'"]
'1'
'109'
'5'
['1', '109', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="3">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'5'", "'3'"]
'0'
'5'
'3'
['0', '5', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'0'", "'5'"]
'109'
'0'
'5'
['109', '0', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="102">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie), allocating memory all at once (eliminating the need for full pointers), and being easy to compile.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="200">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'", "'0'", "'200'"]
'102'
'0'
'200'
['102', '0', '200']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="9">Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf , returning where the probability p(wn|wn−1 f ) and backoff penalties b(wn−1 i ) are given by an already-estimated model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'9'"]
'5'
'0'
'9'
['5', '0', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S><S ssid="1" sid="3">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'3'", "'5'"]
'109'
'3'
'5'
['109', '3', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'1'"]
'5'
'0'
'1'
['5', '0', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'109'"]
'5'
'0'
'109'
['5', '0', '109']
parsed_discourse_facet ['method_citation']



W11-2123
W12-3706
0
method_citation
['method_citation']



W11-2123
W12-3160
0
method_citation
['method_citation']



W11-2123
W12-3131
0
method_citation
['method_citation']



W11-2123
W11-2139
0
method_citation
['method_citation']



W11-2123
P13-2073
0
method_citation
['method_citation']
parsing: input/ref/Task1/P87-1015_swastika.csv
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="149" ssid="34">We can obtain a letter equivalent CFL defined by a CFG in which the for each rule as above, we have the production A &#8212;* A1 Anup where tk (up) = cp.</S>
original cit marker offset is 0
new cit marker offset is 0



['149']
149
['149']
parsed_discourse_facet ['method_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
    <S sid="2" ssid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['2']
2
['2']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="222" ssid="28">However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



['222']
222
['222']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['???']
???
['???']
parsed_discourse_facet ['aim_citation']
<S sid="207" ssid="13">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['207']
207
['207']
parsed_discourse_facet ['result_citation']
<S sid="207" ssid="13">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['207']
207
['207']
parsed_discourse_facet ['result_citation']
<S sid="19" ssid="4">It can be easily shown from Thatcher's result that the path set of every local set is a regular set.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/P87-1015.csv
<S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'207'", "'125'"]
'92'
'207'
'125'
['92', '207', '125']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'125'", "'207'"]
'75'
'125'
'207'
['75', '125', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'75'", "'207'"]
'125'
'75'
'207'
['125', '75', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'125'", "'207'"]
'2'
'125'
'207'
['2', '125', '207']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'92'"]
'207'
'125'
'92'
['207', '125', '92']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'207'", "'75'"]
'2'
'207'
'75'
['2', '207', '75']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'92'"]
'207'
'125'
'92'
['207', '125', '92']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="151">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'207'", "'125'"]
'151'
'207'
'125'
['151', '207', '125']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'75'", "'207'"]
'2'
'75'
'207'
['2', '75', '207']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'75'", "'207'"]
'125'
'75'
'207'
['125', '75', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'125'", "'207'"]
'75'
'125'
'207'
['75', '125', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'75'"]
'207'
'125'
'75'
['207', '125', '75']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'75'", "'207'"]
'125'
'75'
'207'
['125', '75', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="113">A formalism generating tree sets with complex path sets can still generate only semilinear languages if its tree sets have independent paths, and semilinear path sets.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'113'", "'75'"]
'2'
'113'
'75'
['2', '113', '75']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'92'"]
'207'
'125'
'92'
['207', '125', '92']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'75'"]
'207'
'125'
'75'
['207', '125', '75']
parsed_discourse_facet ['result_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P08-1043_swastika.csv
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['result_citation']
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="37">3An English sentence with ambiguous PoS assignment can be trivially represented as a lattice similar to our own, where every pair of consecutive nodes correspond to a word, and every possible PoS assignment for this word is a connecting arc.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="163" ssid="1">The accuracy results for segmentation, tagging and parsing using our different models and our standard data split are summarized in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



['163']
163
['163']
parsed_discourse_facet ['result_citation']
    <S sid="100" ssid="32">This means that the rules in our grammar are of two kinds: (a) syntactic rules relating nonterminals to a sequence of non-terminals and/or PoS tags, and (b) lexical rules relating PoS tags to lattice arcs (lexemes).</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['method_citation']
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['result_citation']
<S sid="188" ssid="2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



['188']
188
['188']
parsed_discourse_facet ['result_citation']
<S sid="86" ssid="18">A morphological analyzer M : W&#8212;* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S>
original cit marker offset is 0
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['result_citation']
<S sid="97" ssid="29">Thus our proposed model is a proper model assigning probability mass to all (7r, L) pairs, where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S>
original cit marker offset is 0
new cit marker offset is 0



['97']
97
['97']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'4'", "'0'"]
'19'
'4'
'0'
['19', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="186">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'186'", "'19'", "'4'"]
'186'
'19'
'4'
['186', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'0'", "'19'"]
'4'
'0'
'19'
['4', '0', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="186">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'186'"]
'0'
'19'
'186'
['0', '19', '186']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'19'"]
'0'
'4'
'19'
['0', '4', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'4'"]
'0'
'19'
'4'
['0', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="116">We simulate lexical constraints by using an external lexical resource against which we verify whether OOV segments are in fact valid Hebrew lexemes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'116'"]
'0'
'19'
'116'
['0', '19', '116']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'19'"]
'0'
'4'
'19'
['0', '4', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'4'"]
'0'
'19'
'4'
['0', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="117">This heuristics is used to prune all segmentation possibilities involving “lexically improper” segments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'117'"]
'0'
'4'
'117'
['0', '4', '117']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'4'"]
'0'
'19'
'4'
['0', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="188">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'188'"]
'0'
'4'
'188'
['0', '4', '188']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="186">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'186'"]
'0'
'19'
'186'
['0', '19', '186']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="136">Lexicon and OOV Handling Our data-driven morphological-analyzer proposes analyses for unknown tokens as described in Section 5.</S><S ssid="1" sid="133">Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'136'", "'133'"]
'0'
'136'
'133'
['0', '136', '133']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="69">We represent all morphological analyses of a given utterance using a lattice structure.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'69'", "'19'"]
'0'
'69'
'19'
['0', '69', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="135">We construct a mapping from all the space-delimited tokens seen in the training sentences to their corresponding analyses.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'135'", "'19'"]
'4'
'135'
'19'
['4', '135', '19']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



P08-1043
P11-1089
0
method_citation
['method_citation']



P08-1043
P11-1141
0
result_citation
['method_citation']



P08-1043
W10-1404
0
method_citation
['method_citation']



P08-1043
D12-1046
0
method_citation
['method_citation']
IGNORE THIS: Key error 5
parsing: input/ref/Task1/A00-2030_sweta.csv
<S sid="18" ssid="1">Almost all approaches to information extraction &#8212; even at the sentence level &#8212; are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="100" ssid="5">Given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a While our focus throughout the project was on TE and TR, we became curious about how well the model did at part-of-speech tagging, syntactic parsing, and at name finding.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="15">Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
 <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="1">Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.<
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'104'"]
'2'
'33'
'104'
['2', '33', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'6'"]
'2'
'33'
'6'
['2', '33', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="19">Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'19'"]
'2'
'33'
'19'
['2', '33', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'6'", "'2'"]
'33'
'6'
'2'
['33', '6', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'2'", "'33'"]
'6'
'2'
'33'
['6', '2', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'6'"]
'2'
'33'
'6'
['2', '33', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="92">2.</S><S ssid="1" sid="69">8.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'92'", "'69'"]
'2'
'92'
'69'
['2', '92', '69']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'104'"]
'2'
'6'
'104'
['2', '6', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'2'", "'104'"]
'6'
'2'
'104'
['6', '2', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'33'"]
'2'
'6'
'33'
['2', '6', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'104'"]
'2'
'6'
'104'
['2', '6', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'104'"]
'2'
'33'
'104'
['2', '33', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'104'"]
'2'
'6'
'104'
['2', '6', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'33'"]
'2'
'6'
'33'
['2', '6', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="0">A Novel Use of Statistical Parsing to Extract Information from Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'0'"]
'2'
'6'
'0'
['2', '6', '0']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1



A00-2030
W10-2924
0
method_citation
['method_citation']



A00-2030
P07-1055
0
method_citation
['method_citation']



A00-2030
P05-1053
0
method_citation
['method_citation']



A00-2030
N06-1037
0
method_citation
['method_citation']



A00-2030
W06-0508
0
method_citation
['method_citation']
parsing: input/ref/Task1/P11-1060_swastika.csv
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['aim_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['aim_citation']
    <S sid="148" ssid="33">This bootstrapping behavior occurs naturally: The &#8220;easy&#8221; examples are processed first, where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
    <S sid="13" ssid="9">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['result_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="47" ssid="23">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
S sid="44" ssid="20">The recurrence is as follows: At each node, we compute the set of tuples v consistent with the predicate at that node (v &#8712; w(p)), and S(x)}, where a set of pairs S is treated as a set-valued function S(x) = {y : (x, y) &#8712; S} with domain S1 = {x : (x, y) &#8712; S}.</S>
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
Error in Reference Offset
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



['106']
106
['106']
parsed_discourse_facet ['aim_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
    <S sid="172" ssid="57">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms, we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus, and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S>
original cit marker offset is 0
new cit marker offset is 0



['172']
172
['172']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



['171']
171
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'2'"]
'100'
'21'
'2'
['100', '21', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="23">We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'21'", "'23'"]
'2'
'21'
'23'
['2', '21', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'2'"]
'0'
'21'
'2'
['0', '21', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'25'"]
'21'
'0'
'25'
['21', '0', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'106'"]
'21'
'0'
'106'
['21', '0', '106']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'100'"]
'21'
'0'
'100'
['21', '0', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'100'"]
'0'
'21'
'100'
['0', '21', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'100'", "'2'"]
'21'
'100'
'2'
['21', '100', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'25'"]
'21'
'0'
'25'
['21', '0', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="23">We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'23'"]
'0'
'21'
'23'
['0', '21', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'25'"]
'21'
'0'
'25'
['21', '0', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'25'"]
'0'
'21'
'25'
['0', '21', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="174">598</S><S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'174'", "'106'"]
'21'
'174'
'106'
['21', '174', '106']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'21'", "'100'"]
'106'
'21'
'100'
['106', '21', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="23">We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'0'", "'23'"]
'100'
'0'
'23'
['100', '0', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'106'"]
'21'
'0'
'106'
['21', '0', '106']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'0'"]
'100'
'21'
'0'
['100', '21', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'100'"]
'0'
'21'
'100'
['0', '21', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="12">We represent logical forms z as labeled trees, induced automatically from (x, y) pairs.</S><S ssid="1" sid="17">The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'12'", "'17'"]
'21'
'12'
'17'
['21', '12', '17']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



P11-1060
P13-1092
0
aim_citation
['method_citation']



P11-1060
D11-1022
0
aim_citation
['method_citation']



P11-1060
D11-1140
0
aim_citation
['method_citation']
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P08-1043_sweta.csv
<S sid="156" ssid="34">To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["4'"]
4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="2">Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.</S>
original cit marker offset is 0
new cit marker offset is 0



["70'"]
70'
['70']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="3">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="9">Tsarfaty (2006) used a morphological analyzer (Segal, 2000), a PoS tagger (Bar-Haim et al., 2005), and a general purpose parser (Schmid, 2000) in an integrated framework in which morphological and syntactic components interact to share information, leading to improved performance on the joint task.</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="107" ssid="39">Firstly, Hebrew unknown tokens are doubly unknown: each unknown token may correspond to several segmentation possibilities, and each segment in such sequences may be able to admit multiple PoS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["107'"]
107'
['107']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="10">The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["14'"]
14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="12">The current work treats both segmental and super-segmental phenomena, yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg, 2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="53" ssid="11">Both (Tsarfaty, 2006; Cohen and Smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has shown a single generative model which handles both tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["53'"]
53'
['53']
parsed_discourse_facet ['method_citation']
 <S sid="48" ssid="6">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["48'"]
48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="141" ssid="19">This analyzer setting is similar to that of (Cohen and Smith, 2007), and models using it are denoted nohsp, Parser and Grammar We used BitPar (Schmid, 2004), an efficient general purpose parser,10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["141'"]
141'
['141']
parsed_discourse_facet ['method_citation']
<S sid="188" ssid="2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



["188'"]
188'
['188']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["94'"]
94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="12">Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training data similar to (Cohen and Smith, 2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
    <S sid="156" ssid="34">To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth, &#8216;tokens&#8217;) that constitute the unanalyzed surface forms (utterances).</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'4'", "'0'"]
'19'
'4'
'0'
['19', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="186">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'186'", "'19'", "'4'"]
'186'
'19'
'4'
['186', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'0'", "'19'"]
'4'
'0'
'19'
['4', '0', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="186">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'186'"]
'0'
'19'
'186'
['0', '19', '186']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'19'"]
'0'
'4'
'19'
['0', '4', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'4'"]
'0'
'19'
'4'
['0', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="116">We simulate lexical constraints by using an external lexical resource against which we verify whether OOV segments are in fact valid Hebrew lexemes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'116'"]
'0'
'19'
'116'
['0', '19', '116']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'19'"]
'0'
'4'
'19'
['0', '4', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'4'"]
'0'
'19'
'4'
['0', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="117">This heuristics is used to prune all segmentation possibilities involving “lexically improper” segments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'117'"]
'0'
'4'
'117'
['0', '4', '117']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'4'"]
'0'
'19'
'4'
['0', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="188">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'188'"]
'0'
'4'
'188'
['0', '4', '188']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="186">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'186'"]
'0'
'19'
'186'
['0', '19', '186']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="136">Lexicon and OOV Handling Our data-driven morphological-analyzer proposes analyses for unknown tokens as described in Section 5.</S><S ssid="1" sid="133">Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'136'", "'133'"]
'0'
'136'
'133'
['0', '136', '133']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="69">We represent all morphological analyses of a given utterance using a lattice structure.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'69'", "'19'"]
'0'
'69'
'19'
['0', '69', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="135">We construct a mapping from all the space-delimited tokens seen in the training sentences to their corresponding analyses.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'135'", "'19'"]
'4'
'135'
'19'
['4', '135', '19']
parsed_discourse_facet ['method_citation']



P08-1043
P11-1141
0
method_citation
['method_citation']
parsing: input/ref/Task1/D10-1044_sweta.csv
<S sid="4" ssid="1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S>
original cit marker offset is 0
new cit marker offset is 0



["4'"]
4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="1">We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="4">For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components (word-alignment model, language model, translation model, etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["7'"]
7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="14">Linear weights are difficult to incorporate into the standard MERT procedure because they are &#8220;hidden&#8221; within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn, 2007), we circumvent this problem by choosing weights to optimize corpus loglikelihood, which is roughly speaking the training criterion used by the LM and TM themselves.</S>
original cit marker offset is 0
new cit marker offset is 0



["50'"]
50'
['50']
parsed_discourse_facet ['method_citation']
<S sid="152" ssid="9">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["152'"]
152'
['152']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
 <S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="97" ssid="1">We carried out translation experiments in two different settings.</S>
original cit marker offset is 0
new cit marker offset is 0



["97'"]
97'
['97']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="12">Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="10">Finally, we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="141" ssid="10">Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L&#168;u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L&#168;u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["141'"]
141'
['141']
parsed_discourse_facet ['method_citation']
 <S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="37" ssid="1">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S>
original cit marker offset is 0
new cit marker offset is 0



["37'"]
37'
['37']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D10-1044.csv
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="62">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'37'", "'62'"]
'1'
'37'
'62'
['1', '37', '62']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="120">This significantly underperforms log-linear combination.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="3">We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'", "'37'", "'3'"]
'120'
'37'
'3'
['120', '37', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="68">First, we learn weights on individual phrase pairs rather than sentences.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'68'", "'71'"]
'37'
'68'
'71'
['37', '68', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="3">We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'3'", "'37'"]
'1'
'3'
'37'
['1', '3', '37']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="28">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="120">This significantly underperforms log-linear combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'", "'37'", "'120'"]
'28'
'37'
'120'
['28', '37', '120']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="22">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S><S ssid="1" sid="146">The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair’s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'22'", "'146'"]
'37'
'22'
'146'
['37', '22', '146']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="146">The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair’s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'146'"]
'37'
'1'
'146'
['37', '1', '146']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'37'", "'71'"]
'1'
'37'
'71'
['1', '37', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="148">Mixing, smoothing, and instance-feature weights are learned at the same time using an efficient maximum-likelihood procedure that relies on only a small in-domain development corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'148'"]
'37'
'1'
'148'
['37', '1', '148']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="148">Mixing, smoothing, and instance-feature weights are learned at the same time using an efficient maximum-likelihood procedure that relies on only a small in-domain development corpus.</S><S ssid="1" sid="151">In future work we plan to try this approach with more competitive SMT systems, and to extend instance weighting to other standard SMT components such as the LM, lexical phrase weights, and lexicalized distortion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'148'", "'151'"]
'37'
'148'
'151'
['37', '148', '151']
parsed_discourse_facet ['method_citation']



D10-1044
P14-2093
0
method_citation
['method_citation']



D10-1044
P14-1012
0
method_citation
['method_citation']
parsing: input/ref/Task1/W06-2932_vardha.csv
    <S sid="19" ssid="1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="5">However, in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="76" ssid="14">For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'"]
'76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="54" ssid="2">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'"]
'54'
['54']
parsed_discourse_facet ['method_citation']
    <S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="58" ssid="6">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'"]
'58'
['58']
parsed_discourse_facet ['method_citation']
    <S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'"]
'22'
['22']
parsed_discourse_facet ['method_citation']
    <S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
    <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'"]
'64'
['64']
parsed_discourse_facet ['method_citation']
    <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'"]
'43'
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="41">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm�1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm−1) in the tree y.</S>
original cit marker offset is 
new cit marker offset is 0



["'0'", "'12'", "'41'"]
'0'
'12'
'41'
['0', '12', '41']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="38">Such a model could easily be trained using the provided training data for each language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'38'"]
'0'
'12'
'38'
['0', '12', '38']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="106">First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'106'"]
'0'
'12'
'106'
['0', '12', '106']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="106">First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'106'"]
'0'
'12'
'106'
['0', '12', '106']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="41">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm�1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm−1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'41'"]
'0'
'12'
'41'
['0', '12', '41']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'1'"]
'0'
'12'
'1'
['0', '12', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'1'"]
'0'
'12'
'1'
['0', '12', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="41">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm�1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm−1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'0'", "'41'"]
'12'
'0'
'41'
['12', '0', '41']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="16">A dependency graph is represented by a set of ordered pairs (i, j) E y in which xj is a dependent and xi is the corresponding head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'16'"]
'0'
'12'
'16'
['0', '12', '16']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="16">A dependency graph is represented by a set of ordered pairs (i, j) E y in which xj is a dependent and xi is the corresponding head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'16'"]
'0'
'12'
'16'
['0', '12', '16']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'1'"]
'0'
'12'
'1'
['0', '12', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="54">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'54'"]
'0'
'12'
'54'
['0', '12', '54']
parsed_discourse_facet ['aim_citation', 'result_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'3'"]
'0'
'12'
'3'
['0', '12', '3']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



W06-2932
I08-1012
0
method_citation
['method_citation']
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P04-1036_swastika.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="24">We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.</S>
original cit marker offset is 0
new cit marker offset is 0



['68']
68
['68']
parsed_discourse_facet ['result_citation']
<S sid="83" ssid="12">The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.</S>
original cit marker offset is 0
new cit marker offset is 0



['83']
83
['83']
parsed_discourse_facet ['result_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
<S sid="101" ssid="30">Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.</S>
original cit marker offset is 0
new cit marker offset is 0



['101']
101
['101']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="83" ssid="12">The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.</S>
original cit marker offset is 0
new cit marker offset is 0



['83']
83
['83']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="46" ssid="2">This provides the nearest neighbours to each target word, along with the distributional similarity score between the target word and its neighbour.</S>
original cit marker offset is 0
new cit marker offset is 0



['46']
46
['46']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents, rather than a lexical sample task, where the target words are manually determined and the results will depend on the skew of the words in the sample.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
<S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['method_citation']
<S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P04-1036.csv
<S ssid="1" sid="107">To disambiguate senses a system should take context into account.</S><S ssid="1" sid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'1'", "'153'"]
'107'
'1'
'153'
['107', '1', '153']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="162">It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'162'"]
'179'
'178'
'162'
['179', '178', '162']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="34">In these each target word is entered with an ordered list of “nearest neighbours”.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'34'"]
'179'
'153'
'34'
['179', '153', '34']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="0">Finding Predominant Word Senses in Untagged Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'0'"]
'179'
'4'
'0'
['179', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'153'"]
'179'
'4'
'153'
['179', '4', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="169">This method obtains precision of 61% and recall 51%.</S><S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'", "'179'", "'4'"]
'169'
'179'
'4'
['169', '179', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'153'"]
'179'
'4'
'153'
['179', '4', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="0">Finding Predominant Word Senses in Untagged Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'0'"]
'179'
'4'
'0'
['179', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="62">2 The WordNet Similarity package supports a range of WordNet similarity scores.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'62'", "'4'"]
'179'
'62'
'4'
['179', '62', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'179'", "'178'"]
'4'
'179'
'178'
['4', '179', '178']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'36'"]
'179'
'4'
'36'
['179', '4', '36']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'153'"]
'179'
'178'
'153'
['179', '178', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'36'"]
'179'
'178'
'36'
['179', '178', '36']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="162">It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'162'", "'178'"]
'4'
'162'
'178'
['4', '162', '178']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'4'"]
'179'
'153'
'4'
['179', '153', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'4'"]
'179'
'153'
'4'
['179', '153', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'36'"]
'179'
'153'
'36'
['179', '153', '36']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/W99-0623_sweta.csv
 <S sid="144" ssid="6">Combining multiple highly-accurate independent parsers yields promising results.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
 <S sid="125" ssid="54">The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["125'"]
125'
['125']
parsed_discourse_facet ['method_citation']
<S sid="125" ssid="54">The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["125'"]
125'
['125']
parsed_discourse_facet ['method_citation']
    <S sid="48" ssid="34">&#8226; Similarly, when the na&#239;ve Bayes classifier is configured such that the constituents require estimated probabilities strictly larger than 0.5 to be accepted, there is not enough probability mass remaining on crossing brackets for them to be included in the hypothesis.</S>
original cit marker offset is 0
new cit marker offset is 0



["48'"]
48'
['48']
parsed_discourse_facet ['method_citation']
 <S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="63">As seen by the drop in average individual parser performance baseline, the introduced parser does not perform very well.</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
    <S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="13" ssid="9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993).</S>
original cit marker offset is 0
new cit marker offset is 0



["13'"]
13'
['13']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="37">From this we see that a finer-grained model for parser combination, at least for the features we have examined, will not give us any additional power.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="98" ssid="27">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["98'"]
98'
['98']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="13">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'"]
27'
['27']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="9">For our experiments we also report the mean of precision and recall, which we denote by (P + R)I2 and F-measure.</S>
    <S sid="81" ssid="10">F-measure is the harmonic mean of precision and recall, 2PR/(P + R).</S>
    <S sid="82" ssid="11">It is closer to the smaller value of precision and recall when there is a large skew in their values.</S>
original cit marker offset is 0
new cit marker offset is 0



["80'", "'81'", "'82'"]
80'
'81'
'82'
['80', '81', '82']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="35">In general, the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["49'"]
49'
['49']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="7">Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["11'"]
11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="98" ssid="27">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["98'"]
98'
['98']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0623.csv
<S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'16'", "'23'"]
'58'
'16'
'23'
['58', '16', '23']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="21">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'21'"]
'23'
'1'
'21'
['23', '1', '21']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'58'", "'16'"]
'23'
'58'
'16'
['23', '58', '16']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'58'", "'16'"]
'23'
'58'
'16'
['23', '58', '16']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="28">The development of a naïve Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'23'", "'28'"]
'58'
'23'
'28'
['58', '23', '28']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="58">We call this approach parser switching.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'58'"]
'23'
'16'
'58'
['23', '16', '58']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'1'"]
'23'
'16'
'1'
['23', '16', '1']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="58">We call this approach parser switching.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'58'"]
'23'
'16'
'58'
['23', '16', '58']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="14">We used these three parsers to explore parser combination techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'23'", "'14'"]
'1'
'23'
'14'
['1', '23', '14']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="58">We call this approach parser switching.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'58'"]
'23'
'16'
'58'
['23', '16', '58']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'23'", "'1'"]
'16'
'23'
'1'
['16', '23', '1']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="28">The development of a naïve Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'23'", "'28'"]
'16'
'23'
'28'
['16', '23', '28']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'16'", "'23'"]
'1'
'16'
'23'
['1', '16', '23']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
IGNORE THIS: key error 1
parsing: input/ref/Task1/W06-2932_sweta.csv
 <S sid="5" ssid="1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="5">However, in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["36'"]
36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="9">In fact, for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="76" ssid="14">For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["76'"]
76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="45" ssid="14">Furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["45'"]
45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="3">First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["106'"]
106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="58" ssid="6">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["58'"]
58'
['58']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
 <S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="2">Ideally one would like to make all parsing and labeling decisions jointly so that the shared knowledge of both decisions will help resolve any ambiguities.</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
 <S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["43'"]
43'
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="41">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm�1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm−1) in the tree y.</S>
original cit marker offset is 
new cit marker offset is 0



["'0'", "'12'", "'41'"]
'0'
'12'
'41'
['0', '12', '41']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="38">Such a model could easily be trained using the provided training data for each language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'38'"]
'0'
'12'
'38'
['0', '12', '38']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="106">First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'106'"]
'0'
'12'
'106'
['0', '12', '106']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="106">First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'106'"]
'0'
'12'
'106'
['0', '12', '106']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="41">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm�1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm−1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'41'"]
'0'
'12'
'41'
['0', '12', '41']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'1'"]
'0'
'12'
'1'
['0', '12', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'1'"]
'0'
'12'
'1'
['0', '12', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="41">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm�1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm−1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'0'", "'41'"]
'12'
'0'
'41'
['12', '0', '41']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="16">A dependency graph is represented by a set of ordered pairs (i, j) E y in which xj is a dependent and xi is the corresponding head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'16'"]
'0'
'12'
'16'
['0', '12', '16']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="16">A dependency graph is represented by a set of ordered pairs (i, j) E y in which xj is a dependent and xi is the corresponding head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'16'"]
'0'
'12'
'16'
['0', '12', '16']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'1'"]
'0'
'12'
'1'
['0', '12', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="54">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'54'"]
'0'
'12'
'54'
['0', '12', '54']
parsed_discourse_facet ['aim_citation', 'result_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'3'"]
'0'
'12'
'3'
['0', '12', '3']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1



W06-2932
I08-1012
0
method_citation
['method_citation']
parsing: input/ref/Task1/P11-1061_swastika.csv
    <S sid="144" ssid="7">For comparison, the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%, and goes up to 88.7% with a treebank dictionary.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
    <S sid="44" ssid="10">Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.</S
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
parsed_discourse_facet ['method_citation']
    <S sid="16" ssid="12">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (&#167;3), and then use graph label propagation to project syntactic information from English to the foreign language (&#167;4).</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="44" ssid="10">Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.</S
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
parsed_discourse_facet ['method_citation']
<S sid="110" ssid="10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S>
original cit marker offset is 0
new cit marker offset is 0



['110']
110
['110']
parsed_discourse_facet ['aim_citation']
    <S sid="115" ssid="15">The taggers were trained on datasets labeled with the universal tags.</S>
original cit marker offset is 0
new cit marker offset is 0



['115']
115
['115']
parsed_discourse_facet ['method_citation']
    <S sid="115" ssid="15">The taggers were trained on datasets labeled with the universal tags.</S>
original cit marker offset is 0
new cit marker offset is 0



['115']
115
['115']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['158']
158
['158']
parsed_discourse_facet ['result_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



['23']
23
['23']
parsed_discourse_facet ['result_citation']
<S sid="24" ssid="1">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['24']
24
['24']
parsed_discourse_facet ['aim_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['???']
???
['???']
parsed_discourse_facet ['method_citation']
    <S sid="16" ssid="12">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (&#167;3), and then use graph label propagation to project syntactic information from English to the foreign language (&#167;4).</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



['23']
23
['23']
parsed_discourse_facet ['result_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['158']
158
['158']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
<S sid="56" ssid="22">To define a similarity function between the English and the foreign vertices, we rely on high-confidence word alignments.</S>
original cit marker offset is 0
new cit marker offset is 0



['56']
56
['56']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



['161']
161
['161']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P11-1061.csv
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="74">This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'74'", "'25'"]
'0'
'74'
'25'
['0', '74', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="24">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'24'", "'25'"]
'0'
'24'
'25'
['0', '24', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'0'", "'23'"]
'25'
'0'
'23'
['25', '0', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="51">For each trigram type x2 x3 x4 in a sequence x1 x2 x3 x4 x5, we count how many times that trigram type co-occurs with the different instantiations of each concept, and compute the point-wise mutual information (PMI) between the two.5 The similarity between two trigram types is given by summing over the PMI values over feature instantiations that they have in common.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'51'"]
'0'
'25'
'51'
['0', '25', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="74">This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'74'"]
'0'
'25'
'74'
['0', '25', '74']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="81">Instead, we resort to an iterative update based method.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'81'", "'25'"]
'0'
'81'
'25'
['0', '81', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'23'"]
'0'
'25'
'23'
['0', '25', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'23'", "'25'"]
'0'
'23'
'25'
['0', '23', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'23'", "'25'"]
'0'
'23'
'25'
['0', '23', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="74">This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'74'", "'16'"]
'0'
'74'
'16'
['0', '74', '16']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="29">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'29'"]
'0'
'25'
'29'
['0', '25', '29']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'16'", "'0'"]
'25'
'16'
'0'
['25', '16', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S ssid="1" sid="87">This locally normalized log-linear model can look at various aspects of the observation x, incorporating overlapping features of the observation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'16'", "'87'"]
'25'
'16'
'87'
['25', '16', '87']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.</S><S ssid="1" sid="24">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'24'"]
'0'
'1'
'24'
['0', '1', '24']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'16'", "'25'"]
'0'
'16'
'25'
['0', '16', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="24">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'24'", "'23'"]
'0'
'24'
'23'
['0', '24', '23']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="29">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'29'"]
'0'
'25'
'29'
['0', '25', '29']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="81">Instead, we resort to an iterative update based method.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'81'", "'25'"]
'0'
'81'
'25'
['0', '81', '25']
parsed_discourse_facet ['method_citation']



P11-1061
P12-3012
0
method_citation
['method_citation']



P11-1061
W11-2205
0
result_citation
['method_citation']
parsing: input/ref/Task1/P05-1013_vardha.csv
 <S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
  <S sid="9" ssid="5">This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
  <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
    <S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
 <S sid="62" ssid="1">In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al., 2004) and English (Nivre and Scholz, 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="23" ssid="19">By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
    <S sid="96" ssid="7">With respect to exact match, the improvement is even more noticeable, which shows quite clearly that even if non-projective dependencies are rare on the token level, they are nevertheless important for getting the global syntactic structure correct.</S>
original cit marker offset is 0
new cit marker offset is 0



["'96'"]
'96'
['96']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
  <S sid="40" ssid="11">Even this may be nondeterministic, in case the graph contains several non-projective arcs whose lifts interact, but we use the following algorithm to construct a minimal projective transformation D0 = (W, A0) of a (nonprojective) dependency graph D = (W, A): The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right).</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
  <S sid="7" ssid="3">From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="14" ssid="10">While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
 <S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'"]
'49'
['49']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P05-1013.csv
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="20">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'20'"]
'0'
'2'
'20'
['0', '2', '20']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="63">The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'63'"]
'0'
'2'
'63'
['0', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'0'", "'109'"]
'2'
'0'
'109'
['2', '0', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="63">The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'63'"]
'0'
'2'
'63'
['0', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'0'", "'109'"]
'2'
'0'
'109'
['2', '0', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']



P05-1013
W10-1403
0
method_citation
['method_citation']



P05-1013
D07-1013
0
method_citation
['method_citation']
parsing: input/ref/Task1/P05-1013_swastika.csv
<S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



['49']
49
['49']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="6">In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
<S sid="38" ssid="9">Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="6">In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
Error in Reference Offset
S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
Error in Reference Offset
    <S sid="86" ssid="13">As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
 <S sid="51" ssid="22">In the second scheme, Head+Path, we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p&#8595;.</S>
original cit marker offset is 0
new cit marker offset is 0



['51']
51
['51']
parsed_discourse_facet ['method_citation']
<S sid="99" ssid="10">This may seem surprising, given the experiments reported in section 4, but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift, where the encoding of path information is often redundant.</S>
original cit marker offset is 0
new cit marker offset is 0



['99']
99
['99']
parsed_discourse_facet ['result_citation']
<S sid="7" ssid="3">From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



['2']
2
['2']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/P05-1013.csv
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="20">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'20'"]
'0'
'2'
'20'
['0', '2', '20']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="63">The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'63'"]
'0'
'2'
'63'
['0', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'0'", "'109'"]
'2'
'0'
'109'
['2', '0', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="63">The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'63'"]
'0'
'2'
'63'
['0', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'0'", "'109'"]
'2'
'0'
'109'
['2', '0', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



P05-1013
N07-1050
0
result_citation
['method_citation']



P05-1013
D07-1111
0
aim_citation
['method_citation']



P05-1013
W10-1403
0
result_citation
['method_citation']



P05-1013
P08-1006
0
method_citation
['method_citation']
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W99-0623_swastika.csv
<S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['aim_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['result_citation']
<S sid="91" ssid="20">Features and context were initially introduced into the models, but they refused to offer any gains in performance.</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



['139']
139
['139']
parsed_discourse_facet ['result_citation']
<S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['method_citation']
<S sid="103" ssid="32">The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.</S>
original cit marker offset is 0
new cit marker offset is 0



['103']
103
['103']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



['139']
139
['139']
parsed_discourse_facet ['result_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
<S sid="140" ssid="2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
<S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['result_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/W99-0623.csv
<S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'16'", "'23'"]
'58'
'16'
'23'
['58', '16', '23']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="21">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'21'"]
'23'
'1'
'21'
['23', '1', '21']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'58'", "'16'"]
'23'
'58'
'16'
['23', '58', '16']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'58'", "'16'"]
'23'
'58'
'16'
['23', '58', '16']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="28">The development of a naïve Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'23'", "'28'"]
'58'
'23'
'28'
['58', '23', '28']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="58">We call this approach parser switching.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'58'"]
'23'
'16'
'58'
['23', '16', '58']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'1'"]
'23'
'16'
'1'
['23', '16', '1']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="58">We call this approach parser switching.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'58'"]
'23'
'16'
'58'
['23', '16', '58']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="14">We used these three parsers to explore parser combination techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'23'", "'14'"]
'1'
'23'
'14'
['1', '23', '14']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="58">We call this approach parser switching.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'58'"]
'23'
'16'
'58'
['23', '16', '58']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'23'", "'1'"]
'16'
'23'
'1'
['16', '23', '1']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="28">The development of a naïve Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'23'", "'28'"]
'16'
'23'
'28'
['16', '23', '28']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'16'", "'23'"]
'1'
'16'
'23'
['1', '16', '23']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
parsing: input/ref/Task1/P87-1015_sweta.csv
<S sid="205" ssid="11">Independence of paths at this level reflects context freeness of rewriting and suggests why they can be recognized efficiently.</S>
original cit marker offset is 0
new cit marker offset is 0



["205'"]
205'
['205']
parsed_discourse_facet ['method_citation']
<S sid="229" ssid="35">In addition, the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars, and the hierarchy defined by Weir).</S>
original cit marker offset is 0
new cit marker offset is 0



["229'"]
229'
['229']
parsed_discourse_facet ['method_citation']
<S sid="146" ssid="31">Since every CFL is known to be semilinear (Parikh, 1966), in order to show semilinearity of some language, we need only show the existence of a letter equivalent CFL Our definition of LCFRS's insists that the composition operations are linear and nonerasing.</S>
original cit marker offset is 0
new cit marker offset is 0



["146'"]
146'
['146']
parsed_discourse_facet ['method_citation']
<S sid="201" ssid="7">It is interesting to note, however, that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG, FUG and IG's.</S>
original cit marker offset is 0
new cit marker offset is 0



["201'"]
201'
['201']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
 <S sid="222" ssid="28">However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



["222'"]
222'
['222']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="7">Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.</S>
    <S sid="23" ssid="8">He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["22'", "'23'"]
22'
'23'
['22', '23']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="221" ssid="27">Members of LCFRS whose operations have this property can be translated into the ILFP notation (Rounds, 1985).</S>
original cit marker offset is 0
new cit marker offset is 0



["221'"]
221'
['221']
parsed_discourse_facet ['method_citation']
<S sid="54" ssid="39">An IG can be viewed as a CFG in which each nonterminal is associated with a stack.</S>
original cit marker offset is 0
new cit marker offset is 0



["54'"]
54'
['54']
parsed_discourse_facet ['method_citation']
<S sid="128" ssid="13">As in the case of the derivation trees of CFG's, nodes are labeled by a member of some finite set of symbols (perhaps only implicit in the grammar as in TAG's) used to denote derived structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["128'"]
128'
['128']
parsed_discourse_facet ['method_citation']
<S sid="217" ssid="23">In considering the recognition of these languages, we were forced to be more specific regarding the relationship between the structures derived by these formalisms and the substrings they span.</S>
original cit marker offset is 0
new cit marker offset is 0



[";217'"]
;217'
[';217']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="1">From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="1">From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="214" ssid="20">LCFRS's share several properties possessed by the class of mildly context-sensitive formalisms discussed by Joshi (1983/85).</S>
original cit marker offset is 0
new cit marker offset is 0



["214'"]
214'
['214']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="20">TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish, for cross serial dependencies in Dutch subordinate clauses, and for the nestings of paired English complementizers.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P87-1015.csv
<S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'207'", "'125'"]
'92'
'207'
'125'
['92', '207', '125']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'125'", "'207'"]
'75'
'125'
'207'
['75', '125', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'75'", "'207'"]
'125'
'75'
'207'
['125', '75', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'125'", "'207'"]
'2'
'125'
'207'
['2', '125', '207']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'92'"]
'207'
'125'
'92'
['207', '125', '92']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'207'", "'75'"]
'2'
'207'
'75'
['2', '207', '75']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'92'"]
'207'
'125'
'92'
['207', '125', '92']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="151">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'207'", "'125'"]
'151'
'207'
'125'
['151', '207', '125']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'75'", "'207'"]
'2'
'75'
'207'
['2', '75', '207']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'75'", "'207'"]
'125'
'75'
'207'
['125', '75', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'125'", "'207'"]
'75'
'125'
'207'
['75', '125', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'75'"]
'207'
'125'
'75'
['207', '125', '75']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'75'", "'207'"]
'125'
'75'
'207'
['125', '75', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="113">A formalism generating tree sets with complex path sets can still generate only semilinear languages if its tree sets have independent paths, and semilinear path sets.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'113'", "'75'"]
'2'
'113'
'75'
['2', '113', '75']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'92'"]
'207'
'125'
'92'
['207', '125', '92']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'75'"]
'207'
'125'
'75'
['207', '125', '75']
parsed_discourse_facet ['result_citation']
parsing: input/ref/Task1/E03-1005_aakansha.csv
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'"]
'105'
['105']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'"]
'145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'"]
'80'
['80']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'"]
'143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid="141" ssid="6">This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'141'"]
'140'
'141'
['140', '141']
parsed_discourse_facet ['result_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="102" ssid="5">In case the shortest derivation is not unique, Bod (2000b) proposes to back off to a frequency ordering of the subtrees.</S>
    <S sid="103" ssid="6">That is, all subtrees of each root label are assigned a rank according to their frequency in the treebank: the most frequent subtree (or subtrees) of each root label gets rank 1, the second most frequent subtree gets rank 2, etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'", "'103'"]
'102'
'103'
['102', '103']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="46" ssid="43">Most previous notions of best parse tree in DOP1 were based on a probabilistic metric, with Bod (2000b) as a notable exception, who used a simplicity metric based on the shortest derivation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'"]
'46'
['46']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="37">For example, Bod (2001) samples a fixed number of subtrees of each depth, which has the effect of assigning roughly equal weight to each node in the training data, and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S>
    <S sid="86" ssid="38">Bod reports state-of-the-art results with this method, and observes no decrease in parse accuracy when larger subtrees are included (using subtrees up to depth 14).</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'", "'86'"]
'85'
'86'
['85', '86']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="115" ssid="18">The only thing that needs to be changed for Simplicity-DOP is that all subtrees should be assigned equal probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="74">Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'74'", "'0'"]
'97'
'74'
'0'
['97', '74', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'31'", "'97'"]
'0'
'31'
'97'
['0', '31', '97']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'108'", "'51'"]
'97'
'108'
'51'
['97', '108', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'97'"]
'0'
'51'
'97'
['0', '51', '97']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'108'"]
'97'
'0'
'108'
['97', '0', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'51'"]
'97'
'0'
'51'
['97', '0', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'108'"]
'97'
'0'
'108'
['97', '0', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="101">We will refer to this model as Simplicity-DOP.</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'31'", "'97'"]
'101'
'31'
'97'
['101', '31', '97']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'108'"]
'97'
'0'
'108'
['97', '0', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'51'", "'0'"]
'97'
'51'
'0'
['97', '51', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="101">We will refer to this model as Simplicity-DOP.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'101'", "'108'"]
'0'
'101'
'108'
['0', '101', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'31'", "'108'"]
'0'
'31'
'108'
['0', '31', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'31'", "'108'"]
'0'
'31'
'108'
['0', '31', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'108'", "'51'"]
'97'
'108'
'51'
['97', '108', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="67">We can create a subtree by choosing any possible left subtree and any possible right subtree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'67'"]
'97'
'0'
'67'
['97', '0', '67']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P11-1060_sweta.csv
<S sid="8" ssid="4">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
 <S sid="45" ssid="21">The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["45'"]
45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="27">It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent&#8217;s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
 <S sid="166" ssid="51">Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language&#8212;think grounded al., 2010). compositional semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
 <S sid="8" ssid="4">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
 <S sid="11" ssid="7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["11'"]
11'
['11']
parsed_discourse_facet ['method_citation']
  <S sid="115" ssid="91">After training, given a new utterance x, our system outputs the most likely y, summing out the latent logical form z: argmaxy p&#952;(T)(y  |x, z &#8712; &#732;ZL,&#952;(T)).</S>
original cit marker offset is 0
new cit marker offset is 0



["115'"]
115'
['115']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="70">We now turn to the task of mapping natural language For the example in Figure 4(b), the de- utterances to DCS trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["94'"]
94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="157" ssid="42">Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["157'"]
157'
['157']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["106'"]
106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="52">In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="23">Next, we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S>
original cit marker offset is 0
new cit marker offset is 0



["138'"]
138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="16">The CSP has two types of constraints: (i) x &#8712; w(p) for each node x labeled with predicate p &#8712; P; and (ii) xj = yj0 (the j-th component of x must equal the j'-th component of y) for each edge (x, y) labeled with j0j &#8712; R. A solution to the CSP is an assignment of nodes to values that satisfies all the constraints.</S>
original cit marker offset is 0
new cit marker offset is 0



["40'"]
40'
['40']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["171'"]
171'
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'2'"]
'100'
'21'
'2'
['100', '21', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="23">We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'21'", "'23'"]
'2'
'21'
'23'
['2', '21', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'2'"]
'0'
'21'
'2'
['0', '21', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'25'"]
'21'
'0'
'25'
['21', '0', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'106'"]
'21'
'0'
'106'
['21', '0', '106']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'100'"]
'21'
'0'
'100'
['21', '0', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'100'"]
'0'
'21'
'100'
['0', '21', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'100'", "'2'"]
'21'
'100'
'2'
['21', '100', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'25'"]
'21'
'0'
'25'
['21', '0', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="23">We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'23'"]
'0'
'21'
'23'
['0', '21', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'25'"]
'21'
'0'
'25'
['21', '0', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'25'"]
'0'
'21'
'25'
['0', '21', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="174">598</S><S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'174'", "'106'"]
'21'
'174'
'106'
['21', '174', '106']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'21'", "'100'"]
'106'
'21'
'100'
['106', '21', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="23">We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'0'", "'23'"]
'100'
'0'
'23'
['100', '0', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'106'"]
'21'
'0'
'106'
['21', '0', '106']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'0'"]
'100'
'21'
'0'
['100', '21', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'100'"]
'0'
'21'
'100'
['0', '21', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="12">We represent logical forms z as labeled trees, induced automatically from (x, y) pairs.</S><S ssid="1" sid="17">The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'12'", "'17'"]
'21'
'12'
'17'
['21', '12', '17']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P04-1036_sweta.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="82" ssid="11">We also calculate the WSD accuracy that would be obtained on SemCor, when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["82'"]
82'
['82']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="20">We briefly summarise the two measures here; for a more detailed summary see (Patwardhan et al., 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="172" ssid="20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="1">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="165" ssid="13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus, whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S>
original cit marker offset is 0
new cit marker offset is 0



["165'"]
165'
['165']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["171'"]
171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["89'"]
89'
['89']
parsed_discourse_facet ['method_citation']
<S sid="172" ssid="20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="12">Additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["189'"]
189'
['189']
parsed_discourse_facet ['method_citation']
<S sid="87" ssid="16">Again, the automatic ranking outperforms this by a large margin.</S>
original cit marker offset is 0
new cit marker offset is 0



["87'"]
87'
['87']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["89'"]
89'
['89']
parsed_discourse_facet ['method_citation']
 <S sid="137" ssid="14">Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["137'"]
137'
['137']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="19">We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline, but because of space limitations give results for the two which perform the best.</S>
original cit marker offset is 0
new cit marker offset is 0



["63'"]
63'
['63']
parsed_discourse_facet ['method_citation']
<S sid="159" ssid="7">Magnini and Cavagli`a (2000) have identified WordNet word senses with particular domains, and this has proven useful for high precision WSD (Magnini et al., 2001); indeed in section 5 we used these domain labels for evaluation.</S>
original cit marker offset is 0
new cit marker offset is 0



["159'"]
159'
['159']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S ssid="1" sid="107">To disambiguate senses a system should take context into account.</S><S ssid="1" sid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'1'", "'153'"]
'107'
'1'
'153'
['107', '1', '153']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="162">It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'162'"]
'179'
'178'
'162'
['179', '178', '162']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="34">In these each target word is entered with an ordered list of “nearest neighbours”.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'34'"]
'179'
'153'
'34'
['179', '153', '34']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="0">Finding Predominant Word Senses in Untagged Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'0'"]
'179'
'4'
'0'
['179', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'153'"]
'179'
'4'
'153'
['179', '4', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="169">This method obtains precision of 61% and recall 51%.</S><S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'", "'179'", "'4'"]
'169'
'179'
'4'
['169', '179', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'153'"]
'179'
'4'
'153'
['179', '4', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="0">Finding Predominant Word Senses in Untagged Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'0'"]
'179'
'4'
'0'
['179', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="62">2 The WordNet Similarity package supports a range of WordNet similarity scores.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'62'", "'4'"]
'179'
'62'
'4'
['179', '62', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'179'", "'178'"]
'4'
'179'
'178'
['4', '179', '178']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'36'"]
'179'
'4'
'36'
['179', '4', '36']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'153'"]
'179'
'178'
'153'
['179', '178', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'36'"]
'179'
'178'
'36'
['179', '178', '36']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="162">It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'162'", "'178'"]
'4'
'162'
'178'
['4', '162', '178']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'4'"]
'179'
'153'
'4'
['179', '153', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'4'"]
'179'
'153'
'4'
['179', '153', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'36'"]
'179'
'153'
'36'
['179', '153', '36']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/W06-3114_sweta.csv
 <S sid="108" ssid="1">The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="27">For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="44">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages, as demonstrated by the results for English-German and English-French.</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
 <S sid="172" ssid="3">Due to many similarly performing systems, we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
 <S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["36'"]
36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="103" ssid="19">Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["103'"]
103'
['103']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="60">One annotator suggested that this was the case for as much as 10% of our test sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["102'"]
102'
['102']
parsed_discourse_facet ['method_citation']
<S sid="123" ssid="16">For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.</S>
original cit marker offset is 0
new cit marker offset is 0



["123'"]
123'
['123']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="27">For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="1">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="19">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["126'"]
126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="173" ssid="4">The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["173'"]
173'
['173']
parsed_discourse_facet ['method_citation']
 <S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["170'"]
170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="84" ssid="23">The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["84'"]
84'
['84']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S><S ssid="1" sid="114">Pairwise comparison is done using the sign test.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'63'", "'114'"]
'1'
'63'
'114'
['1', '63', '114']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="26">Most of these groups follow a phrase-based statistical approach to machine translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'26'"]
'0'
'1'
'26'
['0', '1', '26']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="9">Training and testing is based on the Europarl corpus.</S><S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'0'", "'1'"]
'9'
'0'
'1'
['9', '0', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'0'"]
'1'
'103'
'0'
['1', '103', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="9">Training and testing is based on the Europarl corpus.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'1'", "'0'"]
'9'
'1'
'0'
['9', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="113">The confidence intervals are computed by bootstrap resampling for BLEU, and by standard significance testing for the manual scores, as described earlier in the paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["'103'", "'1'", "'113'"]
'103'
'1'
'113'
['103', '1', '113']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'103'", "'1'"]
'63'
'103'
'1'
['63', '103', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'103'", "'1'"]
'63'
'103'
'1'
['63', '103', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'2'", "'63'"]
'1'
'2'
'63'
['1', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="114">Pairwise comparison is done using the sign test.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'1'", "'103'"]
'114'
'1'
'103'
['114', '1', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'2'", "'103'"]
'1'
'2'
'103'
['1', '2', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="26">Most of these groups follow a phrase-based statistical approach to machine translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'26'"]
'1'
'103'
'26'
['1', '103', '26']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'63'"]
'1'
'103'
'63'
['1', '103', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="9">Training and testing is based on the Europarl corpus.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'1'", "'103'"]
'9'
'1'
'103'
['9', '1', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'2'"]
'1'
'103'
'2'
['1', '103', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'103'"]
'0'
'1'
'103'
['0', '1', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'2'", "'63'"]
'1'
'2'
'63'
['1', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="170">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'170'", "'1'"]
'0'
'170'
'1'
['0', '170', '1']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/W11-2123_vardha.csv
    <S sid="12" ssid="7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
 <S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
 <S sid="131" ssid="3">Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N &#8722; 1 preceding words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'"]
'131'
['131']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="16">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="16">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="12">Compared with SRILM, IRSTLM adds several features: lower memory consumption, a binary file format with memory mapping, caching to increase speed, and quantization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'"]
'108'
['108']
parsed_discourse_facet ['method_citation']
    <S sid="129" ssid="1">In addition to the optimizations specific to each datastructure described in Section 2, we implement several general optimizations for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'129'"]
'129'
['129']
parsed_discourse_facet ['method_citation']
    <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
    <S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
    <S sid="263" ssid="5">Quantization can be improved by jointly encoding probability and backoff.</S>
original cit marker offset is 0
new cit marker offset is 0



["'263'"]
'263'
['263']
parsed_discourse_facet ['method_citation']
    <S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
    <S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="17">If the context wnf will never extend to the right (i.e. wnf v is not present in the model for all words v) then no subsequent query will match the full context.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'"]
'145'
['145']
parsed_discourse_facet ['method_citation']
 <S sid="182" ssid="1">This section measures performance on shared tasks in order of increasing complexity: sparse lookups, evaluating perplexity of a large file, and translation with Moses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'182'"]
'182'
['182']
parsed_discourse_facet ['method_citation']
    <S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
    <S sid="12" ssid="7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
    <S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="12">We have modified Moses (Koehn et al., 2007) to keep our state with hypotheses; to conserve memory, phrases do not keep state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
 <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
 <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W11-2123.csv
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="32">Linear probing places at most one entry in each bucket.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'32'"]
'5'
'0'
'32'
['5', '0', '32']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="187">The data structure was populated with 64-bit integers sampled uniformly without replacement.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'187'"]
'5'
'0'
'187'
['5', '0', '187']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'109'"]
'0'
'25'
'109'
['0', '25', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'5'"]
'1'
'0'
'5'
['1', '0', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'109'"]
'5'
'0'
'109'
['5', '0', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S><S ssid="1" sid="7">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'25'", "'7'"]
'5'
'25'
'7'
['5', '25', '7']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'25'"]
'5'
'1'
'25'
['5', '1', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="7">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'0'", "'7'"]
'3'
'0'
'7'
['3', '0', '7']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="7">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'7'"]
'5'
'1'
'7'
['5', '1', '7']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="32">Linear probing places at most one entry in each bucket.</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'32'", "'109'"]
'5'
'32'
'109'
['5', '32', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'5'", "'1'"]
'0'
'5'
'1'
['0', '5', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'25'"]
'5'
'1'
'25'
['5', '1', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'109'", "'5'"]
'1'
'109'
'5'
['1', '109', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="3">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'5'", "'3'"]
'0'
'5'
'3'
['0', '5', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'0'", "'5'"]
'109'
'0'
'5'
['109', '0', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="102">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie), allocating memory all at once (eliminating the need for full pointers), and being easy to compile.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="200">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'", "'0'", "'200'"]
'102'
'0'
'200'
['102', '0', '200']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="9">Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf , returning where the probability p(wn|wn−1 f ) and backoff penalties b(wn−1 i ) are given by an already-estimated model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'9'"]
'5'
'0'
'9'
['5', '0', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S><S ssid="1" sid="3">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'3'", "'5'"]
'109'
'3'
'5'
['109', '3', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'1'"]
'5'
'0'
'1'
['5', '0', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'109'"]
'5'
'0'
'109'
['5', '0', '109']
parsed_discourse_facet ['method_citation']



W11-2123
W12-3160
0
method_citation
['method_citation']
parsing: input/ref/Task1/A97-1014_swastika.csv
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



['151']
151
['151']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
<S sid="47" ssid="37">Argument structure can be represented in terms of unordered trees (with crossing branches).</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



['167']
167
['167']
parsed_discourse_facet ['method_citation']
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



['160']
160
['160']
parsed_discourse_facet ['method_citation']
<S sid="127" ssid="8">As the need for certain functionalities becomes obvious with growing annotation experience, we have decided to implement the tool in two stages.</S>
original cit marker offset is 0
new cit marker offset is 0



['127']
127
['127']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



['39']
39
['39']
parsed_discourse_facet ['method_citation']
<S sid="72" ssid="17">In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.</S>
original cit marker offset is 0
new cit marker offset is 0



['72']
72
['72']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/A97-1014.csv
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'0'"]
'100'
'12'
'0'
['100', '12', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'0'"]
'100'
'12'
'0'
['100', '12', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="47">Argument structure can be represented in terms of unordered trees (with crossing branches).</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'47'", "'12'"]
'100'
'47'
'12'
['100', '47', '12']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="74">During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'74'"]
'100'
'12'
'74'
['100', '12', '74']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="47">Argument structure can be represented in terms of unordered trees (with crossing branches).</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'0'", "'100'"]
'47'
'0'
'100'
['47', '0', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S ssid="1" sid="74">During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'4'", "'74'"]
'100'
'4'
'74'
['100', '4', '74']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="114">Fig.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'114'", "'12'"]
'100'
'114'
'12'
['100', '114', '12']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="14">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'0'", "'14'"]
'4'
'0'
'14'
['4', '0', '14']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'0'", "'4'"]
'100'
'0'
'4'
['100', '0', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="21">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'0'", "'21'"]
'100'
'0'
'21'
['100', '0', '21']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S ssid="1" sid="14">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'4'", "'14'"]
'12'
'4'
'14'
['12', '4', '14']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="114">Fig.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'114'"]
'100'
'12'
'114'
['100', '12', '114']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="170">Our annotation tool supplies efficient manipulation and immediate visualization of argument structures.</S><S ssid="1" sid="162">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'170'", "'162'"]
'100'
'170'
'162'
['100', '170', '162']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="105">In addition, a. number of clear-cut NP cornponents can be defined outside that, juxtapositional kernel: pre- and postnominal genitives (GL, GR), relative clauses (RC), clausal and sentential complements (OC).</S><S ssid="1" sid="21">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'105'", "'21'"]
'100'
'105'
'21'
['100', '105', '21']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/W99-0613_sweta.csv
<S sid="121" ssid="54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page, and other pages pointing to the page).</S>
original cit marker offset is 0
new cit marker offset is 0



["121'"]
121'
['121']
parsed_discourse_facet ['method_citation']
<S sid="252" ssid="3">The method uses a &amp;quot;soft&amp;quot; measure of the agreement between two classifiers as an objective function; we described an algorithm which directly optimizes this function.</S>
original cit marker offset is 0
new cit marker offset is 0



["252'"]
252'
['252']
parsed_discourse_facet ['method_citation']
 <S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



["213'"]
213'
['213']
parsed_discourse_facet ['method_citation']
 <S sid="250" ssid="1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S>
original cit marker offset is 0
new cit marker offset is 0



["250'"]
250'
['250']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="33">(Brin 98) ,describes a system for extracting (author, book-title) pairs from the World Wide Web using an approach that bootstraps from an initial seed set of examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="202" ssid="69">The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels, and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.</S>
original cit marker offset is 0
new cit marker offset is 0



["202'"]
202'
['202']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="15">The following features were used: full-string=x The full string (e.g., for Maury Cooper, full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word, this feature applies for any words that the string contains (e.g., Maury Cooper contributes two such features, contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g., IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods, and contains at least one period.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="176" ssid="43">(7) is at 0 when: 1) Vi : sign(gi (xi)) = sign(g2 (xi)); 2) Ig3(xi)l oo; and 3) sign(gi (xi)) = yi for i = 1, , m. In fact, Zco provides a bound on the sum of the classification error of the labeled examples and the number of disagreements between the two classifiers on the unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["176'"]
176'
['176']
parsed_discourse_facet ['method_citation']
 <S sid="108" ssid="41">In the cotraining case, (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples, and (2) must agree with each other on the unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S>
    <S sid="28" ssid="22">(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'", "'28'"]
27'
'28'
['27', '28']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["7'"]
7'
['7']
parsed_discourse_facet ['method_citation']
 <S sid="172" ssid="39">To see this, note thai the first two terms in the above equation correspond to the function that AdaBoost attempts to minimize in the standard supervised setting (Equ.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["85'"]
85'
['85']
parsed_discourse_facet ['method_citation']
 <S sid="214" ssid="81">This modification brings the method closer to the DL-CoTrain algorithm described earlier, and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples, preventing one label from dominating &#8212; this deserves more theoretical investigation.</S>
original cit marker offset is 0
new cit marker offset is 0



["214'"]
214'
['214']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'0'", "'179'"]
'137'
'0'
'179'
['137', '0', '179']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="33">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'33'", "'179'"]
'137'
'33'
'179'
['137', '33', '179']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S><S ssid="1" sid="33">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'0'", "'33'"]
'137'
'0'
'33'
['137', '0', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="79">2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'79'", "'0'"]
'137'
'79'
'0'
['137', '79', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'137'", "'0'"]
'179'
'137'
'0'
['179', '137', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="224">The model was parameterized such that the joint probability of a (label, feature-set) pair P(yi, xi) is written as The model assumes that (y, x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'224'"]
'137'
'179'
'224'
['137', '179', '224']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="222">The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'222'", "'179'"]
'137'
'222'
'179'
['137', '222', '179']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
parsing: input/ref/Task1/P87-1015_vardha.csv
<S sid="165" ssid="50">This class of formalisms have the properties that their derivation trees are local sets, and manipulate objects, using a finite number of composition operations that use a finite number of symbols.</S>
original cit marker offset is 0
new cit marker offset is 0



["'165'"]
'165'
['165']
parsed_discourse_facet ['method_citation']
 <S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'"]
'119'
['119']
parsed_discourse_facet ['method_citation']
    <S sid="3" ssid="1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'"]
'3'
['3']
parsed_discourse_facet ['method_citation']
    <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
    <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
    <S sid="133" ssid="18">To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'"]
'133'
['133']
parsed_discourse_facet ['method_citation']
 <S sid="138" ssid="23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
    <S sid="156" ssid="41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'"]
'156'
['156']
parsed_discourse_facet ['method_citation']
 <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
 <S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'"]
'119'
['119']
parsed_discourse_facet ['method_citation']
  <S sid="133" ssid="18">To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'"]
'133'
['133']
parsed_discourse_facet ['method_citation']
 <S sid="164" ssid="49">Although embedding this version of LCFRS's in the framework of ILFP developed by Rounds (1985) is straightforward, our motivation was to capture properties shared by a family of grammatical systems and generalize them defining a class of related formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'164'"]
'164'
['164']
parsed_discourse_facet ['method_citation']
 <S sid="204" ssid="10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's, HG's, TAG's, and MCTAG's are all local sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'"]
'204'
['204']
parsed_discourse_facet ['method_citation']
    <S sid="9" ssid="7">We examine both the complexity of the paths of trees in the tree sets, and the kinds of dependencies that the formalisms can impose between paths.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="28" ssid="13">A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
 <S sid="138" ssid="23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P87-1015.csv
<S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'207'", "'125'"]
'92'
'207'
'125'
['92', '207', '125']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'125'", "'207'"]
'75'
'125'
'207'
['75', '125', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'75'", "'207'"]
'125'
'75'
'207'
['125', '75', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'125'", "'207'"]
'2'
'125'
'207'
['2', '125', '207']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'92'"]
'207'
'125'
'92'
['207', '125', '92']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'207'", "'75'"]
'2'
'207'
'75'
['2', '207', '75']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'92'"]
'207'
'125'
'92'
['207', '125', '92']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="151">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'207'", "'125'"]
'151'
'207'
'125'
['151', '207', '125']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'75'", "'207'"]
'2'
'75'
'207'
['2', '75', '207']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'75'", "'207'"]
'125'
'75'
'207'
['125', '75', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'125'", "'207'"]
'75'
'125'
'207'
['75', '125', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'75'"]
'207'
'125'
'75'
['207', '125', '75']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'75'", "'207'"]
'125'
'75'
'207'
['125', '75', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="113">A formalism generating tree sets with complex path sets can still generate only semilinear languages if its tree sets have independent paths, and semilinear path sets.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'113'", "'75'"]
'2'
'113'
'75'
['2', '113', '75']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'92'"]
'207'
'125'
'92'
['207', '125', '92']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'75'"]
'207'
'125'
'75'
['207', '125', '75']
parsed_discourse_facet ['result_citation']
parsing: input/ref/Task1/W99-0623_vardha.csv
    <S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="117" ssid="46">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'117'"]
'117'
['117']
parsed_discourse_facet ['method_citation']
<S sid="72" ssid="1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
    <S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'"]
'120'
['120']
parsed_discourse_facet ['method_citation']
  <S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'139'"]
'139'
['139']
parsed_discourse_facet ['method_citation']
 <S sid="84" ssid="13">The first shows how constituent features and context do not help in deciding which parser to trust.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'"]
'84'
['84']
parsed_discourse_facet ['method_citation']
    <S sid="76" ssid="5">The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'"]
'76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'"]
'38'
['38']
parsed_discourse_facet ['method_citation']
  <S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
 <S sid="72" ssid="1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
    <S sid="87" ssid="16">It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'"]
'87'
['87']
parsed_discourse_facet ['method_citation']
    <S sid="51" ssid="37">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
    <S sid="79" ssid="8">Precision is the portion of hypothesized constituents that are correct and recall is the portion of the Treebank constituents that are hypothesized.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
  <S sid="27" ssid="13">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'"]
'27'
['27']
parsed_discourse_facet ['method_citation']
 <S sid="77" ssid="6">Each parse is converted into a set of constituents represented as a tuples: (label, start, end).</S>
original cit marker offset is 0
new cit marker offset is 0



["'77'"]
'77'
['77']
parsed_discourse_facet ['method_citation']
  <S sid="11" ssid="7">Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
  <S sid="116" ssid="45">The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'116'"]
'116'
['116']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0623.csv
<S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'16'", "'23'"]
'58'
'16'
'23'
['58', '16', '23']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="21">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'21'"]
'23'
'1'
'21'
['23', '1', '21']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'58'", "'16'"]
'23'
'58'
'16'
['23', '58', '16']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'58'", "'16'"]
'23'
'58'
'16'
['23', '58', '16']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="28">The development of a naïve Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'23'", "'28'"]
'58'
'23'
'28'
['58', '23', '28']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="58">We call this approach parser switching.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'58'"]
'23'
'16'
'58'
['23', '16', '58']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'1'"]
'23'
'16'
'1'
['23', '16', '1']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="58">We call this approach parser switching.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'58'"]
'23'
'16'
'58'
['23', '16', '58']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="14">We used these three parsers to explore parser combination techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'23'", "'14'"]
'1'
'23'
'14'
['1', '23', '14']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="58">We call this approach parser switching.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'58'"]
'23'
'16'
'58'
['23', '16', '58']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'23'", "'1'"]
'16'
'23'
'1'
['16', '23', '1']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="28">The development of a naïve Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'23'", "'28'"]
'16'
'23'
'28'
['16', '23', '28']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'16'", "'23'"]
'1'
'16'
'23'
['1', '16', '23']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P08-1028_sweta.csv
<S sid="65" ssid="13">So, if we assume that only the ith components of u and v contribute to the ith component of p, that these components are not dependent on i, and that the function is symmetric with regard to the interchange of u and v, we obtain a simpler instantiation of an additive model: Analogously, under the same assumptions, we obtain the following simpler multiplicative model: only the ith components of u and v contribute to the ith component of p. Another class of models can be derived by relaxing this constraint.</S>
original cit marker offset is 0
new cit marker offset is 0



["65'"]
65'
['65']
parsed_discourse_facet ['method_citation']
 <S sid="75" ssid="23">An extreme form of this differential in the contribution of constituents is where one of the vectors, say u, contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic, however it can serve as a simple baseline against which to compare more sophisticated models.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["42'"]
42'
['42']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="17">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="195" ssid="7">The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately.</S>
original cit marker offset is 0
new cit marker offset is 0



["195'"]
195'
['195']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["57'"]
57'
['57']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["190'"]
190'
['190']
parsed_discourse_facet ['method_citation']
 <S sid="29" ssid="2">While neural networks can readily represent single distinct objects, in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>
original cit marker offset is 0
new cit marker offset is 0



["29'"]
29'
['29']
parsed_discourse_facet ['method_citation']
    <S sid="38" ssid="11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="12">Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="163" ssid="76">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["163'"]
163'
['163']
parsed_discourse_facet ['method_citation']
<S sid="185" ssid="19">The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.</S>
original cit marker offset is 0
new cit marker offset is 0



["185'"]
185'
['185']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["190'"]
190'
['190']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1028.csv
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'24'"]
'0'
'51'
'24'
['0', '51', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'24'"]
'0'
'51'
'24'
['0', '51', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'52'"]
'0'
'51'
'52'
['0', '51', '52']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'142'", "'24'"]
'51'
'142'
'24'
['51', '142', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="25">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'25'"]
'0'
'51'
'25'
['0', '51', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'142'"]
'0'
'51'
'142'
['0', '51', '142']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'142'"]
'51'
'0'
'142'
['51', '0', '142']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'52'", "'24'"]
'51'
'52'
'24'
['51', '52', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'142'"]
'51'
'0'
'142'
['51', '0', '142']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'52'", "'51'"]
'0'
'52'
'51'
['0', '52', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="38">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'38'"]
'0'
'51'
'38'
['0', '51', '38']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="3">Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'3'"]
'0'
'51'
'3'
['0', '51', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'52'", "'24'"]
'0'
'52'
'24'
['0', '52', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'24'", "'51'"]
'0'
'24'
'51'
['0', '24', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'52'"]
'0'
'51'
'52'
['0', '51', '52']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'24'"]
'51'
'0'
'24'
['51', '0', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="25">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'25'"]
'51'
'0'
'25'
['51', '0', '25']
parsed_discourse_facet ['method_citation']



P08-1028
D11-1094
0
method_citation
['method_citation']



P08-1028
P10-1021
0
method_citation
['method_citation']
parsing: input/ref/Task1/P08-1028_aakansha.csv
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="185" ssid="19">The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.</S>
    <S sid="186" ssid="20">The combined model is best overall with &#961; = 0.19.</S>
    <S sid="187" ssid="21">However, the difference between the two models is not statistically significant.</S>
original cit marker offset is 0
new cit marker offset is 0



["'185'", "'186'"]
'185'
'186'
['185', '186']
parsed_discourse_facet ['result_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="21">The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'"]
'48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="53" ssid="1">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.</S><S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'53'", "'57'"]
'53'
'57'
['53', '57']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="16">Although the composition model in (5) is commonly used in the literature, from a linguistic perspective, the model in (6) is more appealing.</S>
    <S sid="69" ssid="17">Simply adding the vectors u and v lumps their contents together rather than allowing the content of one vector to pick out the relevant content of the other.</S>
    <S sid="70" ssid="18">Instead, it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v, and vice versa.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'69'", "'70'"]
'68'
'69'
'70'
['68', '69', '70']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
    <S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'", "'25'"]
'24'
'25'
['24', '25']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="12">Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'"]
'64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="176" ssid="10">The multiplicative and combined models yield means closer to the human ratings.</S>
    <S sid="177" ssid="11">The difference between High and Low similarity values estimated by these models are statistically significant (p &lt; 0.01 using the Wilcoxon rank sum test).</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'", "'177'"]
'176'
'177'
['176', '177']
parsed_discourse_facet ['method_citation']
<S sid="191" ssid="3">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'191'"]
'191'
['191']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1028.csv
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'24'"]
'0'
'51'
'24'
['0', '51', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'24'"]
'0'
'51'
'24'
['0', '51', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'52'"]
'0'
'51'
'52'
['0', '51', '52']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'142'", "'24'"]
'51'
'142'
'24'
['51', '142', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="25">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'25'"]
'0'
'51'
'25'
['0', '51', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'142'"]
'0'
'51'
'142'
['0', '51', '142']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'142'"]
'51'
'0'
'142'
['51', '0', '142']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'52'", "'24'"]
'51'
'52'
'24'
['51', '52', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'142'"]
'51'
'0'
'142'
['51', '0', '142']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'52'", "'51'"]
'0'
'52'
'51'
['0', '52', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="38">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'38'"]
'0'
'51'
'38'
['0', '51', '38']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="3">Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'3'"]
'0'
'51'
'3'
['0', '51', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'52'", "'24'"]
'0'
'52'
'24'
['0', '52', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'24'", "'51'"]
'0'
'24'
'51'
['0', '24', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'52'"]
'0'
'51'
'52'
['0', '51', '52']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'24'"]
'51'
'0'
'24'
['51', '0', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="25">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'25'"]
'51'
'0'
'25'
['51', '0', '25']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



P08-1028
D08-1094
0
method_citation
['method_citation']



P08-1028
D11-1094
0
method_citation
['method_citation']
IGNORE THIS: Key error 5
parsing: input/ref/Task1/A00-2030_aakansha.csv
<S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="9">By necessity, we adopted the strategy of hand marking only the semantics.</S>
    <S sid="50" ssid="10">Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'50'"]
'49'
'50'
['49', '50']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="8">Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'"]
'16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S><S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'", "'33'", "'34'"]
'23'
'24'
'33'
'34'
['23', '24', '33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="60" ssid="1">In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).</S>
    <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'61'"]
'60'
'61'
['60', '61']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'"]
'33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).</S>
    <S sid="12" ssid="2">The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).</S><S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'12'", "'16'"]
'11'
'12'
'16'
['11', '12', '16']
parsed_discourse_facet ['method_citation']
<S sid="105" ssid="2">A single model proved capable of performing all necessary sentential processing, both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'"]
'105'
['105']
parsed_discourse_facet ['result_citation']
<S sid="60" ssid="1">In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).</S>
    <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'61'"]
'60'
'61'
['60', '61']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'"]
'16'
['16']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'104'"]
'2'
'33'
'104'
['2', '33', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'6'"]
'2'
'33'
'6'
['2', '33', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="19">Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'19'"]
'2'
'33'
'19'
['2', '33', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'6'", "'2'"]
'33'
'6'
'2'
['33', '6', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'2'", "'33'"]
'6'
'2'
'33'
['6', '2', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'6'"]
'2'
'33'
'6'
['2', '33', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="92">2.</S><S ssid="1" sid="69">8.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'92'", "'69'"]
'2'
'92'
'69'
['2', '92', '69']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'104'"]
'2'
'6'
'104'
['2', '6', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'2'", "'104'"]
'6'
'2'
'104'
['6', '2', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'33'"]
'2'
'6'
'33'
['2', '6', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'104'"]
'2'
'6'
'104'
['2', '6', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'104'"]
'2'
'33'
'104'
['2', '33', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'104'"]
'2'
'6'
'104'
['2', '6', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'33'"]
'2'
'6'
'33'
['2', '6', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="0">A Novel Use of Statistical Parsing to Extract Information from Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'0'"]
'2'
'6'
'0'
['2', '6', '0']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W06-3114_aakansha.csv
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="2">Training and testing is based on the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="2">Training and testing is based on the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'"]
'102'
['102']
parsed_discourse_facet ['method_citation']
<S sid="84" ssid="23">The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'"]
'84'
['84']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="4">To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="19">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="6">Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="3">&#8226; We evaluated translation from English, in addition to into English.</S>
    <S sid="6" ssid="4">English was again paired with German, French, and Spanish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'6'"]
'5'
'6'
['5', '6']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S><S ssid="1" sid="114">Pairwise comparison is done using the sign test.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'63'", "'114'"]
'1'
'63'
'114'
['1', '63', '114']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="26">Most of these groups follow a phrase-based statistical approach to machine translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'26'"]
'0'
'1'
'26'
['0', '1', '26']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="9">Training and testing is based on the Europarl corpus.</S><S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'0'", "'1'"]
'9'
'0'
'1'
['9', '0', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'0'"]
'1'
'103'
'0'
['1', '103', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="9">Training and testing is based on the Europarl corpus.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'1'", "'0'"]
'9'
'1'
'0'
['9', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="113">The confidence intervals are computed by bootstrap resampling for BLEU, and by standard significance testing for the manual scores, as described earlier in the paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["'103'", "'1'", "'113'"]
'103'
'1'
'113'
['103', '1', '113']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'103'", "'1'"]
'63'
'103'
'1'
['63', '103', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'103'", "'1'"]
'63'
'103'
'1'
['63', '103', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'2'", "'63'"]
'1'
'2'
'63'
['1', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="114">Pairwise comparison is done using the sign test.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'1'", "'103'"]
'114'
'1'
'103'
['114', '1', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'2'", "'103'"]
'1'
'2'
'103'
['1', '2', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="26">Most of these groups follow a phrase-based statistical approach to machine translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'26'"]
'1'
'103'
'26'
['1', '103', '26']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'63'"]
'1'
'103'
'63'
['1', '103', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="9">Training and testing is based on the Europarl corpus.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'1'", "'103'"]
'9'
'1'
'103'
['9', '1', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'2'"]
'1'
'103'
'2'
['1', '103', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'103'"]
'0'
'1'
'103'
['0', '1', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'2'", "'63'"]
'1'
'2'
'63'
['1', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="170">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'170'", "'1'"]
'0'
'170'
'1'
['0', '170', '1']
parsed_discourse_facet ['method_citation']



W06-3114
C08-1074
0
method_citation
['method_citation']



W06-3114
P07-1083
0
method_citation
['method_citation']
parsing: input/ref/Task1/A97-1014_vardha.csv
<S sid="72" ssid="17">In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
 <S sid="144" ssid="25">We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="4">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
  <S sid="36" ssid="26">As for free word order languages, the following features may cause problems: sition between the two poles.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
 <S sid="24" ssid="14">The typical treebank architecture is as follows: Structures: A context-free backbone is augmented with trace-filler representations of non-local dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
 <S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'"]
'151'
['151']
parsed_discourse_facet ['method_citation']
 <S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
 <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["'167'"]
'167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'"]
'39'
['39']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'"]
'151'
['151']
parsed_discourse_facet ['method_citation']
<S sid="71" ssid="16">However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'71'"]
'71'
['71']
parsed_discourse_facet ['method_citation']
 <S sid="144" ssid="25">We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A97-1014.csv
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'0'"]
'100'
'12'
'0'
['100', '12', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'0'"]
'100'
'12'
'0'
['100', '12', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="47">Argument structure can be represented in terms of unordered trees (with crossing branches).</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'47'", "'12'"]
'100'
'47'
'12'
['100', '47', '12']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="74">During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'74'"]
'100'
'12'
'74'
['100', '12', '74']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="47">Argument structure can be represented in terms of unordered trees (with crossing branches).</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'0'", "'100'"]
'47'
'0'
'100'
['47', '0', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S ssid="1" sid="74">During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'4'", "'74'"]
'100'
'4'
'74'
['100', '4', '74']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="114">Fig.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'114'", "'12'"]
'100'
'114'
'12'
['100', '114', '12']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="14">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'0'", "'14'"]
'4'
'0'
'14'
['4', '0', '14']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'0'", "'4'"]
'100'
'0'
'4'
['100', '0', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="21">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'0'", "'21'"]
'100'
'0'
'21'
['100', '0', '21']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S ssid="1" sid="14">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'4'", "'14'"]
'12'
'4'
'14'
['12', '4', '14']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="114">Fig.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'114'"]
'100'
'12'
'114'
['100', '12', '114']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="170">Our annotation tool supplies efficient manipulation and immediate visualization of argument structures.</S><S ssid="1" sid="162">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'170'", "'162'"]
'100'
'170'
'162'
['100', '170', '162']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="105">In addition, a. number of clear-cut NP cornponents can be defined outside that, juxtapositional kernel: pre- and postnominal genitives (GL, GR), relative clauses (RC), clausal and sentential complements (OC).</S><S ssid="1" sid="21">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'105'", "'21'"]
'100'
'105'
'21'
['100', '105', '21']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
parsing: input/ref/Task1/P11-1061_sweta.csv
 <S sid="9" ssid="5">Unfortunately, the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al., 2010), making its practical usability questionable at best.</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="47" ssid="13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["47'"]
47'
['47']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["10'"]
10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="1">Given the bilingual graph described in the previous section, we can use label propagation to project the English POS labels to the foreign language.</S>
original cit marker offset is 0
new cit marker offset is 0



["70'"]
70'
['70']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="18">This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
 <S sid="83" ssid="14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value &#964;: We describe how we choose &#964; in &#167;6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["83'"]
83'
['83']
parsed_discourse_facet ['method_citation']
<S sid="113" ssid="13">For each language under consideration, Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["113'"]
113'
['113']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
 <S sid="13" ssid="9">(2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available.</S>
original cit marker offset is 0
new cit marker offset is 0



["13'"]
13'
['13']
parsed_discourse_facet ['method_citation']
 <S sid="3" ssid="3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="20">We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Syntactic universals are a well studied concept in linguistics (Carnie, 2002; Newmeyer, 2005), and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["19'"]
19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="16">Figure 2 shows an excerpt of a sentence from the Italian test set and the tags assigned by four different models, as well as the gold tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



["161'"]
161'
['161']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["23'"]
23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["23'"]
23'
['23']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1061.csv
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="74">This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'74'", "'25'"]
'0'
'74'
'25'
['0', '74', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="24">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'24'", "'25'"]
'0'
'24'
'25'
['0', '24', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'0'", "'23'"]
'25'
'0'
'23'
['25', '0', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="51">For each trigram type x2 x3 x4 in a sequence x1 x2 x3 x4 x5, we count how many times that trigram type co-occurs with the different instantiations of each concept, and compute the point-wise mutual information (PMI) between the two.5 The similarity between two trigram types is given by summing over the PMI values over feature instantiations that they have in common.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'51'"]
'0'
'25'
'51'
['0', '25', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="74">This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'74'"]
'0'
'25'
'74'
['0', '25', '74']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="81">Instead, we resort to an iterative update based method.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'81'", "'25'"]
'0'
'81'
'25'
['0', '81', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'23'"]
'0'
'25'
'23'
['0', '25', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'23'", "'25'"]
'0'
'23'
'25'
['0', '23', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'23'", "'25'"]
'0'
'23'
'25'
['0', '23', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="74">This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'74'", "'16'"]
'0'
'74'
'16'
['0', '74', '16']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="29">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'29'"]
'0'
'25'
'29'
['0', '25', '29']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'16'", "'0'"]
'25'
'16'
'0'
['25', '16', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S ssid="1" sid="87">This locally normalized log-linear model can look at various aspects of the observation x, incorporating overlapping features of the observation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'16'", "'87'"]
'25'
'16'
'87'
['25', '16', '87']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.</S><S ssid="1" sid="24">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'24'"]
'0'
'1'
'24'
['0', '1', '24']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'16'", "'25'"]
'0'
'16'
'25'
['0', '16', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="24">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'24'", "'23'"]
'0'
'24'
'23'
['0', '24', '23']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="29">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'29'"]
'0'
'25'
'29'
['0', '25', '29']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="81">Instead, we resort to an iterative update based method.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'81'", "'25'"]
'0'
'81'
'25'
['0', '81', '25']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/P08-1028_swastika.csv
<S sid="21" ssid="17">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="23">Our results show that the multiplicative models are superior and correlate significantly with behavioral data.</S>
original cit marker offset is 0
new cit marker offset is 0



['27']
27
['27']
parsed_discourse_facet ['result_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['189']
189
['189']
parsed_discourse_facet ['aim_citation']
<S sid="53" ssid="1">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.</S>
original cit marker offset is 0
new cit marker offset is 0



['53']
53
['53']
parsed_discourse_facet ['method_citation']
<S sid="76" ssid="24">The models considered so far assume that components do not &#8216;interfere&#8217; with each other, i.e., that It is also possible to re-introduce the dependence on K into the model of vector composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['76']
76
['76']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['57']
57
['57']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['57']
57
['57']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="60" ssid="8">To derive specific models from this general framework requires the identification of appropriate constraints to narrow the space of functions being considered.</S>
original cit marker offset is 0
new cit marker offset is 0



['60']
60
['60']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="73" ssid="21">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware, since semantically important constituents can participate more actively in the composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['73']
73
['73']
parsed_discourse_facet ['result_citation']
<S sid="99" ssid="12">In order to establish an independent measure of sentence similarity, we assembled a set of experimental materials and elicited similarity ratings from human subjects.</S>
original cit marker offset is 0
new cit marker offset is 0



['99']
99
['99']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['189']
189
['189']
parsed_discourse_facet ['aim_citation']
<S sid="191" ssid="3">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S>
original cit marker offset is 0
new cit marker offset is 0



['191']
191
['191']
parsed_discourse_facet ['result_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P08-1028.csv
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'24'"]
'0'
'51'
'24'
['0', '51', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'24'"]
'0'
'51'
'24'
['0', '51', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'52'"]
'0'
'51'
'52'
['0', '51', '52']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'142'", "'24'"]
'51'
'142'
'24'
['51', '142', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="25">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'25'"]
'0'
'51'
'25'
['0', '51', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'142'"]
'0'
'51'
'142'
['0', '51', '142']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'142'"]
'51'
'0'
'142'
['51', '0', '142']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'52'", "'24'"]
'51'
'52'
'24'
['51', '52', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'142'"]
'51'
'0'
'142'
['51', '0', '142']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'52'", "'51'"]
'0'
'52'
'51'
['0', '52', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="38">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'38'"]
'0'
'51'
'38'
['0', '51', '38']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="3">Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'3'"]
'0'
'51'
'3'
['0', '51', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'52'", "'24'"]
'0'
'52'
'24'
['0', '52', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'24'", "'51'"]
'0'
'24'
'51'
['0', '24', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'52'"]
'0'
'51'
'52'
['0', '51', '52']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'24'"]
'51'
'0'
'24'
['51', '0', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="25">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'25'"]
'51'
'0'
'25'
['51', '0', '25']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/W06-3114_swastika.csv
<S sid="47" ssid="13">Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data.</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
    <S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="37">Our suspicion is that BLEU is very sensitive to jargon, to selecting exactly the right words, and not synonyms that human judges may appreciate as equally good.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
<S sid="145" ssid="38">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S>
original cit marker offset is 0
new cit marker offset is 0



['145']
145
['145']
parsed_discourse_facet ['result_citation']
    <S sid="103" ssid="19">Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['103']
103
['103']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="16">Following this method, we repeatedly &#8212; say, 1000 times &#8212; sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.</S>
original cit marker offset is 0
new cit marker offset is 0



['50']
50
['50']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="7">We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.</S>
original cit marker offset is 0
new cit marker offset is 0



['68']
68
['68']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="8">The way judgements are collected, human judges tend to use the scores to rank systems against each other.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['result_citation']
<S sid="145" ssid="38">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S>
original cit marker offset is 0
new cit marker offset is 0



['145']
145
['145']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S><S ssid="1" sid="114">Pairwise comparison is done using the sign test.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'63'", "'114'"]
'1'
'63'
'114'
['1', '63', '114']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="26">Most of these groups follow a phrase-based statistical approach to machine translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'26'"]
'0'
'1'
'26'
['0', '1', '26']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="9">Training and testing is based on the Europarl corpus.</S><S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'0'", "'1'"]
'9'
'0'
'1'
['9', '0', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'0'"]
'1'
'103'
'0'
['1', '103', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="9">Training and testing is based on the Europarl corpus.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'1'", "'0'"]
'9'
'1'
'0'
['9', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="113">The confidence intervals are computed by bootstrap resampling for BLEU, and by standard significance testing for the manual scores, as described earlier in the paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["'103'", "'1'", "'113'"]
'103'
'1'
'113'
['103', '1', '113']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'103'", "'1'"]
'63'
'103'
'1'
['63', '103', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'103'", "'1'"]
'63'
'103'
'1'
['63', '103', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'2'", "'63'"]
'1'
'2'
'63'
['1', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="114">Pairwise comparison is done using the sign test.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'1'", "'103'"]
'114'
'1'
'103'
['114', '1', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'2'", "'103'"]
'1'
'2'
'103'
['1', '2', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="26">Most of these groups follow a phrase-based statistical approach to machine translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'26'"]
'1'
'103'
'26'
['1', '103', '26']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'63'"]
'1'
'103'
'63'
['1', '103', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="9">Training and testing is based on the Europarl corpus.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'1'", "'103'"]
'9'
'1'
'103'
['9', '1', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'2'"]
'1'
'103'
'2'
['1', '103', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'103'"]
'0'
'1'
'103'
['0', '1', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'2'", "'63'"]
'1'
'2'
'63'
['1', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="170">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'170'", "'1'"]
'0'
'170'
'1'
['0', '170', '1']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/W99-0613_vardha.csv
    <S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="35" ssid="29">AdaBoost finds a weighted combination of simple (weak) classifiers, where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
    <S sid="134" ssid="1">This section describes an algorithm based on boosting algorithms, which were previously developed for supervised machine learning problems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'"]
'134'
['134']
parsed_discourse_facet ['method_citation']
    <S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'"]
'236'
['236']
parsed_discourse_facet ['method_citation']
    <S sid="8" ssid="2">Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
    <S sid="42" ssid="36">(Riloff and Shepherd 97) describe a bootstrapping approach for acquiring nouns in particular categories (such as &amp;quot;vehicle&amp;quot; or &amp;quot;weapon&amp;quot; categories).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
    <S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'"]
'236'
['236']
parsed_discourse_facet ['method_citation']
    <S sid="222" ssid="1">The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'222'"]
'222'
['222']
parsed_discourse_facet ['method_citation']
    <S sid="30" ssid="24">(Blum and Mitchell 98) offer a promising formulation of redundancy, also prove some results about how the use of can help classification, and suggest an objective function when training with unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'"]
'30'
['30']
parsed_discourse_facet ['method_citation']
 <S sid="26" ssid="20">We present two algorithms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'"]
'26'
['26']
parsed_discourse_facet ['method_citation']
    <S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="26">The algorithm can be viewed as heuristically optimizing an objective function suggested by (Blum and Mitchell 98); empirically it is shown to be quite successful in optimizing this criterion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'"]
'32'
['32']
parsed_discourse_facet ['method_citation']
    <S sid="47" ssid="1">971,746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'"]
'47'
['47']
parsed_discourse_facet ['method_citation']
    <S sid="127" ssid="60">The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'"]
'127'
['127']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'0'", "'179'"]
'137'
'0'
'179'
['137', '0', '179']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="33">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'33'", "'179'"]
'137'
'33'
'179'
['137', '33', '179']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S><S ssid="1" sid="33">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'0'", "'33'"]
'137'
'0'
'33'
['137', '0', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="79">2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'79'", "'0'"]
'137'
'79'
'0'
['137', '79', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'137'", "'0'"]
'179'
'137'
'0'
['179', '137', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="224">The model was parameterized such that the joint probability of a (label, feature-set) pair P(yi, xi) is written as The model assumes that (y, x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'224'"]
'137'
'179'
'224'
['137', '179', '224']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="222">The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'222'", "'179'"]
'137'
'222'
'179'
['137', '222', '179']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
IGNORE THIS: key error 1
IGNORE THIS: key error 1
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
IGNORE THIS: Key error 5
parsing: input/ref/Task1/A00-2018_akanksha.csv
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'"]
'90'
'91'
['90', '91']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="17">Maximum-entropy models have two benefits for a parser builder.</S>
    <S sid="49" ssid="18">First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &amp;quot;features&amp;quot; suggests that the probability model should be easily changeable &#8212; just change the set of features used.</S>
    <S sid="51" ssid="20">Second, and this is a point we have not yet mentioned, the features used in these models need have no particular independence of one another.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'49'", "'51'"]
'48'
'49'
'51'
['48', '49', '51']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
    <S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
    <S sid="93" ssid="4">This allows the second pass to see expansions not present in the training corpus.</S>
    <S sid="94" ssid="5">We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'", "'92'", "'93'", "'94'"]
'90'
'91'
'92'
'93'
'94'
['90', '91', '92', '93', '94']
parsed_discourse_facet ['method_citation']
“NA”
original cit marker offset is 0
new cit marker offset is 0



['0']
0
['0']
Error in Reference Offset
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
    <S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
    <S sid="93" ssid="4">This allows the second pass to see expansions not present in the training corpus.</S>
    <S sid="94" ssid="5">We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'", "'92'", "'93'", "'94'"]
'90'
'91'
'92'
'93'
'94'
['90', '91', '92', '93', '94']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="7">To compute a probability in a log-linear model one first defines a set of &amp;quot;features&amp;quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.</S>
    <S sid="39" ssid="8">In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.</S>
    <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'39'", "'40'"]
'38'
'39'
'40'
['38', '39', '40']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['result_citation']
<S sid="85" ssid="54">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models, this simplifies the model significantly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="32">As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &amp;quot;before&amp;quot;), and the label of the grandparent of c (la).</S><S sid="143" ssid="34">The first is simply that if we first guess the pre-terminal, when we go to guess the head the first thing we can condition upon is the pre-terminal, i.e., we compute p(h I t).</S><S sid="146" ssid="37">The second major reason why first guessing the pre-terminal makes so much difference is that it can be used when backing off the lexical head in computing the probability of the rule expansion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'143'", "'146'"]
'63'
'143'
'146'
['63', '143', '146']
parsed_discourse_facet ['method_citation']
<S sid="78" ssid="47">With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.</S>                                        <S sid="79" ssid="48">We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.</S>
original cit marker offset is 0
new cit marker offset is 0



["'78'", "'79'"]
'78'
'79'
['78', '79']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/A00-2018.csv
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="7">Following [5,10], our parser is based upon a probabilistic generative model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'0'", "'7'"]
'90'
'0'
'7'
['90', '0', '7']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="55">This method is known as &quot;deleted interpolation&quot; smoothing.</S><S ssid="1" sid="130">This parser achieves an average precision/recall of 86.2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'55'", "'130'"]
'0'
'55'
'130'
['0', '55', '130']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S><S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'90'", "'174'"]
'0'
'90'
'174'
['0', '90', '174']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'174'", "'90'"]
'0'
'174'
'90'
['0', '174', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S ssid="1" sid="20">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar, and a right-hand side that is a sequence of one or more such symbols.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'91'", "'20'"]
'174'
'91'
'20'
['174', '91', '20']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="130">This parser achieves an average precision/recall of 86.2%.</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'130'", "'91'"]
'174'
'130'
'91'
['174', '130', '91']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



A00-2018
N10-1002
0
method_citation
['method_citation']



A00-2018
N06-1039
0
method_citation
['method_citation']



A00-2018
P04-1042
0
result_citation
['method_citation']
parsing: input/ref/Task1/E03-1005_swastika.csv
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
    <S sid="41" ssid="38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



['41']
41
['41']
parsed_discourse_facet ['aim_citation']
<S sid="80" ssid="32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S>
original cit marker offset is 0
new cit marker offset is 0



['80']
80
['80']
parsed_discourse_facet ['result_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



['143']
143
['143']
parsed_discourse_facet ['method_citation']
<S sid="146" ssid="11">This paper also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic depen dencies (as e.g. in Collins 1999 and Charniak 2000).</S>
original cit marker offset is 0
new cit marker offset is 0



['146']
146
['146']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
<S sid="100" ssid="3">In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['result_citation']
    <S sid="30" ssid="27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S>
original cit marker offset is 0
new cit marker offset is 0



['30']
30
['30']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
    <S sid="30" ssid="27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S>
original cit marker offset is 0
new cit marker offset is 0



['30']
30
['30']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="74">Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'74'", "'0'"]
'97'
'74'
'0'
['97', '74', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'31'", "'97'"]
'0'
'31'
'97'
['0', '31', '97']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'108'", "'51'"]
'97'
'108'
'51'
['97', '108', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'97'"]
'0'
'51'
'97'
['0', '51', '97']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'108'"]
'97'
'0'
'108'
['97', '0', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'51'"]
'97'
'0'
'51'
['97', '0', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'108'"]
'97'
'0'
'108'
['97', '0', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="101">We will refer to this model as Simplicity-DOP.</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'31'", "'97'"]
'101'
'31'
'97'
['101', '31', '97']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'108'"]
'97'
'0'
'108'
['97', '0', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'51'", "'0'"]
'97'
'51'
'0'
['97', '51', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="101">We will refer to this model as Simplicity-DOP.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'101'", "'108'"]
'0'
'101'
'108'
['0', '101', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'31'", "'108'"]
'0'
'31'
'108'
['0', '31', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'31'", "'108'"]
'0'
'31'
'108'
['0', '31', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'108'", "'51'"]
'97'
'108'
'51'
['97', '108', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="67">We can create a subtree by choosing any possible left subtree and any possible right subtree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'67'"]
'97'
'0'
'67'
['97', '0', '67']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: Key error 5
parsing: input/ref/Task1/P04-1036_vardha.csv
    <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
    <S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="66" ssid="22">It uses the glosses of semantically related (according to WordNet) senses too. jcn (Jiang and Conrath, 1997) This score uses corpus data to populate classes (synsets) in the WordNet hierarchy with frequency counts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'66'"]
'66'
['66']
parsed_discourse_facet ['method_citation']
 <S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="101" ssid="30">Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'"]
'101'
['101']
parsed_discourse_facet ['method_citation']
 <S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
 <S sid="115" ssid="13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
  <S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'"]
'89'
['89']
parsed_discourse_facet ['method_citation']
 <S sid="137" ssid="14">Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'"]
'137'
['137']
parsed_discourse_facet ['method_citation']
75 ssid="4">We generated a thesaurus entry for all polysemous nouns which occurred in SemCor with a frequency 2, and in the BNC with a frequency 10 in the grammatical relations listed in section 2.1 above.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'"]
'75'
['75']
Error in Reference Offset
 <S sid="155" ssid="3">A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
  <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
 <S sid="155" ssid="3">A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="24">We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'"]
'68'
['68']
parsed_discourse_facet ['method_citation']
    <S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'"]
'13'
['13']
parsed_discourse_facet ['method_citation']
  <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S ssid="1" sid="107">To disambiguate senses a system should take context into account.</S><S ssid="1" sid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'1'", "'153'"]
'107'
'1'
'153'
['107', '1', '153']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="162">It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'162'"]
'179'
'178'
'162'
['179', '178', '162']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="34">In these each target word is entered with an ordered list of “nearest neighbours”.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'34'"]
'179'
'153'
'34'
['179', '153', '34']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="0">Finding Predominant Word Senses in Untagged Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'0'"]
'179'
'4'
'0'
['179', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'153'"]
'179'
'4'
'153'
['179', '4', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="169">This method obtains precision of 61% and recall 51%.</S><S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'", "'179'", "'4'"]
'169'
'179'
'4'
['169', '179', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'153'"]
'179'
'4'
'153'
['179', '4', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="0">Finding Predominant Word Senses in Untagged Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'0'"]
'179'
'4'
'0'
['179', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="62">2 The WordNet Similarity package supports a range of WordNet similarity scores.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'62'", "'4'"]
'179'
'62'
'4'
['179', '62', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'179'", "'178'"]
'4'
'179'
'178'
['4', '179', '178']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'36'"]
'179'
'4'
'36'
['179', '4', '36']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'153'"]
'179'
'178'
'153'
['179', '178', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'36'"]
'179'
'178'
'36'
['179', '178', '36']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="162">It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'162'", "'178'"]
'4'
'162'
'178'
['4', '162', '178']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'4'"]
'179'
'153'
'4'
['179', '153', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'4'"]
'179'
'153'
'4'
['179', '153', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'36'"]
'179'
'153'
'36'
['179', '153', '36']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/A00-2018_sweta.csv
 <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["17'"]
17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
 <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["17'"]
17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="11">Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &amp;quot;correct&amp;quot;, and statistics were collected on the resulting parses.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="10">(It is &amp;quot;soft&amp;quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.)</S>
original cit marker offset is 0
new cit marker offset is 0



["119'"]
119'
['119']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.</S>
original cit marker offset is 0
new cit marker offset is 0



["95'"]
95'
['95']
parsed_discourse_facet ['method_citation']
 <S sid="175" ssid="2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["175'"]
175'
['175']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
original cit marker offset is 0
new cit marker offset is 0



["92'"]
92'
['92']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="17">This is indicated in Figure 2, where the model labeled &amp;quot;Best&amp;quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["126'"]
126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="1">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c, t(c) (t for &amp;quot;tag&amp;quot;), then the lexical head of c, h(c), and then the expansion of c into further constituents e(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["174'"]
174'
['174']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="32">As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &amp;quot;before&amp;quot;), and the label of the grandparent of c (la).</S>
original cit marker offset is 0
new cit marker offset is 0



["63'"]
63'
['63']
parsed_discourse_facet ['method_citation']
<S sid="78" ssid="47">With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.</S>
original cit marker offset is 0
new cit marker offset is 0



["78'"]
78'
['78']
parsed_discourse_facet ['method_citation']
 <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="7">From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.</S>
original cit marker offset is 0
new cit marker offset is 0



["180'"]
180'
['180']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2018.csv
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="7">Following [5,10], our parser is based upon a probabilistic generative model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'0'", "'7'"]
'90'
'0'
'7'
['90', '0', '7']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="55">This method is known as &quot;deleted interpolation&quot; smoothing.</S><S ssid="1" sid="130">This parser achieves an average precision/recall of 86.2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'55'", "'130'"]
'0'
'55'
'130'
['0', '55', '130']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S><S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'90'", "'174'"]
'0'
'90'
'174'
['0', '90', '174']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'174'", "'90'"]
'0'
'174'
'90'
['0', '174', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S ssid="1" sid="20">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar, and a right-hand side that is a sequence of one or more such symbols.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'91'", "'20'"]
'174'
'91'
'20'
['174', '91', '20']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="130">This parser achieves an average precision/recall of 86.2%.</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'130'", "'91'"]
'174'
'130'
'91'
['174', '130', '91']
parsed_discourse_facet ['method_citation']



A00-2018
H05-1035
0
method_citation
['method_citation']



A00-2018
P04-1040
0
method_citation
['method_citation']
parsing: input/ref/Task1/E03-1005_sweta.csv
<S sid="20" ssid="17">Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).</S>
original cit marker offset is 0
new cit marker offset is 0



["20'"]
20'
['20']
parsed_discourse_facet ['method_citation']
 <S sid="74" ssid="26">Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.</S>
original cit marker offset is 0
new cit marker offset is 0



["74'"]
74'
['74']
parsed_discourse_facet ['method_citation']
<S sid="44" ssid="41">But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["44'"]
44'
['44']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["145'"]
145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S>
original cit marker offset is 0
new cit marker offset is 0



["22'"]
22'
['22']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="14">It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).</S>
original cit marker offset is 0
new cit marker offset is 0



["133'"]
133'
['133']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier &amp; Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="35">Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
 <S sid="100" ssid="3">In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="29">However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
 <S sid="130" ssid="11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["130'"]
130'
['130']
parsed_discourse_facet ['method_citation']
 <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["140'"]
140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="24">Goodman (1996, 1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'"]
27'
['27']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="74">Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'74'", "'0'"]
'97'
'74'
'0'
['97', '74', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'31'", "'97'"]
'0'
'31'
'97'
['0', '31', '97']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'108'", "'51'"]
'97'
'108'
'51'
['97', '108', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'97'"]
'0'
'51'
'97'
['0', '51', '97']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'108'"]
'97'
'0'
'108'
['97', '0', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'51'"]
'97'
'0'
'51'
['97', '0', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'108'"]
'97'
'0'
'108'
['97', '0', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="101">We will refer to this model as Simplicity-DOP.</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'31'", "'97'"]
'101'
'31'
'97'
['101', '31', '97']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'108'"]
'97'
'0'
'108'
['97', '0', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'51'", "'0'"]
'97'
'51'
'0'
['97', '51', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="101">We will refer to this model as Simplicity-DOP.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'101'", "'108'"]
'0'
'101'
'108'
['0', '101', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'31'", "'108'"]
'0'
'31'
'108'
['0', '31', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'31'", "'108'"]
'0'
'31'
'108'
['0', '31', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'108'", "'51'"]
'97'
'108'
'51'
['97', '108', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="67">We can create a subtree by choosing any possible left subtree and any possible right subtree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'67'"]
'97'
'0'
'67'
['97', '0', '67']
parsed_discourse_facet ['method_citation']
parsing: input/ref/Task1/A00-2018_vardha.csv
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'91'"]
'91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="8">In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'"]
'39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="162" ssid="53">Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'"]
'162'
['162']
parsed_discourse_facet ['method_citation']
 <S sid="87" ssid="56">In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17].</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'"]
'87'
['87']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'91'"]
'91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="176" ssid="3">That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'"]
'176'
['176']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['method_citation']
 <S sid="101" ssid="12">In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning).</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'"]
'101'
['101']
parsed_discourse_facet ['method_citation']
<S sid="155" ssid="46">For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
  <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="162" ssid="53">Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'"]
'162'
['162']
parsed_discourse_facet ['method_citation']
 <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
 <S sid="79" ssid="48">We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
    <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
    <S sid="175" ssid="2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["'175'"]
'175'
['175']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2018.csv
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="7">Following [5,10], our parser is based upon a probabilistic generative model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'0'", "'7'"]
'90'
'0'
'7'
['90', '0', '7']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="55">This method is known as &quot;deleted interpolation&quot; smoothing.</S><S ssid="1" sid="130">This parser achieves an average precision/recall of 86.2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'55'", "'130'"]
'0'
'55'
'130'
['0', '55', '130']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S><S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'90'", "'174'"]
'0'
'90'
'174'
['0', '90', '174']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'174'", "'90'"]
'0'
'174'
'90'
['0', '174', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S ssid="1" sid="20">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar, and a right-hand side that is a sequence of one or more such symbols.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'91'", "'20'"]
'174'
'91'
'20'
['174', '91', '20']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="130">This parser achieves an average precision/recall of 86.2%.</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'130'", "'91'"]
'174'
'130'
'91'
['174', '130', '91']
parsed_discourse_facet ['method_citation']
IGNORE THIS: key error 2
IGNORE THIS: key error 2
IGNORE THIS: key error 2



A00-2018
W06-3119
0
method_citation
['method_citation']
IGNORE THIS: Key error 5





input/ref/Task1/P04-1036_aakansha.csv
input/res/Task1/P04-1036.csv
parsing: input/ref/Task1/P04-1036_aakansha.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="34">In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
    <S sid="181" ssid="4">This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'181'"]
'180'
'181'
['180', '181']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="4">To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'"]
'48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="169" ssid="17">This method obtains precision of 61% and recall 51%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'"]
'169'
['169']
parsed_discourse_facet ['result_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="115" ssid="13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
    <S sid="181" ssid="4">This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'", "'181'"]
'180'
'181'
['180', '181']
parsed_discourse_facet ['result_citation']
<S sid="180" ssid="3">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'180'"]
'180'
['180']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="34">In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S ssid="1" sid="107">To disambiguate senses a system should take context into account.</S><S ssid="1" sid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'1'", "'153'"]
'107'
'1'
'153'
['107', '1', '153']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="162">It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'162'"]
'179'
'178'
'162'
['179', '178', '162']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="34">In these each target word is entered with an ordered list of “nearest neighbours”.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'34'"]
'179'
'153'
'34'
['179', '153', '34']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="0">Finding Predominant Word Senses in Untagged Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'0'"]
'179'
'4'
'0'
['179', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'153'"]
'179'
'4'
'153'
['179', '4', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="169">This method obtains precision of 61% and recall 51%.</S><S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'", "'179'", "'4'"]
'169'
'179'
'4'
['169', '179', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'153'"]
'179'
'4'
'153'
['179', '4', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="0">Finding Predominant Word Senses in Untagged Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'0'"]
'179'
'4'
'0'
['179', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="62">2 The WordNet Similarity package supports a range of WordNet similarity scores.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'62'", "'4'"]
'179'
'62'
'4'
['179', '62', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'179'", "'178'"]
'4'
'179'
'178'
['4', '179', '178']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'36'"]
'179'
'4'
'36'
['179', '4', '36']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'153'"]
'179'
'178'
'153'
['179', '178', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'36'"]
'179'
'178'
'36'
['179', '178', '36']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="162">It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'162'", "'178'"]
'4'
'162'
'178'
['4', '162', '178']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'4'"]
'179'
'153'
'4'
['179', '153', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'4'"]
'179'
'153'
'4'
['179', '153', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'36'"]
'179'
'153'
'36'
['179', '153', '36']
parsed_discourse_facet ['method_citation']
['In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.']
['Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.', u'In these each target word is entered with an ordered list of \u201cnearest neighbours\u201d.']
['system', 'ROUGE-S*', 'Average_R:', '0.03385', '(95%-conf.int.', '0.03385', '-', '0.03385)']
['system', 'ROUGE-S*', 'Average_P:', '0.10476', '(95%-conf.int.', '0.10476', '-', '0.10476)']
['system', 'ROUGE-S*', 'Average_F:', '0.05116', '(95%-conf.int.', '0.05116', '-', '0.05116)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:325', 'P:105', 'F:11']
['The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.']
['Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.', 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.']
['system', 'ROUGE-S*', 'Average_R:', '0.01783', '(95%-conf.int.', '0.01783', '-', '0.01783)']
['system', 'ROUGE-S*', 'Average_P:', '0.05263', '(95%-conf.int.', '0.05263', '-', '0.05263)']
['system', 'ROUGE-S*', 'Average_F:', '0.02663', '(95%-conf.int.', '0.02663', '-', '0.02663)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:190', 'F:10']
['To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for ranking.']
['Finding Predominant Word Senses in Untagged Text', 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.']
['system', 'ROUGE-S*', 'Average_R:', '0.01425', '(95%-conf.int.', '0.01425', '-', '0.01425)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.02463', '(95%-conf.int.', '0.02463', '-', '0.02463)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:55', 'F:5']
['We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.']
['We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.', 'It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text.', 'We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.']
['system', 'ROUGE-S*', 'Average_R:', '0.00450', '(95%-conf.int.', '0.00450', '-', '0.00450)']
['system', 'ROUGE-S*', 'Average_P:', '0.10714', '(95%-conf.int.', '0.10714', '-', '0.10714)']
['system', 'ROUGE-S*', 'Average_F:', '0.00865', '(95%-conf.int.', '0.00865', '-', '0.00865)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:28', 'F:3']
['The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.', 'This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.']
['Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.', 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.']
['system', 'ROUGE-S*', 'Average_R:', '0.02674', '(95%-conf.int.', '0.02674', '-', '0.02674)']
['system', 'ROUGE-S*', 'Average_P:', '0.02674', '(95%-conf.int.', '0.02674', '-', '0.02674)']
['system', 'ROUGE-S*', 'Average_F:', '0.02674', '(95%-conf.int.', '0.02674', '-', '0.02674)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:561', 'F:15']
['The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.', 'This is just 5% lower than results using the first sense in the manually labelled SemCor, and we obtain 67% precision on polysemous nouns that are not in SemCor.']
['This method obtains precision of 61% and recall 51%.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.', 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.']
['system', 'ROUGE-S*', 'Average_R:', '0.04558', '(95%-conf.int.', '0.04558', '-', '0.04558)']
['system', 'ROUGE-S*', 'Average_P:', '0.02852', '(95%-conf.int.', '0.02852', '-', '0.02852)']
['system', 'ROUGE-S*', 'Average_F:', '0.03509', '(95%-conf.int.', '0.03509', '-', '0.03509)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:561', 'F:16']
['This method obtains precision of 61% and recall 51%.']
['2 The WordNet Similarity package supports a range of WordNet similarity scores.', 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:15', 'F:0']
['In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.']
['Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.', 'Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.']
['system', 'ROUGE-S*', 'Average_R:', '0.02529', '(95%-conf.int.', '0.02529', '-', '0.02529)']
['system', 'ROUGE-S*', 'Average_P:', '0.10476', '(95%-conf.int.', '0.10476', '-', '0.10476)']
['system', 'ROUGE-S*', 'Average_F:', '0.04074', '(95%-conf.int.', '0.04074', '-', '0.04074)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:105', 'F:11']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.', 'We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:66', 'F:0']
['Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.']
['Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.', 'We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.']
['system', 'ROUGE-S*', 'Average_R:', '0.03448', '(95%-conf.int.', '0.03448', '-', '0.03448)']
['system', 'ROUGE-S*', 'Average_P:', '0.14286', '(95%-conf.int.', '0.14286', '-', '0.14286)']
['system', 'ROUGE-S*', 'Average_F:', '0.05556', '(95%-conf.int.', '0.05556', '-', '0.05556)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:105', 'F:15']
0.0658319993417 0.0202519997975 0.0269199997308





input/ref/Task1/A00-2030_vardha.csv
input/res/Task1/A00-2030.csv
parsing: input/ref/Task1/A00-2030_vardha.csv
 <S sid="11" ssid="1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="10">Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'50'"]
'50'
['50']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
 <S sid="106" ssid="3">We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
 <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
 <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="2">The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
 <S sid="3" ssid="1">Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'"]
'3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="15">Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'"]
'32'
['32']
parsed_discourse_facet ['method_citation']
 <S sid="10" ssid="8">Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="13">Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
    <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
 <S sid="26" ssid="9">We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'"]
'26'
['26']
parsed_discourse_facet ['method_citation']
    <S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'"]
'6'
['6']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="2">Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'104'"]
'2'
'33'
'104'
['2', '33', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'6'"]
'2'
'33'
'6'
['2', '33', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="19">Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'19'"]
'2'
'33'
'19'
['2', '33', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'6'", "'2'"]
'33'
'6'
'2'
['33', '6', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'2'", "'33'"]
'6'
'2'
'33'
['6', '2', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'6'"]
'2'
'33'
'6'
['2', '33', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="92">2.</S><S ssid="1" sid="69">8.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'92'", "'69'"]
'2'
'92'
'69'
['2', '92', '69']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'104'"]
'2'
'6'
'104'
['2', '6', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'2'", "'104'"]
'6'
'2'
'104'
['6', '2', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'33'"]
'2'
'6'
'33'
['2', '6', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'104'"]
'2'
'6'
'104'
['2', '6', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'104'"]
'2'
'33'
'104'
['2', '33', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'104'"]
'2'
'6'
'104'
['2', '6', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'33'"]
'2'
'6'
'33'
['2', '6', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="0">A Novel Use of Statistical Parsing to Extract Information from Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'0'"]
'2'
'6'
'0'
['2', '6', '0']
parsed_discourse_facet ['method_citation']
['In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.08418', '(95%-conf.int.', '0.08418', '-', '0.08418)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.15529', '(95%-conf.int.', '0.15529', '-', '0.15529)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:91', 'F:91']
['We were already using a generative statistical model for part-of-speech tagging (Weischedel et al. 1993), and more recently, had begun using a generative statistical model for name finding (Bikel et al.']
['In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:153', 'F:0']
['In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['Our integrated model represents syntax and semantics jointly using augmented parse trees.', 'In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.10078', '(95%-conf.int.', '0.10078', '-', '0.10078)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.18310', '(95%-conf.int.', '0.18310', '-', '0.18310)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:91', 'F:91']
['We were able to use the Penn TREEBANK to estimate the syntactic parameters; no additional syntactic training was required.']
['Our integrated model represents syntax and semantics jointly using augmented parse trees.', 'In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:36', 'F:0']
['In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['Our integrated model represents syntax and semantics jointly using augmented parse trees.', 'In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.10078', '(95%-conf.int.', '0.10078', '-', '0.10078)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.18310', '(95%-conf.int.', '0.18310', '-', '0.18310)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:91', 'F:91']
['In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['Our integrated model represents syntax and semantics jointly using augmented parse trees.', 'In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.10078', '(95%-conf.int.', '0.10078', '-', '0.10078)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.18310', '(95%-conf.int.', '0.18310', '-', '0.18310)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:91', 'F:91']
['The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).']
['Our integrated model represents syntax and semantics jointly using augmented parse trees.', 'In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00332', '(95%-conf.int.', '0.00332', '-', '0.00332)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00612', '(95%-conf.int.', '0.00612', '-', '0.00612)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:78', 'F:3']
['Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.']
['8.', '2.', 'In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:253', 'F:0']
['Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.']
['A Novel Use of Statistical Parsing to Extract Information from Text', 'In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00270', '(95%-conf.int.', '0.00270', '-', '0.00270)']
['system', 'ROUGE-S*', 'Average_P:', '0.00725', '(95%-conf.int.', '0.00725', '-', '0.00725)']
['system', 'ROUGE-S*', 'Average_F:', '0.00393', '(95%-conf.int.', '0.00393', '-', '0.00393)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:276', 'F:2']
['Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.']
['In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:253', 'F:0']
['We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.']
['Our integrated model represents syntax and semantics jointly using augmented parse trees.', 'In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.09059', '(95%-conf.int.', '0.09059', '-', '0.09059)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.16613', '(95%-conf.int.', '0.16613', '-', '0.16613)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:78', 'F:78']
['Given a new sentence, the outcome of this search process is a tree structure that encodes both the syntactic and semantic structure of the sentence.']
['Our integrated model represents syntax and semantics jointly using augmented parse trees.', 'In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:55', 'F:0']
['Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.']
['In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:136', 'F:0']
0.388131535476 0.037163845868 0.0677515379404





input/ref/Task1/P11-1061_aakansha.csv
input/res/Task1/P11-1061.csv
parsing: input/ref/Task1/P11-1061_aakansha.csv
<S sid="40" ssid="6">We extend Subramanya et al.&#8217;s intuitions to our bilingual setup.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="11">First, we use a novel graph-based framework for projecting syntactic information across language boundaries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="2">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="29" ssid="6">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'"]
'158'
['158']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'158'"]
'158'
['158']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="1">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="17" ssid="13">Second, we treat the projected labels as features in an unsupervised model (&#167;5), rather than using them directly for supervised training.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="111" ssid="11">We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).</S>
original cit marker offset is 0
new cit marker offset is 0



["'111'"]
'111'
['111']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="29" ssid="6">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'161'"]
'161'
['161']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1061.csv
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="74">This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'74'", "'25'"]
'0'
'74'
'25'
['0', '74', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="24">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'24'", "'25'"]
'0'
'24'
'25'
['0', '24', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'0'", "'23'"]
'25'
'0'
'23'
['25', '0', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="51">For each trigram type x2 x3 x4 in a sequence x1 x2 x3 x4 x5, we count how many times that trigram type co-occurs with the different instantiations of each concept, and compute the point-wise mutual information (PMI) between the two.5 The similarity between two trigram types is given by summing over the PMI values over feature instantiations that they have in common.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'51'"]
'0'
'25'
'51'
['0', '25', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="74">This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'74'"]
'0'
'25'
'74'
['0', '25', '74']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="81">Instead, we resort to an iterative update based method.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'81'", "'25'"]
'0'
'81'
'25'
['0', '81', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'23'"]
'0'
'25'
'23'
['0', '25', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'23'", "'25'"]
'0'
'23'
'25'
['0', '23', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'23'", "'25'"]
'0'
'23'
'25'
['0', '23', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="74">This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'74'", "'16'"]
'0'
'74'
'16'
['0', '74', '16']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="29">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'29'"]
'0'
'25'
'29'
['0', '25', '29']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'16'", "'0'"]
'25'
'16'
'0'
['25', '16', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S ssid="1" sid="87">This locally normalized log-linear model can look at various aspects of the observation x, incorporating overlapping features of the observation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'16'", "'87'"]
'25'
'16'
'87'
['25', '16', '87']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.</S><S ssid="1" sid="24">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'24'"]
'0'
'1'
'24'
['0', '1', '24']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'16'", "'25'"]
'0'
'16'
'25'
['0', '16', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="24">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'24'", "'23'"]
'0'
'24'
'23'
['0', '24', '23']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="29">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'29'"]
'0'
'25'
'29'
['0', '25', '29']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="81">Instead, we resort to an iterative update based method.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'81'", "'25'"]
'0'
'81'
'25'
['0', '81', '25']
parsed_discourse_facet ['method_citation']
['We use the universal POS tagset of Petrov et al. (2011) in our experiments.10 This set C consists of the following 12 coarse-grained tags: NOUN (nouns), VERB (verbs), ADJ (adjectives), ADV (adverbs), PRON (pronouns), DET (determiners), ADP (prepositions or postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), PUNC (punctuation marks) and X (a catch-all for other categories such as abbreviations or foreign words).']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.', u'To this end, we construct a bilingual graph over word types to establish a connection between the two languages (\xa73), and then use graph label propagation to project syntactic information from English to the foreign language (\xa74).']
['system', 'ROUGE-S*', 'Average_R:', '0.00256', '(95%-conf.int.', '0.00256', '-', '0.00256)']
['system', 'ROUGE-S*', 'Average_P:', '0.00221', '(95%-conf.int.', '0.00221', '-', '0.00221)']
['system', 'ROUGE-S*', 'Average_F:', '0.00238', '(95%-conf.int.', '0.00238', '-', '0.00238)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:903', 'F:2']
['We extend Subramanya et al.&#8217;s intuitions to our bilingual setup.']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', u'This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf\ufffd to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x\u2212 x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and \xb5 and \u03bd are hyperparameters that we discuss in \xa76.4.', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4278', 'P:21', 'F:0']
['The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.']
['Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.', 'This locally normalized log-linear model can look at various aspects of the observation x, incorporating overlapping features of the observation.', u'To this end, we construct a bilingual graph over word types to establish a connection between the two languages (\xa73), and then use graph label propagation to project syntactic information from English to the foreign language (\xa74).']
['system', 'ROUGE-S*', 'Average_R:', '0.00554', '(95%-conf.int.', '0.00554', '-', '0.00554)']
['system', 'ROUGE-S*', 'Average_P:', '0.05495', '(95%-conf.int.', '0.05495', '-', '0.05495)']
['system', 'ROUGE-S*', 'Average_F:', '0.01006', '(95%-conf.int.', '0.01006', '-', '0.01006)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:91', 'F:5']
['These universal POS categories not only facilitate the transfer of POS information from one language to another, but also relieve us from using controversial evaluation metrics,2 by establishing a direct correspondence between the induced hidden states in the foreign language and the observed English labels.']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.', u'Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).']
['system', 'ROUGE-S*', 'Average_R:', '0.00157', '(95%-conf.int.', '0.00157', '-', '0.00157)']
['system', 'ROUGE-S*', 'Average_P:', '0.00725', '(95%-conf.int.', '0.00725', '-', '0.00725)']
['system', 'ROUGE-S*', 'Average_F:', '0.00258', '(95%-conf.int.', '0.00258', '-', '0.00258)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:276', 'F:2']
['To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.']
['Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.', 'Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', u'To this end, we construct a bilingual graph over word types to establish a connection between the two languages (\xa73), and then use graph label propagation to project syntactic information from English to the foreign language (\xa74).']
['system', 'ROUGE-S*', 'Average_R:', '0.01923', '(95%-conf.int.', '0.01923', '-', '0.01923)']
['system', 'ROUGE-S*', 'Average_P:', '0.02841', '(95%-conf.int.', '0.02841', '-', '0.02841)']
['system', 'ROUGE-S*', 'Average_F:', '0.02294', '(95%-conf.int.', '0.02294', '-', '0.02294)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:528', 'F:15']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', u'This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf\ufffd to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x\u2212 x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and \xb5 and \u03bd are hyperparameters that we discuss in \xa76.4.', u'To this end, we construct a bilingual graph over word types to establish a connection between the two languages (\xa73), and then use graph label propagation to project syntactic information from English to the foreign language (\xa74).']
['system', 'ROUGE-S*', 'Average_R:', '0.00515', '(95%-conf.int.', '0.00515', '-', '0.00515)']
['system', 'ROUGE-S*', 'Average_P:', '0.47273', '(95%-conf.int.', '0.47273', '-', '0.47273)']
['system', 'ROUGE-S*', 'Average_F:', '0.01019', '(95%-conf.int.', '0.01019', '-', '0.01019)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5050', 'P:55', 'F:26']
['To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (&#167;3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', u'This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf\ufffd to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x\u2212 x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and \xb5 and \u03bd are hyperparameters that we discuss in \xa76.4.', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00701', '(95%-conf.int.', '0.00701', '-', '0.00701)']
['system', 'ROUGE-S*', 'Average_P:', '0.08547', '(95%-conf.int.', '0.08547', '-', '0.08547)']
['system', 'ROUGE-S*', 'Average_F:', '0.01296', '(95%-conf.int.', '0.01296', '-', '0.01296)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4278', 'P:351', 'F:30']
['To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.']
['The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.', 'Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.04813', '(95%-conf.int.', '0.04813', '-', '0.04813)']
['system', 'ROUGE-S*', 'Average_P:', '0.05114', '(95%-conf.int.', '0.05114', '-', '0.05114)']
['system', 'ROUGE-S*', 'Average_F:', '0.04959', '(95%-conf.int.', '0.04959', '-', '0.04959)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:528', 'F:27']
['Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', 'Instead, we resort to an iterative update based method.', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.01000', '(95%-conf.int.', '0.01000', '-', '0.01000)']
['system', 'ROUGE-S*', 'Average_P:', '0.02206', '(95%-conf.int.', '0.02206', '-', '0.02206)']
['system', 'ROUGE-S*', 'Average_F:', '0.01376', '(95%-conf.int.', '0.01376', '-', '0.01376)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:300', 'P:136', 'F:3']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.', u'Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).']
['system', 'ROUGE-S*', 'Average_R:', '0.00314', '(95%-conf.int.', '0.00314', '-', '0.00314)']
['system', 'ROUGE-S*', 'Average_P:', '0.07273', '(95%-conf.int.', '0.07273', '-', '0.07273)']
['system', 'ROUGE-S*', 'Average_F:', '0.00602', '(95%-conf.int.', '0.00602', '-', '0.00602)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:55', 'F:4']
['To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', u'Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00627', '(95%-conf.int.', '0.00627', '-', '0.00627)']
['system', 'ROUGE-S*', 'Average_P:', '0.12121', '(95%-conf.int.', '0.12121', '-', '0.12121)']
['system', 'ROUGE-S*', 'Average_F:', '0.01193', '(95%-conf.int.', '0.01193', '-', '0.01193)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:66', 'F:8']
0.0834690901503 0.00987272718298 0.0129463635187





input/ref/Task1/P05-1013_aakansha.csv
input/res/Task1/P05-1013.csv
parsing: input/ref/Task1/P05-1013_aakansha.csv
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="17">However, the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech, with a best performance of 73% UAS (Holan, 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="86" ssid="13">As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'86'"]
'86'
['86']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="7">As shown in Table 3, the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.</S>
    <S sid="81"  ssid="8">However, the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'", "'81'"]
'80'
'81'
['80', '81']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'"]
'49'
['49']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="10">While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P05-1013.csv
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="20">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'20'"]
'0'
'2'
'20'
['0', '2', '20']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="63">The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'63'"]
'0'
'2'
'63'
['0', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'0'", "'109'"]
'2'
'0'
'109'
['2', '0', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="63">The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'63'"]
'0'
'2'
'63'
['0', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'0'", "'109'"]
'2'
'0'
'109'
['2', '0', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
['We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.01248', '(95%-conf.int.', '0.01248', '-', '0.01248)']
['system', 'ROUGE-S*', 'Average_P:', '0.12727', '(95%-conf.int.', '0.12727', '-', '0.12727)']
['system', 'ROUGE-S*', 'Average_F:', '0.02273', '(95%-conf.int.', '0.02273', '-', '0.02273)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:55', 'F:7']
['By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.']
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'Pseudo-Projective Dependency Parsing', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.06417', '(95%-conf.int.', '0.06417', '-', '0.06417)']
['system', 'ROUGE-S*', 'Average_P:', '0.07258', '(95%-conf.int.', '0.07258', '-', '0.07258)']
['system', 'ROUGE-S*', 'Average_F:', '0.06812', '(95%-conf.int.', '0.06812', '-', '0.06812)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:496', 'F:36']
['As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.']
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'Pseudo-Projective Dependency Parsing', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.00178', '(95%-conf.int.', '0.00178', '-', '0.00178)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00307', '(95%-conf.int.', '0.00307', '-', '0.00307)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:91', 'F:1']
['As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.']
['The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents.', 'Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.']
['system', 'ROUGE-S*', 'Average_R:', '0.03427', '(95%-conf.int.', '0.03427', '-', '0.03427)']
['system', 'ROUGE-S*', 'Average_P:', '0.04497', '(95%-conf.int.', '0.04497', '-', '0.04497)']
['system', 'ROUGE-S*', 'Average_F:', '0.03890', '(95%-conf.int.', '0.03890', '-', '0.03890)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:378', 'F:17']
['While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.00535', '(95%-conf.int.', '0.00535', '-', '0.00535)']
['system', 'ROUGE-S*', 'Average_P:', '0.03297', '(95%-conf.int.', '0.03297', '-', '0.03297)']
['system', 'ROUGE-S*', 'Average_F:', '0.00920', '(95%-conf.int.', '0.00920', '-', '0.00920)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:91', 'F:3']
['We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).']
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'Pseudo-Projective Dependency Parsing', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.01426', '(95%-conf.int.', '0.01426', '-', '0.01426)']
['system', 'ROUGE-S*', 'Average_P:', '0.14545', '(95%-conf.int.', '0.14545', '-', '0.14545)']
['system', 'ROUGE-S*', 'Average_F:', '0.02597', '(95%-conf.int.', '0.02597', '-', '0.02597)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:55', 'F:8']
['In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.09269', '(95%-conf.int.', '0.09269', '-', '0.09269)']
['system', 'ROUGE-S*', 'Average_P:', '0.57143', '(95%-conf.int.', '0.57143', '-', '0.57143)']
['system', 'ROUGE-S*', 'Average_F:', '0.15951', '(95%-conf.int.', '0.15951', '-', '0.15951)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:91', 'F:52']
['In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.09269', '(95%-conf.int.', '0.09269', '-', '0.09269)']
['system', 'ROUGE-S*', 'Average_P:', '0.57143', '(95%-conf.int.', '0.57143', '-', '0.57143)']
['system', 'ROUGE-S*', 'Average_F:', '0.15951', '(95%-conf.int.', '0.15951', '-', '0.15951)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:91', 'F:52']
['However, the accuracy is considerably higher than previously reported results for robust non-projective parsing of Czech, with a best performance of 73% UAS (Holan, 2004).']
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'Pseudo-Projective Dependency Parsing', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.00178', '(95%-conf.int.', '0.00178', '-', '0.00178)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00300', '(95%-conf.int.', '0.00300', '-', '0.00300)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:105', 'F:1']
['We call this pseudoprojective dependency parsing, since it is based on a notion of pseudo-projectivity (Kahane et al., 1998).']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.01248', '(95%-conf.int.', '0.01248', '-', '0.01248)']
['system', 'ROUGE-S*', 'Average_P:', '0.12727', '(95%-conf.int.', '0.12727', '-', '0.12727)']
['system', 'ROUGE-S*', 'Average_F:', '0.02273', '(95%-conf.int.', '0.02273', '-', '0.02273)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:55', 'F:7']
['The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.02674', '(95%-conf.int.', '0.02674', '-', '0.02674)']
['system', 'ROUGE-S*', 'Average_P:', '0.02024', '(95%-conf.int.', '0.02024', '-', '0.02024)']
['system', 'ROUGE-S*', 'Average_F:', '0.02304', '(95%-conf.int.', '0.02304', '-', '0.02304)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:741', 'F:15']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.08523', '(95%-conf.int.', '0.08523', '-', '0.08523)']
['system', 'ROUGE-S*', 'Average_P:', '0.42857', '(95%-conf.int.', '0.42857', '-', '0.42857)']
['system', 'ROUGE-S*', 'Average_F:', '0.14218', '(95%-conf.int.', '0.14218', '-', '0.14218)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:528', 'P:105', 'F:45']
['In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'Pseudo-Projective Dependency Parsing', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.08378', '(95%-conf.int.', '0.08378', '-', '0.08378)']
['system', 'ROUGE-S*', 'Average_P:', '0.51648', '(95%-conf.int.', '0.51648', '-', '0.51648)']
['system', 'ROUGE-S*', 'Average_F:', '0.14417', '(95%-conf.int.', '0.14417', '-', '0.14417)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:91', 'F:47']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'Pseudo-Projective Dependency Parsing', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.18717', '(95%-conf.int.', '0.18717', '-', '0.18717)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.31532', '(95%-conf.int.', '0.31532', '-', '0.31532)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:105', 'F:105']
['As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.03743', '(95%-conf.int.', '0.03743', '-', '0.03743)']
['system', 'ROUGE-S*', 'Average_P:', '0.05556', '(95%-conf.int.', '0.05556', '-', '0.05556)']
['system', 'ROUGE-S*', 'Average_F:', '0.04473', '(95%-conf.int.', '0.04473', '-', '0.04473)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:378', 'F:21']
['As shown in Table 3, the proportion of sentences containing some non-projective dependency ranges from about 15% in DDT to almost 25% in PDT.', 'However, the overall percentage of non-projective arcs is less than 2% in PDT and less than 1% in DDT.']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.00535', '(95%-conf.int.', '0.00535', '-', '0.00535)']
['system', 'ROUGE-S*', 'Average_P:', '0.01754', '(95%-conf.int.', '0.01754', '-', '0.01754)']
['system', 'ROUGE-S*', 'Average_F:', '0.00820', '(95%-conf.int.', '0.00820', '-', '0.00820)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:171', 'F:3']
0.234516873534 0.047353124704 0.074398749535





input/ref/Task1/W99-0613_aakansha.csv
input/res/Task1/W99-0613.csv
parsing: input/ref/Task1/W99-0613_aakansha.csv
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="30">Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'"]
'137'
['137']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="12">2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="4">The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="12">But we will show that the use of unlabeled data can drastically reduce the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
    <S sid="237" ssid="4">The numbers falling into the location, person, organization categories were 186, 289 and 402 respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'", "'237'"]
'236'
'237'
['236', '237']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
    <S sid="10" ssid="4">The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'10'"]
'9'
'10'
['9', '10']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S sid="139" ssid="6">This section describes AdaBoost, which is the basis for the CoBoost algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'39'"]
'137'
'39'
['137', '39']
parsed_discourse_facet ['method_citation']
<S sid="26" ssid="20">We present two algorithms.</S>
    <S sid="27" ssid="21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'", "'27'"]
'26'
'27'
['26', '27']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="12">But we will show that the use of unlabeled data can drastically reduce the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'18'"]
'18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="2">Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S>
    <S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'", "'9'"]
'8'
'9'
['8', '9']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'0'", "'179'"]
'137'
'0'
'179'
['137', '0', '179']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="33">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'33'", "'179'"]
'137'
'33'
'179'
['137', '33', '179']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S><S ssid="1" sid="33">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'0'", "'33'"]
'137'
'0'
'33'
['137', '0', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="79">2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'79'", "'0'"]
'137'
'79'
'0'
['137', '79', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'137'", "'0'"]
'179'
'137'
'0'
['179', '137', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="224">The model was parameterized such that the joint probability of a (label, feature-set) pair P(yi, xi) is written as The model assumes that (y, x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'224'"]
'137'
'179'
'224'
['137', '179', '224']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="222">The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'222'", "'179'"]
'137'
'222'
'179'
['137', '222', '179']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
['The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.']
['The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).', 'Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.']
['system', 'ROUGE-S*', 'Average_R:', '0.10256', '(95%-conf.int.', '0.10256', '-', '0.10256)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.18605', '(95%-conf.int.', '0.18605', '-', '0.18605)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:36', 'F:36']
['This section describes AdaBoost, which is the basis for the CoBoost algorithm.', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.21645', '(95%-conf.int.', '0.21645', '-', '0.21645)']
['system', 'ROUGE-S*', 'Average_P:', '0.47619', '(95%-conf.int.', '0.47619', '-', '0.47619)']
['system', 'ROUGE-S*', 'Average_F:', '0.29762', '(95%-conf.int.', '0.29762', '-', '0.29762)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:105', 'F:50']
['The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:78', 'F:0']
['This paper discusses the use of unlabeled examples for the problem of named entity classification.', 'Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.05195', '(95%-conf.int.', '0.05195', '-', '0.05195)']
['system', 'ROUGE-S*', 'Average_P:', '0.04743', '(95%-conf.int.', '0.04743', '-', '0.04743)']
['system', 'ROUGE-S*', 'Average_F:', '0.04959', '(95%-conf.int.', '0.04959', '-', '0.04959)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:253', 'F:12']
['We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.', 'The numbers falling into the location, person, organization categories were 186, 289 and 402 respectively.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:171', 'F:0']
['We present two algorithms.', 'The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).']
['The model was parameterized such that the joint probability of a (label, feature-set) pair P(yi, xi) is written as The model assumes that (y, x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00157', '(95%-conf.int.', '0.00157', '-', '0.00157)']
['system', 'ROUGE-S*', 'Average_P:', '0.03636', '(95%-conf.int.', '0.03636', '-', '0.03636)']
['system', 'ROUGE-S*', 'Average_F:', '0.00301', '(95%-conf.int.', '0.00301', '-', '0.00301)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:55', 'F:2']
['This paper discusses the use of unlabeled examples for the problem of named entity classification.', 'The task is to learn a function from an input string (proper name) to its type, which we will assume to be one of the categories Person, Organization, or Location.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.01299', '(95%-conf.int.', '0.01299', '-', '0.01299)']
['system', 'ROUGE-S*', 'Average_P:', '0.01429', '(95%-conf.int.', '0.01429', '-', '0.01429)']
['system', 'ROUGE-S*', 'Average_F:', '0.01361', '(95%-conf.int.', '0.01361', '-', '0.01361)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:210', 'F:3']
['Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.']
['The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00794', '(95%-conf.int.', '0.00794', '-', '0.00794)']
['system', 'ROUGE-S*', 'Average_P:', '0.02857', '(95%-conf.int.', '0.02857', '-', '0.02857)']
['system', 'ROUGE-S*', 'Average_F:', '0.01242', '(95%-conf.int.', '0.01242', '-', '0.01242)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:105', 'F:3']
['(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:630', 'F:0']
['But we will show that the use of unlabeled data can drastically reduce the need for supervision.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00433', '(95%-conf.int.', '0.00433', '-', '0.00433)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00813', '(95%-conf.int.', '0.00813', '-', '0.00813)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:15', 'F:1']
0.16695099833 0.0397789996022 0.0570429994296





input/ref/Task1/P08-1102_aakansha.csv
input/res/Task1/P08-1102.csv
parsing: input/ref/Task1/P08-1102_aakansha.csv
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="14">As predications generated from such templates depend on the current character, we name these templates lexical-target.</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'34'"]
'34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
<S sid="121" ssid="32">Among other features, the 4-gram POS LM plays the most important role, removing this feature causes F-measure decrement of 0.33 points on segmentation and 0.71 points on Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'121'"]
'121'
['121']
parsed_discourse_facet ['method_citation']
<S sid="37" ssid="9">C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character (it has the subscript 0).</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'"]
'37'
['37']
parsed_discourse_facet ['method_citation']
<S sid="73" ssid="24">For instance, if the word w appears N times in training corpus and is labelled as POS t for n times, the probability Pr(t|w) can be estimated by the formula below: The probability Pr(w|t) could be estimated through the same approach.</S>
original cit marker offset is 0
new cit marker offset is 0



["'73'"]
'73'
['73']
parsed_discourse_facet ['method_citation']
<S sid="46" ssid="18">Following Collins, we use a function GEN(x) generating all candidate results of an input x , a representation 4) mapping each training example (x, y) &#8712; X &#215; Y to a feature vector 4)(x, y) &#8712; Rd, and a parameter vector &#945;&#65533; &#8712; Rd corresponding to the feature vector. d means the dimension of the vector space, it equals to the amount of features in the model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'"]
'46'
['46']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="130" ssid="1">We proposed a cascaded linear model for Chinese Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'130'"]
'130'
['130']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'21'"]
'1'
'0'
'21'
['1', '0', '21']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'96'", "'60'"]
'0'
'96'
'60'
['0', '96', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'60'", "'21'"]
'0'
'60'
'21'
['0', '60', '21']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'21'"]
'0'
'1'
'21'
['0', '1', '21']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'3'", "'60'"]
'21'
'3'
'60'
['21', '3', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'60'"]
'21'
'0'
'60'
['21', '0', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="64">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'64'"]
'0'
'21'
'64'
['0', '21', '64']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'60'"]
'0'
'21'
'60'
['0', '21', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="24">It is a better idea to perform segmentation and POS tagging jointly in a uniform framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'24'"]
'1'
'0'
'24'
['1', '0', '24']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'96'"]
'0'
'1'
'96'
['0', '1', '96']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'21'"]
'0'
'1'
'21'
['0', '1', '21']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S ssid="1" sid="72">Suppose the conditional probability Pr(t|w) describes the probability that the word w is labelled as the POS t, while Pr(w|t) describes the probability that the POS t generates the word w, then P(T|W) can be approximated by: Pr(w|t) and Pr(t|w) can be easily acquired by Maximum Likelihood Estimates (MLE) over the corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'96'", "'72'"]
'0'
'96'
'72'
['0', '96', '72']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'96'", "'60'"]
'0'
'96'
'60'
['0', '96', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="2">With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'2'"]
'0'
'1'
'2'
['0', '1', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S><S ssid="1" sid="64">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'60'", "'64'"]
'0'
'60'
'64'
['0', '60', '64']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="2">With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'2'"]
'1'
'0'
'2'
['1', '0', '2']
parsed_discourse_facet ['method_citation']
Length 0 input/ref/Task1/P08-1102_aakansha.csv
0.0 0.0 0.0





input/ref/Task1/J01-2004_aakansha.csv
input/res/Task1/J01-2004.csv
parsing: input/ref/Task1/J01-2004_aakansha.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'"]
'372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="17" ssid="5">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



["'302'"]
'302'
['302']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="37">This underspecification of the nonterminal predictions (e.g., VP-VBD in the example in Figure 2, as opposed to NP), allows lexical items to become part of the left context, and so be used to condition production probabilities, even the production probabilities of constituents that dominate them in the unfactored tree.</S>
    <S sid="80" ssid="38">It also brings words further downstream into the look-ahead at the point of specification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'", "'80'"]
'79'
'80'
['79', '80']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="42">Our approach is found to yield very accurate parses efficiently, and, in addition, to lend itself straightforwardly to estimating word probabilities on-line, that is, in a single pass from left to right.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="291" ssid="47">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'291'"]
'291'
['291']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['method_citation']
<S sid="209" ssid="113">This parser is essentially a stochastic version of the top-down parser described in Aho, Sethi, and Ullman (1986).</S>
    <S sid="210" ssid="114">It uses a PCFG with a conditional probability model of the sort defined in the previous section.</S>
original cit marker offset is 0
new cit marker offset is 0



["'209'", "'210'"]
'209'
'210'
['209', '210']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["'372'"]
'372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="20" ssid="8">Two features of our top-down parsing approach will emerge as key to its success.</S>
    <S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S><S sid="32" ssid="20">A second key feature of our approach is that top-down guidance improves the efficiency of the search as more and more conditioning events are extracted from the derivation for use in the probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'", "'21'", "'32'"]
'20'
'21'
'32'
['20', '21', '32']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="21">Because the rooted partial derivation is fully connected, all of the conditioning information that might be extracted from the top-down left context has already been specified, and a conditional probability model built on this information will not impose any additional burden on the search.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'"]
'33'
['33']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="17">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'0'", "'3'"]
'17'
'0'
'3'
['17', '0', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="4">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S><S ssid="1" sid="10">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'10'"]
'0'
'4'
'10'
['0', '4', '10']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="137">Here we will present a parser that uses simple search heuristics of this sort without DP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'137'"]
'3'
'9'
'137'
['3', '9', '137']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="223">We implement this as a beam search.</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'223'", "'100'"]
'0'
'223'
'100'
['0', '223', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'100'"]
'3'
'9'
'100'
['3', '9', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'100'"]
'3'
'9'
'100'
['3', '9', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'100'", "'3'"]
'0'
'100'
'3'
['0', '100', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="4">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S><S ssid="1" sid="10">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'10'"]
'0'
'4'
'10'
['0', '4', '10']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
['Because the rooted partial derivation is fully connected, all of the conditioning information that might be extracted from the top-down left context has already been specified, and a conditional probability model built on this information will not impose any additional burden on the search.']
['A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'Probabilistic Top-Down Parsing and Language Modeling', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.00135', '(95%-conf.int.', '0.00135', '-', '0.00135)']
['system', 'ROUGE-S*', 'Average_P:', '0.00526', '(95%-conf.int.', '0.00526', '-', '0.00526)']
['system', 'ROUGE-S*', 'Average_F:', '0.00215', '(95%-conf.int.', '0.00215', '-', '0.00215)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:190', 'F:1']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['Probabilistic Top-Down Parsing and Language Modeling', 'The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:91', 'F:0']
['It also brings words further downstream into the look-ahead at the point of specification.', 'This underspecification of the nonterminal predictions (e.g., VP-VBD in the example in Figure 2, as opposed to NP), allows lexical items to become part of the left context, and so be used to condition production probabilities, even the production probabilities of constituents that dominate them in the unfactored tree.']
['A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'Probabilistic Top-Down Parsing and Language Modeling', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:406', 'F:0']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'Probabilistic Top-Down Parsing and Language Modeling', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:91', 'F:0']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:91', 'F:0']
['Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.']
['A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'Probabilistic Top-Down Parsing and Language Modeling', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.02024', '(95%-conf.int.', '0.02024', '-', '0.02024)']
['system', 'ROUGE-S*', 'Average_P:', '0.03448', '(95%-conf.int.', '0.03448', '-', '0.03448)']
['system', 'ROUGE-S*', 'Average_F:', '0.02551', '(95%-conf.int.', '0.02551', '-', '0.02551)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:435', 'F:15']
['This parser is essentially a stochastic version of the top-down parser described in Aho, Sethi, and Ullman (1986).', 'It uses a PCFG with a conditional probability model of the sort defined in the previous section.']
['A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'Probabilistic Top-Down Parsing and Language Modeling', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.00810', '(95%-conf.int.', '0.00810', '-', '0.00810)']
['system', 'ROUGE-S*', 'Average_P:', '0.03922', '(95%-conf.int.', '0.03922', '-', '0.03922)']
['system', 'ROUGE-S*', 'Average_F:', '0.01342', '(95%-conf.int.', '0.01342', '-', '0.01342)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:153', 'F:6']
['This paper will examine language modeling for speech recognition from a natural language processing point of view.']
['Probabilistic Top-Down Parsing and Language Modeling', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'This paper will examine language modeling for speech recognition from a natural language processing point of view.']
['system', 'ROUGE-S*', 'Average_R:', '0.10417', '(95%-conf.int.', '0.10417', '-', '0.10417)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.18868', '(95%-conf.int.', '0.18868', '-', '0.18868)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:528', 'P:55', 'F:55']
0.134869998314 0.0167324997908 0.028719999641





input/ref/Task1/D10-1044_aakansha.csv
input/res/Task1/D10-1044.csv
parsing: input/ref/Task1/D10-1044_aakansha.csv
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="32">Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.</S>
    <S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'", "'96'"]
'95'
'96'
['95', '96']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'"]
'75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="28">For comparison to information-retrieval inspired baselines, eg (L&#168;u et al., 2007), we select sentences from OUT using language model perplexities from IN.</S>
original cit marker offset is 0
new cit marker offset is 0



["'31'"]
'31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'"]
'22'
['22']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="23">The 2nd block contains the IR system, which was tuned by selecting text in multiples of the size of the EMEA training corpus, according to dev set performance.</S>
    <S sid="120" ssid="24">This significantly underperforms log-linear combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'", "'120'"]
'119'
'120'
['119', '120']
parsed_discourse_facet ['result_citation']
<S sid="23" ssid="20">Our second contribution is to apply instance weighting at the level of phrase pairs.</S>
    <S sid="24" ssid="21">Sentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="4">We focus here instead on adapting the two most important features: the language model (LM), which estimates the probability p(wIh) of a target word w following an ngram h; and the translation models (TM) p(slt) and p(t1s), which give the probability of source phrase s translating to target phrase t, and vice versa.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D10-1044.csv
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="62">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'37'", "'62'"]
'1'
'37'
'62'
['1', '37', '62']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="120">This significantly underperforms log-linear combination.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="3">We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'", "'37'", "'3'"]
'120'
'37'
'3'
['120', '37', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="68">First, we learn weights on individual phrase pairs rather than sentences.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'68'", "'71'"]
'37'
'68'
'71'
['37', '68', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="3">We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'3'", "'37'"]
'1'
'3'
'37'
['1', '3', '37']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="28">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="120">This significantly underperforms log-linear combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'", "'37'", "'120'"]
'28'
'37'
'120'
['28', '37', '120']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="22">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S><S ssid="1" sid="146">The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair’s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'22'", "'146'"]
'37'
'22'
'146'
['37', '22', '146']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="146">The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair’s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'146'"]
'37'
'1'
'146'
['37', '1', '146']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'37'", "'71'"]
'1'
'37'
'71'
['1', '37', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="148">Mixing, smoothing, and instance-feature weights are learned at the same time using an efficient maximum-likelihood procedure that relies on only a small in-domain development corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'148'"]
'37'
'1'
'148'
['37', '1', '148']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="148">Mixing, smoothing, and instance-feature weights are learned at the same time using an efficient maximum-likelihood procedure that relies on only a small in-domain development corpus.</S><S ssid="1" sid="151">In future work we plan to try this approach with more competitive SMT systems, and to extend instance weighting to other standard SMT components such as the LM, lexical phrase weights, and lexicalized distortion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'148'", "'151'"]
'37'
'148'
'151'
['37', '148', '151']
parsed_discourse_facet ['method_citation']
['Our second contribution is to apply instance weighting at the level of phrase pairs.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', u'The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair\u2019s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.']
['system', 'ROUGE-S*', 'Average_R:', '0.00468', '(95%-conf.int.', '0.00468', '-', '0.00468)']
['system', 'ROUGE-S*', 'Average_P:', '0.38095', '(95%-conf.int.', '0.38095', '-', '0.38095)']
['system', 'ROUGE-S*', 'Average_F:', '0.00924', '(95%-conf.int.', '0.00924', '-', '0.00924)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:21', 'F:8']
['To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', 'To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.']
['system', 'ROUGE-S*', 'Average_R:', '0.08386', '(95%-conf.int.', '0.08386', '-', '0.08386)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.15474', '(95%-conf.int.', '0.15474', '-', '0.15474)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:120', 'F:120']
['Phrase tables were extracted from the IN and OUT training corpora (not the dev as was used for instance weighting models), and phrase pairs in the intersection of the IN and OUT phrase tables were used as positive examples, with two alternate definitions of negative examples: The classifier trained using the 2nd definition had higher accuracy on a development set.', 'We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.', u'Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c\u03bb(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.']
['system', 'ROUGE-S*', 'Average_R:', '0.02541', '(95%-conf.int.', '0.02541', '-', '0.02541)']
['system', 'ROUGE-S*', 'Average_P:', '0.05974', '(95%-conf.int.', '0.05974', '-', '0.05974)']
['system', 'ROUGE-S*', 'Average_F:', '0.03565', '(95%-conf.int.', '0.03565', '-', '0.03565)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:703', 'F:42']
['In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', 'Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.']
['system', 'ROUGE-S*', 'Average_R:', '0.01739', '(95%-conf.int.', '0.01739', '-', '0.01739)']
['system', 'ROUGE-S*', 'Average_P:', '0.23077', '(95%-conf.int.', '0.23077', '-', '0.23077)']
['system', 'ROUGE-S*', 'Average_F:', '0.03235', '(95%-conf.int.', '0.03235', '-', '0.03235)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:78', 'F:18']
['We focus here instead on adapting the two most important features: the language model (LM), which estimates the probability p(wIh) of a target word w following an ngram h; and the translation models (TM) p(slt) and p(t1s), which give the probability of source phrase s translating to target phrase t, and vice versa.']
['In future work we plan to try this approach with more competitive SMT systems, and to extend instance weighting to other standard SMT components such as the LM, lexical phrase weights, and lexicalized distortion.', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.', 'Mixing, smoothing, and instance-feature weights are learned at the same time using an efficient maximum-likelihood procedure that relies on only a small in-domain development corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00188', '(95%-conf.int.', '0.00188', '-', '0.00188)']
['system', 'ROUGE-S*', 'Average_P:', '0.00855', '(95%-conf.int.', '0.00855', '-', '0.00855)']
['system', 'ROUGE-S*', 'Average_F:', '0.00308', '(95%-conf.int.', '0.00308', '-', '0.00308)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:351', 'F:3']
['In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', 'Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.']
['system', 'ROUGE-S*', 'Average_R:', '0.00290', '(95%-conf.int.', '0.00290', '-', '0.00290)']
['system', 'ROUGE-S*', 'Average_P:', '0.01429', '(95%-conf.int.', '0.01429', '-', '0.01429)']
['system', 'ROUGE-S*', 'Average_F:', '0.00482', '(95%-conf.int.', '0.00482', '-', '0.00482)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:210', 'F:3']
['Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.']
[u'The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair\u2019s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.', 'Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.']
['system', 'ROUGE-S*', 'Average_R:', '0.05065', '(95%-conf.int.', '0.05065', '-', '0.05065)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.09642', '(95%-conf.int.', '0.09642', '-', '0.09642)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:78', 'F:78']
['Sentence pairs are the natural instances for SMT, but sentences often contain a mix of domain-specific and general language.', 'Our second contribution is to apply instance weighting at the level of phrase pairs.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.', u'Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c\u03bb(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.']
['system', 'ROUGE-S*', 'Average_R:', '0.02783', '(95%-conf.int.', '0.02783', '-', '0.02783)']
['system', 'ROUGE-S*', 'Average_P:', '0.30065', '(95%-conf.int.', '0.30065', '-', '0.30065)']
['system', 'ROUGE-S*', 'Average_F:', '0.05094', '(95%-conf.int.', '0.05094', '-', '0.05094)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:153', 'F:46']
['In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', 'Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.']
['system', 'ROUGE-S*', 'Average_R:', '0.00290', '(95%-conf.int.', '0.00290', '-', '0.00290)']
['system', 'ROUGE-S*', 'Average_P:', '0.01429', '(95%-conf.int.', '0.01429', '-', '0.01429)']
['system', 'ROUGE-S*', 'Average_F:', '0.00482', '(95%-conf.int.', '0.00482', '-', '0.00482)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:210', 'F:3']
0.334359996285 0.0241666663981 0.0435622217382





input/ref/Task1/A97-1014_sweta.csv
input/res/Task1/A97-1014.csv
parsing: input/ref/Task1/A97-1014_sweta.csv
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["168'"]
168'
['168']
parsed_discourse_facet ['method_citation']
<S sid="165" ssid="7">In addition the approach provides empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linea.rizations, and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.</S>
original cit marker offset is 0
new cit marker offset is 0



["165'"]
165'
['165']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="26">This amount of data suffices as training material to reliably assign the grammatical functions if the user determines the elements of a phrase and its type (step 1 of the list above).</S>
original cit marker offset is 0
new cit marker offset is 0



["145'"]
145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.<
original cit marker offset is 0
new cit marker offset is 0



["15'"]
15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="31">Apart from this rather technical problem, two further arguments speak against phrase structure as the structural pivot of the annotation scheme: Finally, the structural handling of free word order means stating well-formedness constraints on structures involving many trace-filler dependencies, which has proved tedious.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
<S sid="47" ssid="37">Argument structure can be represented in terms of unordered trees (with crossing branches).</S>
original cit marker offset is 0
new cit marker offset is 0



["47'"]
47'
['47']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
 <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["160'"]
160'
['160']
parsed_discourse_facet ['method_citation']
<S sid="166" ssid="8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
<S sid="166" ssid="8">Syntactically annotated corpora of German have been missing until now.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="24">Sentences annotated in previous steps are used as training material for further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
  <S sid="71" ssid="16">However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["71'"]
71'
['71']
parsed_discourse_facet ['method_citation']
  <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A97-1014.csv
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'0'"]
'100'
'12'
'0'
['100', '12', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'0'"]
'100'
'12'
'0'
['100', '12', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="47">Argument structure can be represented in terms of unordered trees (with crossing branches).</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'47'", "'12'"]
'100'
'47'
'12'
['100', '47', '12']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="74">During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'74'"]
'100'
'12'
'74'
['100', '12', '74']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="47">Argument structure can be represented in terms of unordered trees (with crossing branches).</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'0'", "'100'"]
'47'
'0'
'100'
['47', '0', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S ssid="1" sid="74">During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'4'", "'74'"]
'100'
'4'
'74'
['100', '4', '74']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="114">Fig.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'114'", "'12'"]
'100'
'114'
'12'
['100', '114', '12']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="14">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'0'", "'14'"]
'4'
'0'
'14'
['4', '0', '14']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'0'", "'4'"]
'100'
'0'
'4'
['100', '0', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="21">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'0'", "'21'"]
'100'
'0'
'21'
['100', '0', '21']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S ssid="1" sid="14">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'4'", "'14'"]
'12'
'4'
'14'
['12', '4', '14']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="114">Fig.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'114'"]
'100'
'12'
'114'
['100', '12', '114']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="170">Our annotation tool supplies efficient manipulation and immediate visualization of argument structures.</S><S ssid="1" sid="162">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'170'", "'162'"]
'100'
'170'
'162'
['100', '170', '162']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="105">In addition, a. number of clear-cut NP cornponents can be defined outside that, juxtapositional kernel: pre- and postnominal genitives (GL, GR), relative clauses (RC), clausal and sentential complements (OC).</S><S ssid="1" sid="21">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'105'", "'21'"]
'100'
'105'
'21'
['100', '105', '21']
parsed_discourse_facet ['method_citation']
['In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.']
['We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc.', "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction."]
['system', 'ROUGE-S*', 'Average_R:', '0.00085', '(95%-conf.int.', '0.00085', '-', '0.00085)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00158', '(95%-conf.int.', '0.00158', '-', '0.00158)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:91', 'F:1']
['In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.']
['Fig.', 'We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:136', 'P:91', 'F:0']
['These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.']
['An Annotation Scheme for Free Word Order Languages', 'We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction."]
['system', 'ROUGE-S*', 'Average_R:', '0.00308', '(95%-conf.int.', '0.00308', '-', '0.00308)']
['system', 'ROUGE-S*', 'Average_P:', '0.04762', '(95%-conf.int.', '0.04762', '-', '0.04762)']
['system', 'ROUGE-S*', 'Average_F:', '0.00578', '(95%-conf.int.', '0.00578', '-', '0.00578)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:325', 'P:21', 'F:1']
['Sentences annotated in previous steps are used as training material for further processing.']
['Fig.', 'We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:136', 'P:21', 'F:0']
['However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.']
['We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.', 'Our annotation tool supplies efficient manipulation and immediate visualization of argument structures.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:28', 'F:0']
['For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).']
['We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'Disambiguation is based on human processing skills (cf.', 'In addition, a. number of clear-cut NP cornponents can be defined outside that, juxtapositional kernel: pre- and postnominal genitives (GL, GR), relative clauses (RC), clausal and sentential complements (OC).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:55', 'F:0']
['This amount of data suffices as training material to reliably assign the grammatical functions if the user determines the elements of a phrase and its type (step 1 of the list above).']
['Argument structure can be represented in terms of unordered trees (with crossing branches).', 'We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00362', '(95%-conf.int.', '0.00362', '-', '0.00362)']
['system', 'ROUGE-S*', 'Average_P:', '0.00735', '(95%-conf.int.', '0.00735', '-', '0.00735)']
['system', 'ROUGE-S*', 'Average_F:', '0.00485', '(95%-conf.int.', '0.00485', '-', '0.00485)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:276', 'P:136', 'F:1']
['Syntactically annotated corpora of German have been missing until now.']
['Realworld texts annotated with different strata of linguistic information can be used for grammar induction.', "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.", 'Corpora annotated with syntactic structures are commonly referred to as trctbank.5.']
['system', 'ROUGE-S*', 'Average_R:', '0.00794', '(95%-conf.int.', '0.00794', '-', '0.00794)']
['system', 'ROUGE-S*', 'Average_P:', '0.30000', '(95%-conf.int.', '0.30000', '-', '0.30000)']
['system', 'ROUGE-S*', 'Average_F:', '0.01546', '(95%-conf.int.', '0.01546', '-', '0.01546)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:10', 'F:3']
['Syntactically annotated corpora of German have been missing until now.']
['An Annotation Scheme for Free Word Order Languages', 'We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'Disambiguation is based on human processing skills (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:190', 'P:10', 'F:0']
['In addition the approach provides empirical material for psycholinguistic investigation, since preferences for the choice of certain syntactic constructions, linea.rizations, and attachments that have been observed in online experiments of language production and comprehension can now be put in relation with the frequency of these alternatives in larger amounts of texts.']
['An Annotation Scheme for Free Word Order Languages', 'We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00433', '(95%-conf.int.', '0.00433', '-', '0.00433)']
['system', 'ROUGE-S*', 'Average_P:', '0.00308', '(95%-conf.int.', '0.00308', '-', '0.00308)']
['system', 'ROUGE-S*', 'Average_F:', '0.00360', '(95%-conf.int.', '0.00360', '-', '0.00360)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:325', 'F:1']
['Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.']
['We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.', 'During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc.']
['system', 'ROUGE-S*', 'Average_R:', '0.00101', '(95%-conf.int.', '0.00101', '-', '0.00101)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00183', '(95%-conf.int.', '0.00183', '-', '0.00183)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:105', 'F:1']
0.0344145451417 0.00189363634642 0.00300909088174





input/ref/Task1/P11-1060_aakansha.csv
input/res/Task1/P11-1060.csv
parsing: input/ref/Task1/P11-1060_aakansha.csv
<S sid="11" ssid="7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'"]
'2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="5">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



["'112'"]
'112'
['112']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="11">Although a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">However, we still model the logical form (now as a latent variable) to capture the complexities of language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="12">It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="18">The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'"]
'106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="21">The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="88" ssid="64">Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets, a restrictor A and a nuclear scope B.</S>
original cit marker offset is 0
new cit marker offset is 0



["'88'"]
'88'
['88']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'171'"]
'171'
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'2'"]
'100'
'21'
'2'
['100', '21', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="23">We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'21'", "'23'"]
'2'
'21'
'23'
['2', '21', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'2'"]
'0'
'21'
'2'
['0', '21', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'25'"]
'21'
'0'
'25'
['21', '0', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'106'"]
'21'
'0'
'106'
['21', '0', '106']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'100'"]
'21'
'0'
'100'
['21', '0', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'100'"]
'0'
'21'
'100'
['0', '21', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'100'", "'2'"]
'21'
'100'
'2'
['21', '100', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'25'"]
'21'
'0'
'25'
['21', '0', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="23">We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'23'"]
'0'
'21'
'23'
['0', '21', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'25'"]
'21'
'0'
'25'
['21', '0', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'25'"]
'0'
'21'
'25'
['0', '21', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="174">598</S><S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'174'", "'106'"]
'21'
'174'
'106'
['21', '174', '106']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'21'", "'100'"]
'106'
'21'
'100'
['106', '21', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="23">We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'0'", "'23'"]
'100'
'0'
'23'
['100', '0', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'106'"]
'21'
'0'
'106'
['21', '0', '106']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'0'"]
'100'
'21'
'0'
['100', '21', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'100'"]
'0'
'21'
'100'
['0', '21', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="12">We represent logical forms z as labeled trees, induced automatically from (x, y) pairs.</S><S ssid="1" sid="17">The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'12'", "'17'"]
'21'
'12'
'17'
['21', '12', '17']
parsed_discourse_facet ['method_citation']
['The denotation JzKw (z evaluated on w) is the set of consistent values of the root node (see Figure 2 for an example).']
[u'Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.', u'Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(\u03b8) = E(x,y)ED log p\u03b8(JzKw = y  |x, z \u2208 ZL(x)) \u2212 \u03bbk\u03b8k22, which sums over all DCS trees z that evaluate to the target answer y.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00266', '(95%-conf.int.', '0.00266', '-', '0.00266)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00512', '(95%-conf.int.', '0.00512', '-', '0.00512)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:45', 'F:3']
['The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['Learning Dependency-Based Compositional Semantics', u'Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(\u03b8) = E(x,y)ED log p\u03b8(JzKw = y  |x, z \u2208 ZL(x)) \u2212 \u03bbk\u03b8k22, which sums over all DCS trees z that evaluate to the target answer y.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.12805', '(95%-conf.int.', '0.12805', '-', '0.12805)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.22703', '(95%-conf.int.', '0.22703', '-', '0.22703)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:105', 'F:105']
['The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['Learning Dependency-Based Compositional Semantics', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).', 'We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).']
['system', 'ROUGE-S*', 'Average_R:', '0.22581', '(95%-conf.int.', '0.22581', '-', '0.22581)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.36842', '(95%-conf.int.', '0.36842', '-', '0.36842)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:105', 'F:105']
['Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.']
[u'Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.', 'In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00256', '(95%-conf.int.', '0.00256', '-', '0.00256)']
['system', 'ROUGE-S*', 'Average_P:', '0.00667', '(95%-conf.int.', '0.00667', '-', '0.00667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00370', '(95%-conf.int.', '0.00370', '-', '0.00370)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:300', 'F:2']
['This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.']
['We represent logical forms z as labeled trees, induced automatically from (x, y) pairs.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).', 'The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner.']
['system', 'ROUGE-S*', 'Average_R:', '0.05546', '(95%-conf.int.', '0.05546', '-', '0.05546)']
['system', 'ROUGE-S*', 'Average_P:', '0.17368', '(95%-conf.int.', '0.17368', '-', '0.17368)']
['system', 'ROUGE-S*', 'Average_F:', '0.08408', '(95%-conf.int.', '0.08408', '-', '0.08408)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:190', 'F:33']
['Although a DCS tree is a logical form, note that it looks like a syntactic dependency tree with predicates in place of words.']
['Learning Dependency-Based Compositional Semantics', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).', 'We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.']
['system', 'ROUGE-S*', 'Average_R:', '0.00540', '(95%-conf.int.', '0.00540', '-', '0.00540)']
['system', 'ROUGE-S*', 'Average_P:', '0.07273', '(95%-conf.int.', '0.07273', '-', '0.07273)']
['system', 'ROUGE-S*', 'Average_F:', '0.01005', '(95%-conf.int.', '0.01005', '-', '0.01005)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:55', 'F:4']
['The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.']
['Learning Dependency-Based Compositional Semantics', u'Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(\u03b8) = E(x,y)ED log p\u03b8(JzKw = y  |x, z \u2208 ZL(x)) \u2212 \u03bbk\u03b8k22, which sums over all DCS trees z that evaluate to the target answer y.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00244', '(95%-conf.int.', '0.00244', '-', '0.00244)']
['system', 'ROUGE-S*', 'Average_P:', '0.03030', '(95%-conf.int.', '0.03030', '-', '0.03030)']
['system', 'ROUGE-S*', 'Average_F:', '0.00451', '(95%-conf.int.', '0.00451', '-', '0.00451)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:66', 'F:2']
['Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.']
[u'Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.', 'In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00128', '(95%-conf.int.', '0.00128', '-', '0.00128)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00240', '(95%-conf.int.', '0.00240', '-', '0.00240)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:55', 'F:1']
['Generalized Quantification (d.ri = Q) Generalized quantifiers are predicates on two sets, a restrictor A and a nuclear scope B.']
['Learning Dependency-Based Compositional Semantics', u'Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:45', 'F:0']
0.263136663743 0.0470733328103 0.078367776907





input/ref/Task1/P08-1102_sweta.csv
input/res/Task1/P08-1102.csv
parsing: input/ref/Task1/P08-1102_sweta.csv
<S sid="21" ssid="17">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="24">A subsequence of boundary-POS labelling result indicates a word with POS t only if the boundary tag sequence composed of its boundary part conforms to s or bm*e style, and all POS tags in its POS part equal to t. For example, a tag sequence b NN m NN e NN represents a threecharacter word with POS tag NN.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="43" ssid="15">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["43'"]
43'
['43']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



["92'"]
92'
['92']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="5">To segment and tag a character sequence, there are two strategies to choose: performing POS tagging following segmentation; or joint segmentation and POS tagging (Joint S&amp;T).</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="5">In following subsections, we describe the feature templates and the perceptron training algorithm.</S>
    <S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["33'", "'34'"]
33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="15">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="4">By maintaining a stack of size N at each position i of the sequence, we can preserve the top N best candidate labelled results of subsequence C1:i during decoding.</S>
original cit marker offset is 0
new cit marker offset is 0



["79'"]
79'
['79']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="7">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["96'"]
96'
['96']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="16" ssid="12">Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="7">To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'21'"]
'1'
'0'
'21'
['1', '0', '21']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'96'", "'60'"]
'0'
'96'
'60'
['0', '96', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'60'", "'21'"]
'0'
'60'
'21'
['0', '60', '21']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'21'"]
'0'
'1'
'21'
['0', '1', '21']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'3'", "'60'"]
'21'
'3'
'60'
['21', '3', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'60'"]
'21'
'0'
'60'
['21', '0', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="64">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'64'"]
'0'
'21'
'64'
['0', '21', '64']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'60'"]
'0'
'21'
'60'
['0', '21', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="24">It is a better idea to perform segmentation and POS tagging jointly in a uniform framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'24'"]
'1'
'0'
'24'
['1', '0', '24']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'96'"]
'0'
'1'
'96'
['0', '1', '96']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'21'"]
'0'
'1'
'21'
['0', '1', '21']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S ssid="1" sid="72">Suppose the conditional probability Pr(t|w) describes the probability that the word w is labelled as the POS t, while Pr(w|t) describes the probability that the POS t generates the word w, then P(T|W) can be approximated by: Pr(w|t) and Pr(t|w) can be easily acquired by Maximum Likelihood Estimates (MLE) over the corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'96'", "'72'"]
'0'
'96'
'72'
['0', '96', '72']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'96'", "'60'"]
'0'
'96'
'60'
['0', '96', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="2">With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'2'"]
'0'
'1'
'2'
['0', '1', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S><S ssid="1" sid="64">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'60'", "'64'"]
'0'
'60'
'64'
['0', '60', '64']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="2">With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'2'"]
'1'
'0'
'2'
['1', '0', '2']
parsed_discourse_facet ['method_citation']
Length 0 input/ref/Task1/P08-1102_sweta.csv
0.0 0.0 0.0





input/ref/Task1/D09-1092_sweta.csv
input/res/Task1/D09-1092.csv
parsing: input/ref/Task1/D09-1092_sweta.csv
 <S sid="32" ssid="8">Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="5">Additionally, PLTM assumes that each &#8220;topic&#8221; consists of a set of discrete distributions over words&#8212;one for each language l = 1, ... , L. In other words, rather than using a single set of topics &#934; = {&#966;1, ... , &#966;T}, as in LDA, there are L sets of language-specific topics, &#934;1, ... , &#934;L, each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter &#946;l.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



["138'"]
138'
['138']
parsed_discourse_facet ['method_citation']
 <S sid="196" ssid="5">When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>
original cit marker offset is 0
new cit marker offset is 0



["196'"]
196'
['196']
parsed_discourse_facet ['method_citation']
<S sid="111" ssid="60">In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts &#8211; i.e., we put each of these documents in a single-document tuple.</S>
original cit marker offset is 0
new cit marker offset is 0



["111'"]
111'
['111']
parsed_discourse_facet ['method_citation']
<S sid="128" ssid="77">Although the PLTM is clearly not a substitute for a machine translation system&#8212;it has no way to represent syntax or even multi-word phrases&#8212;it is clear from the examples in figure 2 that the sets of high probability words in different languages for a given topic are likely to include translations.</S>
original cit marker offset is 0
new cit marker offset is 0



["128'"]
128'
['128']
parsed_discourse_facet ['method_citation']
<S sid="122" ssid="71">Divergence drops significantly when the proportion of &#8220;glue&#8221; tuples increases from 0.01 to 0.25.</S>
original cit marker offset is 0
new cit marker offset is 0



["122'"]
122'
['122']
parsed_discourse_facet ['method_citation']
 <S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
 <S sid="110" ssid="59">Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S>
original cit marker offset is 0
new cit marker offset is 0



["110'"]
110'
['110']
parsed_discourse_facet ['method_citation']
<S sid="30" ssid="6">However, they evaluate their model on only two languages (English and Chinese), and do not use the model to detect differences between languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["30'"]
30'
['30']
parsed_discourse_facet ['method_citation']
    <S sid="146" ssid="95">In addition to enhancing lexicons by aligning topic-specific vocabulary, PLTM may also be useful for adapting machine translation systems to new domains by finding translations or near translations in an unstructured corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["146'"]
146'
['146']
parsed_discourse_facet ['method_citation']
<S sid="77" ssid="26">Maximum topic probability in document Although the posterior distribution over topics for each tuple is not concentrated on one topic, it is worth checking that this is not simply because the model is assigning a single topic to the 1We use the R density function. tokens in each of the languages.</
original cit marker offset is 0
new cit marker offset is 0



["77'"]
77'
['77']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="105">We use both Jensen-Shannon divergence and cosine distance.</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="7">They also provide little analysis of the differences between polylingual and single-language topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>
original cit marker offset is 0
new cit marker offset is 0



["19'"]
19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="131" ssid="80">We evaluate sets of high-probability words in each topic and multilingual &#8220;synsets&#8221; by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["131'"]
131'
['131']
parsed_discourse_facet ['method_citation']
<S sid="196" ssid="5">When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.</S>
original cit marker offset is 0
new cit marker offset is 0



["196'"]
196'
['196']
parsed_discourse_facet ['method_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["192'"]
192'
['192']
parsed_discourse_facet ['method_citation']
<S sid="195" ssid="4">Additionally, PLTM can support the creation of bilingual lexica for low resource language pairs, providing candidate translations for more computationally intense alignment processes without the sentence-aligned translations typically used in such tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["195'"]
195'
['195']
parsed_discourse_facet ['method_citation']
<S sid="6" ssid="2">Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["6'"]
6'
['6']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'0'"]
'3'
'192'
'0'
['3', '192', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'0'"]
'3'
'192'
'0'
['3', '192', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'185'"]
'3'
'192'
'185'
['3', '192', '185']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'9'"]
'0'
'192'
'9'
['0', '192', '9']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'185'", "'3'"]
'192'
'185'
'3'
['192', '185', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'185'", "'3'"]
'192'
'185'
'3'
['192', '185', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'3'"]
'0'
'192'
'3'
['0', '192', '3']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="148">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'3'", "'148'"]
'192'
'3'
'148'
['192', '3', '148']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'9'", "'192'"]
'0'
'9'
'192'
['0', '9', '192']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'3'", "'185'"]
'192'
'3'
'185'
['192', '3', '185']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'192'"]
'0'
'3'
'192'
['0', '3', '192']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'9'"]
'0'
'192'
'9'
['0', '192', '9']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'185'", "'9'", "'192'"]
'185'
'9'
'192'
['185', '9', '192']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="153">In order to make the task more difficult, we train a relatively coarse-grained PLTM with 50 topics on the training set.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="19">We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>
original cit marker offset is 0
new cit marker offset is 0



["'153'", "'192'", "'19'"]
'153'
'192'
'19'
['153', '192', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'0'"]
'3'
'192'
'0'
['3', '192', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'3'", "'0'"]
'192'
'3'
'0'
['192', '3', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'0'", "'3'"]
'192'
'0'
'3'
['192', '0', '3']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'3'"]
'0'
'192'
'3'
['0', '192', '3']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'9'", "'3'"]
'192'
'9'
'3'
['192', '9', '3']
parsed_discourse_facet ['aim_citation']
['When applied to comparable document collections such as Wikipedia, PLTM supports data-driven analysis of differences and similarities across all languages for readers who understand any one language.']
['In this paper, we present the polylingual topic model (PLTM).', 'Polylingual Topic Models', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.01170', '(95%-conf.int.', '0.01170', '-', '0.01170)']
['system', 'ROUGE-S*', 'Average_P:', '0.01667', '(95%-conf.int.', '0.01667', '-', '0.01667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01375', '(95%-conf.int.', '0.01375', '-', '0.01375)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:171', 'P:120', 'F:2']
['We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.']
['We introduce a polylingual topic model that discovers topics aligned across multiple languages.', u'Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission \u03b4\u03b9\u03b1\u03c3\u03c4\u03b7\u03bc\u03b9\u03ba\u03cc sts nasa \u03b1\u03b3\u03b3\u03bb small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimm\u0160inen space lento spatiale mission orbite mars satellite spatial \u05ea\u05d9\u05e0\u05db\u05d5\u05ea \u05d0 \u05e8\u05d5\u05d3\u05db \u05dc\u05dc \u05d7 \u05e5\u05e8 \u05d0\u05d4 \u05dc\u05dc\u05d7\u05d4 spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0441\u043e\u044e\u0437 \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0441\u043f\u0443\u0442\u043d\u0438\u043a \u0441\u0442\u0430\u043d\u0446\u0438\u0438 uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la \u03b9\u03c3\u03c0\u03b1\u03bd\u03af\u03b1\u03c2 \u03b9\u03c3\u03c0\u03b1\u03bd\u03af\u03b1 de \u03b9\u03c3\u03c0\u03b1\u03bd\u03cc\u03c2 \u03bd\u03c4\u03b5 \u03bc\u03b1\u03b4\u03c1\u03af\u03c4\u03b7 de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpa\u0144ski hiszpanii la juan y \u0434\u0435 \u043c\u0430\u0434\u0440\u0438\u0434 \u0438\u0441\u043f\u0430\u043d\u0438\u0438 \u0438\u0441\u043f\u0430\u043d\u0438\u044f \u0438\u0441\u043f\u0430\u043d\u0441\u043a\u0438\u0439 de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk \u03c0\u03bf\u03b9\u03b7\u03c4\u03ae\u03c2 \u03c0\u03bf\u03af\u03b7\u03c3\u03b7 \u03c0\u03bf\u03b9\u03b7\u03c4\u03ae \u03ad\u03c1\u03b3\u03bf \u03c0\u03bf\u03b9\u03b7\u03c4\u03ad\u03c2 \u03c0\u03bf\u03b9\u03ae\u03bc\u03b1\u03c4\u03b1 poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:12880', 'P:78', 'F:0']
['Topic models have been used for analyzing topic trends in research literature (Mann et al., 2006; Hall et al., 2008), inferring captions for images (Blei and Jordan, 2003), social network analysis in email (McCallum et al., 2005), and expanding queries with topically related words in information retrieval (Wei and Croft, 2006).']
['In this paper, we present the polylingual topic model (PLTM).', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.02000', '(95%-conf.int.', '0.02000', '-', '0.02000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.01290', '(95%-conf.int.', '0.01290', '-', '0.01290)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:300', 'P:630', 'F:6']
['Additionally, PLTM assumes that each &#8220;topic&#8221; consists of a set of discrete distributions over words&#8212;one for each language l = 1, ... , L. In other words, rather than using a single set of topics &#934; = {&#966;1, ... , &#966;T}, as in LDA, there are L sets of language-specific topics, &#934;1, ... , &#934;L, each of which is drawn from a language-specific symmetric Dirichlet with concentration parameter &#946;l.']
['Polylingual Topic Models', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.06061', '(95%-conf.int.', '0.06061', '-', '0.06061)']
['system', 'ROUGE-S*', 'Average_P:', '0.01991', '(95%-conf.int.', '0.01991', '-', '0.01991)']
['system', 'ROUGE-S*', 'Average_F:', '0.02998', '(95%-conf.int.', '0.02998', '-', '0.02998)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:703', 'F:14']
['Divergence drops significantly when the proportion of &#8220;glue&#8221; tuples increases from 0.01 to 0.25.']
['Polylingual Topic Models', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:78', 'F:0']
['In order to simulate this scenario we create a set of variations of the EuroParl corpus by treating some documents as if they have no parallel/comparable texts &#8211; i.e., we put each of these documents in a single-document tuple.']
['We introduce a polylingual topic model that discovers topics aligned across multiple languages.', u'Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission \u03b4\u03b9\u03b1\u03c3\u03c4\u03b7\u03bc\u03b9\u03ba\u03cc sts nasa \u03b1\u03b3\u03b3\u03bb small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimm\u0160inen space lento spatiale mission orbite mars satellite spatial \u05ea\u05d9\u05e0\u05db\u05d5\u05ea \u05d0 \u05e8\u05d5\u05d3\u05db \u05dc\u05dc \u05d7 \u05e5\u05e8 \u05d0\u05d4 \u05dc\u05dc\u05d7\u05d4 spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0441\u043e\u044e\u0437 \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0441\u043f\u0443\u0442\u043d\u0438\u043a \u0441\u0442\u0430\u043d\u0446\u0438\u0438 uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la \u03b9\u03c3\u03c0\u03b1\u03bd\u03af\u03b1\u03c2 \u03b9\u03c3\u03c0\u03b1\u03bd\u03af\u03b1 de \u03b9\u03c3\u03c0\u03b1\u03bd\u03cc\u03c2 \u03bd\u03c4\u03b5 \u03bc\u03b1\u03b4\u03c1\u03af\u03c4\u03b7 de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpa\u0144ski hiszpanii la juan y \u0434\u0435 \u043c\u0430\u0434\u0440\u0438\u0434 \u0438\u0441\u043f\u0430\u043d\u0438\u0438 \u0438\u0441\u043f\u0430\u043d\u0438\u044f \u0438\u0441\u043f\u0430\u043d\u0441\u043a\u0438\u0439 de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk \u03c0\u03bf\u03b9\u03b7\u03c4\u03ae\u03c2 \u03c0\u03bf\u03af\u03b7\u03c3\u03b7 \u03c0\u03bf\u03b9\u03b7\u03c4\u03ae \u03ad\u03c1\u03b3\u03bf \u03c0\u03bf\u03b9\u03b7\u03c4\u03ad\u03c2 \u03c0\u03bf\u03b9\u03ae\u03bc\u03b1\u03c4\u03b1 poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:12880', 'P:171', 'F:0']
['We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.']
['We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.', 'In order to make the task more difficult, we train a relatively coarse-grained PLTM with 50 topics on the training set.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.14444', '(95%-conf.int.', '0.14444', '-', '0.14444)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.25243', '(95%-conf.int.', '0.25243', '-', '0.25243)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:91', 'F:91']
['We evaluate sets of high-probability words in each topic and multilingual &#8220;synsets&#8221; by comparing them to entries in human-constructed bilingual dictionaries, as done by Haghighi et al. (2008).']
['Polylingual Topic Models', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:171', 'F:0']
['Outside of the field of topic modeling, Kawaba et al. (Kawaba et al., 2008) use a Wikipedia-based model to perform sentiment analysis of blog posts.']
['Polylingual Topic Models', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.01299', '(95%-conf.int.', '0.01299', '-', '0.01299)']
['system', 'ROUGE-S*', 'Average_P:', '0.02500', '(95%-conf.int.', '0.02500', '-', '0.02500)']
['system', 'ROUGE-S*', 'Average_F:', '0.01709', '(95%-conf.int.', '0.01709', '-', '0.01709)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:120', 'F:3']
['We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['Polylingual Topic Models', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.19481', '(95%-conf.int.', '0.19481', '-', '0.19481)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.32609', '(95%-conf.int.', '0.32609', '-', '0.32609)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:45', 'F:45']
0.207109997929 0.0444549995555 0.0652239993478





input/ref/Task1/D09-1092_swastika.csv
input/res/Task1/D09-1092.csv
parsing: input/ref/Task1/D09-1092_swastika.csv
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="114" ssid="63">Ideally, the &#8220;glue&#8221; documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.</S>
original cit marker offset is 0
new cit marker offset is 0



['114']
114
['114']
parsed_discourse_facet ['aim_citation']
<S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



['138']
138
['138']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



['55']
55
['55']
parsed_discourse_facet ['method_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['192']
192
['192']
parsed_discourse_facet ['result_citation']
<S sid="119" ssid="68">The lower the divergence, the more similar the distributions are to each other.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['result_citation']
<S sid="148" ssid="97">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="163" ssid="112">Performance continues to improve with longer documents, most likely due to better topic inference.</S>
original cit marker offset is 0
new cit marker offset is 0



['163']
163
['163']
parsed_discourse_facet ['result_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="148" ssid="97">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
<S sid="184" ssid="18">Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.</S>
original cit marker offset is 0
new cit marker offset is 0



['184']
184
['184']
parsed_discourse_facet ['result_citation']
<S sid="193" ssid="2">We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.</S>
original cit marker offset is 0
new cit marker offset is 0



['193']
193
['193']
parsed_discourse_facet ['result_citation']
<S sid="192" ssid="1">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['192']
192
['192']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="54">An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['method_citation']
    <S sid="10" ssid="6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'0'"]
'3'
'192'
'0'
['3', '192', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'0'"]
'3'
'192'
'0'
['3', '192', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'185'"]
'3'
'192'
'185'
['3', '192', '185']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'9'"]
'0'
'192'
'9'
['0', '192', '9']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'185'", "'3'"]
'192'
'185'
'3'
['192', '185', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'185'", "'3'"]
'192'
'185'
'3'
['192', '185', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'3'"]
'0'
'192'
'3'
['0', '192', '3']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="148">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'3'", "'148'"]
'192'
'3'
'148'
['192', '3', '148']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'9'", "'192'"]
'0'
'9'
'192'
['0', '9', '192']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'3'", "'185'"]
'192'
'3'
'185'
['192', '3', '185']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'192'"]
'0'
'3'
'192'
['0', '3', '192']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'9'"]
'0'
'192'
'9'
['0', '192', '9']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'185'", "'9'", "'192'"]
'185'
'9'
'192'
['185', '9', '192']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="153">In order to make the task more difficult, we train a relatively coarse-grained PLTM with 50 topics on the training set.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="19">We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>
original cit marker offset is 0
new cit marker offset is 0



["'153'", "'192'", "'19'"]
'153'
'192'
'19'
['153', '192', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'0'"]
'3'
'192'
'0'
['3', '192', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'3'", "'0'"]
'192'
'3'
'0'
['192', '3', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'0'", "'3'"]
'192'
'0'
'3'
['192', '0', '3']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'3'"]
'0'
'192'
'3'
['0', '192', '3']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'9'", "'3'"]
'192'
'9'
'3'
['192', '9', '3']
parsed_discourse_facet ['aim_citation']
['In this paper, we use two polylingual corpora to answer various critical questions related to polylingual topic models.']
['In this paper, we present the polylingual topic model (PLTM).', 'Polylingual Topic Models', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.05848', '(95%-conf.int.', '0.05848', '-', '0.05848)']
['system', 'ROUGE-S*', 'Average_P:', '0.22222', '(95%-conf.int.', '0.22222', '-', '0.22222)']
['system', 'ROUGE-S*', 'Average_F:', '0.09259', '(95%-conf.int.', '0.09259', '-', '0.09259)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:171', 'P:45', 'F:10']
['We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.']
['We introduce a polylingual topic model that discovers topics aligned across multiple languages.', u'Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission \u03b4\u03b9\u03b1\u03c3\u03c4\u03b7\u03bc\u03b9\u03ba\u03cc sts nasa \u03b1\u03b3\u03b3\u03bb small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimm\u0160inen space lento spatiale mission orbite mars satellite spatial \u05ea\u05d9\u05e0\u05db\u05d5\u05ea \u05d0 \u05e8\u05d5\u05d3\u05db \u05dc\u05dc \u05d7 \u05e5\u05e8 \u05d0\u05d4 \u05dc\u05dc\u05d7\u05d4 spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0441\u043e\u044e\u0437 \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0441\u043f\u0443\u0442\u043d\u0438\u043a \u0441\u0442\u0430\u043d\u0446\u0438\u0438 uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la \u03b9\u03c3\u03c0\u03b1\u03bd\u03af\u03b1\u03c2 \u03b9\u03c3\u03c0\u03b1\u03bd\u03af\u03b1 de \u03b9\u03c3\u03c0\u03b1\u03bd\u03cc\u03c2 \u03bd\u03c4\u03b5 \u03bc\u03b1\u03b4\u03c1\u03af\u03c4\u03b7 de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpa\u0144ski hiszpanii la juan y \u0434\u0435 \u043c\u0430\u0434\u0440\u0438\u0434 \u0438\u0441\u043f\u0430\u043d\u0438\u0438 \u0438\u0441\u043f\u0430\u043d\u0438\u044f \u0438\u0441\u043f\u0430\u043d\u0441\u043a\u0438\u0439 de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk \u03c0\u03bf\u03b9\u03b7\u03c4\u03ae\u03c2 \u03c0\u03bf\u03af\u03b7\u03c3\u03b7 \u03c0\u03bf\u03b9\u03b7\u03c4\u03ae \u03ad\u03c1\u03b3\u03bf \u03c0\u03bf\u03b9\u03b7\u03c4\u03ad\u03c2 \u03c0\u03bf\u03b9\u03ae\u03bc\u03b1\u03c4\u03b1 poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:12880', 'P:78', 'F:0']
['We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).']
['In this paper, we present the polylingual topic model (PLTM).', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.01000', '(95%-conf.int.', '0.01000', '-', '0.01000)']
['system', 'ROUGE-S*', 'Average_P:', '0.02500', '(95%-conf.int.', '0.02500', '-', '0.02500)']
['system', 'ROUGE-S*', 'Average_F:', '0.01429', '(95%-conf.int.', '0.01429', '-', '0.01429)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:300', 'P:120', 'F:3']
['Ideally, the &#8220;glue&#8221; documents in g will be sufficient to align the topics across languages, and will cause comparable documents in S to have similar distributions over topics even though they are modeled independently.']
['Polylingual Topic Models', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.04329', '(95%-conf.int.', '0.04329', '-', '0.04329)']
['system', 'ROUGE-S*', 'Average_P:', '0.08333', '(95%-conf.int.', '0.08333', '-', '0.08333)']
['system', 'ROUGE-S*', 'Average_F:', '0.05698', '(95%-conf.int.', '0.05698', '-', '0.05698)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:120', 'F:10']
['The lower the divergence, the more similar the distributions are to each other.']
['Polylingual Topic Models', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:6', 'F:0']
['The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.']
['We introduce a polylingual topic model that discovers topics aligned across multiple languages.', u'Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission \u03b4\u03b9\u03b1\u03c3\u03c4\u03b7\u03bc\u03b9\u03ba\u03cc sts nasa \u03b1\u03b3\u03b3\u03bb small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimm\u0160inen space lento spatiale mission orbite mars satellite spatial \u05ea\u05d9\u05e0\u05db\u05d5\u05ea \u05d0 \u05e8\u05d5\u05d3\u05db \u05dc\u05dc \u05d7 \u05e5\u05e8 \u05d0\u05d4 \u05dc\u05dc\u05d7\u05d4 spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0441\u043e\u044e\u0437 \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0441\u043f\u0443\u0442\u043d\u0438\u043a \u0441\u0442\u0430\u043d\u0446\u0438\u0438 uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la \u03b9\u03c3\u03c0\u03b1\u03bd\u03af\u03b1\u03c2 \u03b9\u03c3\u03c0\u03b1\u03bd\u03af\u03b1 de \u03b9\u03c3\u03c0\u03b1\u03bd\u03cc\u03c2 \u03bd\u03c4\u03b5 \u03bc\u03b1\u03b4\u03c1\u03af\u03c4\u03b7 de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpa\u0144ski hiszpanii la juan y \u0434\u0435 \u043c\u0430\u0434\u0440\u0438\u0434 \u0438\u0441\u043f\u0430\u043d\u0438\u0438 \u0438\u0441\u043f\u0430\u043d\u0438\u044f \u0438\u0441\u043f\u0430\u043d\u0441\u043a\u0438\u0439 de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk \u03c0\u03bf\u03b9\u03b7\u03c4\u03ae\u03c2 \u03c0\u03bf\u03af\u03b7\u03c3\u03b7 \u03c0\u03bf\u03b9\u03b7\u03c4\u03ae \u03ad\u03c1\u03b3\u03bf \u03c0\u03bf\u03b9\u03b7\u03c4\u03ad\u03c2 \u03c0\u03bf\u03b9\u03ae\u03bc\u03b1\u03c4\u03b1 poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00016', '(95%-conf.int.', '0.00016', '-', '0.00016)']
['system', 'ROUGE-S*', 'Average_P:', '0.01053', '(95%-conf.int.', '0.01053', '-', '0.01053)']
['system', 'ROUGE-S*', 'Average_F:', '0.00031', '(95%-conf.int.', '0.00031', '-', '0.00031)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:12880', 'P:190', 'F:2']
['To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.']
['We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.', 'In order to make the task more difficult, we train a relatively coarse-grained PLTM with 50 topics on the training set.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.08730', '(95%-conf.int.', '0.08730', '-', '0.08730)']
['system', 'ROUGE-S*', 'Average_P:', '0.26190', '(95%-conf.int.', '0.26190', '-', '0.26190)']
['system', 'ROUGE-S*', 'Average_F:', '0.13095', '(95%-conf.int.', '0.13095', '-', '0.13095)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:210', 'F:55']
['Interestingly, we find that almost all languages in our corpus, including several pairs that have historically been in conflict, show average JS divergences of between approximately 0.08 and 0.12 for T = 400, consistent with our findings for EuroParl translations.']
['Polylingual Topic Models', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:231', 'F:0']
['We analyzed the characteristics of PLTM in comparison to monolingual LDA, and demonstrated that it is possible to discover aligned topics.']
['Polylingual Topic Models', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.02597', '(95%-conf.int.', '0.02597', '-', '0.02597)']
['system', 'ROUGE-S*', 'Average_P:', '0.13333', '(95%-conf.int.', '0.13333', '-', '0.13333)']
['system', 'ROUGE-S*', 'Average_F:', '0.04348', '(95%-conf.int.', '0.04348', '-', '0.04348)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:45', 'F:6']
['We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['Polylingual Topic Models', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.19481', '(95%-conf.int.', '0.19481', '-', '0.19481)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.32609', '(95%-conf.int.', '0.32609', '-', '0.32609)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:45', 'F:45']
0.173630998264 0.04200099958 0.0664689993353





input/ref/Task1/J01-2004_sweta.csv
input/res/Task1/J01-2004.csv
parsing: input/ref/Task1/J01-2004_sweta.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



["372'"]
372'
['372']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="28">The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.</S>
    <S sid="41" ssid="29">There will also be a brief review of previous work using syntactic information for language modeling, before we introduce our model in Section 4.</S>
    <S sid="42" ssid="30">Three parse trees: (a) a complete parse tree; (b) a complete parse tree with an explicit stop symbol; and (c) a partial parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["40'", "'41'", "'42'"]
40'
'41'
'42'
['40', '41', '42']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="13">A parser that is not left to right, but which has rooted derivations, e.g., a headfirst parser, will be able to calculate generative joint probabilities for entire strings; however, it will not be able to calculate probabilities for each word conditioned on previously generated words, unless each derivation generates the words in the string in exactly the same order.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="364" ssid="120">We follow Chelba (2000) in dealing with this problem: for parsing purposes, we use the Penn Treebank tokenization; for interpolation with the provided trigram model, and for evaluation, the lattice tokenization is used.</S>
original cit marker offset is 0
new cit marker offset is 0



["364'"]
364'
['364']
parsed_discourse_facet ['method_citation']
<S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



["302'"]
302'
['302']
parsed_discourse_facet ['method_citation']
 <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="231" ssid="135">Since we do not know the POS for the word, we must sum the LAP for all POS For a PCFG G, a stack S = Ao An$ (which we will write AN and a look-ahead terminal item wi, we define the look-ahead probability as follows: We recursively estimate this with two empirically observed conditional probabilities for every nonterminal A,: 13(A, w,a) and P(A, c).</S>
original cit marker offset is 0
new cit marker offset is 0



["231'"]
231'
['231']
parsed_discourse_facet ['method_citation']
<S sid="297" ssid="53">The differences between a k-best and a beam-search parser (not to mention the use of dynamic programming) make a running time difference unsurprising.</S>
original cit marker offset is 0
new cit marker offset is 0



["297'"]
297'
['297']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="37">Statistically based heuristic best-first or beam-search strategies (Caraballo and Charniak 1998; Charniak, Goldwater, and Johnson 1998; Goodman 1997) have yielded an enormous improvement in the quality and speed of parsers, even without any guarantee that the parse returned is, in fact, that with the maximum likelihood for the probability model.</S>
original cit marker offset is 0
new cit marker offset is 0



["133'"]
133'
['133']
parsed_discourse_facet ['method_citation']
<S sid="291" ssid="47">Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["291'"]
291'
['291']
parsed_discourse_facet ['method_citation']
<S sid="355" ssid="111">In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.</S>
original cit marker offset is 0
new cit marker offset is 0



["355'"]
355'
['355']
parsed_discourse_facet ['method_citation']
<S sid="59" ssid="17">A PCFG is a CFG with a probability assigned to each rule; specifically, each righthand side has a probability given the left-hand side of the rule.</S>
original cit marker offset is 0
new cit marker offset is 0



["59'"]
59'
['59']
parsed_discourse_facet ['method_citation']
<S sid="100" ssid="4">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="12">Another approach that uses syntactic structure for language modeling has been to use a shift-reduce parser to &amp;quot;surface&amp;quot; c-commanding phrasal headwords or part-of-speech (POS) tags from arbitrarily far back in the prefix string, for use in a trigram-like model.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
<S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



["31'"]
31'
['31']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="17">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'0'", "'3'"]
'17'
'0'
'3'
['17', '0', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="4">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S><S ssid="1" sid="10">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'10'"]
'0'
'4'
'10'
['0', '4', '10']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="137">Here we will present a parser that uses simple search heuristics of this sort without DP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'137'"]
'3'
'9'
'137'
['3', '9', '137']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="223">We implement this as a beam search.</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'223'", "'100'"]
'0'
'223'
'100'
['0', '223', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'100'"]
'3'
'9'
'100'
['3', '9', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'100'"]
'3'
'9'
'100'
['3', '9', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'100'", "'3'"]
'0'
'100'
'3'
['0', '100', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="4">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S><S ssid="1" sid="10">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'10'"]
'0'
'4'
'10'
['0', '4', '10']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
['Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).']
['A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'Probabilistic Top-Down Parsing and Language Modeling', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.01080', '(95%-conf.int.', '0.01080', '-', '0.01080)']
['system', 'ROUGE-S*', 'Average_P:', '0.02667', '(95%-conf.int.', '0.02667', '-', '0.02667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01537', '(95%-conf.int.', '0.01537', '-', '0.01537)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:300', 'F:8']
['The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.']
['Probabilistic Top-Down Parsing and Language Modeling', 'The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.29847', '(95%-conf.int.', '0.29847', '-', '0.29847)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.45972', '(95%-conf.int.', '0.45972', '-', '0.45972)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:351', 'F:351']
['Since we do not know the POS for the word, we must sum the LAP for all POS For a PCFG G, a stack S = Ao An$ (which we will write AN and a look-ahead terminal item wi, we define the look-ahead probability as follows: We recursively estimate this with two empirically observed conditional probabilities for every nonterminal A,: 13(A, w,a) and P(A, c).']
['A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'Probabilistic Top-Down Parsing and Language Modeling', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:276', 'F:0']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'Probabilistic Top-Down Parsing and Language Modeling', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:91', 'F:0']
['In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.']
['A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:105', 'F:0']
['Also, the parser returns a set of candidate parses, from which we have been choosing the top ranked; if we use an oracle to choose the parse with the highest accuracy from among the candidates (which averaged 70.0 in number per sentence), we find an average labeled precision/recall of 94.1, for sentences of length &lt; 100.']
['A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'Probabilistic Top-Down Parsing and Language Modeling', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.02024', '(95%-conf.int.', '0.02024', '-', '0.02024)']
['system', 'ROUGE-S*', 'Average_P:', '0.03448', '(95%-conf.int.', '0.03448', '-', '0.03448)']
['system', 'ROUGE-S*', 'Average_F:', '0.02551', '(95%-conf.int.', '0.02551', '-', '0.02551)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:435', 'F:15']
['A PCFG is a CFG with a probability assigned to each rule; specifically, each righthand side has a probability given the left-hand side of the rule.']
['A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'Probabilistic Top-Down Parsing and Language Modeling', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:78', 'F:0']
['Three parse trees: (a) a complete parse tree; (b) a complete parse tree with an explicit stop symbol; and (c) a partial parse tree.', 'The following section will provide some background in probabilistic context-free grammars and language modeling for speech recognition.', 'There will also be a brief review of previous work using syntactic information for language modeling, before we introduce our model in Section 4.']
['Probabilistic Top-Down Parsing and Language Modeling', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'This paper will examine language modeling for speech recognition from a natural language processing point of view.']
['system', 'ROUGE-S*', 'Average_R:', '0.06439', '(95%-conf.int.', '0.06439', '-', '0.06439)']
['system', 'ROUGE-S*', 'Average_P:', '0.05397', '(95%-conf.int.', '0.05397', '-', '0.05397)']
['system', 'ROUGE-S*', 'Average_F:', '0.05872', '(95%-conf.int.', '0.05872', '-', '0.05872)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:528', 'P:630', 'F:34']
0.139389998258 0.0492374993845 0.0699149991261





input/ref/Task1/W11-2123_swastika.csv
input/res/Task1/W11-2123.csv
parsing: input/ref/Task1/W11-2123_swastika.csv
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



['52']
52
['52']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="177" ssid="49">However, TRIE partitions storage by n-gram length, so walking the trie reads N disjoint pages.</S>
original cit marker offset is 0
new cit marker offset is 0



['177']
177
['177']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['1']
1
['1']
parsed_discourse_facet ['aim_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="200" ssid="19">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



['200']
200
['200']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="278" ssid="5">We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns.</S>
original cit marker offset is 0
new cit marker offset is 0



['278']
278
['278']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/W11-2123.csv
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="32">Linear probing places at most one entry in each bucket.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'32'"]
'5'
'0'
'32'
['5', '0', '32']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="187">The data structure was populated with 64-bit integers sampled uniformly without replacement.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'187'"]
'5'
'0'
'187'
['5', '0', '187']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'109'"]
'0'
'25'
'109'
['0', '25', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'5'"]
'1'
'0'
'5'
['1', '0', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'109'"]
'5'
'0'
'109'
['5', '0', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S><S ssid="1" sid="7">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'25'", "'7'"]
'5'
'25'
'7'
['5', '25', '7']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'25'"]
'5'
'1'
'25'
['5', '1', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="7">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'0'", "'7'"]
'3'
'0'
'7'
['3', '0', '7']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="7">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'7'"]
'5'
'1'
'7'
['5', '1', '7']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="32">Linear probing places at most one entry in each bucket.</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'32'", "'109'"]
'5'
'32'
'109'
['5', '32', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'5'", "'1'"]
'0'
'5'
'1'
['0', '5', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'25'"]
'5'
'1'
'25'
['5', '1', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'109'", "'5'"]
'1'
'109'
'5'
['1', '109', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="3">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'5'", "'3'"]
'0'
'5'
'3'
['0', '5', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'0'", "'5'"]
'109'
'0'
'5'
['109', '0', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="102">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie), allocating memory all at once (eliminating the need for full pointers), and being easy to compile.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="200">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'", "'0'", "'200'"]
'102'
'0'
'200'
['102', '0', '200']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="9">Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf , returning where the probability p(wn|wn−1 f ) and backoff penalties b(wn−1 i ) are given by an already-estimated model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'9'"]
'5'
'0'
'9'
['5', '0', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S><S ssid="1" sid="3">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'3'", "'5'"]
'109'
'3'
'5'
['109', '3', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'1'"]
'5'
'0'
'1'
['5', '0', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'109'"]
'5'
'0'
'109'
['5', '0', '109']
parsed_discourse_facet ['method_citation']
['The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.', 'Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:120', 'F:0']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.', 'An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.11712', '(95%-conf.int.', '0.11712', '-', '0.11712)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.20968', '(95%-conf.int.', '0.20968', '-', '0.20968)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:78', 'F:78']
['We attain these results using several optimizations: hashing, custom lookup tables, bit-level packing, and state for left-to-right query patterns.']
['Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', 'Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00726', '(95%-conf.int.', '0.00726', '-', '0.00726)']
['system', 'ROUGE-S*', 'Average_P:', '0.10989', '(95%-conf.int.', '0.10989', '-', '0.10989)']
['system', 'ROUGE-S*', 'Average_F:', '0.01361', '(95%-conf.int.', '0.01361', '-', '0.01361)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:91', 'F:10']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['KenLM: Faster and Smaller Language Model Queries', 'Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.02381', '(95%-conf.int.', '0.02381', '-', '0.02381)']
['system', 'ROUGE-S*', 'Average_P:', '0.11538', '(95%-conf.int.', '0.11538', '-', '0.11538)']
['system', 'ROUGE-S*', 'Average_F:', '0.03947', '(95%-conf.int.', '0.03947', '-', '0.03947)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:78', 'F:9']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['KenLM: Faster and Smaller Language Model Queries', 'Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.', 'An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.']
['system', 'ROUGE-S*', 'Average_R:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_P:', '0.10909', '(95%-conf.int.', '0.10909', '-', '0.10909)']
['system', 'ROUGE-S*', 'Average_F:', '0.01752', '(95%-conf.int.', '0.01752', '-', '0.01752)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:55', 'F:6']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.', 'KenLM: Faster and Smaller Language Model Queries', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.22222', '(95%-conf.int.', '0.22222', '-', '0.22222)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.36364', '(95%-conf.int.', '0.36364', '-', '0.36364)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:78', 'F:78']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.', 'KenLM: Faster and Smaller Language Model Queries', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.04558', '(95%-conf.int.', '0.04558', '-', '0.04558)']
['system', 'ROUGE-S*', 'Average_P:', '0.29091', '(95%-conf.int.', '0.29091', '-', '0.29091)']
['system', 'ROUGE-S*', 'Average_F:', '0.07882', '(95%-conf.int.', '0.07882', '-', '0.07882)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:55', 'F:16']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
[u'Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf , returning where the probability p(wn|wn\u22121 f ) and backoff penalties b(wn\u22121 i ) are given by an already-estimated model.', 'KenLM: Faster and Smaller Language Model Queries', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00300', '(95%-conf.int.', '0.00300', '-', '0.00300)']
['system', 'ROUGE-S*', 'Average_P:', '0.03636', '(95%-conf.int.', '0.03636', '-', '0.03636)']
['system', 'ROUGE-S*', 'Average_F:', '0.00555', '(95%-conf.int.', '0.00555', '-', '0.00555)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:55', 'F:2']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['KenLM: Faster and Smaller Language Model Queries', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.', 'The data structure was populated with 64-bit integers sampled uniformly without replacement.']
['system', 'ROUGE-S*', 'Average_R:', '0.05929', '(95%-conf.int.', '0.05929', '-', '0.05929)']
['system', 'ROUGE-S*', 'Average_P:', '0.19231', '(95%-conf.int.', '0.19231', '-', '0.19231)']
['system', 'ROUGE-S*', 'Average_F:', '0.09063', '(95%-conf.int.', '0.09063', '-', '0.09063)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:253', 'P:78', 'F:15']
['The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.']
['The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie), allocating memory all at once (eliminating the need for full pointers), and being easy to compile.', 'KenLM: Faster and Smaller Language Model Queries', 'The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.']
['system', 'ROUGE-S*', 'Average_R:', '0.15385', '(95%-conf.int.', '0.15385', '-', '0.15385)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.26667', '(95%-conf.int.', '0.26667', '-', '0.26667)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:120', 'F:120']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['KenLM: Faster and Smaller Language Model Queries', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.', 'Linear probing places at most one entry in each bucket.']
['system', 'ROUGE-S*', 'Average_R:', '0.04678', '(95%-conf.int.', '0.04678', '-', '0.04678)']
['system', 'ROUGE-S*', 'Average_P:', '0.10256', '(95%-conf.int.', '0.10256', '-', '0.10256)']
['system', 'ROUGE-S*', 'Average_F:', '0.06426', '(95%-conf.int.', '0.06426', '-', '0.06426)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:171', 'P:78', 'F:8']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.', 'An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.11712', '(95%-conf.int.', '0.11712', '-', '0.11712)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.20968', '(95%-conf.int.', '0.20968', '-', '0.20968)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:78', 'F:78']
['Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.']
['An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.', 'This paper presents methods to query N-gram language models, minimizing time and space costs.']
['system', 'ROUGE-S*', 'Average_R:', '0.01345', '(95%-conf.int.', '0.01345', '-', '0.01345)']
['system', 'ROUGE-S*', 'Average_P:', '0.12121', '(95%-conf.int.', '0.12121', '-', '0.12121)']
['system', 'ROUGE-S*', 'Average_F:', '0.02421', '(95%-conf.int.', '0.02421', '-', '0.02421)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:66', 'F:8']
['The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.']
['KenLM: Faster and Smaller Language Model Queries', 'Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:120', 'F:0']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['KenLM: Faster and Smaller Language Model Queries', 'Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00529', '(95%-conf.int.', '0.00529', '-', '0.00529)']
['system', 'ROUGE-S*', 'Average_P:', '0.03636', '(95%-conf.int.', '0.03636', '-', '0.03636)']
['system', 'ROUGE-S*', 'Average_F:', '0.00924', '(95%-conf.int.', '0.00924', '-', '0.00924)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:55', 'F:2']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['KenLM: Faster and Smaller Language Model Queries', 'Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', 'This paper presents methods to query N-gram language models, minimizing time and space costs.']
['system', 'ROUGE-S*', 'Average_R:', '0.02305', '(95%-conf.int.', '0.02305', '-', '0.02305)']
['system', 'ROUGE-S*', 'Average_P:', '0.33333', '(95%-conf.int.', '0.33333', '-', '0.33333)']
['system', 'ROUGE-S*', 'Average_F:', '0.04312', '(95%-conf.int.', '0.04312', '-', '0.04312)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:78', 'F:26']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['KenLM: Faster and Smaller Language Model Queries', 'Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00505', '(95%-conf.int.', '0.00505', '-', '0.00505)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.00957', '(95%-conf.int.', '0.00957', '-', '0.00957)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:55', 'F:5']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.', 'KenLM: Faster and Smaller Language Model Queries', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.04558', '(95%-conf.int.', '0.04558', '-', '0.04558)']
['system', 'ROUGE-S*', 'Average_P:', '0.29091', '(95%-conf.int.', '0.29091', '-', '0.29091)']
['system', 'ROUGE-S*', 'Average_F:', '0.07882', '(95%-conf.int.', '0.07882', '-', '0.07882)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:55', 'F:16']
0.323845553756 0.0498872219451 0.0846938884184





input/ref/Task1/P08-1043_aakansha.csv
input/res/Task1/P08-1043.csv
parsing: input/ref/Task1/P08-1043_aakansha.csv
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="69" ssid="1">We represent all morphological analyses of a given utterance using a lattice structure.</S>
    <S sid="70" ssid="2">Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'69'", "'70'"]
'69'
'70'
['69', '70']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="17">The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="21" ssid="17">Morphological segmentation decisions in our model are delegated to a lexeme-based PCFG and we show that using a simple treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling our model outperforms (Tsarfaty, 2006) and (Cohen and Smith, 2007) on the joint task and achieves state-of-the-art results on a par with current respective standalone models.2</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'94'"]
'94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="11">Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
    <S sid="134" ssid="12">Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training 	similar to (Cohen and Smith, 2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'", "'134'"]
'133'
'134'
['133', '134']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="17">The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="155" ssid="33">Evaluation We use 8 different measures to evaluate the performance of our system on the joint disambiguation task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'4'", "'0'"]
'19'
'4'
'0'
['19', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="186">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'186'", "'19'", "'4'"]
'186'
'19'
'4'
['186', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'0'", "'19'"]
'4'
'0'
'19'
['4', '0', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="186">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'186'"]
'0'
'19'
'186'
['0', '19', '186']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'19'"]
'0'
'4'
'19'
['0', '4', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'4'"]
'0'
'19'
'4'
['0', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="116">We simulate lexical constraints by using an external lexical resource against which we verify whether OOV segments are in fact valid Hebrew lexemes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'116'"]
'0'
'19'
'116'
['0', '19', '116']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'19'"]
'0'
'4'
'19'
['0', '4', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'4'"]
'0'
'19'
'4'
['0', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="117">This heuristics is used to prune all segmentation possibilities involving “lexically improper” segments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'117'"]
'0'
'4'
'117'
['0', '4', '117']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'4'"]
'0'
'19'
'4'
['0', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="188">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'188'"]
'0'
'4'
'188'
['0', '4', '188']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="186">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'186'"]
'0'
'19'
'186'
['0', '19', '186']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="136">Lexicon and OOV Handling Our data-driven morphological-analyzer proposes analyses for unknown tokens as described in Section 5.</S><S ssid="1" sid="133">Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'136'", "'133'"]
'0'
'136'
'133'
['0', '136', '133']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="69">We represent all morphological analyses of a given utterance using a lattice structure.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'69'", "'19'"]
'0'
'69'
'19'
['0', '69', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="135">We construct a mapping from all the space-delimited tokens seen in the training sentences to their corresponding analyses.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'135'", "'19'"]
'4'
'135'
'19'
['4', '135', '19']
parsed_discourse_facet ['method_citation']
['The Input The set of analyses for a token is thus represented as a lattice in which every arc corresponds to a specific lexeme l, as shown in Figure 1.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:78', 'F:0']
['Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['system', 'ROUGE-S*', 'Average_R:', '0.00452', '(95%-conf.int.', '0.00452', '-', '0.00452)']
['system', 'ROUGE-S*', 'Average_P:', '0.03922', '(95%-conf.int.', '0.03922', '-', '0.03922)']
['system', 'ROUGE-S*', 'Average_F:', '0.00811', '(95%-conf.int.', '0.00811', '-', '0.00811)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:153', 'F:6']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', u'This heuristics is used to prune all segmentation possibilities involving \u201clexically improper\u201d segments.', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['system', 'ROUGE-S*', 'Average_R:', '0.01586', '(95%-conf.int.', '0.01586', '-', '0.01586)']
['system', 'ROUGE-S*', 'Average_P:', '0.12500', '(95%-conf.int.', '0.12500', '-', '0.12500)']
['system', 'ROUGE-S*', 'Average_F:', '0.02814', '(95%-conf.int.', '0.02814', '-', '0.02814)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:120', 'F:15']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.']
['system', 'ROUGE-S*', 'Average_R:', '0.18018', '(95%-conf.int.', '0.18018', '-', '0.18018)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.30534', '(95%-conf.int.', '0.30534', '-', '0.30534)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:120', 'F:120']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['system', 'ROUGE-S*', 'Average_R:', '0.09050', '(95%-conf.int.', '0.09050', '-', '0.09050)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.16598', '(95%-conf.int.', '0.16598', '-', '0.16598)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:120', 'F:120']
['Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['system', 'ROUGE-S*', 'Average_R:', '0.23684', '(95%-conf.int.', '0.23684', '-', '0.23684)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.38298', '(95%-conf.int.', '0.38298', '-', '0.38298)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:378', 'F:378']
['We represent all morphological analyses of a given utterance using a lattice structure.', 'Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'We simulate lexical constraints by using an external lexical resource against which we verify whether OOV segments are in fact valid Hebrew lexemes.']
['system', 'ROUGE-S*', 'Average_R:', '0.00901', '(95%-conf.int.', '0.00901', '-', '0.00901)']
['system', 'ROUGE-S*', 'Average_P:', '0.03509', '(95%-conf.int.', '0.03509', '-', '0.03509)']
['system', 'ROUGE-S*', 'Average_F:', '0.01434', '(95%-conf.int.', '0.01434', '-', '0.01434)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:171', 'F:6']
['Evaluation We use 8 different measures to evaluate the performance of our system on the joint disambiguation task.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.', 'We construct a mapping from all the space-delimited tokens seen in the training sentences to their corresponding analyses.']
['system', 'ROUGE-S*', 'Average_R:', '0.00151', '(95%-conf.int.', '0.00151', '-', '0.00151)']
['system', 'ROUGE-S*', 'Average_P:', '0.05556', '(95%-conf.int.', '0.05556', '-', '0.05556)']
['system', 'ROUGE-S*', 'Average_F:', '0.00294', '(95%-conf.int.', '0.00294', '-', '0.00294)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:36', 'F:2']
['Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['system', 'ROUGE-S*', 'Average_R:', '0.28507', '(95%-conf.int.', '0.28507', '-', '0.28507)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.44366', '(95%-conf.int.', '0.44366', '-', '0.44366)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:378', 'F:378']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['system', 'ROUGE-S*', 'Average_R:', '0.09050', '(95%-conf.int.', '0.09050', '-', '0.09050)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.16598', '(95%-conf.int.', '0.16598', '-', '0.16598)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:120', 'F:120']
0.525486994745 0.091398999086 0.151746998483





input/ref/Task1/D10-1044_swastika.csv
input/res/Task1/D10-1044.csv
parsing: input/ref/Task1/D10-1044_swastika.csv
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
<S sid="71" ssid="8">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c&#955;(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



['71']
71
['71']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
<S sid="71" ssid="8">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c&#955;(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



['71']
71
['71']
parsed_discourse_facet ['result_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="9">An easy way to achieve this is to put the domain-specific LMs and TMs into the top-level log-linear model and learn optimal weights with MERT (Och, 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



['45']
45
['45']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



['75']
75
['75']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



['62']
62
['62']
parsed_discourse_facet ['method_citation']
  <S sid="42" ssid="6">The natural baseline approach is to concatenate data from IN and OUT.</S>
original cit marker offset is 0
new cit marker offset is 0



['42']
42
['42']
parsed_discourse_facet ['aim_citation']
<S sid="96" ssid="33">We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.</S>
original cit marker offset is 0
new cit marker offset is 0



['96']
96
['96']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/D10-1044.csv
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="62">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'37'", "'62'"]
'1'
'37'
'62'
['1', '37', '62']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="120">This significantly underperforms log-linear combination.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="3">We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'", "'37'", "'3'"]
'120'
'37'
'3'
['120', '37', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="68">First, we learn weights on individual phrase pairs rather than sentences.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'68'", "'71'"]
'37'
'68'
'71'
['37', '68', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="3">We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'3'", "'37'"]
'1'
'3'
'37'
['1', '3', '37']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="28">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="120">This significantly underperforms log-linear combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'", "'37'", "'120'"]
'28'
'37'
'120'
['28', '37', '120']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="22">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S><S ssid="1" sid="146">The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair’s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'22'", "'146'"]
'37'
'22'
'146'
['37', '22', '146']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="146">The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair’s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'146'"]
'37'
'1'
'146'
['37', '1', '146']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'37'", "'71'"]
'1'
'37'
'71'
['1', '37', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="148">Mixing, smoothing, and instance-feature weights are learned at the same time using an efficient maximum-likelihood procedure that relies on only a small in-domain development corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'148'"]
'37'
'1'
'148'
['37', '1', '148']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="148">Mixing, smoothing, and instance-feature weights are learned at the same time using an efficient maximum-likelihood procedure that relies on only a small in-domain development corpus.</S><S ssid="1" sid="151">In future work we plan to try this approach with more competitive SMT systems, and to extend instance weighting to other standard SMT components such as the LM, lexical phrase weights, and lexicalized distortion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'148'", "'151'"]
'37'
'148'
'151'
['37', '148', '151']
parsed_discourse_facet ['method_citation']
['To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', 'To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.']
['system', 'ROUGE-S*', 'Average_R:', '0.08386', '(95%-conf.int.', '0.08386', '-', '0.08386)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.15474', '(95%-conf.int.', '0.15474', '-', '0.15474)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:120', 'F:120']
['In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', 'Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.']
['system', 'ROUGE-S*', 'Average_R:', '0.00290', '(95%-conf.int.', '0.00290', '-', '0.00290)']
['system', 'ROUGE-S*', 'Average_P:', '0.01429', '(95%-conf.int.', '0.01429', '-', '0.01429)']
['system', 'ROUGE-S*', 'Average_F:', '0.00482', '(95%-conf.int.', '0.00482', '-', '0.00482)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:210', 'F:3']
['Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.']
[u'The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair\u2019s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.', 'Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.']
['system', 'ROUGE-S*', 'Average_R:', '0.05065', '(95%-conf.int.', '0.05065', '-', '0.05065)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.09642', '(95%-conf.int.', '0.09642', '-', '0.09642)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:78', 'F:78']
['To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', 'Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.']
['system', 'ROUGE-S*', 'Average_R:', '0.00290', '(95%-conf.int.', '0.00290', '-', '0.00290)']
['system', 'ROUGE-S*', 'Average_P:', '0.02500', '(95%-conf.int.', '0.02500', '-', '0.02500)']
['system', 'ROUGE-S*', 'Average_F:', '0.00519', '(95%-conf.int.', '0.00519', '-', '0.00519)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:120', 'F:3']
['We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', u'The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair\u2019s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.']
['system', 'ROUGE-S*', 'Average_R:', '0.00584', '(95%-conf.int.', '0.00584', '-', '0.00584)']
['system', 'ROUGE-S*', 'Average_P:', '0.22222', '(95%-conf.int.', '0.22222', '-', '0.22222)']
['system', 'ROUGE-S*', 'Average_F:', '0.01139', '(95%-conf.int.', '0.01139', '-', '0.01139)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:45', 'F:10']
['We used it to score all phrase pairs in the OUT table, in order to provide a feature for the instance-weighting model.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.', u'Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c\u03bb(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.']
['system', 'ROUGE-S*', 'Average_R:', '0.00907', '(95%-conf.int.', '0.00907', '-', '0.00907)']
['system', 'ROUGE-S*', 'Average_P:', '0.33333', '(95%-conf.int.', '0.33333', '-', '0.33333)']
['system', 'ROUGE-S*', 'Average_F:', '0.01767', '(95%-conf.int.', '0.01767', '-', '0.01767)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:45', 'F:15']
['In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', 'Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.']
['system', 'ROUGE-S*', 'Average_R:', '0.00290', '(95%-conf.int.', '0.00290', '-', '0.00290)']
['system', 'ROUGE-S*', 'Average_P:', '0.01429', '(95%-conf.int.', '0.01429', '-', '0.01429)']
['system', 'ROUGE-S*', 'Average_F:', '0.00482', '(95%-conf.int.', '0.00482', '-', '0.00482)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:210', 'F:3']
0.372732851818 0.0225885711059 0.0421499993979





input/ref/Task1/W99-0613_swastika.csv
input/res/Task1/W99-0613.csv
parsing: input/ref/Task1/W99-0613_swastika.csv
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
<S sid="159" ssid="26">To prevent this we &amp;quot;smooth&amp;quot; the confidence by adding a small value, e, to both W+ and W_, giving at = Plugging the value of at from Equ.</S>
original cit marker offset is 0
new cit marker offset is 0



['159']
159
['159']
parsed_discourse_facet ['method_citation']
<S sid="137" ssid="4">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S>
original cit marker offset is 0
new cit marker offset is 0



['137']
137
['137']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['method_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
 <S sid="250" ssid="1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S>
original cit marker offset is 0
new cit marker offset is 0



['250']
250
['250']
parsed_discourse_facet ['result_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



['9']
9
['9']
parsed_discourse_facet ['aim_citation']
    <S sid="36" ssid="30">Roughly speaking, the new algorithm presented in this paper performs a similar search, but instead minimizes a bound on the number of (unlabeled) examples on which two classifiers disagree.</S>
original cit marker offset is 0
new cit marker offset is 0



['36']
36
['36']
parsed_discourse_facet ['result_citation']
<S sid="29" ssid="23">Unfortunately, Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective function.</S>
original cit marker offset is 0
new cit marker offset is 0



['29']
29
['29']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['aim_citation']
    <S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['method_citation']
    <S sid="95" ssid="28">(Specifically, the limit n starts at 5 and increases by 5 at each iteration.)</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['method_citation']
<S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



['213']
213
['213']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'0'", "'179'"]
'137'
'0'
'179'
['137', '0', '179']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="33">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'33'", "'179'"]
'137'
'33'
'179'
['137', '33', '179']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S><S ssid="1" sid="33">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'0'", "'33'"]
'137'
'0'
'33'
['137', '0', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="79">2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'79'", "'0'"]
'137'
'79'
'0'
['137', '79', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'137'", "'0'"]
'179'
'137'
'0'
['179', '137', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="224">The model was parameterized such that the joint probability of a (label, feature-set) pair P(yi, xi) is written as The model assumes that (y, x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'224'"]
'137'
'179'
'224'
['137', '179', '224']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="222">The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'222'", "'179'"]
'137'
'222'
'179'
['137', '222', '179']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
['The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.']
['The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).', 'Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.']
['system', 'ROUGE-S*', 'Average_R:', '0.10256', '(95%-conf.int.', '0.10256', '-', '0.10256)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.18605', '(95%-conf.int.', '0.18605', '-', '0.18605)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:36', 'F:36']
["Unfortunately, Yarowsky's method is not well understood from a theoretical viewpoint: we would like to formalize the notion of redundancy in unlabeled data, and set up the learning task as optimization of some appropriate objective function."]
['The model was parameterized such that the joint probability of a (label, feature-set) pair P(yi, xi) is written as The model assumes that (y, x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00078', '(95%-conf.int.', '0.00078', '-', '0.00078)']
['system', 'ROUGE-S*', 'Average_P:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00143', '(95%-conf.int.', '0.00143', '-', '0.00143)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:120', 'F:1']
['Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00433', '(95%-conf.int.', '0.00433', '-', '0.00433)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00699', '(95%-conf.int.', '0.00699', '-', '0.00699)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:55', 'F:1']
['Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00433', '(95%-conf.int.', '0.00433', '-', '0.00433)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00699', '(95%-conf.int.', '0.00699', '-', '0.00699)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:55', 'F:1']
['Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00433', '(95%-conf.int.', '0.00433', '-', '0.00433)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00699', '(95%-conf.int.', '0.00699', '-', '0.00699)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:55', 'F:1']
['This paper discusses the use of unlabeled examples for the problem of named entity classification.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.01299', '(95%-conf.int.', '0.01299', '-', '0.01299)']
['system', 'ROUGE-S*', 'Average_P:', '0.10714', '(95%-conf.int.', '0.10714', '-', '0.10714)']
['system', 'ROUGE-S*', 'Average_F:', '0.02317', '(95%-conf.int.', '0.02317', '-', '0.02317)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:28', 'F:3']
['To prevent this we &quot;smooth&quot; the confidence by adding a small value, e, to both W+ and W_, giving at = Plugging the value of at from Equ.']
['The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:45', 'F:0']
['(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:630', 'F:0']
['Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.01299', '(95%-conf.int.', '0.01299', '-', '0.01299)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.02098', '(95%-conf.int.', '0.02098', '-', '0.02098)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:55', 'F:3']
0.13606222071 0.0158122220465 0.0280666663548





input/ref/Task1/W06-2932_swastika.csv
input/res/Task1/W06-2932.csv
parsing: input/ref/Task1/W06-2932_swastika.csv
<S sid="86" ssid="8">Unlabeled accuracy for all verbs increases from 71% to 73% and for all conjunctions from 71% to 74%.</S>
original cit marker offset is 
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="54" ssid="2">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



['54']
54
['54']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



['12']
12
['12']
parsed_discourse_facet ['method_citation']
    <S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



['22']
22
['22']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



['41']
41
['41']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="6">Its power lies in the ability to define a rich set of features over parsing decisions, as well as surface level features relative to these decisions.</S>
original cit marker offset is 0
new cit marker offset is 0



['24']
24
['24']
parsed_discourse_facet ['method_citation']
<S sid="79" ssid="1">Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['result_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



['12']
12
['12']
parsed_discourse_facet ['method_citation']
<S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



['43']
43
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="41">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm�1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm−1) in the tree y.</S>
original cit marker offset is 
new cit marker offset is 0



["'0'", "'12'", "'41'"]
'0'
'12'
'41'
['0', '12', '41']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="38">Such a model could easily be trained using the provided training data for each language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'38'"]
'0'
'12'
'38'
['0', '12', '38']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="106">First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'106'"]
'0'
'12'
'106'
['0', '12', '106']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="106">First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'106'"]
'0'
'12'
'106'
['0', '12', '106']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="41">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm�1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm−1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'41'"]
'0'
'12'
'41'
['0', '12', '41']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'1'"]
'0'
'12'
'1'
['0', '12', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'1'"]
'0'
'12'
'1'
['0', '12', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="41">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm�1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm−1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'0'", "'41'"]
'12'
'0'
'41'
['12', '0', '41']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="16">A dependency graph is represented by a set of ordered pairs (i, j) E y in which xj is a dependent and xi is the corresponding head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'16'"]
'0'
'12'
'16'
['0', '12', '16']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="16">A dependency graph is represented by a set of ordered pairs (i, j) E y in which xj is a dependent and xi is the corresponding head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'16'"]
'0'
'12'
'16'
['0', '12', '16']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'1'"]
'0'
'12'
'1'
['0', '12', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="54">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'54'"]
'0'
'12'
'54'
['0', '12', '54']
parsed_discourse_facet ['aim_citation', 'result_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'3'"]
'0'
'12'
'3'
['0', '12', '3']
parsed_discourse_facet ['method_citation']
['An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.']
['Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.', u'To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm\ufffd1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm\u22121) in the tree y.']
['system', 'ROUGE-S*', 'Average_R:', '0.00423', '(95%-conf.int.', '0.00423', '-', '0.00423)']
['system', 'ROUGE-S*', 'Average_P:', '0.02941', '(95%-conf.int.', '0.02941', '-', '0.02941)']
['system', 'ROUGE-S*', 'Average_F:', '0.00739', '(95%-conf.int.', '0.00739', '-', '0.00739)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:136', 'F:4']
['An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.']
['Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.', u'To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm\ufffd1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm\u22121) in the tree y.']
['system', 'ROUGE-S*', 'Average_R:', '0.00423', '(95%-conf.int.', '0.00423', '-', '0.00423)']
['system', 'ROUGE-S*', 'Average_P:', '0.02941', '(95%-conf.int.', '0.02941', '-', '0.02941)']
['system', 'ROUGE-S*', 'Average_F:', '0.00739', '(95%-conf.int.', '0.00739', '-', '0.00739)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:136', 'F:4']
['Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.']
['Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.', 'First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:171', 'F:0']
['Its power lies in the ability to define a rich set of features over parsing decisions, as well as surface level features relative to these decisions.']
['Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.', 'A dependency graph is represented by a set of ordered pairs (i, j) E y in which xj is a dependent and xi is the corresponding head.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:91', 'F:0']
['For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.']
['Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['system', 'ROUGE-S*', 'Average_R:', '0.00178', '(95%-conf.int.', '0.00178', '-', '0.00178)']
['system', 'ROUGE-S*', 'Average_P:', '0.00433', '(95%-conf.int.', '0.00433', '-', '0.00433)']
['system', 'ROUGE-S*', 'Average_F:', '0.00253', '(95%-conf.int.', '0.00253', '-', '0.00253)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:231', 'F:1']
['In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.', 'Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['system', 'ROUGE-S*', 'Average_R:', '0.18803', '(95%-conf.int.', '0.18803', '-', '0.18803)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.31655', '(95%-conf.int.', '0.31655', '-', '0.31655)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:66', 'F:66']
['Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.']
['present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.', 'Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:300', 'F:0']
['To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.']
['Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.', 'A dependency graph is represented by a set of ordered pairs (i, j) E y in which xj is a dependent and xi is the corresponding head.']
['system', 'ROUGE-S*', 'Average_R:', '0.00794', '(95%-conf.int.', '0.00794', '-', '0.00794)']
['system', 'ROUGE-S*', 'Average_P:', '0.00794', '(95%-conf.int.', '0.00794', '-', '0.00794)']
['system', 'ROUGE-S*', 'Average_F:', '0.00794', '(95%-conf.int.', '0.00794', '-', '0.00794)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:378', 'F:3']
['Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.']
['Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.', 'First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:171', 'F:0']
['Although overall unlabeled accuracy is 86%, most verbs and some conjunctions attach to their head words with much lower accuracy: 69% for main verbs, 75% for the verb ser, and 65% for coordinating conjunctions.']
['present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.', 'Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:171', 'F:0']
0.107108998929 0.0206209997938 0.0341799996582





input/ref/Task1/P08-1102_swastika.csv
input/res/Task1/P08-1102.csv
parsing: input/ref/Task1/P08-1102_swastika.csv
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="10">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="8">All feature templates and their instances are shown in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



['36']
36
['36']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="4">We trained a character-based perceptron for Chinese Joint S&amp;T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['32']
32
['32']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
<S sid="97" ssid="8">Figure 3 shows their learning curves depicting the F-measure on the development set after 1 to 10 training iterations.</S>
original cit marker offset is 0
new cit marker offset is 0



['97']
97
['97']
parsed_discourse_facet ['result_citation']
<S sid="91" ssid="2">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['aim_citation']
<S sid="16" ssid="12">Shown in Figure 1, the cascaded model has a two-layer architecture, with a characterbased perceptron as the core combined with other real-valued features such as language models.</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="34" ssid="6">The feature templates we adopted are selected from those of Ng and Low (2004).</S>
original cit marker offset is 0
new cit marker offset is 0



['34']
34
['34']
parsed_discourse_facet ['method_citation']
    <S sid="92" ssid="3">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1102.csv
<S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'21'"]
'1'
'0'
'21'
['1', '0', '21']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'96'", "'60'"]
'0'
'96'
'60'
['0', '96', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'60'", "'21'"]
'0'
'60'
'21'
['0', '60', '21']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'21'"]
'0'
'1'
'21'
['0', '1', '21']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="3">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'3'", "'60'"]
'21'
'3'
'60'
['21', '3', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'60'"]
'21'
'0'
'60'
['21', '0', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="64">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'64'"]
'0'
'21'
'64'
['0', '21', '64']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'60'"]
'0'
'21'
'60'
['0', '21', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="24">It is a better idea to perform segmentation and POS tagging jointly in a uniform framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'24'"]
'1'
'0'
'24'
['1', '0', '24']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'96'"]
'0'
'1'
'96'
['0', '1', '96']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="21">Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'21'"]
'0'
'1'
'21'
['0', '1', '21']
parsed_discourse_facet ['method_citation', 'result_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S ssid="1" sid="72">Suppose the conditional probability Pr(t|w) describes the probability that the word w is labelled as the POS t, while Pr(w|t) describes the probability that the POS t generates the word w, then P(T|W) can be approximated by: Pr(w|t) and Pr(t|w) can be easily acquired by Maximum Likelihood Estimates (MLE) over the corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'96'", "'72'"]
'0'
'96'
'72'
['0', '96', '72']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="96">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'96'", "'60'"]
'0'
'96'
'60'
['0', '96', '60']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="2">With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'2'"]
'0'
'1'
'2'
['0', '1', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="60">The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.</S><S ssid="1" sid="64">In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'60'", "'64'"]
'0'
'60'
'64'
['0', '60', '64']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S><S ssid="1" sid="0">A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</S><S ssid="1" sid="2">With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'2'"]
'1'
'0'
'2'
['1', '0', '2']
parsed_discourse_facet ['method_citation']
['Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.']
['The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.', 'A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging', 'Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:66', 'F:0']
['All feature templates and their instances are shown in Table 1.']
['The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.', 'A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging', 'Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:15', 'F:0']
['The feature templates we adopted are selected from those of Ng and Low (2004).']
['The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.', 'A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging', 'In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:21', 'F:0']
['The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.']
['The outside-layer linear model, similar to those in SMT, can synthetically utilize different knowledge sources to conduct more accurate comparison between candidates.', 'Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.', 'Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.']
['system', 'ROUGE-S*', 'Average_R:', '0.01350', '(95%-conf.int.', '0.01350', '-', '0.01350)']
['system', 'ROUGE-S*', 'Average_P:', '0.10989', '(95%-conf.int.', '0.10989', '-', '0.10989)']
['system', 'ROUGE-S*', 'Average_F:', '0.02404', '(95%-conf.int.', '0.02404', '-', '0.02404)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:91', 'F:10']
['The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.']
['We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.', 'A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging', 'In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.']
['system', 'ROUGE-S*', 'Average_R:', '0.00877', '(95%-conf.int.', '0.00877', '-', '0.00877)']
['system', 'ROUGE-S*', 'Average_P:', '0.16484', '(95%-conf.int.', '0.16484', '-', '0.16484)']
['system', 'ROUGE-S*', 'Average_F:', '0.01665', '(95%-conf.int.', '0.01665', '-', '0.01665)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:91', 'F:15']
['We trained a character-based perceptron for Chinese Joint S&T, and found that the perceptron itself could achieve considerably high accuracy on segmentation and Joint S&T.']
['A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging', 'In our experiments we trained a 3-gram word language model measuring the fluency of the segmentation result, a 4-gram POS language model functioning as the product of statetransition probabilities in HMM, and a word-POS co-occurrence model describing how much probably a word sequence coexists with a POS sequence.', 'Experiments show that our cascaded model can utilize different knowledge sources effectively and obtain accuracy improvements on both segmentation and Joint S&T.']
['system', 'ROUGE-S*', 'Average_R:', '0.00839', '(95%-conf.int.', '0.00839', '-', '0.00839)']
['system', 'ROUGE-S*', 'Average_P:', '0.13187', '(95%-conf.int.', '0.13187', '-', '0.13187)']
['system', 'ROUGE-S*', 'Average_F:', '0.01577', '(95%-conf.int.', '0.01577', '-', '0.01577)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:91', 'F:12']
['The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&T.']
['We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.', 'A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging', 'With a character-based perceptron as the core, combined with realvalued features such as language models, the cascaded model is able to efficiently utilize knowledge sources that are inconvenient to incorporate into the perceptron directly.']
['system', 'ROUGE-S*', 'Average_R:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_P:', '0.10989', '(95%-conf.int.', '0.10989', '-', '0.10989)']
['system', 'ROUGE-S*', 'Average_F:', '0.02296', '(95%-conf.int.', '0.02296', '-', '0.02296)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:91', 'F:10']
0.0737842846602 0.00621142848269 0.0113457141236





input/ref/Task1/J01-2004_swastika.csv
input/res/Task1/J01-2004.csv
parsing: input/ref/Task1/J01-2004_swastika.csv
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="3">In the past few years, however, some improvements have been made over these language models through the use of statistical methods of natural language processing, and the development of innovative, linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
<S sid="215" ssid="119">The first word in the string remaining to be parsed, w1, we will call the look-ahead word.</S>
original cit marker offset is 0
new cit marker offset is 0



['215']
215
['215']
parsed_discourse_facet ['method_citation']
    <S sid="302" ssid="58">In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds.</S>
original cit marker offset is 0
new cit marker offset is 0



['302']
302
['302']
parsed_discourse_facet ['method_citation']
<S sid="372" ssid="128">The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.</S>
original cit marker offset is 0
new cit marker offset is 0



['372']
372
['372']
parsed_discourse_facet ['result_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
    <S sid="100" ssid="4">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['method_citation']
    <S sid="31" ssid="19">Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).</S>
original cit marker offset is 0
new cit marker offset is 0



['31']
31
['31']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="9">First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/J01-2004.csv
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="17">This paper will examine language modeling for speech recognition from a natural language processing point of view.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'", "'0'", "'3'"]
'17'
'0'
'3'
['17', '0', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="4">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S><S ssid="1" sid="10">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'10'"]
'0'
'4'
'10'
['0', '4', '10']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="137">Here we will present a parser that uses simple search heuristics of this sort without DP.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'137'"]
'3'
'9'
'137'
['3', '9', '137']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="223">We implement this as a beam search.</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'223'", "'100'"]
'0'
'223'
'100'
['0', '223', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'100'"]
'3'
'9'
'100'
['3', '9', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'0'"]
'3'
'9'
'0'
['3', '9', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'9'", "'100'"]
'3'
'9'
'100'
['3', '9', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="100">The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'100'", "'3'"]
'0'
'100'
'3'
['0', '100', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="4">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S><S ssid="1" sid="10">A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'10'"]
'0'
'4'
'10'
['0', '4', '10']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Probabilistic Top-Down Parsing and Language Modeling</S><S ssid="1" sid="3">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S><S ssid="1" sid="9">A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'9'"]
'0'
'3'
'9'
['0', '3', '9']
parsed_discourse_facet ['method_citation']
['First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which allows it to calculate a generative probability for each prefix string from the probabilistic grammar, and hence a conditional probability for each word given the previous words and the probabilistic grammar.']
['A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'Probabilistic Top-Down Parsing and Language Modeling', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.01080', '(95%-conf.int.', '0.01080', '-', '0.01080)']
['system', 'ROUGE-S*', 'Average_P:', '0.02462', '(95%-conf.int.', '0.02462', '-', '0.02462)']
['system', 'ROUGE-S*', 'Average_F:', '0.01501', '(95%-conf.int.', '0.01501', '-', '0.01501)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:325', 'F:8']
['The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.']
['Probabilistic Top-Down Parsing and Language Modeling', 'The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.29847', '(95%-conf.int.', '0.29847', '-', '0.29847)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.45972', '(95%-conf.int.', '0.45972', '-', '0.45972)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:351', 'F:351']
['The first word in the string remaining to be parsed, w1, we will call the look-ahead word.']
['A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'Probabilistic Top-Down Parsing and Language Modeling', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:36', 'F:0']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'Probabilistic Top-Down Parsing and Language Modeling', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:91', 'F:0']
['The small size of our training data, as well as the fact that we are rescoring n-best lists, rather than working directly on lattices, makes comparison with the other models not particularly informative.']
['A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'The approach that we will subsequently present uses the probabilistic grammar as its language model, but only includes probability mass from those parses that are found, that is, it uses the parser to find a subset of the total set of parses (hopefully most of the high-probability parses) and uses the sum of their probabilities as an estimate of the true probability given the grammar.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1830', 'P:91', 'F:0']
["In the beam search approach outlined above, we can estimate the string's probability in the same manner, by summing the probabilities of the parses that the algorithm finds."]
['A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'Probabilistic Top-Down Parsing and Language Modeling', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:78', 'F:0']
['Thus, our top-down parser allows for the incremental calculation of generative conditional word probabilities, a property it shares with other left-to-right parsers with rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers (Rosenkrantz and Lewis 11 1970).']
['A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'Probabilistic Top-Down Parsing and Language Modeling', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.']
['system', 'ROUGE-S*', 'Average_R:', '0.01080', '(95%-conf.int.', '0.01080', '-', '0.01080)']
['system', 'ROUGE-S*', 'Average_P:', '0.02667', '(95%-conf.int.', '0.02667', '-', '0.02667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01537', '(95%-conf.int.', '0.01537', '-', '0.01537)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:300', 'F:8']
['In the past few years, however, some improvements have been made over these language models through the use of statistical methods of natural language processing, and the development of innovative, linguistically well-motivated techniques for improving language models for speech recognition is generating more interest among computational linguists.']
['Probabilistic Top-Down Parsing and Language Modeling', 'A lexicalized probabilistic topdown parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers.', 'This paper will examine language modeling for speech recognition from a natural language processing point of view.']
['system', 'ROUGE-S*', 'Average_R:', '0.06439', '(95%-conf.int.', '0.06439', '-', '0.06439)']
['system', 'ROUGE-S*', 'Average_P:', '0.11333', '(95%-conf.int.', '0.11333', '-', '0.11333)']
['system', 'ROUGE-S*', 'Average_F:', '0.08213', '(95%-conf.int.', '0.08213', '-', '0.08213)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:528', 'P:300', 'F:34']
0.14557749818 0.0480574993993 0.0715287491059





input/ref/Task1/D09-1092_vardha.csv
input/res/Task1/D09-1092.csv
parsing: input/ref/Task1/D09-1092_vardha.csv
<S sid="17" ssid="13">We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
 <S sid="20" ssid="16">We also explore how the characteristics of different languages affect topic model performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
    <S sid="138" ssid="87">We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).</S>
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
   <S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'"]
'55'
['55']
parsed_discourse_facet ['method_citation']
 sid="9" ssid="5">In this paper, we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
Error in Reference Offset
<S sid="118" ssid="67">We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="1">The polylingual topic model (PLTM) is an extension of latent Dirichlet allocation (LDA) (Blei et al., 2003) for modeling polylingual document tuples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
 <S sid="55" ssid="4">The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'55'"]
'55'
['55']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="51">In contrast, PLTM assigns a significant number of tokens to almost all 800 topics, in very similar proportions in both languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'"]
'102'
['102']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="4">This is unlike LDA, in which each document is assumed to have its own document-specific distribution over topics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'"]
'38'
['38']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="105">We use both Jensen-Shannon divergence and cosine distance.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'"]
'156'
['156']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="2">Each tuple is a set of documents that are loosely equivalent to each other, but written in different languages, e.g., corresponding Wikipedia articles in French, English and German.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
  <S sid="170" ssid="4">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
    <S sid="29" ssid="5">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
  <S sid="170" ssid="4">First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="168" ssid="2">However, the growth of the web, and in particular Wikipedia, has made comparable text corpora &#8211; documents that are topically similar but are not direct translations of one another &#8211; considerably more abundant than true parallel corpora.</S>
original cit marker offset is 0
new cit marker offset is 0



["'168'"]
'168'
['168']
parsed_discourse_facet ['method_citation']
    <S sid="29" ssid="5">A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'29'"]
'29'
['29']
parsed_discourse_facet ['method_citation']
 <S sid="110" ssid="59">Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.</S>
original cit marker offset is 0
new cit marker offset is 0



["'110'"]
'110'
['110']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D09-1092.csv
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'0'"]
'3'
'192'
'0'
['3', '192', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'0'"]
'3'
'192'
'0'
['3', '192', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'185'"]
'3'
'192'
'185'
['3', '192', '185']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'9'"]
'0'
'192'
'9'
['0', '192', '9']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'185'", "'3'"]
'192'
'185'
'3'
['192', '185', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'185'", "'3'"]
'192'
'185'
'3'
['192', '185', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'3'"]
'0'
'192'
'3'
['0', '192', '3']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="148">To evaluate this scenario, we train PLTM on a set of document tuples from EuroParl, infer topic distributions for a set of held-out documents, and then measure our ability to align documents in one language with their translations in another language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'3'", "'148'"]
'192'
'3'
'148'
['192', '3', '148']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'9'", "'192'"]
'0'
'9'
'192'
['0', '9', '192']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'3'", "'185'"]
'192'
'3'
'185'
['192', '3', '185']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'3'", "'192'"]
'0'
'3'
'192'
['0', '3', '192']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'9'"]
'0'
'192'
'9'
['0', '192', '9']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="185">Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission διαστημικό sts nasa αγγλ small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimmŠinen space lento spatiale mission orbite mars satellite spatial תינכות א רודכ לל ח ץר אה ללחה spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa космический союз космического спутник станции uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la ισπανίας ισπανία de ισπανός ντε μαδρίτη de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpański hiszpanii la juan y де мадрид испании испания испанский de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk ποιητής ποίηση ποιητή έργο ποιητές ποιήματα poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'185'", "'9'", "'192'"]
'185'
'9'
'192'
['185', '9', '192']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="153">In order to make the task more difficult, we train a relatively coarse-grained PLTM with 50 topics on the training set.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="19">We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.</S>
original cit marker offset is 0
new cit marker offset is 0



["'153'", "'192'", "'19'"]
'153'
'192'
'19'
['153', '192', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'192'", "'0'"]
'3'
'192'
'0'
['3', '192', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'3'", "'0'"]
'192'
'3'
'0'
['192', '3', '0']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'0'", "'3'"]
'192'
'0'
'3'
['192', '0', '3']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Polylingual Topic Models</S><S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'192'", "'3'"]
'0'
'192'
'3'
['0', '192', '3']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="192">We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.</S><S ssid="1" sid="9">In this paper, we present the polylingual topic model (PLTM).</S><S ssid="1" sid="3">We introduce a polylingual topic model that discovers topics aligned across multiple languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'192'", "'9'", "'3'"]
'192'
'9'
'3'
['192', '9', '3']
parsed_discourse_facet ['aim_citation']
['We demonstrate its utility and explore its characteristics using two polylingual corpora: proceedings of the European parliament (in eleven languages) and a collection of Wikipedia articles (in twelve languages).']
['In this paper, we present the polylingual topic model (PLTM).', 'Polylingual Topic Models', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.01170', '(95%-conf.int.', '0.01170', '-', '0.01170)']
['system', 'ROUGE-S*', 'Average_P:', '0.01667', '(95%-conf.int.', '0.01667', '-', '0.01667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01375', '(95%-conf.int.', '0.01375', '-', '0.01375)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:171', 'P:120', 'F:2']
['We then add the Cartesian product of these sets for every topic to a set of candidate translations C. We report the number of elements of C that appear in the reference lexica.']
['We introduce a polylingual topic model that discovers topics aligned across multiple languages.', u'Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission \u03b4\u03b9\u03b1\u03c3\u03c4\u03b7\u03bc\u03b9\u03ba\u03cc sts nasa \u03b1\u03b3\u03b3\u03bb small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimm\u0160inen space lento spatiale mission orbite mars satellite spatial \u05ea\u05d9\u05e0\u05db\u05d5\u05ea \u05d0 \u05e8\u05d5\u05d3\u05db \u05dc\u05dc \u05d7 \u05e5\u05e8 \u05d0\u05d4 \u05dc\u05dc\u05d7\u05d4 spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0441\u043e\u044e\u0437 \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0441\u043f\u0443\u0442\u043d\u0438\u043a \u0441\u0442\u0430\u043d\u0446\u0438\u0438 uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la \u03b9\u03c3\u03c0\u03b1\u03bd\u03af\u03b1\u03c2 \u03b9\u03c3\u03c0\u03b1\u03bd\u03af\u03b1 de \u03b9\u03c3\u03c0\u03b1\u03bd\u03cc\u03c2 \u03bd\u03c4\u03b5 \u03bc\u03b1\u03b4\u03c1\u03af\u03c4\u03b7 de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpa\u0144ski hiszpanii la juan y \u0434\u0435 \u043c\u0430\u0434\u0440\u0438\u0434 \u0438\u0441\u043f\u0430\u043d\u0438\u0438 \u0438\u0441\u043f\u0430\u043d\u0438\u044f \u0438\u0441\u043f\u0430\u043d\u0441\u043a\u0438\u0439 de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk \u03c0\u03bf\u03b9\u03b7\u03c4\u03ae\u03c2 \u03c0\u03bf\u03af\u03b7\u03c3\u03b7 \u03c0\u03bf\u03b9\u03b7\u03c4\u03ae \u03ad\u03c1\u03b3\u03bf \u03c0\u03bf\u03b9\u03b7\u03c4\u03ad\u03c2 \u03c0\u03bf\u03b9\u03ae\u03bc\u03b1\u03c4\u03b1 poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:12880', 'P:78', 'F:0']
['Continuing with the example above, one might extract a set of connected Wikipedia articles related to the focus of the journal and then train PLTM on a joint corpus consisting of journal papers and Wikipedia articles.']
['In this paper, we present the polylingual topic model (PLTM).', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:300', 'P:153', 'F:0']
['We also explore how the characteristics of different languages affect topic model performance.']
['Polylingual Topic Models', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.01299', '(95%-conf.int.', '0.01299', '-', '0.01299)']
['system', 'ROUGE-S*', 'Average_P:', '0.14286', '(95%-conf.int.', '0.14286', '-', '0.14286)']
['system', 'ROUGE-S*', 'Average_F:', '0.02381', '(95%-conf.int.', '0.02381', '-', '0.02381)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:21', 'F:3']
['We calculate the Jensen-Shannon divergence between the topic distributions for each pair of individual documents in S that were originally part of the same tuple prior to separation.']
['Polylingual Topic Models', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:91', 'F:0']
['The EuroParl corpus consists of parallel texts in eleven western European languages: Danish, German, Greek, English, Spanish, Finnish, French, Italian, Dutch, Portuguese and Swedish.']
['We introduce a polylingual topic model that discovers topics aligned across multiple languages.', u'Subtle differences of sentiment may be below the granularity of the model. sadwrn blaned gallair at lloeren mytholeg space nasa sojus flug mission \u03b4\u03b9\u03b1\u03c3\u03c4\u03b7\u03bc\u03b9\u03ba\u03cc sts nasa \u03b1\u03b3\u03b3\u03bb small space mission launch satellite nasa spacecraft sojuz nasa apollo ensimm\u0160inen space lento spatiale mission orbite mars satellite spatial \u05ea\u05d9\u05e0\u05db\u05d5\u05ea \u05d0 \u05e8\u05d5\u05d3\u05db \u05dc\u05dc \u05d7 \u05e5\u05e8 \u05d0\u05d4 \u05dc\u05dc\u05d7\u05d4 spaziale missione programma space sojuz stazione misja kosmicznej stacji misji space nasa \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u0441\u043e\u044e\u0437 \u043a\u043e\u0441\u043c\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0441\u043f\u0443\u0442\u043d\u0438\u043a \u0441\u0442\u0430\u043d\u0446\u0438\u0438 uzay soyuz ay uzaya salyut sovyetler sbaen madrid el la jos6 sbaeneg de spanischer spanischen spanien madrid la \u03b9\u03c3\u03c0\u03b1\u03bd\u03af\u03b1\u03c2 \u03b9\u03c3\u03c0\u03b1\u03bd\u03af\u03b1 de \u03b9\u03c3\u03c0\u03b1\u03bd\u03cc\u03c2 \u03bd\u03c4\u03b5 \u03bc\u03b1\u03b4\u03c1\u03af\u03c4\u03b7 de spanish spain la madrid y espanja de espanjan madrid la real espagnol espagne madrid espagnole juan y de spagna spagnolo spagnola madrid el de hiszpa\u0144ski hiszpanii la juan y \u0434\u0435 \u043c\u0430\u0434\u0440\u0438\u0434 \u0438\u0441\u043f\u0430\u043d\u0438\u0438 \u0438\u0441\u043f\u0430\u043d\u0438\u044f \u0438\u0441\u043f\u0430\u043d\u0441\u043a\u0438\u0439 de ispanya ispanyol madrid la kOba real bardd gerddi iaith beirdd fardd gymraeg dichter schriftsteller literatur gedichte gedicht werk \u03c0\u03bf\u03b9\u03b7\u03c4\u03ae\u03c2 \u03c0\u03bf\u03af\u03b7\u03c3\u03b7 \u03c0\u03bf\u03b9\u03b7\u03c4\u03ae \u03ad\u03c1\u03b3\u03bf \u03c0\u03bf\u03b9\u03b7\u03c4\u03ad\u03c2 \u03c0\u03bf\u03b9\u03ae\u03bc\u03b1\u03c4\u03b1 poet poetry literature literary poems poem runoilija kirjailija kirjallisuuden kirjoitti runo julkaisi poste 6crivain litt6rature po6sie litt6raire ses Overall, these scores indicate that although individual pages may show disagreement, Wikipedia is on average consistent between languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00016', '(95%-conf.int.', '0.00016', '-', '0.00016)']
['system', 'ROUGE-S*', 'Average_P:', '0.01053', '(95%-conf.int.', '0.01053', '-', '0.01053)']
['system', 'ROUGE-S*', 'Average_F:', '0.00031', '(95%-conf.int.', '0.00031', '-', '0.00031)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:12880', 'P:190', 'F:2']
['First, we explore whether comparable document tuples support the alignment of fine-grained topics, as demonstrated earlier using parallel documents.']
['We employ a set of direct translations, the EuroParl corpus, to evaluate whether PLTM can accurately infer topics when documents genuinely contain the same content.', 'In order to make the task more difficult, we train a relatively coarse-grained PLTM with 50 topics on the training set.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00794', '(95%-conf.int.', '0.00794', '-', '0.00794)']
['system', 'ROUGE-S*', 'Average_P:', '0.05495', '(95%-conf.int.', '0.05495', '-', '0.05495)']
['system', 'ROUGE-S*', 'Average_F:', '0.01387', '(95%-conf.int.', '0.01387', '-', '0.01387)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:91', 'F:5']
['A recent extended abstract, developed concurrently by Ni et al. (Ni et al., 2009), discusses a multilingual topic model similar to the one presented here.']
['Polylingual Topic Models', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00433', '(95%-conf.int.', '0.00433', '-', '0.00433)']
['system', 'ROUGE-S*', 'Average_P:', '0.00833', '(95%-conf.int.', '0.00833', '-', '0.00833)']
['system', 'ROUGE-S*', 'Average_F:', '0.00570', '(95%-conf.int.', '0.00570', '-', '0.00570)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:120', 'F:1']
['We argue that topic modeling is both a useful and appropriate tool for leveraging correspondences between semantically comparable documents in multiple different languages.']
['Polylingual Topic Models', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.02597', '(95%-conf.int.', '0.02597', '-', '0.02597)']
['system', 'ROUGE-S*', 'Average_P:', '0.10909', '(95%-conf.int.', '0.10909', '-', '0.10909)']
['system', 'ROUGE-S*', 'Average_F:', '0.04196', '(95%-conf.int.', '0.04196', '-', '0.04196)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:55', 'F:6']
['However, the growth of the web, and in particular Wikipedia, has made comparable text corpora &#8211; documents that are topically similar but are not direct translations of one another &#8211; considerably more abundant than true parallel corpora.']
['Polylingual Topic Models', 'We introduce a polylingual topic model that discovers topics aligned across multiple languages.', 'We introduced a polylingual topic model (PLTM) that discovers topics aligned across multiple languages.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:171', 'F:0']
0.0342429996576 0.00630899993691 0.0099399999006





input/ref/Task1/W11-2123_aakansha.csv
input/res/Task1/W11-2123.csv
parsing: input/ref/Task1/W11-2123_aakansha.csv
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="23">The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="136" ssid="8">We offer a state function s(wn1) = wn&#65533; where substring wn&#65533; is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'136'"]
'136'
['136']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="3">Queries take the form p(wn|wn&#8722;1 1 ) where wn1 is an n-gram.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="205" ssid="24">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al., 2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["'205'"]
'205'
['205']
parsed_discourse_facet ['method_citation']
<S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
<S sid="204" ssid="23">For RandLM, we used the settings in the documentation: 8 bits per value and false positive probability 1 256.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'"]
'204'
['204']
parsed_discourse_facet ['method_citation']
<S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="229" ssid="48">Then we ran binary search to determine the least amount of memory with which it would run.</S>
original cit marker offset is 0
new cit marker offset is 0



["'229'"]
'229'
['229']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="93" ssid="71">The cost of storing these averages, in bits, is Because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.</S>
original cit marker offset is 0
new cit marker offset is 0



["'93'"]
'93'
['93']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W11-2123.csv
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="32">Linear probing places at most one entry in each bucket.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'32'"]
'5'
'0'
'32'
['5', '0', '32']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="187">The data structure was populated with 64-bit integers sampled uniformly without replacement.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'187'"]
'5'
'0'
'187'
['5', '0', '187']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'109'"]
'0'
'25'
'109'
['0', '25', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'5'"]
'1'
'0'
'5'
['1', '0', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'109'"]
'5'
'0'
'109'
['5', '0', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S><S ssid="1" sid="7">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'25'", "'7'"]
'5'
'25'
'7'
['5', '25', '7']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'25'"]
'5'
'1'
'25'
['5', '1', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="7">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'0'", "'7'"]
'3'
'0'
'7'
['3', '0', '7']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="7">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'7'"]
'5'
'1'
'7'
['5', '1', '7']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="32">Linear probing places at most one entry in each bucket.</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'32'", "'109'"]
'5'
'32'
'109'
['5', '32', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'5'", "'1'"]
'0'
'5'
'1'
['0', '5', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'25'"]
'5'
'1'
'25'
['5', '1', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'109'", "'5'"]
'1'
'109'
'5'
['1', '109', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="3">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'5'", "'3'"]
'0'
'5'
'3'
['0', '5', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'0'", "'5'"]
'109'
'0'
'5'
['109', '0', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="102">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie), allocating memory all at once (eliminating the need for full pointers), and being easy to compile.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="200">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'", "'0'", "'200'"]
'102'
'0'
'200'
['102', '0', '200']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="9">Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf , returning where the probability p(wn|wn−1 f ) and backoff penalties b(wn−1 i ) are given by an already-estimated model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'9'"]
'5'
'0'
'9'
['5', '0', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S><S ssid="1" sid="3">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'3'", "'5'"]
'109'
'3'
'5'
['109', '3', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'1'"]
'5'
'0'
'1'
['5', '0', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'109'"]
'5'
'0'
'109'
['5', '0', '109']
parsed_discourse_facet ['method_citation']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.', 'Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.13109', '(95%-conf.int.', '0.13109', '-', '0.13109)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.23180', '(95%-conf.int.', '0.23180', '-', '0.23180)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:78', 'F:78']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.', 'An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.11712', '(95%-conf.int.', '0.11712', '-', '0.11712)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.20968', '(95%-conf.int.', '0.20968', '-', '0.20968)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:78', 'F:78']
['The cost of storing these averages, in bits, is Because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.']
['Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', 'Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00145', '(95%-conf.int.', '0.00145', '-', '0.00145)']
['system', 'ROUGE-S*', 'Average_P:', '0.02198', '(95%-conf.int.', '0.02198', '-', '0.02198)']
['system', 'ROUGE-S*', 'Average_F:', '0.00272', '(95%-conf.int.', '0.00272', '-', '0.00272)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:91', 'F:2']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['KenLM: Faster and Smaller Language Model Queries', 'Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00529', '(95%-conf.int.', '0.00529', '-', '0.00529)']
['system', 'ROUGE-S*', 'Average_P:', '0.03636', '(95%-conf.int.', '0.03636', '-', '0.03636)']
['system', 'ROUGE-S*', 'Average_F:', '0.00924', '(95%-conf.int.', '0.00924', '-', '0.00924)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:55', 'F:2']
['We offer a state function s(wn1) = wn&#65533; where substring wn&#65533; is guaranteed to extend (to the right) in the same way that wn1 does for purposes of language modeling.']
['KenLM: Faster and Smaller Language Model Queries', 'Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.', 'An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.']
['system', 'ROUGE-S*', 'Average_R:', '0.00476', '(95%-conf.int.', '0.00476', '-', '0.00476)']
['system', 'ROUGE-S*', 'Average_P:', '0.02857', '(95%-conf.int.', '0.02857', '-', '0.02857)']
['system', 'ROUGE-S*', 'Average_F:', '0.00816', '(95%-conf.int.', '0.00816', '-', '0.00816)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:105', 'F:3']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.', 'KenLM: Faster and Smaller Language Model Queries', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.22222', '(95%-conf.int.', '0.22222', '-', '0.22222)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.36364', '(95%-conf.int.', '0.36364', '-', '0.36364)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:78', 'F:78']
['We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.', 'KenLM: Faster and Smaller Language Model Queries', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.04274', '(95%-conf.int.', '0.04274', '-', '0.04274)']
['system', 'ROUGE-S*', 'Average_P:', '0.33333', '(95%-conf.int.', '0.33333', '-', '0.33333)']
['system', 'ROUGE-S*', 'Average_F:', '0.07576', '(95%-conf.int.', '0.07576', '-', '0.07576)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:45', 'F:15']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
[u'Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf , returning where the probability p(wn|wn\u22121 f ) and backoff penalties b(wn\u22121 i ) are given by an already-estimated model.', 'KenLM: Faster and Smaller Language Model Queries', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.01201', '(95%-conf.int.', '0.01201', '-', '0.01201)']
['system', 'ROUGE-S*', 'Average_P:', '0.10256', '(95%-conf.int.', '0.10256', '-', '0.10256)']
['system', 'ROUGE-S*', 'Average_F:', '0.02151', '(95%-conf.int.', '0.02151', '-', '0.02151)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:78', 'F:8']
['The PROBING data structure is a rather straightforward application of these hash tables to store Ngram language models.']
['KenLM: Faster and Smaller Language Model Queries', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.', 'The data structure was populated with 64-bit integers sampled uniformly without replacement.']
['system', 'ROUGE-S*', 'Average_R:', '0.00791', '(95%-conf.int.', '0.00791', '-', '0.00791)']
['system', 'ROUGE-S*', 'Average_P:', '0.03636', '(95%-conf.int.', '0.03636', '-', '0.03636)']
['system', 'ROUGE-S*', 'Average_F:', '0.01299', '(95%-conf.int.', '0.01299', '-', '0.01299)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:253', 'P:55', 'F:2']
['Then we ran binary search to determine the least amount of memory with which it would run.']
['The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie), allocating memory all at once (eliminating the need for full pointers), and being easy to compile.', 'KenLM: Faster and Smaller Language Model Queries', 'The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:21', 'F:0']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['KenLM: Faster and Smaller Language Model Queries', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.', 'Linear probing places at most one entry in each bucket.']
['system', 'ROUGE-S*', 'Average_R:', '0.01170', '(95%-conf.int.', '0.01170', '-', '0.01170)']
['system', 'ROUGE-S*', 'Average_P:', '0.03636', '(95%-conf.int.', '0.03636', '-', '0.03636)']
['system', 'ROUGE-S*', 'Average_F:', '0.01770', '(95%-conf.int.', '0.01770', '-', '0.01770)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:171', 'P:55', 'F:2']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.', 'An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.11712', '(95%-conf.int.', '0.11712', '-', '0.11712)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.20968', '(95%-conf.int.', '0.20968', '-', '0.20968)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:78', 'F:78']
['Queries take the form p(wn|wn&#8722;1 1 ) where wn1 is an n-gram.']
['An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.', 'This paper presents methods to query N-gram language models, minimizing time and space costs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00168', '(95%-conf.int.', '0.00168', '-', '0.00168)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00317', '(95%-conf.int.', '0.00317', '-', '0.00317)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:36', 'F:1']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['KenLM: Faster and Smaller Language Model Queries', 'Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.02381', '(95%-conf.int.', '0.02381', '-', '0.02381)']
['system', 'ROUGE-S*', 'Average_P:', '0.11538', '(95%-conf.int.', '0.11538', '-', '0.11538)']
['system', 'ROUGE-S*', 'Average_F:', '0.03947', '(95%-conf.int.', '0.03947', '-', '0.03947)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:78', 'F:9']
['For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.']
['KenLM: Faster and Smaller Language Model Queries', 'Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00265', '(95%-conf.int.', '0.00265', '-', '0.00265)']
['system', 'ROUGE-S*', 'Average_P:', '0.00308', '(95%-conf.int.', '0.00308', '-', '0.00308)']
['system', 'ROUGE-S*', 'Average_F:', '0.00284', '(95%-conf.int.', '0.00284', '-', '0.00284)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:325', 'F:1']
['We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al., 2009).']
['KenLM: Faster and Smaller Language Model Queries', 'Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', 'This paper presents methods to query N-gram language models, minimizing time and space costs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00355', '(95%-conf.int.', '0.00355', '-', '0.00355)']
['system', 'ROUGE-S*', 'Average_P:', '0.02941', '(95%-conf.int.', '0.02941', '-', '0.02941)']
['system', 'ROUGE-S*', 'Average_F:', '0.00633', '(95%-conf.int.', '0.00633', '-', '0.00633)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:136', 'F:4']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['KenLM: Faster and Smaller Language Model Queries', 'Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.01313', '(95%-conf.int.', '0.01313', '-', '0.01313)']
['system', 'ROUGE-S*', 'Average_P:', '0.16667', '(95%-conf.int.', '0.16667', '-', '0.16667)']
['system', 'ROUGE-S*', 'Average_F:', '0.02434', '(95%-conf.int.', '0.02434', '-', '0.02434)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:78', 'F:13']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.', 'KenLM: Faster and Smaller Language Model Queries', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.22222', '(95%-conf.int.', '0.22222', '-', '0.22222)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.36364', '(95%-conf.int.', '0.36364', '-', '0.36364)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:78', 'F:78']
0.329879998167 0.052247221932 0.0890372217276





input/ref/Task1/P87-1015_swastika.csv
input/res/Task1/P87-1015.csv
parsing: input/ref/Task1/P87-1015_swastika.csv
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="149" ssid="34">We can obtain a letter equivalent CFL defined by a CFG in which the for each rule as above, we have the production A &#8212;* A1 Anup where tk (up) = cp.</S>
original cit marker offset is 0
new cit marker offset is 0



['149']
149
['149']
parsed_discourse_facet ['method_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
    <S sid="2" ssid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['2']
2
['2']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
<S sid="222" ssid="28">However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



['222']
222
['222']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['???']
???
['???']
parsed_discourse_facet ['aim_citation']
<S sid="207" ssid="13">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['207']
207
['207']
parsed_discourse_facet ['result_citation']
<S sid="207" ssid="13">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['207']
207
['207']
parsed_discourse_facet ['result_citation']
<S sid="19" ssid="4">It can be easily shown from Thatcher's result that the path set of every local set is a regular set.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



['119']
119
['119']
parsed_discourse_facet ['aim_citation']
<S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



['118']
118
['118']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/P87-1015.csv
<S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'207'", "'125'"]
'92'
'207'
'125'
['92', '207', '125']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'125'", "'207'"]
'75'
'125'
'207'
['75', '125', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'75'", "'207'"]
'125'
'75'
'207'
['125', '75', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'125'", "'207'"]
'2'
'125'
'207'
['2', '125', '207']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'92'"]
'207'
'125'
'92'
['207', '125', '92']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'207'", "'75'"]
'2'
'207'
'75'
['2', '207', '75']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'92'"]
'207'
'125'
'92'
['207', '125', '92']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="151">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'207'", "'125'"]
'151'
'207'
'125'
['151', '207', '125']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'75'", "'207'"]
'2'
'75'
'207'
['2', '75', '207']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'75'", "'207'"]
'125'
'75'
'207'
['125', '75', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'125'", "'207'"]
'75'
'125'
'207'
['75', '125', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'75'"]
'207'
'125'
'75'
['207', '125', '75']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'75'", "'207'"]
'125'
'75'
'207'
['125', '75', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="113">A formalism generating tree sets with complex path sets can still generate only semilinear languages if its tree sets have independent paths, and semilinear path sets.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'113'", "'75'"]
'2'
'113'
'75'
['2', '113', '75']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'92'"]
'207'
'125'
'92'
['207', '125', '92']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'75'"]
'207'
'125'
'75'
['207', '125', '75']
parsed_discourse_facet ['result_citation']
['In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.']
['We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.27094', '(95%-conf.int.', '0.27094', '-', '0.27094)']
['system', 'ROUGE-S*', 'Average_P:', '0.11628', '(95%-conf.int.', '0.11628', '-', '0.11628)']
['system', 'ROUGE-S*', 'Average_F:', '0.16272', '(95%-conf.int.', '0.16272', '-', '0.16272)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:406', 'P:946', 'F:110']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
['We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.06404', '(95%-conf.int.', '0.06404', '-', '0.06404)']
['system', 'ROUGE-S*', 'Average_P:', '0.21667', '(95%-conf.int.', '0.21667', '-', '0.21667)']
['system', 'ROUGE-S*', 'Average_F:', '0.09886', '(95%-conf.int.', '0.09886', '-', '0.09886)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:406', 'P:120', 'F:26']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
['A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.03427', '(95%-conf.int.', '0.03427', '-', '0.03427)']
['system', 'ROUGE-S*', 'Average_P:', '0.14167', '(95%-conf.int.', '0.14167', '-', '0.14167)']
['system', 'ROUGE-S*', 'Average_F:', '0.05519', '(95%-conf.int.', '0.05519', '-', '0.05519)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:120', 'F:17']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
['A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.03427', '(95%-conf.int.', '0.03427', '-', '0.03427)']
['system', 'ROUGE-S*', 'Average_P:', '0.14167', '(95%-conf.int.', '0.14167', '-', '0.14167)']
['system', 'ROUGE-S*', 'Average_F:', '0.05519', '(95%-conf.int.', '0.05519', '-', '0.05519)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:120', 'F:17']
["In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs."]
['We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.01232', '(95%-conf.int.', '0.01232', '-', '0.01232)']
['system', 'ROUGE-S*', 'Average_P:', '0.07576', '(95%-conf.int.', '0.07576', '-', '0.07576)']
['system', 'ROUGE-S*', 'Average_F:', '0.02119', '(95%-conf.int.', '0.02119', '-', '0.02119)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:406', 'P:66', 'F:5']
["It can be easily shown from Thatcher's result that the path set of every local set is a regular set."]
['A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', 'In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.', 'A formalism generating tree sets with complex path sets can still generate only semilinear languages if its tree sets have independent paths, and semilinear path sets.']
['system', 'ROUGE-S*', 'Average_R:', '0.00228', '(95%-conf.int.', '0.00228', '-', '0.00228)']
['system', 'ROUGE-S*', 'Average_P:', '0.13333', '(95%-conf.int.', '0.13333', '-', '0.13333)']
['system', 'ROUGE-S*', 'Average_F:', '0.00449', '(95%-conf.int.', '0.00449', '-', '0.00449)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:45', 'F:6']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
['A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', 'In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.01756', '(95%-conf.int.', '0.01756', '-', '0.01756)']
['system', 'ROUGE-S*', 'Average_P:', '0.33333', '(95%-conf.int.', '0.33333', '-', '0.33333)']
['system', 'ROUGE-S*', 'Average_F:', '0.03336', '(95%-conf.int.', '0.03336', '-', '0.03336)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:120', 'F:40']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
['A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.03427', '(95%-conf.int.', '0.03427', '-', '0.03427)']
['system', 'ROUGE-S*', 'Average_P:', '0.14167', '(95%-conf.int.', '0.14167', '-', '0.14167)']
['system', 'ROUGE-S*', 'Average_F:', '0.05519', '(95%-conf.int.', '0.05519', '-', '0.05519)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:120', 'F:17']
['We can obtain a letter equivalent CFL defined by a CFG in which the for each rule as above, we have the production A &#8212;* A1 Anup where tk (up) = cp.']
['A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:78', 'F:0']
['However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.']
['A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', 'In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.00966', '(95%-conf.int.', '0.00966', '-', '0.00966)']
['system', 'ROUGE-S*', 'Average_P:', '0.11579', '(95%-conf.int.', '0.11579', '-', '0.11579)']
['system', 'ROUGE-S*', 'Average_F:', '0.01783', '(95%-conf.int.', '0.01783', '-', '0.01783)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:190', 'F:22']
0.141616998584 0.0479609995204 0.050401999496





input/ref/Task1/P08-1043_swastika.csv
input/res/Task1/P08-1043.csv
parsing: input/ref/Task1/P08-1043_swastika.csv
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['result_citation']
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="37">3An English sentence with ambiguous PoS assignment can be trivially represented as a lattice similar to our own, where every pair of consecutive nodes correspond to a word, and every possible PoS assignment for this word is a connecting arc.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
    <S sid="19" ssid="15">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



['19']
19
['19']
parsed_discourse_facet ['method_citation']
<S sid="163" ssid="1">The accuracy results for segmentation, tagging and parsing using our different models and our standard data split are summarized in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



['163']
163
['163']
parsed_discourse_facet ['result_citation']
    <S sid="100" ssid="32">This means that the rules in our grammar are of two kinds: (a) syntactic rules relating nonterminals to a sequence of non-terminals and/or PoS tags, and (b) lexical rules relating PoS tags to lattice arcs (lexemes).</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['method_citation']
    <S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



['94']
94
['94']
parsed_discourse_facet ['result_citation']
<S sid="188" ssid="2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



['188']
188
['188']
parsed_discourse_facet ['result_citation']
<S sid="86" ssid="18">A morphological analyzer M : W&#8212;* L is a function mapping sentences in Hebrew (W E W) to their corresponding lattices (M(W) = L E L).</S>
original cit marker offset is 0
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['result_citation']
<S sid="97" ssid="29">Thus our proposed model is a proper model assigning probability mass to all (7r, L) pairs, where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.</S>
original cit marker offset is 0
new cit marker offset is 0



['97']
97
['97']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'4'", "'0'"]
'19'
'4'
'0'
['19', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="186">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'186'", "'19'", "'4'"]
'186'
'19'
'4'
['186', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'0'", "'19'"]
'4'
'0'
'19'
['4', '0', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="186">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'186'"]
'0'
'19'
'186'
['0', '19', '186']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'19'"]
'0'
'4'
'19'
['0', '4', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'4'"]
'0'
'19'
'4'
['0', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="116">We simulate lexical constraints by using an external lexical resource against which we verify whether OOV segments are in fact valid Hebrew lexemes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'116'"]
'0'
'19'
'116'
['0', '19', '116']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'19'"]
'0'
'4'
'19'
['0', '4', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'4'"]
'0'
'19'
'4'
['0', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="117">This heuristics is used to prune all segmentation possibilities involving “lexically improper” segments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'117'"]
'0'
'4'
'117'
['0', '4', '117']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'4'"]
'0'
'19'
'4'
['0', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="188">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'188'"]
'0'
'4'
'188'
['0', '4', '188']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="186">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'186'"]
'0'
'19'
'186'
['0', '19', '186']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="136">Lexicon and OOV Handling Our data-driven morphological-analyzer proposes analyses for unknown tokens as described in Section 5.</S><S ssid="1" sid="133">Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'136'", "'133'"]
'0'
'136'
'133'
['0', '136', '133']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="69">We represent all morphological analyses of a given utterance using a lattice structure.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'69'", "'19'"]
'0'
'69'
'19'
['0', '69', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="135">We construct a mapping from all the space-delimited tokens seen in the training sentences to their corresponding analyses.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'135'", "'19'"]
'4'
'135'
'19'
['4', '135', '19']
parsed_discourse_facet ['method_citation']
['Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['system', 'ROUGE-S*', 'Average_R:', '0.00452', '(95%-conf.int.', '0.00452', '-', '0.00452)']
['system', 'ROUGE-S*', 'Average_P:', '0.03922', '(95%-conf.int.', '0.03922', '-', '0.03922)']
['system', 'ROUGE-S*', 'Average_F:', '0.00811', '(95%-conf.int.', '0.00811', '-', '0.00811)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:153', 'F:6']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', u'This heuristics is used to prune all segmentation possibilities involving \u201clexically improper\u201d segments.', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['system', 'ROUGE-S*', 'Average_R:', '0.01586', '(95%-conf.int.', '0.01586', '-', '0.01586)']
['system', 'ROUGE-S*', 'Average_P:', '0.12500', '(95%-conf.int.', '0.12500', '-', '0.12500)']
['system', 'ROUGE-S*', 'Average_F:', '0.02814', '(95%-conf.int.', '0.02814', '-', '0.02814)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:120', 'F:15']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.']
['system', 'ROUGE-S*', 'Average_R:', '0.18018', '(95%-conf.int.', '0.18018', '-', '0.18018)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.30534', '(95%-conf.int.', '0.30534', '-', '0.30534)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:120', 'F:120']
['Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['system', 'ROUGE-S*', 'Average_R:', '0.00452', '(95%-conf.int.', '0.00452', '-', '0.00452)']
['system', 'ROUGE-S*', 'Average_P:', '0.03922', '(95%-conf.int.', '0.03922', '-', '0.03922)']
['system', 'ROUGE-S*', 'Average_F:', '0.00811', '(95%-conf.int.', '0.00811', '-', '0.00811)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:153', 'F:6']
['Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['system', 'ROUGE-S*', 'Average_R:', '0.23684', '(95%-conf.int.', '0.23684', '-', '0.23684)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.38298', '(95%-conf.int.', '0.38298', '-', '0.38298)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:378', 'F:378']
['3An English sentence with ambiguous PoS assignment can be trivially represented as a lattice similar to our own, where every pair of consecutive nodes correspond to a word, and every possible PoS assignment for this word is a connecting arc.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'We simulate lexical constraints by using an external lexical resource against which we verify whether OOV segments are in fact valid Hebrew lexemes.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:190', 'F:0']
['Thus our proposed model is a proper model assigning probability mass to all (7r, L) pairs, where 7r is a parse tree and L is the one and only lattice that a sequence of characters (and spaces) W over our alpha-beth gives rise to.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.', 'We construct a mapping from all the space-delimited tokens seen in the training sentences to their corresponding analyses.']
['system', 'ROUGE-S*', 'Average_R:', '0.00226', '(95%-conf.int.', '0.00226', '-', '0.00226)']
['system', 'ROUGE-S*', 'Average_P:', '0.01754', '(95%-conf.int.', '0.01754', '-', '0.01754)']
['system', 'ROUGE-S*', 'Average_F:', '0.00401', '(95%-conf.int.', '0.00401', '-', '0.00401)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:171', 'F:3']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['system', 'ROUGE-S*', 'Average_R:', '0.09050', '(95%-conf.int.', '0.09050', '-', '0.09050)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.16598', '(95%-conf.int.', '0.16598', '-', '0.16598)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:120', 'F:120']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['system', 'ROUGE-S*', 'Average_R:', '0.09050', '(95%-conf.int.', '0.09050', '-', '0.09050)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.16598', '(95%-conf.int.', '0.16598', '-', '0.16598)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:120', 'F:120']
0.468997772567 0.0694644436726 0.11873888757





input/ref/Task1/A00-2030_sweta.csv
input/res/Task1/A00-2030.csv
parsing: input/ref/Task1/A00-2030_sweta.csv
<S sid="18" ssid="1">Almost all approaches to information extraction &#8212; even at the sentence level &#8212; are based on the divide-and-conquer strategy of reducing a complex problem to a set of simpler ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="100" ssid="5">Given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a While our focus throughout the project was on TE and TR, we became curious about how well the model did at part-of-speech tagging, syntactic parsing, and at name finding.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="15">Because generative statistical models had already proven successful for each of the first three stages, we were optimistic that some of their properties &#8212; especially their ability to learn from large amounts of data, and their robustness when presented with unexpected inputs &#8212; would also benefit semantic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
 <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="1">Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="1">In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.<
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'104'"]
'2'
'33'
'104'
['2', '33', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'6'"]
'2'
'33'
'6'
['2', '33', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="19">Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'19'"]
'2'
'33'
'19'
['2', '33', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'6'", "'2'"]
'33'
'6'
'2'
['33', '6', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'2'", "'33'"]
'6'
'2'
'33'
['6', '2', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'6'"]
'2'
'33'
'6'
['2', '33', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="92">2.</S><S ssid="1" sid="69">8.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'92'", "'69'"]
'2'
'92'
'69'
['2', '92', '69']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'104'"]
'2'
'6'
'104'
['2', '6', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'2'", "'104'"]
'6'
'2'
'104'
['6', '2', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'33'"]
'2'
'6'
'33'
['2', '6', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'104'"]
'2'
'6'
'104'
['2', '6', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'104'"]
'2'
'33'
'104'
['2', '33', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'104'"]
'2'
'6'
'104'
['2', '6', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'33'"]
'2'
'6'
'33'
['2', '6', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="0">A Novel Use of Statistical Parsing to Extract Information from Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'0'"]
'2'
'6'
'0'
['2', '6', '0']
parsed_discourse_facet ['method_citation']
['We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.']
['In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.07216', '(95%-conf.int.', '0.07216', '-', '0.07216)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.13460', '(95%-conf.int.', '0.13460', '-', '0.13460)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:78', 'F:78']
['In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.']
['In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.17576', '(95%-conf.int.', '0.17576', '-', '0.17576)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.29898', '(95%-conf.int.', '0.29898', '-', '0.29898)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:190', 'F:190']
['We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction']
['Our integrated model represents syntax and semantics jointly using augmented parse trees.', 'In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.03987', '(95%-conf.int.', '0.03987', '-', '0.03987)']
['system', 'ROUGE-S*', 'Average_P:', '0.46154', '(95%-conf.int.', '0.46154', '-', '0.46154)']
['system', 'ROUGE-S*', 'Average_F:', '0.07339', '(95%-conf.int.', '0.07339', '-', '0.07339)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:78', 'F:36']
['Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard.']
['Our integrated model represents syntax and semantics jointly using augmented parse trees.', 'In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:78', 'F:0']
['The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.']
['Our integrated model represents syntax and semantics jointly using augmented parse trees.', 'In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00332', '(95%-conf.int.', '0.00332', '-', '0.00332)']
['system', 'ROUGE-S*', 'Average_P:', '0.01961', '(95%-conf.int.', '0.01961', '-', '0.01961)']
['system', 'ROUGE-S*', 'Average_F:', '0.00568', '(95%-conf.int.', '0.00568', '-', '0.00568)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:153', 'F:3']
['We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.']
['Our integrated model represents syntax and semantics jointly using augmented parse trees.', 'In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.03987', '(95%-conf.int.', '0.03987', '-', '0.03987)']
['system', 'ROUGE-S*', 'Average_P:', '0.46154', '(95%-conf.int.', '0.46154', '-', '0.46154)']
['system', 'ROUGE-S*', 'Average_F:', '0.07339', '(95%-conf.int.', '0.07339', '-', '0.07339)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:78', 'F:36']
['In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.']
['Our integrated model represents syntax and semantics jointly using augmented parse trees.', 'In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.21041', '(95%-conf.int.', '0.21041', '-', '0.21041)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.34767', '(95%-conf.int.', '0.34767', '-', '0.34767)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:190', 'F:190']
['The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.']
['8.', '2.', 'In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:153', 'F:0']
['In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.']
['A Novel Use of Statistical Parsing to Extract Information from Text', 'In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.25641', '(95%-conf.int.', '0.25641', '-', '0.25641)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.40816', '(95%-conf.int.', '0.40816', '-', '0.40816)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:190', 'F:190']
['In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.']
['In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00093', '(95%-conf.int.', '0.00093', '-', '0.00093)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00178', '(95%-conf.int.', '0.00178', '-', '0.00178)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:45', 'F:1']
['We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.']
['Our integrated model represents syntax and semantics jointly using augmented parse trees.', 'In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.09059', '(95%-conf.int.', '0.09059', '-', '0.09059)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.16613', '(95%-conf.int.', '0.16613', '-', '0.16613)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:861', 'P:78', 'F:78']
['In this section, we describe the algorithm that was used to automatically produce augmented trees, starting with a) human-generated semantic annotations and b) machinegenerated syntactic parse trees.']
['Our integrated model represents syntax and semantics jointly using augmented parse trees.', 'In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00554', '(95%-conf.int.', '0.00554', '-', '0.00554)']
['system', 'ROUGE-S*', 'Average_P:', '0.04167', '(95%-conf.int.', '0.04167', '-', '0.04167)']
['system', 'ROUGE-S*', 'Average_F:', '0.00978', '(95%-conf.int.', '0.00978', '-', '0.00978)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:120', 'F:5']
['Since 1995, a few statistical parsing algorithms (Magerman, 1995; Collins, 1996 and 1997; Charniak, 1997; Rathnaparki, 1997) demonstrated a breakthrough in parsing accuracy, as measured against the University of Pennsylvania TREEBANK as a gold standard.']
['In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.', 'We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.', 'In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1081', 'P:253', 'F:0']
0.46204461183 0.0688353840859 0.11688922987





input/ref/Task1/P11-1060_swastika.csv
input/res/Task1/P11-1060.csv
parsing: input/ref/Task1/P11-1060_swastika.csv
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['aim_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['aim_citation']
    <S sid="148" ssid="33">This bootstrapping behavior occurs naturally: The &#8220;easy&#8221; examples are processed first, where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S>
original cit marker offset is 0
new cit marker offset is 0



['148']
148
['148']
parsed_discourse_facet ['method_citation']
    <S sid="13" ssid="9">We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['result_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="47" ssid="23">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
<S sid="112" ssid="88">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>
original cit marker offset is 0
new cit marker offset is 0



['112']
112
['112']
parsed_discourse_facet ['method_citation']
S sid="44" ssid="20">The recurrence is as follows: At each node, we compute the set of tuples v consistent with the predicate at that node (v &#8712; w(p)), and S(x)}, where a set of pairs S is treated as a set-valued function S(x) = {y : (x, y) &#8712; S} with domain S1 = {x : (x, y) &#8712; S}.</S>
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
Error in Reference Offset
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



['106']
106
['106']
parsed_discourse_facet ['aim_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
    <S sid="172" ssid="57">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms, we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus, and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S>
original cit marker offset is 0
new cit marker offset is 0



['172']
172
['172']
parsed_discourse_facet ['result_citation']
<S sid="21" ssid="17">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['aim_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



['171']
171
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'2'"]
'100'
'21'
'2'
['100', '21', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="23">We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'21'", "'23'"]
'2'
'21'
'23'
['2', '21', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'2'"]
'0'
'21'
'2'
['0', '21', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'25'"]
'21'
'0'
'25'
['21', '0', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'106'"]
'21'
'0'
'106'
['21', '0', '106']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'100'"]
'21'
'0'
'100'
['21', '0', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'100'"]
'0'
'21'
'100'
['0', '21', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'100'", "'2'"]
'21'
'100'
'2'
['21', '100', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'25'"]
'21'
'0'
'25'
['21', '0', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="23">We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'23'"]
'0'
'21'
'23'
['0', '21', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'25'"]
'21'
'0'
'25'
['21', '0', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'25'"]
'0'
'21'
'25'
['0', '21', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="174">598</S><S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'174'", "'106'"]
'21'
'174'
'106'
['21', '174', '106']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'21'", "'100'"]
'106'
'21'
'100'
['106', '21', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="23">We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'0'", "'23'"]
'100'
'0'
'23'
['100', '0', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'106'"]
'21'
'0'
'106'
['21', '0', '106']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'0'"]
'100'
'21'
'0'
['100', '21', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'100'"]
'0'
'21'
'100'
['0', '21', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="12">We represent logical forms z as labeled trees, induced automatically from (x, y) pairs.</S><S ssid="1" sid="17">The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'12'", "'17'"]
'21'
'12'
'17'
['21', '12', '17']
parsed_discourse_facet ['method_citation']
['Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.']
[u'Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.', u'Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(\u03b8) = E(x,y)ED log p\u03b8(JzKw = y  |x, z \u2208 ZL(x)) \u2212 \u03bbk\u03b8k22, which sums over all DCS trees z that evaluate to the target answer y.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00887', '(95%-conf.int.', '0.00887', '-', '0.00887)']
['system', 'ROUGE-S*', 'Average_P:', '0.03333', '(95%-conf.int.', '0.03333', '-', '0.03333)']
['system', 'ROUGE-S*', 'Average_F:', '0.01401', '(95%-conf.int.', '0.01401', '-', '0.01401)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:300', 'F:10']
['The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['Learning Dependency-Based Compositional Semantics', u'Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(\u03b8) = E(x,y)ED log p\u03b8(JzKw = y  |x, z \u2208 ZL(x)) \u2212 \u03bbk\u03b8k22, which sums over all DCS trees z that evaluate to the target answer y.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.12805', '(95%-conf.int.', '0.12805', '-', '0.12805)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.22703', '(95%-conf.int.', '0.22703', '-', '0.22703)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:105', 'F:105']
['Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.']
['Learning Dependency-Based Compositional Semantics', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).', 'We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).']
['system', 'ROUGE-S*', 'Average_R:', '0.00430', '(95%-conf.int.', '0.00430', '-', '0.00430)']
['system', 'ROUGE-S*', 'Average_P:', '0.00667', '(95%-conf.int.', '0.00667', '-', '0.00667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00523', '(95%-conf.int.', '0.00523', '-', '0.00523)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:300', 'F:2']
['This bootstrapping behavior occurs naturally: The &#8220;easy&#8221; examples are processed first, where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.']
[u'Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.', 'In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00128', '(95%-conf.int.', '0.00128', '-', '0.00128)']
['system', 'ROUGE-S*', 'Average_P:', '0.00476', '(95%-conf.int.', '0.00476', '-', '0.00476)']
['system', 'ROUGE-S*', 'Average_F:', '0.00202', '(95%-conf.int.', '0.00202', '-', '0.00202)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:210', 'F:1']
['This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.']
['We represent logical forms z as labeled trees, induced automatically from (x, y) pairs.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).', 'The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner.']
['system', 'ROUGE-S*', 'Average_R:', '0.05546', '(95%-conf.int.', '0.05546', '-', '0.05546)']
['system', 'ROUGE-S*', 'Average_P:', '0.17368', '(95%-conf.int.', '0.17368', '-', '0.17368)']
['system', 'ROUGE-S*', 'Average_F:', '0.08408', '(95%-conf.int.', '0.08408', '-', '0.08408)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:190', 'F:33']
['The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['Learning Dependency-Based Compositional Semantics', u'Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.21169', '(95%-conf.int.', '0.21169', '-', '0.21169)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.34942', '(95%-conf.int.', '0.34942', '-', '0.34942)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:105', 'F:105']
['We want to induce latent logical forms z (and parameters 0) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs.']
['Learning Dependency-Based Compositional Semantics', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).', 'We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:66', 'F:0']
['The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['Learning Dependency-Based Compositional Semantics', u'Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(\u03b8) = E(x,y)ED log p\u03b8(JzKw = y  |x, z \u2208 ZL(x)) \u2212 \u03bbk\u03b8k22, which sums over all DCS trees z that evaluate to the target answer y.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.12805', '(95%-conf.int.', '0.12805', '-', '0.12805)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.22703', '(95%-conf.int.', '0.22703', '-', '0.22703)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:105', 'F:105']
['Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.']
[u'Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.', 'In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00256', '(95%-conf.int.', '0.00256', '-', '0.00256)']
['system', 'ROUGE-S*', 'Average_P:', '0.00667', '(95%-conf.int.', '0.00667', '-', '0.00667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00370', '(95%-conf.int.', '0.00370', '-', '0.00370)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:300', 'F:2']
['Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms, we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus, and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.']
['Learning Dependency-Based Compositional Semantics', u'Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:630', 'F:0']
['We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.']
['Learning Dependency-Based Compositional Semantics', u'Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.05847', '(95%-conf.int.', '0.05847', '-', '0.05847)']
['system', 'ROUGE-S*', 'Average_P:', '0.16959', '(95%-conf.int.', '0.16959', '-', '0.16959)']
['system', 'ROUGE-S*', 'Average_F:', '0.08696', '(95%-conf.int.', '0.08696', '-', '0.08696)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:171', 'F:29']
0.308609088104 0.0544299995052 0.0908618173558





input/ref/Task1/P08-1043_sweta.csv
input/res/Task1/P08-1043.csv
parsing: input/ref/Task1/P08-1043_sweta.csv
<S sid="156" ssid="34">To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["4'"]
4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="2">Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.</S>
original cit marker offset is 0
new cit marker offset is 0



["70'"]
70'
['70']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="3">Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="9">Tsarfaty (2006) used a morphological analyzer (Segal, 2000), a PoS tagger (Bar-Haim et al., 2005), and a general purpose parser (Schmid, 2000) in an integrated framework in which morphological and syntactic components interact to share information, leading to improved performance on the joint task.</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="107" ssid="39">Firstly, Hebrew unknown tokens are doubly unknown: each unknown token may correspond to several segmentation possibilities, and each segment in such sequences may be able to admit multiple PoS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["107'"]
107'
['107']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="10">The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["14'"]
14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="12">The current work treats both segmental and super-segmental phenomena, yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg, 2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="53" ssid="11">Both (Tsarfaty, 2006; Cohen and Smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has shown a single generative model which handles both tasks.</S>
original cit marker offset is 0
new cit marker offset is 0



["53'"]
53'
['53']
parsed_discourse_facet ['method_citation']
 <S sid="48" ssid="6">Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["48'"]
48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="141" ssid="19">This analyzer setting is similar to that of (Cohen and Smith, 2007), and models using it are denoted nohsp, Parser and Grammar We used BitPar (Schmid, 2004), an efficient general purpose parser,10 together with various treebank grammars to parse the input sentences and propose compatible morphological segmentation and syntactic analysis.</S>
original cit marker offset is 0
new cit marker offset is 0



["141'"]
141'
['141']
parsed_discourse_facet ['method_citation']
<S sid="188" ssid="2">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



["188'"]
188'
['188']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="26">Our use of an unweighted lattice reflects our belief that all the segmentations of the given input sentence are a-priori equally likely; the only reason to prefer one segmentation over the another is due to the overall syntactic context which is modeled via the PCFG derivations.</S>
original cit marker offset is 0
new cit marker offset is 0



["94'"]
94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="12">Such resources exist for Hebrew (Itai et al., 2006), but unfortunately use a tagging scheme which is incompatible with the one of the Hebrew Treebank.s For this reason, we use a data-driven morphological analyzer derived from the training data similar to (Cohen and Smith, 2007).</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
    <S sid="156" ssid="34">To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth, &#8216;tokens&#8217;) that constitute the unanalyzed surface forms (utterances).</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1043.csv
<S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'", "'4'", "'0'"]
'19'
'4'
'0'
['19', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="186">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'186'", "'19'", "'4'"]
'186'
'19'
'4'
['186', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'0'", "'19'"]
'4'
'0'
'19'
['4', '0', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="186">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'186'"]
'0'
'19'
'186'
['0', '19', '186']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'19'"]
'0'
'4'
'19'
['0', '4', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'4'"]
'0'
'19'
'4'
['0', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="116">We simulate lexical constraints by using an external lexical resource against which we verify whether OOV segments are in fact valid Hebrew lexemes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'116'"]
'0'
'19'
'116'
['0', '19', '116']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'19'"]
'0'
'4'
'19'
['0', '4', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'4'"]
'0'
'19'
'4'
['0', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="117">This heuristics is used to prune all segmentation possibilities involving “lexically improper” segments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'117'"]
'0'
'4'
'117'
['0', '4', '117']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'4'"]
'0'
'19'
'4'
['0', '19', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="188">The overall performance of our joint framework demonstrates that a probability distribution obtained over mere syntactic contexts using a Treebank grammar and a data-driven lexicon outperforms upper bounds proposed by previous joint disambiguation systems and achieves segmentation and parsing results on a par with state-of-the-art standalone applications results.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'4'", "'188'"]
'0'
'4'
'188'
['0', '4', '188']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S><S ssid="1" sid="186">This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'19'", "'186'"]
'0'
'19'
'186'
['0', '19', '186']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="136">Lexicon and OOV Handling Our data-driven morphological-analyzer proposes analyses for unknown tokens as described in Section 5.</S><S ssid="1" sid="133">Morphological Analyzer Ideally, we would use an of-the-shelf morphological analyzer for mapping each input token to its possible analyses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'136'", "'133'"]
'0'
'136'
'133'
['0', '136', '133']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing</S><S ssid="1" sid="69">We represent all morphological analyses of a given utterance using a lattice structure.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'69'", "'19'"]
'0'
'69'
'19'
['0', '69', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.</S><S ssid="1" sid="135">We construct a mapping from all the space-delimited tokens seen in the training sentences to their corresponding analyses.</S><S ssid="1" sid="19">Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'135'", "'19'"]
'4'
'135'
'19'
['4', '135', '19']
parsed_discourse_facet ['method_citation']
['The current work treats both segmental and super-segmental phenomena, yet we note that there may be more adequate ways to treat supersegmental phenomena assuming Word-Based morphology as we explore in (Tsarfaty and Goldberg, 2008).']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['system', 'ROUGE-S*', 'Average_R:', '0.00226', '(95%-conf.int.', '0.00226', '-', '0.00226)']
['system', 'ROUGE-S*', 'Average_P:', '0.01429', '(95%-conf.int.', '0.01429', '-', '0.01429)']
['system', 'ROUGE-S*', 'Average_F:', '0.00391', '(95%-conf.int.', '0.00391', '-', '0.00391)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:210', 'F:3']
['To evaluate the performance on the segmentation task, we report SEG, the standard harmonic means for segmentation Precision and Recall F1 (as defined in Bar-Haim et al. (2005); Tsarfaty (2006)) as well as the segmentation accuracy SEGTok measure indicating the percentage of input tokens assigned the correct exact segmentation (as reported by Cohen and Smith (2007)).']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['system', 'ROUGE-S*', 'Average_R:', '0.00452', '(95%-conf.int.', '0.00452', '-', '0.00452)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00613', '(95%-conf.int.', '0.00613', '-', '0.00613)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:630', 'F:6']
['Tsarfaty (2006) was the first to demonstrate that fully automatic Hebrew parsing is feasible using the newly available 5000 sentences treebank.']
['A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', u'This heuristics is used to prune all segmentation possibilities involving \u201clexically improper\u201d segments.', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['system', 'ROUGE-S*', 'Average_R:', '0.00106', '(95%-conf.int.', '0.00106', '-', '0.00106)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00195', '(95%-conf.int.', '0.00195', '-', '0.00195)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:78', 'F:1']
['Here we propose a single joint model for performing both morphological segmentation and syntactic disambiguation which bypasses the associated circularity.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.']
['system', 'ROUGE-S*', 'Average_R:', '0.03904', '(95%-conf.int.', '0.03904', '-', '0.03904)']
['system', 'ROUGE-S*', 'Average_P:', '0.47273', '(95%-conf.int.', '0.47273', '-', '0.47273)']
['system', 'ROUGE-S*', 'Average_F:', '0.07212', '(95%-conf.int.', '0.07212', '-', '0.07212)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:55', 'F:26']
['Each lattice arc corresponds to a segment and its corresponding PoS tag, and a path through the lattice corresponds to a specific morphological segmentation of the utterance.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['system', 'ROUGE-S*', 'Average_R:', '0.00226', '(95%-conf.int.', '0.00226', '-', '0.00226)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00427', '(95%-conf.int.', '0.00427', '-', '0.00427)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:78', 'F:3']
['Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'This fully generative model caters for real interaction between the syntactic and morphological levels as a part of a single coherent process.', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['system', 'ROUGE-S*', 'Average_R:', '0.23684', '(95%-conf.int.', '0.23684', '-', '0.23684)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.38298', '(95%-conf.int.', '0.38298', '-', '0.38298)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:378', 'F:378']
['The input for the segmentation task is however highly ambiguous for Semitic languages, and surface forms (tokens) may admit multiple possible analyses as in (BarHaim et al., 2007; Adler and Elhadad, 2006).']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'We simulate lexical constraints by using an external lexical resource against which we verify whether OOV segments are in fact valid Hebrew lexemes.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:171', 'F:0']
['Current state-of-the-art broad-coverage parsers assume a direct correspondence between the lexical items ingrained in the proposed syntactic analyses (the yields of syntactic parse-trees) and the spacedelimited tokens (henceforth, &#8216;tokens&#8217;) that constitute the unanalyzed surface forms (utterances).']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.', 'We construct a mapping from all the space-delimited tokens seen in the training sentences to their corresponding analyses.']
['system', 'ROUGE-S*', 'Average_R:', '0.00603', '(95%-conf.int.', '0.00603', '-', '0.00603)']
['system', 'ROUGE-S*', 'Average_P:', '0.01839', '(95%-conf.int.', '0.01839', '-', '0.01839)']
['system', 'ROUGE-S*', 'Average_F:', '0.00909', '(95%-conf.int.', '0.00909', '-', '0.00909)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:435', 'F:8']
['Tsarfaty (2006) used a morphological analyzer (Segal, 2000), a PoS tagger (Bar-Haim et al., 2005), and a general purpose parser (Schmid, 2000) in an integrated framework in which morphological and syntactic components interact to share information, leading to improved performance on the joint task.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['system', 'ROUGE-S*', 'Average_R:', '0.01508', '(95%-conf.int.', '0.01508', '-', '0.01508)']
['system', 'ROUGE-S*', 'Average_P:', '0.04598', '(95%-conf.int.', '0.04598', '-', '0.04598)']
['system', 'ROUGE-S*', 'Average_F:', '0.02271', '(95%-conf.int.', '0.02271', '-', '0.02271)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:435', 'F:20']
['Both (Tsarfaty, 2006; Cohen and Smith, 2007) have shown that a single integrated framework outperforms a completely streamlined implementation, yet neither has shown a single generative model which handles both tasks.']
['Here we push the single-framework conjecture across the board and present a single model that performs morphological segmentation and syntactic disambiguation in a fully generative framework.', 'A Single Generative Model for Joint Morphological Segmentation and Syntactic Parsing', 'Using a treebank grammar, a data-driven lexicon, and a linguistically motivated unknown-tokens handling technique our model outperforms previous pipelined, integrated or factorized systems for Hebrew morphological and syntactic processing, yielding an error reduction of 12% over the best published results so far.']
['system', 'ROUGE-S*', 'Average_R:', '0.01357', '(95%-conf.int.', '0.01357', '-', '0.01357)']
['system', 'ROUGE-S*', 'Average_P:', '0.10526', '(95%-conf.int.', '0.10526', '-', '0.10526)']
['system', 'ROUGE-S*', 'Average_F:', '0.02405', '(95%-conf.int.', '0.02405', '-', '0.02405)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1326', 'P:171', 'F:18']
0.171744998283 0.0320659996793 0.0527209994728





input/ref/Task1/D10-1044_sweta.csv
input/res/Task1/D10-1044.csv
parsing: input/ref/Task1/D10-1044_sweta.csv
<S sid="4" ssid="1">Domain adaptation is a common concern when optimizing empirical NLP applications.</S>
original cit marker offset is 0
new cit marker offset is 0



["4'"]
4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="1">We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="4">For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components (word-alignment model, language model, translation model, etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["7'"]
7'
['7']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="14">Linear weights are difficult to incorporate into the standard MERT procedure because they are &#8220;hidden&#8221; within a top-level probability that represents the linear combination.1 Following previous work (Foster and Kuhn, 2007), we circumvent this problem by choosing weights to optimize corpus loglikelihood, which is roughly speaking the training criterion used by the LM and TM themselves.</S>
original cit marker offset is 0
new cit marker offset is 0



["50'"]
50'
['50']
parsed_discourse_facet ['method_citation']
<S sid="152" ssid="9">We will also directly compare with a baseline similar to the Matsoukas et al approach in order to measure the benefit from weighting phrase pairs (or ngrams) rather than full sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["152'"]
152'
['152']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
 <S sid="9" ssid="6">In this paper we study the problem of using a parallel corpus from a background domain (OUT) to improve performance on a target domain (IN) for which a smaller amount of parallel training material&#8212;though adequate for reasonable performance&#8212;is also available.</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="97" ssid="1">We carried out translation experiments in two different settings.</S>
original cit marker offset is 0
new cit marker offset is 0



["97'"]
97'
['97']
parsed_discourse_facet ['method_citation']
<S sid="75" ssid="12">However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="12">Other work includes transferring latent topic distributions from source to target language for LM adaptation, (Tam et al., 2007) and adapting features at the sentence level to different categories of sentence (Finch and Sumita, 2008).</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="10">Finally, we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="1">In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="26">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="141" ssid="10">Moving beyond directly related work, major themes in SMT adaptation include the IR (Hildebrand et al., 2005; L&#168;u et al., 2007; Zhao et al., 2004) and mixture (Finch and Sumita, 2008; Foster and Kuhn, 2007; Koehn and Schroeder, 2007; L&#168;u et al., 2007) approaches for LMs and TMs described above, as well as methods for exploiting monolingual in-domain text, typically by translating it automatically and then performing self training (Bertoldi and Federico, 2009; Ueffing et al., 2007; Schwenk and Senellart, 2009).</S>
original cit marker offset is 0
new cit marker offset is 0



["141'"]
141'
['141']
parsed_discourse_facet ['method_citation']
 <S sid="28" ssid="25">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S>
original cit marker offset is 0
new cit marker offset is 0



["28'"]
28'
['28']
parsed_discourse_facet ['method_citation']
<S sid="37" ssid="1">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S>
original cit marker offset is 0
new cit marker offset is 0



["37'"]
37'
['37']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/D10-1044.csv
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="62">To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'37'", "'62'"]
'1'
'37'
'62'
['1', '37', '62']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="120">This significantly underperforms log-linear combination.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="3">We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'", "'37'", "'3'"]
'120'
'37'
'3'
['120', '37', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="68">First, we learn weights on individual phrase pairs rather than sentences.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'68'", "'71'"]
'37'
'68'
'71'
['37', '68', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="3">We incorporate instance weighting into a mixture-model framework, and find that it yields consistent improvements over a wide range of baselines.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'3'", "'37'"]
'1'
'3'
'37'
['1', '3', '37']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="0">Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'0'"]
'37'
'1'
'0'
['37', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="28">We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="120">This significantly underperforms log-linear combination.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'", "'37'", "'120'"]
'28'
'37'
'120'
['28', '37', '120']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="22">Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.</S><S ssid="1" sid="146">The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair’s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'22'", "'146'"]
'37'
'22'
'146'
['37', '22', '146']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="146">The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair’s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'146'"]
'37'
'1'
'146'
['37', '1', '146']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'37'", "'71'"]
'1'
'37'
'71'
['1', '37', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="148">Mixing, smoothing, and instance-feature weights are learned at the same time using an efficient maximum-likelihood procedure that relies on only a small in-domain development corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'148'"]
'37'
'1'
'148'
['37', '1', '148']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="1">We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.</S><S ssid="1" sid="71">Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where cλ(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'1'", "'71'"]
'37'
'1'
'71'
['37', '1', '71']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="37">Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.</S><S ssid="1" sid="148">Mixing, smoothing, and instance-feature weights are learned at the same time using an efficient maximum-likelihood procedure that relies on only a small in-domain development corpus.</S><S ssid="1" sid="151">In future work we plan to try this approach with more competitive SMT systems, and to extend instance weighting to other standard SMT components such as the LM, lexical phrase weights, and lexicalized distortion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'37'", "'148'", "'151'"]
'37'
'148'
'151'
['37', '148', '151']
parsed_discourse_facet ['method_citation']
['In this paper we have proposed an approach for instance-weighting phrase pairs in an out-of-domain corpus in order to improve in-domain performance.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', u'The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair\u2019s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.']
['system', 'ROUGE-S*', 'Average_R:', '0.00760', '(95%-conf.int.', '0.00760', '-', '0.00760)']
['system', 'ROUGE-S*', 'Average_P:', '0.16667', '(95%-conf.int.', '0.16667', '-', '0.16667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01453', '(95%-conf.int.', '0.01453', '-', '0.01453)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1711', 'P:78', 'F:13']
['To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', 'To approximate these baselines, we implemented a very simple sentence selection algorithm in which parallel sentence pairs from OUT are ranked by the perplexity of their target half according to the IN language model.', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.']
['system', 'ROUGE-S*', 'Average_R:', '0.08386', '(95%-conf.int.', '0.08386', '-', '0.08386)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.15474', '(95%-conf.int.', '0.15474', '-', '0.15474)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1431', 'P:120', 'F:120']
['We have already mentioned the closely related work by Matsoukas et al (2009) on discriminative corpus weighting, and Jiang and Zhai (2007) on (nondiscriminative) instance weighting.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.', u'Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c\u03bb(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.']
['system', 'ROUGE-S*', 'Average_R:', '0.00181', '(95%-conf.int.', '0.00181', '-', '0.00181)']
['system', 'ROUGE-S*', 'Average_P:', '0.02500', '(95%-conf.int.', '0.02500', '-', '0.02500)']
['system', 'ROUGE-S*', 'Average_F:', '0.00338', '(95%-conf.int.', '0.00338', '-', '0.00338)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:120', 'F:3']
['Domain adaptation is a common concern when optimizing empirical NLP applications.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', 'Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.']
['system', 'ROUGE-S*', 'Average_R:', '0.00097', '(95%-conf.int.', '0.00097', '-', '0.00097)']
['system', 'ROUGE-S*', 'Average_P:', '0.03571', '(95%-conf.int.', '0.03571', '-', '0.03571)']
['system', 'ROUGE-S*', 'Average_F:', '0.00188', '(95%-conf.int.', '0.00188', '-', '0.00188)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:28', 'F:1']
['Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.']
['In future work we plan to try this approach with more competitive SMT systems, and to extend instance weighting to other standard SMT components such as the LM, lexical phrase weights, and lexicalized distortion.', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.', 'Mixing, smoothing, and instance-feature weights are learned at the same time using an efficient maximum-likelihood procedure that relies on only a small in-domain development corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.14474', '(95%-conf.int.', '0.14474', '-', '0.14474)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.25287', '(95%-conf.int.', '0.25287', '-', '0.25287)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1596', 'P:231', 'F:231']
['However, it is robust, efficient, and easy to implement.4 To perform the maximization in (7), we used the popular L-BFGS algorithm (Liu and Nocedal, 1989), which requires gradient information.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', 'Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:136', 'F:0']
['Finally, we intend to explore more sophisticated instanceweighting features for capturing the degree of generality of phrase pairs.']
[u'The features are weighted within a logistic model to give an overall weight that is applied to the phrase pair\u2019s frequency prior to making MAP-smoothed relative-frequency estimates (different weights are learned for each conditioning direction).', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.', 'Within this framework, we use features intended to capture degree of generality, including the output from an SVM classifier that uses the intersection between IN and OUT as positive examples.']
['system', 'ROUGE-S*', 'Average_R:', '0.00779', '(95%-conf.int.', '0.00779', '-', '0.00779)']
['system', 'ROUGE-S*', 'Average_P:', '0.21818', '(95%-conf.int.', '0.21818', '-', '0.21818)']
['system', 'ROUGE-S*', 'Average_F:', '0.01505', '(95%-conf.int.', '0.01505', '-', '0.01505)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:55', 'F:12']
['We train linear mixture models for conditional phrase pair probabilities over IN and OUT so as to maximize the likelihood of an empirical joint phrase-pair distribution extracted from a development set.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.', u'Finally, we incorporate the instance-weighting model into a general linear combination, and learn weights and mixing parameters simultaneously. where c\u03bb(s, t) is a modified count for pair (s, t) in OUT, u(s|t) is a prior distribution, and y is a prior weight.']
['system', 'ROUGE-S*', 'Average_R:', '0.01210', '(95%-conf.int.', '0.01210', '-', '0.01210)']
['system', 'ROUGE-S*', 'Average_P:', '0.13072', '(95%-conf.int.', '0.13072', '-', '0.13072)']
['system', 'ROUGE-S*', 'Average_F:', '0.02215', '(95%-conf.int.', '0.02215', '-', '0.02215)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1653', 'P:153', 'F:20']
['For developers of Statistical Machine Translation (SMT) systems, an additional complication is the heterogeneous nature of SMT components (word-alignment model, language model, translation model, etc.']
['We describe a new approach to SMT adaptation that weights out-of-domain phrase pairs according to their relevance to the target domain, determined by both how similar to it they appear to be, and whether they belong to general language or not.', 'Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation', 'Standard SMT systems have a hierarchical parameter structure: top-level log-linear weights are used to combine a small set of complex features, interpreted as log probabilities, many of which have their own internal parameters and objectives.']
['system', 'ROUGE-S*', 'Average_R:', '0.01353', '(95%-conf.int.', '0.01353', '-', '0.01353)']
['system', 'ROUGE-S*', 'Average_P:', '0.08187', '(95%-conf.int.', '0.08187', '-', '0.08187)']
['system', 'ROUGE-S*', 'Average_F:', '0.02322', '(95%-conf.int.', '0.02322', '-', '0.02322)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:171', 'F:14']
0.295349996718 0.0302666663304 0.05420222162





input/ref/Task1/W06-2932_vardha.csv
input/res/Task1/W06-2932.csv
parsing: input/ref/Task1/W06-2932_vardha.csv
    <S sid="19" ssid="1">The first stage of our system creates an unlabeled parse y for an input sentence x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'19'"]
'19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="5">However, in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="76" ssid="14">For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'"]
'76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="54" ssid="2">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'54'"]
'54'
['54']
parsed_discourse_facet ['method_citation']
    <S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="58" ssid="6">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'"]
'58'
['58']
parsed_discourse_facet ['method_citation']
    <S sid="22" ssid="4">An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.</S>
original cit marker offset is 0
new cit marker offset is 0



["'22'"]
'22'
['22']
parsed_discourse_facet ['method_citation']
    <S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'41'"]
'41'
['41']
parsed_discourse_facet ['method_citation']
    <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'"]
'64'
['64']
parsed_discourse_facet ['method_citation']
    <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="57" ssid="5">Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the edge correctly in the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'57'"]
'57'
['57']
parsed_discourse_facet ['method_citation']
    <S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["'43'"]
'43'
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="41">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm�1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm−1) in the tree y.</S>
original cit marker offset is 
new cit marker offset is 0



["'0'", "'12'", "'41'"]
'0'
'12'
'41'
['0', '12', '41']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="38">Such a model could easily be trained using the provided training data for each language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'38'"]
'0'
'12'
'38'
['0', '12', '38']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="106">First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'106'"]
'0'
'12'
'106'
['0', '12', '106']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="106">First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'106'"]
'0'
'12'
'106'
['0', '12', '106']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="41">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm�1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm−1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'41'"]
'0'
'12'
'41'
['0', '12', '41']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'1'"]
'0'
'12'
'1'
['0', '12', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'1'"]
'0'
'12'
'1'
['0', '12', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="41">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm�1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm−1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'0'", "'41'"]
'12'
'0'
'41'
['12', '0', '41']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="16">A dependency graph is represented by a set of ordered pairs (i, j) E y in which xj is a dependent and xi is the corresponding head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'16'"]
'0'
'12'
'16'
['0', '12', '16']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="16">A dependency graph is represented by a set of ordered pairs (i, j) E y in which xj is a dependent and xi is the corresponding head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'16'"]
'0'
'12'
'16'
['0', '12', '16']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'1'"]
'0'
'12'
'1'
['0', '12', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="54">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'54'"]
'0'
'12'
'54'
['0', '12', '54']
parsed_discourse_facet ['aim_citation', 'result_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'3'"]
'0'
'12'
'3'
['0', '12', '3']
parsed_discourse_facet ['method_citation']
['Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.']
['Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.', u'To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm\ufffd1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm\u22121) in the tree y.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:300', 'F:0']
['An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge.']
['Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.', u'To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm\ufffd1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm\u22121) in the tree y.']
['system', 'ROUGE-S*', 'Average_R:', '0.00423', '(95%-conf.int.', '0.00423', '-', '0.00423)']
['system', 'ROUGE-S*', 'Average_P:', '0.02941', '(95%-conf.int.', '0.02941', '-', '0.02941)']
['system', 'ROUGE-S*', 'Average_F:', '0.00739', '(95%-conf.int.', '0.00739', '-', '0.00739)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:136', 'F:4']
['Performance is measured through unlabeled accuracy, which is the percentage of words that modify the correct head in the dependency graph, and labeled accuracy, which is the percentage of words that modify the correct head and label the dependency edge correctly in the graph.']
['Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.', 'First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.']
['system', 'ROUGE-S*', 'Average_R:', '0.02759', '(95%-conf.int.', '0.02759', '-', '0.02759)']
['system', 'ROUGE-S*', 'Average_P:', '0.04743', '(95%-conf.int.', '0.04743', '-', '0.04743)']
['system', 'ROUGE-S*', 'Average_F:', '0.03488', '(95%-conf.int.', '0.03488', '-', '0.03488)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:253', 'F:12']
['N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.']
['Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.', 'A dependency graph is represented by a set of ordered pairs (i, j) E y in which xj is a dependent and xi is the corresponding head.']
['system', 'ROUGE-S*', 'Average_R:', '0.00794', '(95%-conf.int.', '0.00794', '-', '0.00794)']
['system', 'ROUGE-S*', 'Average_P:', '0.01186', '(95%-conf.int.', '0.01186', '-', '0.01186)']
['system', 'ROUGE-S*', 'Average_F:', '0.00951', '(95%-conf.int.', '0.00951', '-', '0.00951)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:253', 'F:3']
['For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.']
['Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['system', 'ROUGE-S*', 'Average_R:', '0.00178', '(95%-conf.int.', '0.00178', '-', '0.00178)']
['system', 'ROUGE-S*', 'Average_P:', '0.00433', '(95%-conf.int.', '0.00433', '-', '0.00433)']
['system', 'ROUGE-S*', 'Average_F:', '0.00253', '(95%-conf.int.', '0.00253', '-', '0.00253)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:231', 'F:1']
['In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.', 'Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['system', 'ROUGE-S*', 'Average_R:', '0.18803', '(95%-conf.int.', '0.18803', '-', '0.18803)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.31655', '(95%-conf.int.', '0.31655', '-', '0.31655)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:66', 'F:66']
['We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.']
['present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.', 'Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['system', 'ROUGE-S*', 'Average_R:', '0.01140', '(95%-conf.int.', '0.01140', '-', '0.01140)']
['system', 'ROUGE-S*', 'Average_P:', '0.02339', '(95%-conf.int.', '0.02339', '-', '0.02339)']
['system', 'ROUGE-S*', 'Average_F:', '0.01533', '(95%-conf.int.', '0.01533', '-', '0.01533)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:171', 'F:4']
['To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.']
['Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.', 'A dependency graph is represented by a set of ordered pairs (i, j) E y in which xj is a dependent and xi is the corresponding head.']
['system', 'ROUGE-S*', 'Average_R:', '0.00794', '(95%-conf.int.', '0.00794', '-', '0.00794)']
['system', 'ROUGE-S*', 'Average_P:', '0.00794', '(95%-conf.int.', '0.00794', '-', '0.00794)']
['system', 'ROUGE-S*', 'Average_F:', '0.00794', '(95%-conf.int.', '0.00794', '-', '0.00794)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:378', 'F:3']
['For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.']
['Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.', 'First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.']
['system', 'ROUGE-S*', 'Average_R:', '0.02299', '(95%-conf.int.', '0.02299', '-', '0.02299)']
['system', 'ROUGE-S*', 'Average_P:', '0.01587', '(95%-conf.int.', '0.01587', '-', '0.01587)']
['system', 'ROUGE-S*', 'Average_F:', '0.01878', '(95%-conf.int.', '0.01878', '-', '0.01878)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:630', 'F:10']
0.126692220815 0.0302111107754 0.0458788883791





input/ref/Task1/P04-1036_swastika.csv
input/res/Task1/P04-1036.csv
parsing: input/ref/Task1/P04-1036_swastika.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="24">We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.</S>
original cit marker offset is 0
new cit marker offset is 0



['68']
68
['68']
parsed_discourse_facet ['result_citation']
<S sid="83" ssid="12">The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.</S>
original cit marker offset is 0
new cit marker offset is 0



['83']
83
['83']
parsed_discourse_facet ['result_citation']
<S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
<S sid="101" ssid="30">Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.</S>
original cit marker offset is 0
new cit marker offset is 0



['101']
101
['101']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="83" ssid="12">The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.</S>
original cit marker offset is 0
new cit marker offset is 0



['83']
83
['83']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="46" ssid="2">This provides the nearest neighbours to each target word, along with the distributional similarity score between the target word and its neighbour.</S>
original cit marker offset is 0
new cit marker offset is 0



['46']
46
['46']
parsed_discourse_facet ['result_citation']
<S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



['126']
126
['126']
parsed_discourse_facet ['result_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="3">We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents, rather than a lexical sample task, where the target words are manually determined and the results will depend on the skew of the words in the sample.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
<S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['method_citation']
<S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



['13']
13
['13']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P04-1036.csv
<S ssid="1" sid="107">To disambiguate senses a system should take context into account.</S><S ssid="1" sid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'1'", "'153'"]
'107'
'1'
'153'
['107', '1', '153']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="162">It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'162'"]
'179'
'178'
'162'
['179', '178', '162']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="34">In these each target word is entered with an ordered list of “nearest neighbours”.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'34'"]
'179'
'153'
'34'
['179', '153', '34']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="0">Finding Predominant Word Senses in Untagged Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'0'"]
'179'
'4'
'0'
['179', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'153'"]
'179'
'4'
'153'
['179', '4', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="169">This method obtains precision of 61% and recall 51%.</S><S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'", "'179'", "'4'"]
'169'
'179'
'4'
['169', '179', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'153'"]
'179'
'4'
'153'
['179', '4', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="0">Finding Predominant Word Senses in Untagged Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'0'"]
'179'
'4'
'0'
['179', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="62">2 The WordNet Similarity package supports a range of WordNet similarity scores.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'62'", "'4'"]
'179'
'62'
'4'
['179', '62', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'179'", "'178'"]
'4'
'179'
'178'
['4', '179', '178']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'36'"]
'179'
'4'
'36'
['179', '4', '36']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'153'"]
'179'
'178'
'153'
['179', '178', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'36'"]
'179'
'178'
'36'
['179', '178', '36']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="162">It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'162'", "'178'"]
'4'
'162'
'178'
['4', '162', '178']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'4'"]
'179'
'153'
'4'
['179', '153', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'4'"]
'179'
'153'
'4'
['179', '153', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'36'"]
'179'
'153'
'36'
['179', '153', '36']
parsed_discourse_facet ['method_citation']
['We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.']
['Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.', u'In these each target word is entered with an ordered list of \u201cnearest neighbours\u201d.']
['system', 'ROUGE-S*', 'Average_R:', '0.00308', '(95%-conf.int.', '0.00308', '-', '0.00308)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00465', '(95%-conf.int.', '0.00465', '-', '0.00465)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:325', 'P:105', 'F:1']
['The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.']
['Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.', 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.']
['system', 'ROUGE-S*', 'Average_R:', '0.00535', '(95%-conf.int.', '0.00535', '-', '0.00535)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00974', '(95%-conf.int.', '0.00974', '-', '0.00974)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:55', 'F:3']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['Finding Predominant Word Senses in Untagged Text', 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:66', 'F:0']
['We use an allwords task because the predominant senses will reflect the sense distributions of all nouns within the documents, rather than a lexical sample task, where the target words are manually determined and the results will depend on the skew of the words in the sample.']
['We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.', 'It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text.', 'We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.']
['system', 'ROUGE-S*', 'Average_R:', '0.00901', '(95%-conf.int.', '0.00901', '-', '0.00901)']
['system', 'ROUGE-S*', 'Average_P:', '0.02857', '(95%-conf.int.', '0.02857', '-', '0.02857)']
['system', 'ROUGE-S*', 'Average_F:', '0.01370', '(95%-conf.int.', '0.01370', '-', '0.01370)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:210', 'F:6']
['The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.']
['Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.', 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.']
['system', 'ROUGE-S*', 'Average_R:', '0.00535', '(95%-conf.int.', '0.00535', '-', '0.00535)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00974', '(95%-conf.int.', '0.00974', '-', '0.00974)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:55', 'F:3']
['Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.']
['This method obtains precision of 61% and recall 51%.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.', 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:55', 'F:0']
['The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with the jcn and lesk WordNet similarity measures.']
['2 The WordNet Similarity package supports a range of WordNet similarity scores.', 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.']
['system', 'ROUGE-S*', 'Average_R:', '0.01379', '(95%-conf.int.', '0.01379', '-', '0.01379)']
['system', 'ROUGE-S*', 'Average_P:', '0.03509', '(95%-conf.int.', '0.03509', '-', '0.03509)']
['system', 'ROUGE-S*', 'Average_F:', '0.01980', '(95%-conf.int.', '0.01980', '-', '0.01980)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:171', 'F:6']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.', 'Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.']
['system', 'ROUGE-S*', 'Average_R:', '0.00460', '(95%-conf.int.', '0.00460', '-', '0.00460)']
['system', 'ROUGE-S*', 'Average_P:', '0.03030', '(95%-conf.int.', '0.03030', '-', '0.03030)']
['system', 'ROUGE-S*', 'Average_F:', '0.00798', '(95%-conf.int.', '0.00798', '-', '0.00798)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:66', 'F:2']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.', 'We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:66', 'F:0']
['We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.']
['Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.', 'We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:28', 'F:0']
0.0212579997874 0.00411799995882 0.00656099993439





input/ref/Task1/W99-0623_sweta.csv
input/res/Task1/W99-0623.csv
parsing: input/ref/Task1/W99-0623_sweta.csv
 <S sid="144" ssid="6">Combining multiple highly-accurate independent parsers yields promising results.</S>
original cit marker offset is 0
new cit marker offset is 0



["144'"]
144'
['144']
parsed_discourse_facet ['method_citation']
 <S sid="125" ssid="54">The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["125'"]
125'
['125']
parsed_discourse_facet ['method_citation']
<S sid="125" ssid="54">The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["125'"]
125'
['125']
parsed_discourse_facet ['method_citation']
    <S sid="48" ssid="34">&#8226; Similarly, when the na&#239;ve Bayes classifier is configured such that the constituents require estimated probabilities strictly larger than 0.5 to be accepted, there is not enough probability mass remaining on crossing brackets for them to be included in the hypothesis.</S>
original cit marker offset is 0
new cit marker offset is 0



["48'"]
48'
['48']
parsed_discourse_facet ['method_citation']
 <S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="63">As seen by the drop in average individual parser performance baseline, the introduced parser does not perform very well.</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
    <S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="13" ssid="9">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993).</S>
original cit marker offset is 0
new cit marker offset is 0



["13'"]
13'
['13']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="37">From this we see that a finer-grained model for parser combination, at least for the features we have examined, will not give us any additional power.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["139'"]
139'
['139']
parsed_discourse_facet ['method_citation']
<S sid="98" ssid="27">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["98'"]
98'
['98']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="13">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'"]
27'
['27']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="9">For our experiments we also report the mean of precision and recall, which we denote by (P + R)I2 and F-measure.</S>
    <S sid="81" ssid="10">F-measure is the harmonic mean of precision and recall, 2PR/(P + R).</S>
    <S sid="82" ssid="11">It is closer to the smaller value of precision and recall when there is a large skew in their values.</S>
original cit marker offset is 0
new cit marker offset is 0



["80'", "'81'", "'82'"]
80'
'81'
'82'
['80', '81', '82']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="35">In general, the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["49'"]
49'
['49']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="7">Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["11'"]
11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="98" ssid="27">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S>
original cit marker offset is 0
new cit marker offset is 0



["98'"]
98'
['98']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0623.csv
<S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'16'", "'23'"]
'58'
'16'
'23'
['58', '16', '23']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="21">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'21'"]
'23'
'1'
'21'
['23', '1', '21']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'58'", "'16'"]
'23'
'58'
'16'
['23', '58', '16']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'58'", "'16'"]
'23'
'58'
'16'
['23', '58', '16']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="28">The development of a naïve Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'23'", "'28'"]
'58'
'23'
'28'
['58', '23', '28']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="58">We call this approach parser switching.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'58'"]
'23'
'16'
'58'
['23', '16', '58']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'1'"]
'23'
'16'
'1'
['23', '16', '1']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="58">We call this approach parser switching.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'58'"]
'23'
'16'
'58'
['23', '16', '58']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="14">We used these three parsers to explore parser combination techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'23'", "'14'"]
'1'
'23'
'14'
['1', '23', '14']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="58">We call this approach parser switching.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'58'"]
'23'
'16'
'58'
['23', '16', '58']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'23'", "'1'"]
'16'
'23'
'1'
['16', '23', '1']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="28">The development of a naïve Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'23'", "'28'"]
'16'
'23'
'28'
['16', '23', '28']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'16'", "'23'"]
'1'
'16'
'23'
['1', '16', '23']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
['Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.']
['Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.', 'We call this approach parse hybridization.', 'We call this technique constituent voting.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:105', 'F:0']
['It is closer to the smaller value of precision and recall when there is a large skew in their values.', 'For our experiments we also report the mean of precision and recall, which we denote by (P + R)I2 and F-measure.', 'F-measure is the harmonic mean of precision and recall, 2PR/(P + R).']
['We call this technique constituent voting.', u'The development of a na\xefve Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.', 'We call this approach parse hybridization.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:171', 'P:171', 'F:0']
['We have presented two general approaches to studying parser combination: parser switching and parse hybridization.']
['Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.', 'We call this approach parse hybridization.', 'We call this technique constituent voting.']
['system', 'ROUGE-S*', 'Average_R:', '0.04286', '(95%-conf.int.', '0.04286', '-', '0.04286)']
['system', 'ROUGE-S*', 'Average_P:', '0.20000', '(95%-conf.int.', '0.20000', '-', '0.20000)']
['system', 'ROUGE-S*', 'Average_F:', '0.07059', '(95%-conf.int.', '0.07059', '-', '0.07059)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:45', 'F:9']
['In general, the lemma of the previous section does not ensure that all the productions in the combined parse are found in the grammars of the member parsers.']
['Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.', 'We call this technique constituent voting.', 'We call this approach parse hybridization.']
['system', 'ROUGE-S*', 'Average_R:', '0.00476', '(95%-conf.int.', '0.00476', '-', '0.00476)']
['system', 'ROUGE-S*', 'Average_P:', '0.01515', '(95%-conf.int.', '0.01515', '-', '0.01515)']
['system', 'ROUGE-S*', 'Average_F:', '0.00725', '(95%-conf.int.', '0.00725', '-', '0.00725)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:66', 'F:1']
['The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.']
['Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.', "One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.", 'We call this technique constituent voting.']
['system', 'ROUGE-S*', 'Average_R:', '0.01667', '(95%-conf.int.', '0.01667', '-', '0.01667)']
['system', 'ROUGE-S*', 'Average_P:', '0.04762', '(95%-conf.int.', '0.04762', '-', '0.04762)']
['system', 'ROUGE-S*', 'Average_F:', '0.02469', '(95%-conf.int.', '0.02469', '-', '0.02469)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:300', 'P:105', 'F:5']
['Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.']
['Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.', 'We call this approach parse hybridization.', 'We call this technique constituent voting.']
['system', 'ROUGE-S*', 'Average_R:', '0.02857', '(95%-conf.int.', '0.02857', '-', '0.02857)']
['system', 'ROUGE-S*', 'Average_P:', '0.05000', '(95%-conf.int.', '0.05000', '-', '0.05000)']
['system', 'ROUGE-S*', 'Average_F:', '0.03636', '(95%-conf.int.', '0.03636', '-', '0.03636)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:120', 'F:6']
['The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.']
['Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.', 'We call this approach parse hybridization.', 'We call this technique constituent voting.']
['system', 'ROUGE-S*', 'Average_R:', '0.04286', '(95%-conf.int.', '0.04286', '-', '0.04286)']
['system', 'ROUGE-S*', 'Average_P:', '0.03000', '(95%-conf.int.', '0.03000', '-', '0.03000)']
['system', 'ROUGE-S*', 'Average_F:', '0.03529', '(95%-conf.int.', '0.03529', '-', '0.03529)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:300', 'F:9']
['The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.']
['We call this approach parse hybridization.', 'We call this approach parser switching.', 'We call this technique constituent voting.']
['system', 'ROUGE-S*', 'Average_R:', '0.01515', '(95%-conf.int.', '0.01515', '-', '0.01515)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.01170', '(95%-conf.int.', '0.01170', '-', '0.01170)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:66', 'P:105', 'F:1']
['From this we see that a finer-grained model for parser combination, at least for the features we have examined, will not give us any additional power.']
['We call this approach parse hybridization.', 'We call this approach parser switching.', 'We call this technique constituent voting.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:66', 'P:45', 'F:0']
0.0391433328984 0.0167633331471 0.0206533331039





input/ref/Task1/W06-2932_sweta.csv
input/res/Task1/W06-2932.csv
parsing: input/ref/Task1/W06-2932_sweta.csv
 <S sid="5" ssid="1">Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing.</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="5">However, in a two stage system we can incorporate features over the entire output of the unlabeled parser since that structure is fixed as input.</S>
original cit marker offset is 0
new cit marker offset is 0



["36'"]
36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="9">In fact, for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="76" ssid="14">For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.</S>
original cit marker offset is 0
new cit marker offset is 0



["76'"]
76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="45" ssid="14">Furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira, 2006).</S>
original cit marker offset is 0
new cit marker offset is 0



["45'"]
45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="3">First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["106'"]
106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="8">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="58" ssid="6">These results show that the discriminative spanning tree parsing framework (McDonald et al., 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["58'"]
58'
['58']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="3">That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
 <S sid="64" ssid="2">N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="1">We have presented results showing that the spanning tree dependency parsing framework of McDonald et al. (McDonald et al., 2005b; McDonald and Pereira, 2006) generalizes well to languages other than English.</S>
original cit marker offset is 0
new cit marker offset is 0



["104'"]
104'
['104']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="2">Ideally one would like to make all parsing and labeling decisions jointly so that the shared knowledge of both decisions will help resolve any ambiguities.</S>
original cit marker offset is 0
new cit marker offset is 0



["33'"]
33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="41" ssid="10">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["41'"]
41'
['41']
parsed_discourse_facet ['method_citation']
 <S sid="43" ssid="12">For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.</S>
original cit marker offset is 0
new cit marker offset is 0



["43'"]
43'
['43']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-2932.csv
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="41">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm�1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm−1) in the tree y.</S>
original cit marker offset is 
new cit marker offset is 0



["'0'", "'12'", "'41'"]
'0'
'12'
'41'
['0', '12', '41']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="38">Such a model could easily be trained using the provided training data for each language.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'38'"]
'0'
'12'
'38'
['0', '12', '38']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="106">First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'106'"]
'0'
'12'
'106'
['0', '12', '106']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="106">First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'106'"]
'0'
'12'
'106'
['0', '12', '106']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="41">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm�1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm−1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'41'"]
'0'
'12'
'41'
['0', '12', '41']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'1'"]
'0'
'12'
'1'
['0', '12', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'1'"]
'0'
'12'
'1'
['0', '12', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="41">To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm�1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm−1) in the tree y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'0'", "'41'"]
'12'
'0'
'41'
['12', '0', '41']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="16">A dependency graph is represented by a set of ordered pairs (i, j) E y in which xj is a dependent and xi is the corresponding head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'16'"]
'0'
'12'
'16'
['0', '12', '16']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="16">A dependency graph is represented by a set of ordered pairs (i, j) E y in which xj is a dependent and xi is the corresponding head.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'16'"]
'0'
'12'
'16'
['0', '12', '16']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="1">present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'1'"]
'0'
'12'
'1'
['0', '12', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="54">Based on performance from a held-out section of the training data, we used non-projective parsing algorithms for Czech, Danish, Dutch, German, Japanese, Portuguese and Slovene, and projective parsing algorithms for Arabic, Bulgarian, Chinese, Spanish, Swedish and Turkish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'54'"]
'0'
'12'
'54'
['0', '12', '54']
parsed_discourse_facet ['aim_citation', 'result_citation']
<S ssid="1" sid="0">Multilingual Dependency Analysis with a Two-Stage Discriminative Parser</S><S ssid="1" sid="12">In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.</S><S ssid="1" sid="3">The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'12'", "'3'"]
'0'
'12'
'3'
['0', '12', '3']
parsed_discourse_facet ['method_citation']
['Furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira, 2006).']
['Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.', u'To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm\ufffd1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm\u22121) in the tree y.']
['system', 'ROUGE-S*', 'Average_R:', '0.00106', '(95%-conf.int.', '0.00106', '-', '0.00106)']
['system', 'ROUGE-S*', 'Average_P:', '0.01515', '(95%-conf.int.', '0.01515', '-', '0.01515)']
['system', 'ROUGE-S*', 'Average_F:', '0.00198', '(95%-conf.int.', '0.00198', '-', '0.00198)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:66', 'F:1']
['N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.']
['Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.', u'To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm\ufffd1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm\u22121) in the tree y.']
['system', 'ROUGE-S*', 'Average_R:', '0.00634', '(95%-conf.int.', '0.00634', '-', '0.00634)']
['system', 'ROUGE-S*', 'Average_P:', '0.02372', '(95%-conf.int.', '0.02372', '-', '0.02372)']
['system', 'ROUGE-S*', 'Average_F:', '0.01001', '(95%-conf.int.', '0.01001', '-', '0.01001)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:946', 'P:253', 'F:6']
['In fact, for every language our models perform significantly higher than the average performance for all the systems reported in Buchholz et al. (2006).']
['Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.', 'First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:78', 'F:0']
['N/P: Allow non-projective/Force projective, S/A: Sequential labeling/Atomic labeling, M/B: Include morphology features/No morphology features. assignment of edge labels instead of individual assignment, and a rich feature set that incorporates morphological properties when available.']
['Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.', 'A dependency graph is represented by a set of ordered pairs (i, j) E y in which xj is a dependent and xi is the corresponding head.']
['system', 'ROUGE-S*', 'Average_R:', '0.00794', '(95%-conf.int.', '0.00794', '-', '0.00794)']
['system', 'ROUGE-S*', 'Average_P:', '0.01186', '(95%-conf.int.', '0.01186', '-', '0.01186)']
['system', 'ROUGE-S*', 'Average_F:', '0.00951', '(95%-conf.int.', '0.00951', '-', '0.00951)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:253', 'F:3']
['For score functions, we use simple dot products between high dimensional feature representations and a weight vector Assuming we have an appropriate feature representation, we can find the highest scoring label sequence with Viterbi&#8217;s algorithm.']
['Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph.', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['system', 'ROUGE-S*', 'Average_R:', '0.00178', '(95%-conf.int.', '0.00178', '-', '0.00178)']
['system', 'ROUGE-S*', 'Average_P:', '0.00433', '(95%-conf.int.', '0.00433', '-', '0.00433)']
['system', 'ROUGE-S*', 'Average_F:', '0.00253', '(95%-conf.int.', '0.00253', '-', '0.00253)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:231', 'F:1']
['In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.', 'Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['system', 'ROUGE-S*', 'Average_R:', '0.18803', '(95%-conf.int.', '0.18803', '-', '0.18803)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.31655', '(95%-conf.int.', '0.31655', '-', '0.31655)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:66', 'F:66']
['First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.']
['present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.', 'Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['system', 'ROUGE-S*', 'Average_R:', '0.02564', '(95%-conf.int.', '0.02564', '-', '0.02564)']
['system', 'ROUGE-S*', 'Average_P:', '0.13636', '(95%-conf.int.', '0.13636', '-', '0.13636)']
['system', 'ROUGE-S*', 'Average_F:', '0.04317', '(95%-conf.int.', '0.04317', '-', '0.04317)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:66', 'F:9']
['To model this we treat the labeling of the edges (i, j1), ... , (i, jM) as a sequence labeling problem, We use a first-order Markov factorization of the score s(l(i,jm), l(i,jm&#65533;1), i, y, x) in which each factor is the score of labeling the adjacent edges (i, jm) and (i, jm&#8722;1) in the tree y.']
['Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.', 'A dependency graph is represented by a set of ordered pairs (i, j) E y in which xj is a dependent and xi is the corresponding head.']
['system', 'ROUGE-S*', 'Average_R:', '0.00794', '(95%-conf.int.', '0.00794', '-', '0.00794)']
['system', 'ROUGE-S*', 'Average_P:', '0.00794', '(95%-conf.int.', '0.00794', '-', '0.00794)']
['system', 'ROUGE-S*', 'Average_F:', '0.00794', '(95%-conf.int.', '0.00794', '-', '0.00794)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:378', 'F:3']
['For instance, sequential labeling improves the labeling of 2This difference was much larger for experiments in which gold standard unlabeled dependencies are used. objects from 81.7%/75.6% to 84.2%/81.3% (labeled precision/recall) and the labeling of subjects from 86.8%/88.2% to 90.5%/90.4% for Swedish.']
['Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.', 'First, we plan on examining the performance difference between two-staged dependency parsing (as presented here) and joint parsing plus labeling.']
['system', 'ROUGE-S*', 'Average_R:', '0.02299', '(95%-conf.int.', '0.02299', '-', '0.02299)']
['system', 'ROUGE-S*', 'Average_P:', '0.01587', '(95%-conf.int.', '0.01587', '-', '0.01587)']
['system', 'ROUGE-S*', 'Average_F:', '0.01878', '(95%-conf.int.', '0.01878', '-', '0.01878)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:630', 'F:10']
['Ideally one would like to make all parsing and labeling decisions jointly so that the shared knowledge of both decisions will help resolve any ambiguities.']
['present a two-stage multilingual pendency parser and evaluate it on 13 diverse languages.', 'Multilingual Dependency Analysis with a Two-Stage Discriminative Parser', 'In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler.']
['system', 'ROUGE-S*', 'Average_R:', '0.00285', '(95%-conf.int.', '0.00285', '-', '0.00285)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00493', '(95%-conf.int.', '0.00493', '-', '0.00493)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:55', 'F:1']
0.123340998767 0.0264569997354 0.0415399995846





input/ref/Task1/P11-1061_swastika.csv
input/res/Task1/P11-1061.csv
parsing: input/ref/Task1/P11-1061_swastika.csv
    <S sid="144" ssid="7">For comparison, the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%, and goes up to 88.7% with a treebank dictionary.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
    <S sid="44" ssid="10">Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.</S
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
parsed_discourse_facet ['method_citation']
    <S sid="16" ssid="12">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (&#167;3), and then use graph label propagation to project syntactic information from English to the foreign language (&#167;4).</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="44" ssid="10">Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.</S
original cit marker offset is 0
new cit marker offset is 0



['44']
44
['44']
parsed_discourse_facet ['method_citation']
<S sid="110" ssid="10">We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.</S>
original cit marker offset is 0
new cit marker offset is 0



['110']
110
['110']
parsed_discourse_facet ['aim_citation']
    <S sid="115" ssid="15">The taggers were trained on datasets labeled with the universal tags.</S>
original cit marker offset is 0
new cit marker offset is 0



['115']
115
['115']
parsed_discourse_facet ['method_citation']
    <S sid="115" ssid="15">The taggers were trained on datasets labeled with the universal tags.</S>
original cit marker offset is 0
new cit marker offset is 0



['115']
115
['115']
parsed_discourse_facet ['method_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['158']
158
['158']
parsed_discourse_facet ['result_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



['23']
23
['23']
parsed_discourse_facet ['result_citation']
<S sid="24" ssid="1">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['24']
24
['24']
parsed_discourse_facet ['aim_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['???']
???
['???']
parsed_discourse_facet ['method_citation']
    <S sid="16" ssid="12">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (&#167;3), and then use graph label propagation to project syntactic information from English to the foreign language (&#167;4).</S>
original cit marker offset is 0
new cit marker offset is 0



['16']
16
['16']
parsed_discourse_facet ['method_citation']
    <S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



['23']
23
['23']
parsed_discourse_facet ['result_citation']
<S sid="158" ssid="1">We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.</S>
original cit marker offset is 0
new cit marker offset is 0



['158']
158
['158']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



['10']
10
['10']
parsed_discourse_facet ['method_citation']
<S sid="56" ssid="22">To define a similarity function between the English and the foreign vertices, we rely on high-confidence word alignments.</S>
original cit marker offset is 0
new cit marker offset is 0



['56']
56
['56']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



['161']
161
['161']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P11-1061.csv
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="74">This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'74'", "'25'"]
'0'
'74'
'25'
['0', '74', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="24">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'24'", "'25'"]
'0'
'24'
'25'
['0', '24', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'0'", "'23'"]
'25'
'0'
'23'
['25', '0', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="51">For each trigram type x2 x3 x4 in a sequence x1 x2 x3 x4 x5, we count how many times that trigram type co-occurs with the different instantiations of each concept, and compute the point-wise mutual information (PMI) between the two.5 The similarity between two trigram types is given by summing over the PMI values over feature instantiations that they have in common.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'51'"]
'0'
'25'
'51'
['0', '25', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="74">This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'74'"]
'0'
'25'
'74'
['0', '25', '74']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="81">Instead, we resort to an iterative update based method.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'81'", "'25'"]
'0'
'81'
'25'
['0', '81', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'23'"]
'0'
'25'
'23'
['0', '25', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'23'", "'25'"]
'0'
'23'
'25'
['0', '23', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'23'", "'25'"]
'0'
'23'
'25'
['0', '23', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="74">This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'74'", "'16'"]
'0'
'74'
'16'
['0', '74', '16']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="29">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'29'"]
'0'
'25'
'29'
['0', '25', '29']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'16'", "'0'"]
'25'
'16'
'0'
['25', '16', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S ssid="1" sid="87">This locally normalized log-linear model can look at various aspects of the observation x, incorporating overlapping features of the observation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'16'", "'87'"]
'25'
'16'
'87'
['25', '16', '87']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.</S><S ssid="1" sid="24">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'24'"]
'0'
'1'
'24'
['0', '1', '24']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'16'", "'25'"]
'0'
'16'
'25'
['0', '16', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="24">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'24'", "'23'"]
'0'
'24'
'23'
['0', '24', '23']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="29">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'29'"]
'0'
'25'
'29'
['0', '25', '29']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="81">Instead, we resort to an iterative update based method.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'81'", "'25'"]
'0'
'81'
'25'
['0', '81', '25']
parsed_discourse_facet ['method_citation']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.', u'To this end, we construct a bilingual graph over word types to establish a connection between the two languages (\xa73), and then use graph label propagation to project syntactic information from English to the foreign language (\xa74).']
['system', 'ROUGE-S*', 'Average_R:', '0.03333', '(95%-conf.int.', '0.03333', '-', '0.03333)']
['system', 'ROUGE-S*', 'Average_P:', '0.47273', '(95%-conf.int.', '0.47273', '-', '0.47273)']
['system', 'ROUGE-S*', 'Average_F:', '0.06228', '(95%-conf.int.', '0.06228', '-', '0.06228)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:55', 'F:26']
['For comparison, the completely unsupervised feature-HMM baseline accuracy on the universal POS tags for English is 79.4%, and goes up to 88.7% with a treebank dictionary.']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', u'This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf\ufffd to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x\u2212 x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and \xb5 and \u03bd are hyperparameters that we discuss in \xa76.4.', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00210', '(95%-conf.int.', '0.00210', '-', '0.00210)']
['system', 'ROUGE-S*', 'Average_P:', '0.06618', '(95%-conf.int.', '0.06618', '-', '0.06618)']
['system', 'ROUGE-S*', 'Average_F:', '0.00408', '(95%-conf.int.', '0.00408', '-', '0.00408)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4278', 'P:136', 'F:9']
['To this end, we construct a bilingual graph over word types to establish a connection between the two languages (&#167;3), and then use graph label propagation to project syntactic information from English to the foreign language (&#167;4).']
['Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.', 'This locally normalized log-linear model can look at various aspects of the observation x, incorporating overlapping features of the observation.', u'To this end, we construct a bilingual graph over word types to establish a connection between the two languages (\xa73), and then use graph label propagation to project syntactic information from English to the foreign language (\xa74).']
['system', 'ROUGE-S*', 'Average_R:', '0.21041', '(95%-conf.int.', '0.21041', '-', '0.21041)']
['system', 'ROUGE-S*', 'Average_P:', '0.82251', '(95%-conf.int.', '0.82251', '-', '0.82251)']
['system', 'ROUGE-S*', 'Average_F:', '0.33510', '(95%-conf.int.', '0.33510', '-', '0.33510)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:231', 'F:190']
['Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.', u'Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).']
['system', 'ROUGE-S*', 'Average_R:', '0.36471', '(95%-conf.int.', '0.36471', '-', '0.36471)']
['system', 'ROUGE-S*', 'Average_P:', '0.93750', '(95%-conf.int.', '0.93750', '-', '0.93750)']
['system', 'ROUGE-S*', 'Average_F:', '0.52513', '(95%-conf.int.', '0.52513', '-', '0.52513)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:496', 'F:465']
['To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.']
['Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.', 'Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', u'To this end, we construct a bilingual graph over word types to establish a connection between the two languages (\xa73), and then use graph label propagation to project syntactic information from English to the foreign language (\xa74).']
['system', 'ROUGE-S*', 'Average_R:', '0.01923', '(95%-conf.int.', '0.01923', '-', '0.01923)']
['system', 'ROUGE-S*', 'Average_P:', '0.02841', '(95%-conf.int.', '0.02841', '-', '0.02841)']
['system', 'ROUGE-S*', 'Average_F:', '0.02294', '(95%-conf.int.', '0.02294', '-', '0.02294)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:528', 'F:15']
['The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', u'This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf\ufffd to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x\u2212 x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and \xb5 and \u03bd are hyperparameters that we discuss in \xa76.4.', u'To this end, we construct a bilingual graph over word types to establish a connection between the two languages (\xa73), and then use graph label propagation to project syntactic information from English to the foreign language (\xa74).']
['system', 'ROUGE-S*', 'Average_R:', '0.00297', '(95%-conf.int.', '0.00297', '-', '0.00297)']
['system', 'ROUGE-S*', 'Average_P:', '0.16484', '(95%-conf.int.', '0.16484', '-', '0.16484)']
['system', 'ROUGE-S*', 'Average_F:', '0.00584', '(95%-conf.int.', '0.00584', '-', '0.00584)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5050', 'P:91', 'F:15']
['We hope that this will allow practitioners to apply our approach directly to languages for which no resources are available.']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', u'This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf\ufffd to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x\u2212 x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and \xb5 and \u03bd are hyperparameters that we discuss in \xa76.4.', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4278', 'P:21', 'F:0']
['Because all English vertices are going to be labeled, we do not need to disambiguate them by embedding them in trigrams.']
['The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.', 'Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:15', 'F:0']
['Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', 'Instead, we resort to an iterative update based method.', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.01000', '(95%-conf.int.', '0.01000', '-', '0.01000)']
['system', 'ROUGE-S*', 'Average_P:', '0.02206', '(95%-conf.int.', '0.02206', '-', '0.02206)']
['system', 'ROUGE-S*', 'Average_F:', '0.01376', '(95%-conf.int.', '0.01376', '-', '0.01376)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:300', 'P:136', 'F:3']
['We have shown the efficacy of graph-based label propagation for projecting part-of-speech information across languages.']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.', u'Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).']
['system', 'ROUGE-S*', 'Average_R:', '0.00314', '(95%-conf.int.', '0.00314', '-', '0.00314)']
['system', 'ROUGE-S*', 'Average_P:', '0.07273', '(95%-conf.int.', '0.07273', '-', '0.07273)']
['system', 'ROUGE-S*', 'Average_F:', '0.00602', '(95%-conf.int.', '0.00602', '-', '0.00602)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:55', 'F:4']
['The taggers were trained on datasets labeled with the universal tags.']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', u'Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:15', 'F:0']
0.23517817968 0.0587172721935 0.0886499991941





input/ref/Task1/P05-1013_vardha.csv
input/res/Task1/P05-1013.csv
parsing: input/ref/Task1/P05-1013_vardha.csv
 <S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
  <S sid="9" ssid="5">This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
  <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'"]
'109'
['109']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
    <S sid="20" ssid="16">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'20'"]
'20'
['20']
parsed_discourse_facet ['method_citation']
    <S sid="36" ssid="7">As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
 <S sid="62" ssid="1">In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al., 2004) and English (Nivre and Scholz, 2004).</S>
original cit marker offset is 0
new cit marker offset is 0



["'62'"]
'62'
['62']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
    <S sid="23" ssid="19">By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'"]
'23'
['23']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
    <S sid="96" ssid="7">With respect to exact match, the improvement is even more noticeable, which shows quite clearly that even if non-projective dependencies are rare on the token level, they are nevertheless important for getting the global syntactic structure correct.</S>
original cit marker offset is 0
new cit marker offset is 0



["'96'"]
'96'
['96']
parsed_discourse_facet ['method_citation']
 <S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



["'95'"]
'95'
['95']
parsed_discourse_facet ['method_citation']
  <S sid="40" ssid="11">Even this may be nondeterministic, in case the graph contains several non-projective arcs whose lifts interact, but we use the following algorithm to construct a minimal projective transformation D0 = (W, A0) of a (nonprojective) dependency graph D = (W, A): The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right).</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
  <S sid="7" ssid="3">From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="14" ssid="10">While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
 <S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'"]
'49'
['49']
parsed_discourse_facet ['method_citation']
 <S sid="104" ssid="15">The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'104'"]
'104'
['104']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P05-1013.csv
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="20">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'20'"]
'0'
'2'
'20'
['0', '2', '20']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="63">The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'63'"]
'0'
'2'
'63'
['0', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'0'", "'109'"]
'2'
'0'
'109'
['2', '0', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="63">The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'63'"]
'0'
'2'
'63'
['0', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'0'", "'109'"]
'2'
'0'
'109'
['2', '0', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
['The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.01426', '(95%-conf.int.', '0.01426', '-', '0.01426)']
['system', 'ROUGE-S*', 'Average_P:', '0.22222', '(95%-conf.int.', '0.22222', '-', '0.22222)']
['system', 'ROUGE-S*', 'Average_F:', '0.02680', '(95%-conf.int.', '0.02680', '-', '0.02680)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:36', 'F:8']
['In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al., 2004) and English (Nivre and Scholz, 2004).']
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'Pseudo-Projective Dependency Parsing', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.04991', '(95%-conf.int.', '0.04991', '-', '0.04991)']
['system', 'ROUGE-S*', 'Average_P:', '0.11067', '(95%-conf.int.', '0.11067', '-', '0.11067)']
['system', 'ROUGE-S*', 'Average_F:', '0.06880', '(95%-conf.int.', '0.06880', '-', '0.06880)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:253', 'F:28']
['The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'Pseudo-Projective Dependency Parsing', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.00891', '(95%-conf.int.', '0.00891', '-', '0.00891)']
['system', 'ROUGE-S*', 'Average_P:', '0.13889', '(95%-conf.int.', '0.13889', '-', '0.13889)']
['system', 'ROUGE-S*', 'Average_F:', '0.01675', '(95%-conf.int.', '0.01675', '-', '0.01675)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:36', 'F:5']
['As observed by Kahane et al. (1998), any (nonprojective) dependency graph can be transformed into a projective one by a lifting operation, which replaces each non-projective arc wj wk by a projective arc wi &#8212;* wk such that wi &#8212;*&#8727; wj holds in the original graph.']
['The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents.', 'Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.']
['system', 'ROUGE-S*', 'Average_R:', '0.03427', '(95%-conf.int.', '0.03427', '-', '0.03427)']
['system', 'ROUGE-S*', 'Average_P:', '0.04497', '(95%-conf.int.', '0.04497', '-', '0.04497)']
['system', 'ROUGE-S*', 'Average_F:', '0.03890', '(95%-conf.int.', '0.03890', '-', '0.03890)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:378', 'F:17']
['While the proportion of sentences containing non-projective dependencies is often 15&#8211;25%, the total proportion of non-projective arcs is normally only 1&#8211;2%.']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.00535', '(95%-conf.int.', '0.00535', '-', '0.00535)']
['system', 'ROUGE-S*', 'Average_P:', '0.03297', '(95%-conf.int.', '0.03297', '-', '0.03297)']
['system', 'ROUGE-S*', 'Average_F:', '0.00920', '(95%-conf.int.', '0.00920', '-', '0.00920)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:91', 'F:3']
['The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.']
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'Pseudo-Projective Dependency Parsing', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.02317', '(95%-conf.int.', '0.02317', '-', '0.02317)']
['system', 'ROUGE-S*', 'Average_P:', '0.01754', '(95%-conf.int.', '0.01754', '-', '0.01754)']
['system', 'ROUGE-S*', 'Average_F:', '0.01997', '(95%-conf.int.', '0.01997', '-', '0.01997)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:741', 'F:13']
['The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'Pseudo-Projective Dependency Parsing', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.00891', '(95%-conf.int.', '0.00891', '-', '0.00891)']
['system', 'ROUGE-S*', 'Average_P:', '0.13889', '(95%-conf.int.', '0.13889', '-', '0.13889)']
['system', 'ROUGE-S*', 'Average_F:', '0.01675', '(95%-conf.int.', '0.01675', '-', '0.01675)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:36', 'F:5']
['The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.01426', '(95%-conf.int.', '0.01426', '-', '0.01426)']
['system', 'ROUGE-S*', 'Average_P:', '0.22222', '(95%-conf.int.', '0.22222', '-', '0.22222)']
['system', 'ROUGE-S*', 'Average_F:', '0.02680', '(95%-conf.int.', '0.02680', '-', '0.02680)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:36', 'F:8']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.18717', '(95%-conf.int.', '0.18717', '-', '0.18717)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.31532', '(95%-conf.int.', '0.31532', '-', '0.31532)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:105', 'F:105']
['The overall parsing accuracy obtained with the pseudo-projective approach is still lower than for the best projective parsers.']
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'Pseudo-Projective Dependency Parsing', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.00891', '(95%-conf.int.', '0.00891', '-', '0.00891)']
['system', 'ROUGE-S*', 'Average_P:', '0.13889', '(95%-conf.int.', '0.13889', '-', '0.13889)']
['system', 'ROUGE-S*', 'Average_F:', '0.01675', '(95%-conf.int.', '0.01675', '-', '0.01675)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:36', 'F:5']
['This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al., 2004).']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.01783', '(95%-conf.int.', '0.01783', '-', '0.01783)']
['system', 'ROUGE-S*', 'Average_P:', '0.02646', '(95%-conf.int.', '0.02646', '-', '0.02646)']
['system', 'ROUGE-S*', 'Average_F:', '0.02130', '(95%-conf.int.', '0.02130', '-', '0.02130)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:378', 'F:10']
['Even this may be nondeterministic, in case the graph contains several non-projective arcs whose lifts interact, but we use the following algorithm to construct a minimal projective transformation D0 = (W, A0) of a (nonprojective) dependency graph D = (W, A): The function SMALLEST-NONP-ARC returns the non-projective arc with the shortest distance from head to dependent (breaking ties from left to right).']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.04635', '(95%-conf.int.', '0.04635', '-', '0.04635)']
['system', 'ROUGE-S*', 'Average_P:', '0.05591', '(95%-conf.int.', '0.05591', '-', '0.05591)']
['system', 'ROUGE-S*', 'Average_F:', '0.05068', '(95%-conf.int.', '0.05068', '-', '0.05068)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:465', 'F:26']
['In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.17235', '(95%-conf.int.', '0.17235', '-', '0.17235)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.29402', '(95%-conf.int.', '0.29402', '-', '0.29402)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:528', 'P:91', 'F:91']
['In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'Pseudo-Projective Dependency Parsing', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.08378', '(95%-conf.int.', '0.08378', '-', '0.08378)']
['system', 'ROUGE-S*', 'Average_P:', '0.51648', '(95%-conf.int.', '0.51648', '-', '0.51648)']
['system', 'ROUGE-S*', 'Average_F:', '0.14417', '(95%-conf.int.', '0.14417', '-', '0.14417)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:91', 'F:47']
['From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.']
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'Pseudo-Projective Dependency Parsing', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.00535', '(95%-conf.int.', '0.00535', '-', '0.00535)']
['system', 'ROUGE-S*', 'Average_P:', '0.02206', '(95%-conf.int.', '0.02206', '-', '0.02206)']
['system', 'ROUGE-S*', 'Average_F:', '0.00861', '(95%-conf.int.', '0.00861', '-', '0.00861)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:136', 'F:3']
['The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.01070', '(95%-conf.int.', '0.01070', '-', '0.01070)']
['system', 'ROUGE-S*', 'Average_P:', '0.04412', '(95%-conf.int.', '0.04412', '-', '0.04412)']
['system', 'ROUGE-S*', 'Average_F:', '0.01722', '(95%-conf.int.', '0.01722', '-', '0.01722)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:136', 'F:6']
['By applying an inverse transformation to the output of the parser, arcs with non-standard labels can be lowered to their proper place in the dependency graph, giving rise 1The dependency graph has been modified to make the final period a dependent of the main verb instead of being a dependent of a special root node for the sentence. to non-projective structures.']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.06239', '(95%-conf.int.', '0.06239', '-', '0.06239)']
['system', 'ROUGE-S*', 'Average_P:', '0.07056', '(95%-conf.int.', '0.07056', '-', '0.07056)']
['system', 'ROUGE-S*', 'Average_F:', '0.06623', '(95%-conf.int.', '0.06623', '-', '0.06623)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:496', 'F:35']
0.223697057508 0.0443452938568 0.068133529011





input/ref/Task1/P05-1013_swastika.csv
input/res/Task1/P05-1013.csv
parsing: input/ref/Task1/P05-1013_swastika.csv
<S sid="49" ssid="20">The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.</S>
original cit marker offset is 0
new cit marker offset is 0



['49']
49
['49']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="6">In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
<S sid="38" ssid="9">Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
<S sid="79" ssid="6">In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.</S>
original cit marker offset is 0
new cit marker offset is 0



['79']
79
['79']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
Error in Reference Offset
S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
Error in Reference Offset
    <S sid="86" ssid="13">As expected, the most informative encoding, Head+Path, gives the highest accuracy with over 99% of all non-projective arcs being recovered correctly in both data sets.</S>
original cit marker offset is 0
new cit marker offset is 0



['86']
86
['86']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.</S>
original cit marker offset is 0
new cit marker offset is 0



['95']
95
['95']
parsed_discourse_facet ['method_citation']
<S sid="109" ssid="1">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



['109']
109
['109']
parsed_discourse_facet ['result_citation']
 <S sid="51" ssid="22">In the second scheme, Head+Path, we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p&#8595;.</S>
original cit marker offset is 0
new cit marker offset is 0



['51']
51
['51']
parsed_discourse_facet ['method_citation']
<S sid="99" ssid="10">This may seem surprising, given the experiments reported in section 4, but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift, where the encoding of path information is often redundant.</S>
original cit marker offset is 0
new cit marker offset is 0



['99']
99
['99']
parsed_discourse_facet ['result_citation']
<S sid="7" ssid="3">From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.</S>
original cit marker offset is 0
new cit marker offset is 0



['7']
7
['7']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



['2']
2
['2']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/P05-1013.csv
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="20">In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'20'"]
'0'
'2'
'20'
['0', '2', '20']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="63">The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'63'"]
'0'
'2'
'63'
['0', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'0'", "'109'"]
'2'
'0'
'109'
['2', '0', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="63">The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'63'"]
'0'
'2'
'63'
['0', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'0'", "'109'"]
'2'
'0'
'109'
['2', '0', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'109'", "'2'"]
'0'
'109'
'2'
['0', '109', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Pseudo-Projective Dependency Parsing</S><S ssid="1" sid="2">We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.</S><S ssid="1" sid="109">We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'2'", "'109'"]
'0'
'2'
'109'
['0', '2', '109']
parsed_discourse_facet ['method_citation']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'Pseudo-Projective Dependency Parsing', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.18717', '(95%-conf.int.', '0.18717', '-', '0.18717)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.31532', '(95%-conf.int.', '0.31532', '-', '0.31532)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:105', 'F:105']
['In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.']
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'Pseudo-Projective Dependency Parsing', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.00178', '(95%-conf.int.', '0.00178', '-', '0.00178)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00330', '(95%-conf.int.', '0.00330', '-', '0.00330)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:45', 'F:1']
['In the first part of the experiment, dependency graphs from the treebanks were projectivized using the algorithm described in section 2.']
['The parser builds dependency graphs by traversing the input from left to right, using a stack to store tokens that are not yet complete with respect to their dependents.', 'Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.']
['system', 'ROUGE-S*', 'Average_R:', '0.00202', '(95%-conf.int.', '0.00202', '-', '0.00202)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00370', '(95%-conf.int.', '0.00370', '-', '0.00370)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:45', 'F:1']
['From the point of view of computational implementation this can be problematic, since the inclusion of non-projective structures makes the parsing problem more complex and therefore compromises efficiency and in practice also accuracy and robustness.']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.00535', '(95%-conf.int.', '0.00535', '-', '0.00535)']
['system', 'ROUGE-S*', 'Average_P:', '0.02206', '(95%-conf.int.', '0.02206', '-', '0.02206)']
['system', 'ROUGE-S*', 'Average_F:', '0.00861', '(95%-conf.int.', '0.00861', '-', '0.00861)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:136', 'F:3']
no Reference Text in gold P05-1013
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.18717', '(95%-conf.int.', '0.18717', '-', '0.18717)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.31532', '(95%-conf.int.', '0.31532', '-', '0.31532)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:105', 'F:105']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.18717', '(95%-conf.int.', '0.18717', '-', '0.18717)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.31532', '(95%-conf.int.', '0.31532', '-', '0.31532)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:105', 'F:105']
['The second main result is that the pseudo-projective approach to parsing (using special arc labels to guide an inverse transformation) gives a further improvement of about one percentage point on attachment score.']
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'Pseudo-Projective Dependency Parsing', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.01070', '(95%-conf.int.', '0.01070', '-', '0.01070)']
['system', 'ROUGE-S*', 'Average_P:', '0.04412', '(95%-conf.int.', '0.04412', '-', '0.04412)']
['system', 'ROUGE-S*', 'Average_F:', '0.01722', '(95%-conf.int.', '0.01722', '-', '0.01722)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:136', 'F:6']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.18717', '(95%-conf.int.', '0.18717', '-', '0.18717)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.31532', '(95%-conf.int.', '0.31532', '-', '0.31532)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:105', 'F:105']
['In the second scheme, Head+Path, we in addition modify the label of every arc along the lifting path from the syntactic to the linear head so that if the original label is p the new label is p&#8595;.']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:120', 'F:0']
['We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'In this paper, we show how non-projective dependency parsing can be achieved by combining a datadriven projective parser with special graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.08523', '(95%-conf.int.', '0.08523', '-', '0.08523)']
['system', 'ROUGE-S*', 'Average_P:', '0.42857', '(95%-conf.int.', '0.42857', '-', '0.42857)']
['system', 'ROUGE-S*', 'Average_F:', '0.14218', '(95%-conf.int.', '0.14218', '-', '0.14218)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:528', 'P:105', 'F:45']
['The baseline simply retains the original labels for all arcs, regardless of whether they have been lifted or not, and the number of distinct labels is therefore simply the number n of distinct dependency types.2 In the first encoding scheme, called Head, we use a new label d&#8593;h for each lifted arc, where d is the dependency relation between the syntactic head and the dependent in the non-projective representation, and h is the dependency relation that the syntactic head has to its own head in the underlying structure.']
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'Pseudo-Projective Dependency Parsing', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.02317', '(95%-conf.int.', '0.02317', '-', '0.02317)']
['system', 'ROUGE-S*', 'Average_P:', '0.01754', '(95%-conf.int.', '0.01754', '-', '0.01754)']
['system', 'ROUGE-S*', 'Average_F:', '0.01997', '(95%-conf.int.', '0.01997', '-', '0.01997)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:741', 'F:13']
['This may seem surprising, given the experiments reported in section 4, but the explanation is probably that the non-projective dependencies that can be recovered at all are of the simple kind that only requires a single lift, where the encoding of path information is often redundant.']
['We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'Pseudo-Projective Dependency Parsing', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.00178', '(95%-conf.int.', '0.00178', '-', '0.00178)']
['system', 'ROUGE-S*', 'Average_P:', '0.00654', '(95%-conf.int.', '0.00654', '-', '0.00654)']
['system', 'ROUGE-S*', 'Average_F:', '0.00280', '(95%-conf.int.', '0.00280', '-', '0.00280)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:153', 'F:1']
['Projectivizing a dependency graph by lifting nonprojective arcs is a nondeterministic operation in the general case.']
['Pseudo-Projective Dependency Parsing', 'We show how a datadriven deterministic dependency parser, in itself restricted to projective structures, can be combined with graph transformation techniques to produce non-projective structures.', 'We have presented a new method for non-projective dependency parsing, based on a combination of data-driven projective dependency parsing and graph transformation techniques.']
['system', 'ROUGE-S*', 'Average_R:', '0.00178', '(95%-conf.int.', '0.00178', '-', '0.00178)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00330', '(95%-conf.int.', '0.00330', '-', '0.00330)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:45', 'F:1']
no Reference Text in gold P05-1013
0.352729997287 0.067729999479 0.112489229904





input/ref/Task1/W99-0623_swastika.csv
input/res/Task1/W99-0623.csv
parsing: input/ref/Task1/W99-0623_swastika.csv
<S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['aim_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



['38']
38
['38']
parsed_discourse_facet ['result_citation']
<S sid="91" ssid="20">Features and context were initially introduced into the models, but they refused to offer any gains in performance.</S>
original cit marker offset is 0
new cit marker offset is 0



['91']
91
['91']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



['120']
120
['120']
parsed_discourse_facet ['result_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



['139']
139
['139']
parsed_discourse_facet ['result_citation']
<S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



['25']
25
['25']
parsed_discourse_facet ['method_citation']
<S sid="103" ssid="32">The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.</S>
original cit marker offset is 0
new cit marker offset is 0



['103']
103
['103']
parsed_discourse_facet ['method_citation']
<S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



['139']
139
['139']
parsed_discourse_facet ['result_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
<S sid="140" ssid="2">For each experiment we gave an nonparametric and a parametric technique for combining parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
<S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['85']
85
['85']
parsed_discourse_facet ['result_citation']
<S sid="70" ssid="56">In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



['70']
70
['70']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/W99-0623.csv
<S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'16'", "'23'"]
'58'
'16'
'23'
['58', '16', '23']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="21">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'21'"]
'23'
'1'
'21'
['23', '1', '21']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'58'", "'16'"]
'23'
'58'
'16'
['23', '58', '16']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'58'", "'16'"]
'23'
'58'
'16'
['23', '58', '16']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="28">The development of a naïve Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'23'", "'28'"]
'58'
'23'
'28'
['58', '23', '28']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="58">We call this approach parser switching.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'58'"]
'23'
'16'
'58'
['23', '16', '58']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'1'"]
'23'
'16'
'1'
['23', '16', '1']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="58">We call this approach parser switching.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'58'"]
'23'
'16'
'58'
['23', '16', '58']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="14">We used these three parsers to explore parser combination techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'23'", "'14'"]
'1'
'23'
'14'
['1', '23', '14']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="58">We call this approach parser switching.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'58'"]
'23'
'16'
'58'
['23', '16', '58']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'23'", "'1'"]
'16'
'23'
'1'
['16', '23', '1']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="28">The development of a naïve Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'23'", "'28'"]
'16'
'23'
'28'
['16', '23', '28']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'16'", "'23'"]
'1'
'16'
'23'
['1', '16', '23']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
["In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers."]
['Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.', 'We call this approach parse hybridization.', 'We call this technique constituent voting.']
['system', 'ROUGE-S*', 'Average_R:', '0.01905', '(95%-conf.int.', '0.01905', '-', '0.01905)']
['system', 'ROUGE-S*', 'Average_P:', '0.01905', '(95%-conf.int.', '0.01905', '-', '0.01905)']
['system', 'ROUGE-S*', 'Average_F:', '0.01905', '(95%-conf.int.', '0.01905', '-', '0.01905)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:210', 'F:4']
["In this case we are interested in finding' the maximum probability parse, ri, and Mi is the set of relevant (binary) parsing decisions made by parser i. ri is a parse selected from among the outputs of the individual parsers."]
['We call this technique constituent voting.', u'The development of a na\xefve Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.', 'We call this approach parse hybridization.']
['system', 'ROUGE-S*', 'Average_R:', '0.01170', '(95%-conf.int.', '0.01170', '-', '0.01170)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.01050', '(95%-conf.int.', '0.01050', '-', '0.01050)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:171', 'P:210', 'F:2']
['We have presented two general approaches to studying parser combination: parser switching and parse hybridization.']
['Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.', 'We call this approach parse hybridization.', 'We call this technique constituent voting.']
['system', 'ROUGE-S*', 'Average_R:', '0.04286', '(95%-conf.int.', '0.04286', '-', '0.04286)']
['system', 'ROUGE-S*', 'Average_P:', '0.20000', '(95%-conf.int.', '0.20000', '-', '0.20000)']
['system', 'ROUGE-S*', 'Average_F:', '0.07059', '(95%-conf.int.', '0.07059', '-', '0.07059)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:45', 'F:9']
['We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.']
['Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.', 'We call this technique constituent voting.', 'We call this approach parse hybridization.']
['system', 'ROUGE-S*', 'Average_R:', '0.02381', '(95%-conf.int.', '0.02381', '-', '0.02381)']
['system', 'ROUGE-S*', 'Average_P:', '0.13889', '(95%-conf.int.', '0.13889', '-', '0.13889)']
['system', 'ROUGE-S*', 'Average_F:', '0.04065', '(95%-conf.int.', '0.04065', '-', '0.04065)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:36', 'F:5']
['The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.']
['Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.', "One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.", 'We call this technique constituent voting.']
['system', 'ROUGE-S*', 'Average_R:', '0.06000', '(95%-conf.int.', '0.06000', '-', '0.06000)']
['system', 'ROUGE-S*', 'Average_P:', '0.06000', '(95%-conf.int.', '0.06000', '-', '0.06000)']
['system', 'ROUGE-S*', 'Average_F:', '0.06000', '(95%-conf.int.', '0.06000', '-', '0.06000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:300', 'P:300', 'F:18']
['The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.']
['Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.', 'We call this approach parse hybridization.', 'We call this technique constituent voting.']
['system', 'ROUGE-S*', 'Average_R:', '0.04286', '(95%-conf.int.', '0.04286', '-', '0.04286)']
['system', 'ROUGE-S*', 'Average_P:', '0.03000', '(95%-conf.int.', '0.03000', '-', '0.03000)']
['system', 'ROUGE-S*', 'Average_F:', '0.03529', '(95%-conf.int.', '0.03529', '-', '0.03529)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:300', 'F:9']
['The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.']
['Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.', 'We call this approach parse hybridization.', 'We call this technique constituent voting.']
['system', 'ROUGE-S*', 'Average_R:', '0.04286', '(95%-conf.int.', '0.04286', '-', '0.04286)']
['system', 'ROUGE-S*', 'Average_P:', '0.03000', '(95%-conf.int.', '0.03000', '-', '0.03000)']
['system', 'ROUGE-S*', 'Average_F:', '0.03529', '(95%-conf.int.', '0.03529', '-', '0.03529)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:300', 'F:9']
['In our particular case the majority requires the agreement of only two parsers because we have only three.']
['We call this approach parse hybridization.', 'We call this approach parser switching.', 'We call this technique constituent voting.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:66', 'P:10', 'F:0']
['The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.']
['We call this approach parse hybridization.', 'We call this approach parser switching.', 'We call this technique constituent voting.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:66', 'P:45', 'F:0']
0.0541622216204 0.0270155552554 0.0301522218872





input/ref/Task1/P87-1015_sweta.csv
input/res/Task1/P87-1015.csv
parsing: input/ref/Task1/P87-1015_sweta.csv
<S sid="205" ssid="11">Independence of paths at this level reflects context freeness of rewriting and suggests why they can be recognized efficiently.</S>
original cit marker offset is 0
new cit marker offset is 0



["205'"]
205'
['205']
parsed_discourse_facet ['method_citation']
<S sid="229" ssid="35">In addition, the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars, and the hierarchy defined by Weir).</S>
original cit marker offset is 0
new cit marker offset is 0



["229'"]
229'
['229']
parsed_discourse_facet ['method_citation']
<S sid="146" ssid="31">Since every CFL is known to be semilinear (Parikh, 1966), in order to show semilinearity of some language, we need only show the existence of a letter equivalent CFL Our definition of LCFRS's insists that the composition operations are linear and nonerasing.</S>
original cit marker offset is 0
new cit marker offset is 0



["146'"]
146'
['146']
parsed_discourse_facet ['method_citation']
<S sid="201" ssid="7">It is interesting to note, however, that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG, FUG and IG's.</S>
original cit marker offset is 0
new cit marker offset is 0



["201'"]
201'
['201']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="36">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
 <S sid="222" ssid="28">However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.</S>
original cit marker offset is 0
new cit marker offset is 0



["222'"]
222'
['222']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="7">Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.</S>
    <S sid="23" ssid="8">He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["22'", "'23'"]
22'
'23'
['22', '23']
parsed_discourse_facet ['method_citation']
 <S sid="156" ssid="41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["156'"]
156'
['156']
parsed_discourse_facet ['method_citation']
<S sid="221" ssid="27">Members of LCFRS whose operations have this property can be translated into the ILFP notation (Rounds, 1985).</S>
original cit marker offset is 0
new cit marker offset is 0



["221'"]
221'
['221']
parsed_discourse_facet ['method_citation']
<S sid="54" ssid="39">An IG can be viewed as a CFG in which each nonterminal is associated with a stack.</S>
original cit marker offset is 0
new cit marker offset is 0



["54'"]
54'
['54']
parsed_discourse_facet ['method_citation']
<S sid="128" ssid="13">As in the case of the derivation trees of CFG's, nodes are labeled by a member of some finite set of symbols (perhaps only implicit in the grammar as in TAG's) used to denote derived structures.</S>
original cit marker offset is 0
new cit marker offset is 0



["128'"]
128'
['128']
parsed_discourse_facet ['method_citation']
<S sid="217" ssid="23">In considering the recognition of these languages, we were forced to be more specific regarding the relationship between the structures derived by these formalisms and the substrings they span.</S>
original cit marker offset is 0
new cit marker offset is 0



[";217'"]
;217'
[';217']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="1">From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="1">From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="214" ssid="20">LCFRS's share several properties possessed by the class of mildly context-sensitive formalisms discussed by Joshi (1983/85).</S>
original cit marker offset is 0
new cit marker offset is 0



["214'"]
214'
['214']
parsed_discourse_facet ['method_citation']
<S sid="35" ssid="20">TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish, for cross serial dependencies in Dutch subordinate clauses, and for the nestings of paired English complementizers.</S>
original cit marker offset is 0
new cit marker offset is 0



["35'"]
35'
['35']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P87-1015.csv
<S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'207'", "'125'"]
'92'
'207'
'125'
['92', '207', '125']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'125'", "'207'"]
'75'
'125'
'207'
['75', '125', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'75'", "'207'"]
'125'
'75'
'207'
['125', '75', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'125'", "'207'"]
'2'
'125'
'207'
['2', '125', '207']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'92'"]
'207'
'125'
'92'
['207', '125', '92']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'207'", "'75'"]
'2'
'207'
'75'
['2', '207', '75']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'92'"]
'207'
'125'
'92'
['207', '125', '92']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="151">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'207'", "'125'"]
'151'
'207'
'125'
['151', '207', '125']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'75'", "'207'"]
'2'
'75'
'207'
['2', '75', '207']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'75'", "'207'"]
'125'
'75'
'207'
['125', '75', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'125'", "'207'"]
'75'
'125'
'207'
['75', '125', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'75'"]
'207'
'125'
'75'
['207', '125', '75']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'75'", "'207'"]
'125'
'75'
'207'
['125', '75', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="113">A formalism generating tree sets with complex path sets can still generate only semilinear languages if its tree sets have independent paths, and semilinear path sets.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'113'", "'75'"]
'2'
'113'
'75'
['2', '113', '75']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'92'"]
'207'
'125'
'92'
['207', '125', '92']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'75'"]
'207'
'125'
'75'
['207', '125', '75']
parsed_discourse_facet ['result_citation']
['Gazdar (1985) argues this is the appropriate analysis of unbounded dependencies in the hypothetical Scandinavian language Norwedish.', 'He also argues that paired English complementizers may also require structural descriptions whose path sets have nested dependencies.']
['We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:406', 'P:210', 'F:0']
['Independence of paths at this level reflects context freeness of rewriting and suggests why they can be recognized efficiently.']
['We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.00246', '(95%-conf.int.', '0.00246', '-', '0.00246)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00443', '(95%-conf.int.', '0.00443', '-', '0.00443)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:406', 'P:45', 'F:1']
["In addition, the restricted version of CG's (discussed in Section 6) generates tree sets with independent paths and we hope that it can be included in a more general definition of LCFRS's containing formalisms whose tree sets have path sets that are themselves LCFRL's (as in the case of the restricted indexed grammars, and the hierarchy defined by Weir)."]
['A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.04032', '(95%-conf.int.', '0.04032', '-', '0.04032)']
['system', 'ROUGE-S*', 'Average_P:', '0.04598', '(95%-conf.int.', '0.04598', '-', '0.04598)']
['system', 'ROUGE-S*', 'Average_F:', '0.04296', '(95%-conf.int.', '0.04296', '-', '0.04296)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:435', 'F:20']
["TAG's can be used to give the structural descriptions discussed by Gazdar (1985) for the unbounded nested dependencies in Norwedish, for cross serial dependencies in Dutch subordinate clauses, and for the nestings of paired English complementizers."]
['A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:210', 'F:0']
["LCFRS's share several properties possessed by the class of mildly context-sensitive formalisms discussed by Joshi (1983/85)."]
['We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.00739', '(95%-conf.int.', '0.00739', '-', '0.00739)']
['system', 'ROUGE-S*', 'Average_P:', '0.03846', '(95%-conf.int.', '0.03846', '-', '0.03846)']
['system', 'ROUGE-S*', 'Average_F:', '0.01240', '(95%-conf.int.', '0.01240', '-', '0.01240)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:406', 'P:78', 'F:3']
["From Thatcher's (1973) work, it is obvious that the complexity of the set of paths from root to frontier of trees in a local set (the tree set of a CFG) is regular'."]
['A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', 'In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.', 'A formalism generating tree sets with complex path sets can still generate only semilinear languages if its tree sets have independent paths, and semilinear path sets.']
['system', 'ROUGE-S*', 'Average_R:', '0.00723', '(95%-conf.int.', '0.00723', '-', '0.00723)']
['system', 'ROUGE-S*', 'Average_P:', '0.15833', '(95%-conf.int.', '0.15833', '-', '0.15833)']
['system', 'ROUGE-S*', 'Average_F:', '0.01383', '(95%-conf.int.', '0.01383', '-', '0.01383)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:120', 'F:19']
['However, in order to capture the properties of various grammatical systems under consideration, our notation is more restrictive that ILFP, which was designed as a general logical notation to characterize the complete class of languages that are recognizable in polynomial time.']
['A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', 'In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.00966', '(95%-conf.int.', '0.00966', '-', '0.00966)']
['system', 'ROUGE-S*', 'Average_P:', '0.11579', '(95%-conf.int.', '0.11579', '-', '0.11579)']
['system', 'ROUGE-S*', 'Average_F:', '0.01783', '(95%-conf.int.', '0.01783', '-', '0.01783)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:190', 'F:22']
["As in the case of the derivation trees of CFG's, nodes are labeled by a member of some finite set of symbols (perhaps only implicit in the grammar as in TAG's) used to denote derived structures."]
['A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.02621', '(95%-conf.int.', '0.02621', '-', '0.02621)']
['system', 'ROUGE-S*', 'Average_P:', '0.10833', '(95%-conf.int.', '0.10833', '-', '0.10833)']
['system', 'ROUGE-S*', 'Average_F:', '0.04221', '(95%-conf.int.', '0.04221', '-', '0.04221)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:120', 'F:13']
["Since every CFL is known to be semilinear (Parikh, 1966), in order to show semilinearity of some language, we need only show the existence of a letter equivalent CFL Our definition of LCFRS's insists that the composition operations are linear and nonerasing."]
['A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.00202', '(95%-conf.int.', '0.00202', '-', '0.00202)']
['system', 'ROUGE-S*', 'Average_P:', '0.00526', '(95%-conf.int.', '0.00526', '-', '0.00526)']
['system', 'ROUGE-S*', 'Average_F:', '0.00292', '(95%-conf.int.', '0.00292', '-', '0.00292)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:190', 'F:1']
["It is interesting to note, however, that the ability to produce a bounded number of dependent paths (where two dependent paths can share an unbounded amount of information) does not require machinery as powerful as that used in LFG, FUG and IG's."]
['In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:190', 'F:0']
['Members of LCFRS whose operations have this property can be translated into the ILFP notation (Rounds, 1985).']
['A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', 'In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:36', 'F:0']
0.0449427268642 0.00866272719398 0.0124163635235





input/ref/Task1/E03-1005_aakansha.csv
input/res/Task1/E03-1005.csv
parsing: input/ref/Task1/E03-1005_aakansha.csv
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'"]
'105'
['105']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'"]
'145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="80" ssid="32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S>
original cit marker offset is 0
new cit marker offset is 0



["'80'"]
'80'
['80']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["'143'"]
'143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S><S sid="141" ssid="6">This is roughly an 11% relative reduction in error rate over Charniak (2000) and Bods PCFG-reduction reported in Table 1.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'", "'141'"]
'140'
'141'
['140', '141']
parsed_discourse_facet ['result_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="102" ssid="5">In case the shortest derivation is not unique, Bod (2000b) proposes to back off to a frequency ordering of the subtrees.</S>
    <S sid="103" ssid="6">That is, all subtrees of each root label are assigned a rank according to their frequency in the treebank: the most frequent subtree (or subtrees) of each root label gets rank 1, the second most frequent subtree gets rank 2, etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'", "'103'"]
'102'
'103'
['102', '103']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="46" ssid="43">Most previous notions of best parse tree in DOP1 were based on a probabilistic metric, with Bod (2000b) as a notable exception, who used a simplicity metric based on the shortest derivation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'46'"]
'46'
['46']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="37">For example, Bod (2001) samples a fixed number of subtrees of each depth, which has the effect of assigning roughly equal weight to each node in the training data, and roughly exponentially less probability for larger trees (see Goodman 2002: 12).</S>
    <S sid="86" ssid="38">Bod reports state-of-the-art results with this method, and observes no decrease in parse accuracy when larger subtrees are included (using subtrees up to depth 14).</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'", "'86'"]
'85'
'86'
['85', '86']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['result_citation']
<S sid="115" ssid="18">The only thing that needs to be changed for Simplicity-DOP is that all subtrees should be assigned equal probabilities.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="74">Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'74'", "'0'"]
'97'
'74'
'0'
['97', '74', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'31'", "'97'"]
'0'
'31'
'97'
['0', '31', '97']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'108'", "'51'"]
'97'
'108'
'51'
['97', '108', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'97'"]
'0'
'51'
'97'
['0', '51', '97']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'108'"]
'97'
'0'
'108'
['97', '0', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'51'"]
'97'
'0'
'51'
['97', '0', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'108'"]
'97'
'0'
'108'
['97', '0', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="101">We will refer to this model as Simplicity-DOP.</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'31'", "'97'"]
'101'
'31'
'97'
['101', '31', '97']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'108'"]
'97'
'0'
'108'
['97', '0', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'51'", "'0'"]
'97'
'51'
'0'
['97', '51', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="101">We will refer to this model as Simplicity-DOP.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'101'", "'108'"]
'0'
'101'
'108'
['0', '101', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'31'", "'108'"]
'0'
'31'
'108'
['0', '31', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'31'", "'108'"]
'0'
'31'
'108'
['0', '31', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'108'", "'51'"]
'97'
'108'
'51'
['97', '108', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="67">We can create a subtree by choosing any possible left subtree and any possible right subtree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'67'"]
'97'
'0'
'67'
['97', '0', '67']
parsed_discourse_facet ['method_citation']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['An Efficient Implementation of a New DOP Model', "That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.", 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:78', 'F:0']
['The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.']
['An Efficient Implementation of a New DOP Model', "Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.", 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.01961', '(95%-conf.int.', '0.01961', '-', '0.01961)']
['system', 'ROUGE-S*', 'Average_P:', '0.03667', '(95%-conf.int.', '0.03667', '-', '0.03667)']
['system', 'ROUGE-S*', 'Average_F:', '0.02555', '(95%-conf.int.', '0.02555', '-', '0.02555)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:300', 'F:11']
['Bod reports state-of-the-art results with this method, and observes no decrease in parse accuracy when larger subtrees are included (using subtrees up to depth 14).', 'For example, Bod (2001) samples a fixed number of subtrees of each depth, which has the effect of assigning roughly equal weight to each node in the training data, and roughly exponentially less probability for larger trees (see Goodman 2002: 12).']
['An Efficient Implementation of a New DOP Model', 'This resulted in a statistically consistent model dubbed ML-DOP.', 'The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.']
['system', 'ROUGE-S*', 'Average_R:', '0.01000', '(95%-conf.int.', '0.01000', '-', '0.01000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00405', '(95%-conf.int.', '0.00405', '-', '0.00405)']
['system', 'ROUGE-S*', 'Average_F:', '0.00576', '(95%-conf.int.', '0.00576', '-', '0.00576)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:300', 'P:741', 'F:3']
['While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.']
['An Efficient Implementation of a New DOP Model', "That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.", 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.00325', '(95%-conf.int.', '0.00325', '-', '0.00325)']
['system', 'ROUGE-S*', 'Average_P:', '0.01425', '(95%-conf.int.', '0.01425', '-', '0.01425)']
['system', 'ROUGE-S*', 'Average_F:', '0.00529', '(95%-conf.int.', '0.00529', '-', '0.00529)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:351', 'F:5']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.', "That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.", 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:78', 'F:0']
['The only thing that needs to be changed for Simplicity-DOP is that all subtrees should be assigned equal probabilities.']
['An Efficient Implementation of a New DOP Model', 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.', 'We can create a subtree by choosing any possible left subtree and any possible right subtree.']
['system', 'ROUGE-S*', 'Average_R:', '0.00529', '(95%-conf.int.', '0.00529', '-', '0.00529)']
['system', 'ROUGE-S*', 'Average_P:', '0.07143', '(95%-conf.int.', '0.07143', '-', '0.07143)']
['system', 'ROUGE-S*', 'Average_F:', '0.00985', '(95%-conf.int.', '0.00985', '-', '0.00985)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:28', 'F:2']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['An Efficient Implementation of a New DOP Model', "That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.", 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:78', 'F:0']
['DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).']
['The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.', "That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.", 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.00326', '(95%-conf.int.', '0.00326', '-', '0.00326)']
['system', 'ROUGE-S*', 'Average_P:', '0.15556', '(95%-conf.int.', '0.15556', '-', '0.15556)']
['system', 'ROUGE-S*', 'Average_F:', '0.00639', '(95%-conf.int.', '0.00639', '-', '0.00639)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:45', 'F:7']
['In case the shortest derivation is not unique, Bod (2000b) proposes to back off to a frequency ordering of the subtrees.', 'That is, all subtrees of each root label are assigned a rank according to their frequency in the treebank: the most frequent subtree (or subtrees) of each root label gets rank 1, the second most frequent subtree gets rank 2, etc.']
['An Efficient Implementation of a New DOP Model', 'The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.', 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:406', 'F:0']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['An Efficient Implementation of a New DOP Model', 'The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.', 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:78', 'F:0']
0.028195999718 0.00414099995859 0.00528399994716





input/ref/Task1/P11-1060_sweta.csv
input/res/Task1/P11-1060.csv
parsing: input/ref/Task1/P11-1060_sweta.csv
<S sid="8" ssid="4">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
 <S sid="45" ssid="21">The logical forms in DCS are called DCS trees, where nodes are labeled with predicates, and edges are labeled with relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["45'"]
45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="27">It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent&#8217;s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
 <S sid="166" ssid="51">Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language&#8212;think grounded al., 2010). compositional semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["166'"]
166'
['166']
parsed_discourse_facet ['method_citation']
 <S sid="8" ssid="4">On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
 <S sid="11" ssid="7">Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["11'"]
11'
['11']
parsed_discourse_facet ['method_citation']
  <S sid="115" ssid="91">After training, given a new utterance x, our system outputs the most likely y, summing out the latent logical form z: argmaxy p&#952;(T)(y  |x, z &#8712; &#732;ZL,&#952;(T)).</S>
original cit marker offset is 0
new cit marker offset is 0



["115'"]
115'
['115']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="1">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="94" ssid="70">We now turn to the task of mapping natural language For the example in Figure 4(b), the de- utterances to DCS trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["94'"]
94'
['94']
parsed_discourse_facet ['method_citation']
<S sid="132" ssid="17">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["132'"]
132'
['132']
parsed_discourse_facet ['method_citation']
<S sid="157" ssid="42">Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["157'"]
157'
['157']
parsed_discourse_facet ['method_citation']
<S sid="106" ssid="82">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(&#952;) = E(x,y)ED log p&#952;(JzKw = y  |x, z &#8712; ZL(x)) &#8722; &#955;k&#952;k22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["106'"]
106'
['106']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="52">In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="138" ssid="23">Next, we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).</S>
original cit marker offset is 0
new cit marker offset is 0



["138'"]
138'
['138']
parsed_discourse_facet ['method_citation']
<S sid="40" ssid="16">The CSP has two types of constraints: (i) x &#8712; w(p) for each node x labeled with predicate p &#8712; P; and (ii) xj = yj0 (the j-th component of x must equal the j'-th component of y) for each edge (x, y) labeled with j0j &#8712; R. A solution to the CSP is an assignment of nodes to values that satisfies all the constraints.</S>
original cit marker offset is 0
new cit marker offset is 0



["40'"]
40'
['40']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="56">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>
original cit marker offset is 0
new cit marker offset is 0



["171'"]
171'
['171']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1060.csv
<S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'2'"]
'100'
'21'
'2'
['100', '21', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="23">We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'21'", "'23'"]
'2'
'21'
'23'
['2', '21', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'2'"]
'0'
'21'
'2'
['0', '21', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'25'"]
'21'
'0'
'25'
['21', '0', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'106'"]
'21'
'0'
'106'
['21', '0', '106']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'100'"]
'21'
'0'
'100'
['21', '0', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'100'"]
'0'
'21'
'100'
['0', '21', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="2">In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'100'", "'2'"]
'21'
'100'
'2'
['21', '100', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'25'"]
'21'
'0'
'25'
['21', '0', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="23">We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'23'"]
'0'
'21'
'23'
['0', '21', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'25'"]
'21'
'0'
'25'
['21', '0', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="25">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'25'"]
'0'
'21'
'25'
['0', '21', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="174">598</S><S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'174'", "'106'"]
'21'
'174'
'106'
['21', '174', '106']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'106'", "'21'", "'100'"]
'106'
'21'
'100'
['106', '21', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="23">We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'0'", "'23'"]
'100'
'0'
'23'
['100', '0', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="106">Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(θ) = E(x,y)ED log pθ(JzKw = y  |x, z ∈ ZL(x)) − λkθk22, which sums over all DCS trees z that evaluate to the target answer y.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'0'", "'106'"]
'21'
'0'
'106'
['21', '0', '106']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'21'", "'0'"]
'100'
'21'
'0'
['100', '21', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Learning Dependency-Based Compositional Semantics</S><S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="100">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z ∈ ZL(x) given an utterance x.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'21'", "'100'"]
'0'
'21'
'100'
['0', '21', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="21">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S><S ssid="1" sid="12">We represent logical forms z as labeled trees, induced automatically from (x, y) pairs.</S><S ssid="1" sid="17">The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'", "'12'", "'17'"]
'21'
'12'
'17'
['21', '12', '17']
parsed_discourse_facet ['method_citation']
['Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem.']
[u'Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.', u'Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(\u03b8) = E(x,y)ED log p\u03b8(JzKw = y  |x, z \u2208 ZL(x)) \u2212 \u03bbk\u03b8k22, which sums over all DCS trees z that evaluate to the target answer y.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:120', 'F:0']
['It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent&#8217;s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.']
['Learning Dependency-Based Compositional Semantics', u'Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(\u03b8) = E(x,y)ED log p\u03b8(JzKw = y  |x, z \u2208 ZL(x)) \u2212 \u03bbk\u03b8k22, which sums over all DCS trees z that evaluate to the target answer y.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00732', '(95%-conf.int.', '0.00732', '-', '0.00732)']
['system', 'ROUGE-S*', 'Average_P:', '0.01478', '(95%-conf.int.', '0.01478', '-', '0.01478)']
['system', 'ROUGE-S*', 'Average_F:', '0.00979', '(95%-conf.int.', '0.00979', '-', '0.00979)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:406', 'F:6']
['Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.']
['Learning Dependency-Based Compositional Semantics', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).', 'We trained our model using an EM-like algorithm (Section 3) on two benchmarks, GEO and JOBS (Section 4).']
['system', 'ROUGE-S*', 'Average_R:', '0.00215', '(95%-conf.int.', '0.00215', '-', '0.00215)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00351', '(95%-conf.int.', '0.00351', '-', '0.00351)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:105', 'F:1']
['Figure 1 shows our probabilistic model: with respect to a world w (database of facts), producing an answer y.']
[u'Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.', 'In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00128', '(95%-conf.int.', '0.00128', '-', '0.00128)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00240', '(95%-conf.int.', '0.00240', '-', '0.00240)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:55', 'F:1']
['This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.']
['We represent logical forms z as labeled trees, induced automatically from (x, y) pairs.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).', 'The dominant paradigm in compositional semantics is Montague semantics, which constructs lambda calculus forms in a bottom-up manner.']
['system', 'ROUGE-S*', 'Average_R:', '0.05546', '(95%-conf.int.', '0.05546', '-', '0.05546)']
['system', 'ROUGE-S*', 'Average_P:', '0.17368', '(95%-conf.int.', '0.17368', '-', '0.17368)']
['system', 'ROUGE-S*', 'Average_F:', '0.08408', '(95%-conf.int.', '0.08408', '-', '0.08408)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:190', 'F:33']
["The CSP has two types of constraints: (i) x &#8712; w(p) for each node x labeled with predicate p &#8712; P; and (ii) xj = yj0 (the j-th component of x must equal the j'-th component of y) for each edge (x, y) labeled with j0j &#8712; R. A solution to the CSP is an assignment of nodes to values that satisfies all the constraints."]
['Learning Dependency-Based Compositional Semantics', u'Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:300', 'F:0']
['After training, given a new utterance x, our system outputs the most likely y, summing out the latent logical form z: argmaxy p&#952;(T)(y  |x, z &#8712; &#732;ZL,&#952;(T)).']
['Learning Dependency-Based Compositional Semantics', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).', 'We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:741', 'P:91', 'F:0']
['On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.']
['Learning Dependency-Based Compositional Semantics', u'Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:105', 'F:0']
['In DCS, we start with lexical triggers, which are 6 Conclusion more basic than CCG lexical entries.']
['Learning Dependency-Based Compositional Semantics', u'Learning Given a training dataset D containing (x, y) pairs, we define the regularized marginal log-likelihood objective O(\u03b8) = E(x,y)ED log p\u03b8(JzKw = y  |x, z \u2208 ZL(x)) \u2212 \u03bbk\u03b8k22, which sums over all DCS trees z that evaluate to the target answer y.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:45', 'F:0']
['On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives.']
[u'Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.', 'In this paper, we learn to map questions to answers via latent logical forms, which are induced automatically from question-answer pairs.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:105', 'F:0']
['Next, we compared our systems (DCS and DCS+) with the state-of-the-art semantic parsers on the full dataset for both GEO and JOBS (see Table 3).']
['Learning Dependency-Based Compositional Semantics', u'Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:91', 'F:0']
['Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language&#8212;think grounded al., 2010). compositional semantics.']
['Learning Dependency-Based Compositional Semantics', u'Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z \u2208 ZL(x) given an utterance x.', 'The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).']
['system', 'ROUGE-S*', 'Average_R:', '0.01210', '(95%-conf.int.', '0.01210', '-', '0.01210)']
['system', 'ROUGE-S*', 'Average_P:', '0.02372', '(95%-conf.int.', '0.02372', '-', '0.02372)']
['system', 'ROUGE-S*', 'Average_F:', '0.01602', '(95%-conf.int.', '0.01602', '-', '0.01602)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:253', 'F:6']
0.0199899998334 0.00652583327895 0.00964999991958





input/ref/Task1/P04-1036_sweta.csv
input/res/Task1/P04-1036.csv
parsing: input/ref/Task1/P04-1036_sweta.csv
<S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="82" ssid="11">We also calculate the WSD accuracy that would be obtained on SemCor, when using our first sense in all contexts ( ).</S>
original cit marker offset is 0
new cit marker offset is 0



["82'"]
82'
['82']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="20">We briefly summarise the two measures here; for a more detailed summary see (Patwardhan et al., 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="172" ssid="20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="1">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="165" ssid="13">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus, whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S>
original cit marker offset is 0
new cit marker offset is 0



["165'"]
165'
['165']
parsed_discourse_facet ['method_citation']
<S sid="171" ssid="19">In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["171'"]
171'
['171']
parsed_discourse_facet ['method_citation']
<S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["89'"]
89'
['89']
parsed_discourse_facet ['method_citation']
<S sid="172" ssid="20">We believe automatic ranking techniques such as ours will be useful for systems that rely on WordNet, for example those that use it for lexical acquisition or WSD.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="12">Additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses.</S>
original cit marker offset is 0
new cit marker offset is 0



["189'"]
189'
['189']
parsed_discourse_facet ['method_citation']
<S sid="87" ssid="16">Again, the automatic ranking outperforms this by a large margin.</S>
original cit marker offset is 0
new cit marker offset is 0



["87'"]
87'
['87']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["89'"]
89'
['89']
parsed_discourse_facet ['method_citation']
 <S sid="137" ssid="14">Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["137'"]
137'
['137']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="19">We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline, but because of space limitations give results for the two which perform the best.</S>
original cit marker offset is 0
new cit marker offset is 0



["63'"]
63'
['63']
parsed_discourse_facet ['method_citation']
<S sid="159" ssid="7">Magnini and Cavagli`a (2000) have identified WordNet word senses with particular domains, and this has proven useful for high precision WSD (Magnini et al., 2001); indeed in section 5 we used these domain labels for evaluation.</S>
original cit marker offset is 0
new cit marker offset is 0



["159'"]
159'
['159']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S ssid="1" sid="107">To disambiguate senses a system should take context into account.</S><S ssid="1" sid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'1'", "'153'"]
'107'
'1'
'153'
['107', '1', '153']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="162">It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'162'"]
'179'
'178'
'162'
['179', '178', '162']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="34">In these each target word is entered with an ordered list of “nearest neighbours”.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'34'"]
'179'
'153'
'34'
['179', '153', '34']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="0">Finding Predominant Word Senses in Untagged Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'0'"]
'179'
'4'
'0'
['179', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'153'"]
'179'
'4'
'153'
['179', '4', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="169">This method obtains precision of 61% and recall 51%.</S><S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'", "'179'", "'4'"]
'169'
'179'
'4'
['169', '179', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'153'"]
'179'
'4'
'153'
['179', '4', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="0">Finding Predominant Word Senses in Untagged Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'0'"]
'179'
'4'
'0'
['179', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="62">2 The WordNet Similarity package supports a range of WordNet similarity scores.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'62'", "'4'"]
'179'
'62'
'4'
['179', '62', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'179'", "'178'"]
'4'
'179'
'178'
['4', '179', '178']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'36'"]
'179'
'4'
'36'
['179', '4', '36']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'153'"]
'179'
'178'
'153'
['179', '178', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'36'"]
'179'
'178'
'36'
['179', '178', '36']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="162">It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'162'", "'178'"]
'4'
'162'
'178'
['4', '162', '178']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'4'"]
'179'
'153'
'4'
['179', '153', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'4'"]
'179'
'153'
'4'
['179', '153', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'36'"]
'179'
'153'
'36'
['179', '153', '36']
parsed_discourse_facet ['method_citation']
['We briefly summarise the two measures here; for a more detailed summary see (Patwardhan et al., 2003).']
['Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.', u'In these each target word is entered with an ordered list of \u201cnearest neighbours\u201d.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:325', 'P:28', 'F:0']
['We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline, but because of space limitations give results for the two which perform the best.']
['Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.', 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:66', 'F:0']
['In contrast, we use the neighbours lists and WordNet similarity measures to impose a prevalence ranking on the WordNet senses.']
['Finding Predominant Word Senses in Untagged Text', 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.']
['system', 'ROUGE-S*', 'Average_R:', '0.01994', '(95%-conf.int.', '0.01994', '-', '0.01994)']
['system', 'ROUGE-S*', 'Average_P:', '0.12727', '(95%-conf.int.', '0.12727', '-', '0.12727)']
['system', 'ROUGE-S*', 'Average_F:', '0.03448', '(95%-conf.int.', '0.03448', '-', '0.03448)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:55', 'F:7']
['Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.']
['We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.', 'It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text.', 'We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.']
['system', 'ROUGE-S*', 'Average_R:', '0.00450', '(95%-conf.int.', '0.00450', '-', '0.00450)']
['system', 'ROUGE-S*', 'Average_P:', '0.01754', '(95%-conf.int.', '0.01754', '-', '0.01754)']
['system', 'ROUGE-S*', 'Average_F:', '0.00717', '(95%-conf.int.', '0.00717', '-', '0.00717)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:171', 'F:3']
['Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.']
['Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.', 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:136', 'F:0']
['word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.']
['This method obtains precision of 61% and recall 51%.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.', 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:78', 'F:0']
['Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.']
['2 The WordNet Similarity package supports a range of WordNet similarity scores.', 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.']
['system', 'ROUGE-S*', 'Average_R:', '0.00690', '(95%-conf.int.', '0.00690', '-', '0.00690)']
['system', 'ROUGE-S*', 'Average_P:', '0.01754', '(95%-conf.int.', '0.01754', '-', '0.01754)']
['system', 'ROUGE-S*', 'Average_F:', '0.00990', '(95%-conf.int.', '0.00990', '-', '0.00990)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:171', 'F:3']
['Magnini and Cavagli`a (2000) have identified WordNet word senses with particular domains, and this has proven useful for high precision WSD (Magnini et al., 2001); indeed in section 5 we used these domain labels for evaluation.']
['Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.', 'Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.']
['system', 'ROUGE-S*', 'Average_R:', '0.00230', '(95%-conf.int.', '0.00230', '-', '0.00230)']
['system', 'ROUGE-S*', 'Average_P:', '0.00526', '(95%-conf.int.', '0.00526', '-', '0.00526)']
['system', 'ROUGE-S*', 'Average_F:', '0.00320', '(95%-conf.int.', '0.00320', '-', '0.00320)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:190', 'F:1']
['word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.']
['Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.', 'We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.']
['system', 'ROUGE-S*', 'Average_R:', '0.01323', '(95%-conf.int.', '0.01323', '-', '0.01323)']
['system', 'ROUGE-S*', 'Average_P:', '0.06410', '(95%-conf.int.', '0.06410', '-', '0.06410)']
['system', 'ROUGE-S*', 'Average_F:', '0.02193', '(95%-conf.int.', '0.02193', '-', '0.02193)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:78', 'F:5']
['Again, the automatic ranking outperforms this by a large margin.']
['Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.', 'We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:10', 'F:0']
0.0231709997683 0.00468699995313 0.00766799992332





input/ref/Task1/W06-3114_sweta.csv
input/res/Task1/W06-3114.csv
parsing: input/ref/Task1/W06-3114_sweta.csv
 <S sid="108" ssid="1">The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="27">For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="44">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages, as demonstrated by the results for English-German and English-French.</S>
original cit marker offset is 0
new cit marker offset is 0



["151'"]
151'
['151']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="9">The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.</S>
original cit marker offset is 0
new cit marker offset is 0



["16'"]
16'
['16']
parsed_discourse_facet ['method_citation']
 <S sid="172" ssid="3">Due to many similarly performing systems, we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
 <S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["36'"]
36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="103" ssid="19">Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["103'"]
103'
['103']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="60">One annotator suggested that this was the case for as much as 10% of our test sentences.</S>
original cit marker offset is 0
new cit marker offset is 0



["167'"]
167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["102'"]
102'
['102']
parsed_discourse_facet ['method_citation']
<S sid="123" ssid="16">For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.</S>
original cit marker offset is 0
new cit marker offset is 0



["123'"]
123'
['123']
parsed_discourse_facet ['method_citation']
<S sid="34" ssid="27">For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.</S>
original cit marker offset is 0
new cit marker offset is 0



["34'"]
34'
['34']
parsed_discourse_facet ['method_citation']
<S sid="62" ssid="1">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.</S>
original cit marker offset is 0
new cit marker offset is 0



["62'"]
62'
['62']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="19">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["126'"]
126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="173" ssid="4">The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["173'"]
173'
['173']
parsed_discourse_facet ['method_citation']
 <S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["170'"]
170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="84" ssid="23">The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["84'"]
84'
['84']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["8'"]
8'
['8']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S><S ssid="1" sid="114">Pairwise comparison is done using the sign test.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'63'", "'114'"]
'1'
'63'
'114'
['1', '63', '114']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="26">Most of these groups follow a phrase-based statistical approach to machine translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'26'"]
'0'
'1'
'26'
['0', '1', '26']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="9">Training and testing is based on the Europarl corpus.</S><S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'0'", "'1'"]
'9'
'0'
'1'
['9', '0', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'0'"]
'1'
'103'
'0'
['1', '103', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="9">Training and testing is based on the Europarl corpus.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'1'", "'0'"]
'9'
'1'
'0'
['9', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="113">The confidence intervals are computed by bootstrap resampling for BLEU, and by standard significance testing for the manual scores, as described earlier in the paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["'103'", "'1'", "'113'"]
'103'
'1'
'113'
['103', '1', '113']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'103'", "'1'"]
'63'
'103'
'1'
['63', '103', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'103'", "'1'"]
'63'
'103'
'1'
['63', '103', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'2'", "'63'"]
'1'
'2'
'63'
['1', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="114">Pairwise comparison is done using the sign test.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'1'", "'103'"]
'114'
'1'
'103'
['114', '1', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'2'", "'103'"]
'1'
'2'
'103'
['1', '2', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="26">Most of these groups follow a phrase-based statistical approach to machine translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'26'"]
'1'
'103'
'26'
['1', '103', '26']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'63'"]
'1'
'103'
'63'
['1', '103', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="9">Training and testing is based on the Europarl corpus.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'1'", "'103'"]
'9'
'1'
'103'
['9', '1', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'2'"]
'1'
'103'
'2'
['1', '103', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'103'"]
'0'
'1'
'103'
['0', '1', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'2'", "'63'"]
'1'
'2'
'63'
['1', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="170">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'170'", "'1'"]
'0'
'170'
'1'
['0', '170', '1']
parsed_discourse_facet ['method_citation']
['The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'Manual and Automatic Evaluation of Machine Translation between European Languages', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:940506', 'P:45', 'F:0']
['The results of the manual and automatic evaluation of the participating system translations is detailed in the figures at the end of this paper.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'Pairwise comparison is done using the sign test.', 'Many human evaluation metrics have been proposed.']
['system', 'ROUGE-S*', 'Average_R:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Average_P:', '0.14545', '(95%-conf.int.', '0.14545', '-', '0.14545)']
['system', 'ROUGE-S*', 'Average_F:', '0.00002', '(95%-conf.int.', '0.00002', '-', '0.00002)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:926841', 'P:55', 'F:8']
['One annotator suggested that this was the case for as much as 10% of our test sentences.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', u'0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 \u2022systran \u2022 ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency \u2022systran \u2022nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p \ufffd 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain \u2022upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv 23 24 25 26 27 28 29 30 31 32 \u2022upc-mr \u2022utd \u2022upc-jmc \u2022uedin-birch \u2022ntt \u2022rali \u2022uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy \u2022upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency \u2022ntt \u2022nrc \u2022rali \u2022uedin-birch -0.2 -0.3 -0.5 \u2022upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr \u2022 rali Fluency -0.4 \u2022upc-mr utd \u2022upc-jmc -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 \u2022upv 0.5 0.4 \u2022systran \u2022upc-mr \u2022 \u2022rali 0.3 \u2022ntt 0.2 0.1 -0.0 -0.1 \u2022systran \u2022upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022upv \u2022systran \u2022upc-mr \u2022 Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022systran \u2022ntt', 'Many human evaluation metrics have been proposed.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1599366', 'P:15', 'F:1']
['In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.']
['Training and testing is based on the Europarl corpus.', 'Manual and Automatic Evaluation of Machine Translation between European Languages', u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .']
['system', 'ROUGE-S*', 'Average_R:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Average_P:', '0.05495', '(95%-conf.int.', '0.05495', '-', '0.05495)']
['system', 'ROUGE-S*', 'Average_F:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:932295', 'P:91', 'F:5']
['The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages, as demonstrated by the results for English-German and English-French.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', u'Given a set of n sentences, we can compute the sample mean x\ufffd and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x\u2212d, x+df can be computed by d = 1.96 \xb7\ufffdn (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.', 'Manual and Automatic Evaluation of Machine Translation between European Languages']
['system', 'ROUGE-S*', 'Average_R:', '0.00002', '(95%-conf.int.', '0.00002', '-', '0.00002)']
['system', 'ROUGE-S*', 'Average_P:', '0.11765', '(95%-conf.int.', '0.11765', '-', '0.11765)']
['system', 'ROUGE-S*', 'Average_F:', '0.00003', '(95%-conf.int.', '0.00003', '-', '0.00003)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:969528', 'P:136', 'F:16']
['While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', u'Given a set of n sentences, we can compute the sample mean x\ufffd and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x\u2212d, x+df can be computed by d = 1.96 \xb7\ufffdn (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.', 'Many human evaluation metrics have been proposed.']
['system', 'ROUGE-S*', 'Average_R:', '0.00002', '(95%-conf.int.', '0.00002', '-', '0.00002)']
['system', 'ROUGE-S*', 'Average_P:', '0.07792', '(95%-conf.int.', '0.07792', '-', '0.07792)']
['system', 'ROUGE-S*', 'Average_F:', '0.00004', '(95%-conf.int.', '0.00004', '-', '0.00004)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:965355', 'P:231', 'F:18']
['The test data was again drawn from a segment of the Europarl corpus from the fourth quarter of 2000, which is excluded from the training data.']
['Training and testing is based on the Europarl corpus.', u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'Manual and Automatic Evaluation of Machine Translation between European Languages']
['system', 'ROUGE-S*', 'Average_R:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Average_P:', '0.13636', '(95%-conf.int.', '0.13636', '-', '0.13636)']
['system', 'ROUGE-S*', 'Average_F:', '0.00002', '(95%-conf.int.', '0.00002', '-', '0.00002)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:932295', 'P:66', 'F:9']
['For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', u'Given a set of n sentences, we can compute the sample mean x\ufffd and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x\u2212d, x+df can be computed by d = 1.96 \xb7\ufffdn (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.', 'Most of these groups follow a phrase-based statistical approach to machine translation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.03571', '(95%-conf.int.', '0.03571', '-', '0.03571)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:970921', 'P:28', 'F:1']
['The bias of automatic methods in favor of statistical systems seems to be less pronounced on out-of-domain test data.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', u'Given a set of n sentences, we can compute the sample mean x\ufffd and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x\u2212d, x+df can be computed by d = 1.96 \xb7\ufffdn (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.', u'0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 \u2022systran \u2022 ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency \u2022systran \u2022nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p \ufffd 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain \u2022upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv 23 24 25 26 27 28 29 30 31 32 \u2022upc-mr \u2022utd \u2022upc-jmc \u2022uedin-birch \u2022ntt \u2022rali \u2022uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy \u2022upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency \u2022ntt \u2022nrc \u2022rali \u2022uedin-birch -0.2 -0.3 -0.5 \u2022upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr \u2022 rali Fluency -0.4 \u2022upc-mr utd \u2022upc-jmc -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 \u2022upv 0.5 0.4 \u2022systran \u2022upc-mr \u2022 \u2022rali 0.3 \u2022ntt 0.2 0.1 -0.0 -0.1 \u2022systran \u2022upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022upv \u2022systran \u2022upc-mr \u2022 Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022systran \u2022ntt']
['system', 'ROUGE-S*', 'Average_R:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Average_P:', '0.20000', '(95%-conf.int.', '0.20000', '-', '0.20000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1649836', 'P:45', 'F:9']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'Manual and Automatic Evaluation of Machine Translation between European Languages', u'Given a set of n sentences, we can compute the sample mean x\ufffd and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x\u2212d, x+df can be computed by d = 1.96 \xb7\ufffdn (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.00003', '(95%-conf.int.', '0.00003', '-', '0.00003)']
['system', 'ROUGE-S*', 'Average_P:', '0.50909', '(95%-conf.int.', '0.50909', '-', '0.50909)']
['system', 'ROUGE-S*', 'Average_F:', '0.00006', '(95%-conf.int.', '0.00006', '-', '0.00006)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:969528', 'P:55', 'F:28']
['For the manual scoring, we can distinguish only half of the systems, both in terms of fluency and adequacy.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', u'Given a set of n sentences, we can compute the sample mean x\ufffd and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x\u2212d, x+df can be computed by d = 1.96 \xb7\ufffdn (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.', u'0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 \u2022systran \u2022 ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency \u2022systran \u2022nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p \ufffd 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain \u2022upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv 23 24 25 26 27 28 29 30 31 32 \u2022upc-mr \u2022utd \u2022upc-jmc \u2022uedin-birch \u2022ntt \u2022rali \u2022uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy \u2022upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency \u2022ntt \u2022nrc \u2022rali \u2022uedin-birch -0.2 -0.3 -0.5 \u2022upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr \u2022 rali Fluency -0.4 \u2022upc-mr utd \u2022upc-jmc -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 \u2022upv 0.5 0.4 \u2022systran \u2022upc-mr \u2022 \u2022rali 0.3 \u2022ntt 0.2 0.1 -0.0 -0.1 \u2022systran \u2022upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022upv \u2022systran \u2022upc-mr \u2022 Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022systran \u2022ntt']
['system', 'ROUGE-S*', 'Average_R:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Average_P:', '0.35714', '(95%-conf.int.', '0.35714', '-', '0.35714)']
['system', 'ROUGE-S*', 'Average_F:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1649836', 'P:28', 'F:10']
['For more on the participating systems, please refer to the respective system description in the proceedings of the workshop.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'Manual and Automatic Evaluation of Machine Translation between European Languages', 'Most of these groups follow a phrase-based statistical approach to machine translation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:936396', 'P:28', 'F:0']
['The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', u'0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 \u2022systran \u2022 ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency \u2022systran \u2022nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p \ufffd 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain \u2022upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv 23 24 25 26 27 28 29 30 31 32 \u2022upc-mr \u2022utd \u2022upc-jmc \u2022uedin-birch \u2022ntt \u2022rali \u2022uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy \u2022upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency \u2022ntt \u2022nrc \u2022rali \u2022uedin-birch -0.2 -0.3 -0.5 \u2022upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr \u2022 rali Fluency -0.4 \u2022upc-mr utd \u2022upc-jmc -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 \u2022upv 0.5 0.4 \u2022systran \u2022upc-mr \u2022 \u2022rali 0.3 \u2022ntt 0.2 0.1 -0.0 -0.1 \u2022systran \u2022upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022upv \u2022systran \u2022upc-mr \u2022 Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022systran \u2022ntt', 'Many human evaluation metrics have been proposed.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.03571', '(95%-conf.int.', '0.03571', '-', '0.03571)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1599366', 'P:28', 'F:1']
0.133588460511 9.23076915976e-06 1.53846152663e-05





input/ref/Task1/W11-2123_vardha.csv
input/res/Task1/W11-2123.csv
parsing: input/ref/Task1/W11-2123_vardha.csv
    <S sid="12" ssid="7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
 <S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
 <S sid="131" ssid="3">Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N &#8722; 1 preceding words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'131'"]
'131'
['131']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="16">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="16">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'21'"]
'21'
['21']
parsed_discourse_facet ['method_citation']
<S sid="108" ssid="12">Compared with SRILM, IRSTLM adds several features: lower memory consumption, a binary file format with memory mapping, caching to increase speed, and quantization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'108'"]
'108'
['108']
parsed_discourse_facet ['method_citation']
    <S sid="129" ssid="1">In addition to the optimizations specific to each datastructure described in Section 2, we implement several general optimizations for language modeling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'129'"]
'129'
['129']
parsed_discourse_facet ['method_citation']
    <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
    <S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
    <S sid="263" ssid="5">Quantization can be improved by jointly encoding probability and backoff.</S>
original cit marker offset is 0
new cit marker offset is 0



["'263'"]
'263'
['263']
parsed_discourse_facet ['method_citation']
    <S sid="52" ssid="30">Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'52'"]
'52'
['52']
parsed_discourse_facet ['method_citation']
    <S sid="1" ssid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'"]
'1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="17">If the context wnf will never extend to the right (i.e. wnf v is not present in the model for all words v) then no subsequent query will match the full context.</S>
original cit marker offset is 0
new cit marker offset is 0



["'145'"]
'145'
['145']
parsed_discourse_facet ['method_citation']
 <S sid="182" ssid="1">This section measures performance on shared tasks in order of increasing complexity: sparse lookups, evaluating perplexity of a large file, and translation with Moses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'182'"]
'182'
['182']
parsed_discourse_facet ['method_citation']
    <S sid="274" ssid="1">We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.</S>
original cit marker offset is 0
new cit marker offset is 0



["'274'"]
'274'
['274']
parsed_discourse_facet ['method_citation']
    <S sid="12" ssid="7">Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'"]
'12'
['12']
parsed_discourse_facet ['method_citation']
    <S sid="7" ssid="2">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="12">We have modified Moses (Koehn et al., 2007) to keep our state with hypotheses; to conserve memory, phrases do not keep state.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
 <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
 <S sid="199" ssid="18">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'199'"]
'199'
['199']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W11-2123.csv
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="32">Linear probing places at most one entry in each bucket.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'32'"]
'5'
'0'
'32'
['5', '0', '32']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="187">The data structure was populated with 64-bit integers sampled uniformly without replacement.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'187'"]
'5'
'0'
'187'
['5', '0', '187']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'109'"]
'0'
'25'
'109'
['0', '25', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'0'", "'5'"]
'1'
'0'
'5'
['1', '0', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'109'"]
'5'
'0'
'109'
['5', '0', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S><S ssid="1" sid="7">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'25'", "'7'"]
'5'
'25'
'7'
['5', '25', '7']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'25'"]
'5'
'1'
'25'
['5', '1', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="3">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="7">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'", "'0'", "'7'"]
'3'
'0'
'7'
['3', '0', '7']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="7">This paper presents methods to query N-gram language models, minimizing time and space costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'7'"]
'5'
'1'
'7'
['5', '1', '7']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="32">Linear probing places at most one entry in each bucket.</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'32'", "'109'"]
'5'
'32'
'109'
['5', '32', '109']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'5'", "'1'"]
'0'
'5'
'1'
['0', '5', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="25">An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'1'", "'25'"]
'5'
'1'
'25'
['5', '1', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'109'", "'5'"]
'1'
'109'
'5'
['1', '109', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="3">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'5'", "'3'"]
'0'
'5'
'3'
['0', '5', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'0'", "'5'"]
'109'
'0'
'5'
['109', '0', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="102">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie), allocating memory all at once (eliminating the need for full pointers), and being easy to compile.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="200">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'", "'0'", "'200'"]
'102'
'0'
'200'
['102', '0', '200']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="9">Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf , returning where the probability p(wn|wn−1 f ) and backoff penalties b(wn−1 i ) are given by an already-estimated model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'9'"]
'5'
'0'
'9'
['5', '0', '9']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S><S ssid="1" sid="3">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S><S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'109'", "'3'", "'5'"]
'109'
'3'
'5'
['109', '3', '5']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="1">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'1'"]
'5'
'0'
'1'
['5', '0', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="5">This paper describes the several performance techniques used and presents benchmarks against alternative implementations.</S><S ssid="1" sid="0">KenLM: Faster and Smaller Language Model Queries</S><S ssid="1" sid="109">Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'0'", "'109'"]
'5'
'0'
'109'
['5', '0', '109']
parsed_discourse_facet ['method_citation']
['If the context wnf will never extend to the right (i.e. wnf v is not present in the model for all words v) then no subsequent query will match the full context.']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.', 'Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00504', '(95%-conf.int.', '0.00504', '-', '0.00504)']
['system', 'ROUGE-S*', 'Average_P:', '0.04545', '(95%-conf.int.', '0.04545', '-', '0.04545)']
['system', 'ROUGE-S*', 'Average_F:', '0.00908', '(95%-conf.int.', '0.00908', '-', '0.00908)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:66', 'F:3']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.', 'An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.11712', '(95%-conf.int.', '0.11712', '-', '0.11712)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.20968', '(95%-conf.int.', '0.20968', '-', '0.20968)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:78', 'F:78']
['We have modified Moses (Koehn et al., 2007) to keep our state with hypotheses; to conserve memory, phrases do not keep state.']
['Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', 'Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00073', '(95%-conf.int.', '0.00073', '-', '0.00073)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00140', '(95%-conf.int.', '0.00140', '-', '0.00140)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1378', 'P:55', 'F:1']
['Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.']
['KenLM: Faster and Smaller Language Model Queries', 'Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:171', 'F:0']
['Dynamic programming efficiently scores many hypotheses by exploiting the fact that an N-gram language model conditions on at most N &#8722; 1 preceding words.']
['KenLM: Faster and Smaller Language Model Queries', 'Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.', 'An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.']
['system', 'ROUGE-S*', 'Average_R:', '0.00159', '(95%-conf.int.', '0.00159', '-', '0.00159)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00272', '(95%-conf.int.', '0.00272', '-', '0.00272)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:105', 'F:1']
['Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.', 'KenLM: Faster and Smaller Language Model Queries', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:171', 'F:0']
['Our implementation permits jumping to any n-gram of any length with a single lookup; this appears to be unique among language model implementations.']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.', 'KenLM: Faster and Smaller Language Model Queries', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.01709', '(95%-conf.int.', '0.01709', '-', '0.01709)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.02878', '(95%-conf.int.', '0.02878', '-', '0.02878)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:66', 'F:6']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
[u'Backoff-smoothed models estimate this probability based on the observed entry with longest matching history wnf , returning where the probability p(wn|wn\u22121 f ) and backoff penalties b(wn\u22121 i ) are given by an already-estimated model.', 'KenLM: Faster and Smaller Language Model Queries', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00300', '(95%-conf.int.', '0.00300', '-', '0.00300)']
['system', 'ROUGE-S*', 'Average_P:', '0.03636', '(95%-conf.int.', '0.03636', '-', '0.03636)']
['system', 'ROUGE-S*', 'Average_F:', '0.00555', '(95%-conf.int.', '0.00555', '-', '0.00555)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:55', 'F:2']
['This paper presents methods to query N-gram language models, minimizing time and space costs.']
['KenLM: Faster and Smaller Language Model Queries', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.', 'The data structure was populated with 64-bit integers sampled uniformly without replacement.']
['system', 'ROUGE-S*', 'Average_R:', '0.00791', '(95%-conf.int.', '0.00791', '-', '0.00791)']
['system', 'ROUGE-S*', 'Average_P:', '0.03636', '(95%-conf.int.', '0.03636', '-', '0.03636)']
['system', 'ROUGE-S*', 'Average_F:', '0.01299', '(95%-conf.int.', '0.01299', '-', '0.01299)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:253', 'P:55', 'F:2']
['Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.']
['The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie), allocating memory all at once (eliminating the need for full pointers), and being easy to compile.', 'KenLM: Faster and Smaller Language Model Queries', 'The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:78', 'F:0']
['Throughout this paper we compare with several packages: SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders.']
['KenLM: Faster and Smaller Language Model Queries', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.', 'Linear probing places at most one entry in each bucket.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:171', 'P:78', 'F:0']
['In addition to the optimizations specific to each datastructure described in Section 2, we implement several general optimizations for language modeling.']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.', 'An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00450', '(95%-conf.int.', '0.00450', '-', '0.00450)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00832', '(95%-conf.int.', '0.00832', '-', '0.00832)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:55', 'F:3']
['Compared with SRILM, IRSTLM adds several features: lower memory consumption, a binary file format with memory mapping, caching to increase speed, and quantization.']
['An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values given keys using little time.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.', 'This paper presents methods to query N-gram language models, minimizing time and space costs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:136', 'F:0']
['We have described two data structures for language modeling that achieve substantial reductions in time and memory cost.']
['KenLM: Faster and Smaller Language Model Queries', 'Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00265', '(95%-conf.int.', '0.00265', '-', '0.00265)']
['system', 'ROUGE-S*', 'Average_P:', '0.02222', '(95%-conf.int.', '0.02222', '-', '0.02222)']
['system', 'ROUGE-S*', 'Average_F:', '0.00473', '(95%-conf.int.', '0.00473', '-', '0.00473)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:45', 'F:1']
['For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.']
['KenLM: Faster and Smaller Language Model Queries', 'Our TRIE implementation is designed to improve upon IRSTLM using a reverse trie with improved search, bit level packing, and stateful queries.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00265', '(95%-conf.int.', '0.00265', '-', '0.00265)']
['system', 'ROUGE-S*', 'Average_P:', '0.00308', '(95%-conf.int.', '0.00308', '-', '0.00308)']
['system', 'ROUGE-S*', 'Average_F:', '0.00284', '(95%-conf.int.', '0.00284', '-', '0.00284)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:325', 'F:1']
['For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.']
['KenLM: Faster and Smaller Language Model Queries', 'Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', 'This paper presents methods to query N-gram language models, minimizing time and space costs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00532', '(95%-conf.int.', '0.00532', '-', '0.00532)']
['system', 'ROUGE-S*', 'Average_P:', '0.01846', '(95%-conf.int.', '0.01846', '-', '0.01846)']
['system', 'ROUGE-S*', 'Average_F:', '0.00826', '(95%-conf.int.', '0.00826', '-', '0.00826)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1128', 'P:325', 'F:6']
['This section measures performance on shared tasks in order of increasing complexity: sparse lookups, evaluating perplexity of a large file, and translation with Moses.']
['KenLM: Faster and Smaller Language Model Queries', 'Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:120', 'F:0']
['For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.']
['We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.', 'KenLM: Faster and Smaller Language Model Queries', 'This paper describes the several performance techniques used and presents benchmarks against alternative implementations.']
['system', 'ROUGE-S*', 'Average_R:', '0.00285', '(95%-conf.int.', '0.00285', '-', '0.00285)']
['system', 'ROUGE-S*', 'Average_P:', '0.00308', '(95%-conf.int.', '0.00308', '-', '0.00308)']
['system', 'ROUGE-S*', 'Average_F:', '0.00296', '(95%-conf.int.', '0.00296', '-', '0.00296)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:325', 'F:1']
0.0743427773648 0.00946944439184 0.0165172221305





input/ref/Task1/A97-1014_swastika.csv
input/res/Task1/A97-1014.csv
parsing: input/ref/Task1/A97-1014_swastika.csv
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



['151']
151
['151']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.</S>
original cit marker offset is 0
new cit marker offset is 0



['15']
15
['15']
parsed_discourse_facet ['result_citation']
<S sid="47" ssid="37">Argument structure can be represented in terms of unordered trees (with crossing branches).</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
<S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



['167']
167
['167']
parsed_discourse_facet ['method_citation']
<S sid="168" ssid="10">We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



['168']
168
['168']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



['160']
160
['160']
parsed_discourse_facet ['method_citation']
<S sid="127" ssid="8">As the need for certain functionalities becomes obvious with growing annotation experience, we have decided to implement the tool in two stages.</S>
original cit marker offset is 0
new cit marker offset is 0



['127']
127
['127']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



['39']
39
['39']
parsed_discourse_facet ['method_citation']
<S sid="72" ssid="17">In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.</S>
original cit marker offset is 0
new cit marker offset is 0



['72']
72
['72']
parsed_discourse_facet ['method_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
<S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



['4']
4
['4']
parsed_discourse_facet ['aim_citation']
parsing: input/res/Task1/A97-1014.csv
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'0'"]
'100'
'12'
'0'
['100', '12', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'0'"]
'100'
'12'
'0'
['100', '12', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="47">Argument structure can be represented in terms of unordered trees (with crossing branches).</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'47'", "'12'"]
'100'
'47'
'12'
['100', '47', '12']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="74">During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'74'"]
'100'
'12'
'74'
['100', '12', '74']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="47">Argument structure can be represented in terms of unordered trees (with crossing branches).</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'0'", "'100'"]
'47'
'0'
'100'
['47', '0', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S ssid="1" sid="74">During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'4'", "'74'"]
'100'
'4'
'74'
['100', '4', '74']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="114">Fig.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'114'", "'12'"]
'100'
'114'
'12'
['100', '114', '12']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="14">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'0'", "'14'"]
'4'
'0'
'14'
['4', '0', '14']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'0'", "'4'"]
'100'
'0'
'4'
['100', '0', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="21">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'0'", "'21'"]
'100'
'0'
'21'
['100', '0', '21']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S ssid="1" sid="14">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'4'", "'14'"]
'12'
'4'
'14'
['12', '4', '14']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="114">Fig.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'114'"]
'100'
'12'
'114'
['100', '12', '114']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="170">Our annotation tool supplies efficient manipulation and immediate visualization of argument structures.</S><S ssid="1" sid="162">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'170'", "'162'"]
'100'
'170'
'162'
['100', '170', '162']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="105">In addition, a. number of clear-cut NP cornponents can be defined outside that, juxtapositional kernel: pre- and postnominal genitives (GL, GR), relative clauses (RC), clausal and sentential complements (OC).</S><S ssid="1" sid="21">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'105'", "'21'"]
'100'
'105'
'21'
['100', '105', '21']
parsed_discourse_facet ['method_citation']
['In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.']
['We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc.', "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction."]
['system', 'ROUGE-S*', 'Average_R:', '0.00085', '(95%-conf.int.', '0.00085', '-', '0.00085)']
['system', 'ROUGE-S*', 'Average_P:', '0.01099', '(95%-conf.int.', '0.01099', '-', '0.01099)']
['system', 'ROUGE-S*', 'Average_F:', '0.00158', '(95%-conf.int.', '0.00158', '-', '0.00158)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:91', 'F:1']
['We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.']
['Fig.', 'We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:136', 'P:55', 'F:0']
['These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.']
['An Annotation Scheme for Free Word Order Languages', 'We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction."]
['system', 'ROUGE-S*', 'Average_R:', '0.00308', '(95%-conf.int.', '0.00308', '-', '0.00308)']
['system', 'ROUGE-S*', 'Average_P:', '0.04762', '(95%-conf.int.', '0.04762', '-', '0.04762)']
['system', 'ROUGE-S*', 'Average_F:', '0.00578', '(95%-conf.int.', '0.00578', '-', '0.00578)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:325', 'P:21', 'F:1']
['In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.']
['Fig.', 'We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:136', 'P:36', 'F:0']
["The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction."]
['We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.', 'Our annotation tool supplies efficient manipulation and immediate visualization of argument structures.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:66', 'F:0']
["The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction."]
['We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'Disambiguation is based on human processing skills (cf.', 'In addition, a. number of clear-cut NP cornponents can be defined outside that, juxtapositional kernel: pre- and postnominal genitives (GL, GR), relative clauses (RC), clausal and sentential complements (OC).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:66', 'F:0']
['For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).']
['Argument structure can be represented in terms of unordered trees (with crossing branches).', 'We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:276', 'P:55', 'F:0']
["Consider the German sentence (1) daran wird ihn Anna erkennen, &di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes."]
['Realworld texts annotated with different strata of linguistic information can be used for grammar induction.', "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.", 'Corpora annotated with syntactic structures are commonly referred to as trctbank.5.']
['system', 'ROUGE-S*', 'Average_R:', '0.00265', '(95%-conf.int.', '0.00265', '-', '0.00265)']
['system', 'ROUGE-S*', 'Average_P:', '0.00246', '(95%-conf.int.', '0.00246', '-', '0.00246)']
['system', 'ROUGE-S*', 'Average_F:', '0.00255', '(95%-conf.int.', '0.00255', '-', '0.00255)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:406', 'F:1']
['As the need for certain functionalities becomes obvious with growing annotation experience, we have decided to implement the tool in two stages.']
['An Annotation Scheme for Free Word Order Languages', 'We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'Disambiguation is based on human processing skills (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:190', 'P:36', 'F:0']
['We will closely coordinate the further development of our corpus with the annotation work in Verbmobil and with other German efforts in corpus annotation.']
['An Annotation Scheme for Free Word Order Languages', 'We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00433', '(95%-conf.int.', '0.00433', '-', '0.00433)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00699', '(95%-conf.int.', '0.00699', '-', '0.00699)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:55', 'F:1']
['Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.']
['We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.', 'During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc.']
['system', 'ROUGE-S*', 'Average_R:', '0.00101', '(95%-conf.int.', '0.00101', '-', '0.00101)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00183', '(95%-conf.int.', '0.00183', '-', '0.00183)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:105', 'F:1']
0.00806999992664 0.00108363635379 0.00170272725725





input/ref/Task1/W99-0613_sweta.csv
input/res/Task1/W99-0613.csv
parsing: input/ref/Task1/W99-0613_sweta.csv
<S sid="121" ssid="54">They also describe an application of cotraining to classifying web pages (the to feature sets are the words on the page, and other pages pointing to the page).</S>
original cit marker offset is 0
new cit marker offset is 0



["121'"]
121'
['121']
parsed_discourse_facet ['method_citation']
<S sid="252" ssid="3">The method uses a &amp;quot;soft&amp;quot; measure of the agreement between two classifiers as an objective function; we described an algorithm which directly optimizes this function.</S>
original cit marker offset is 0
new cit marker offset is 0



["252'"]
252'
['252']
parsed_discourse_facet ['method_citation']
 <S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="24">There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="213" ssid="80">Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.</S>
original cit marker offset is 0
new cit marker offset is 0



["213'"]
213'
['213']
parsed_discourse_facet ['method_citation']
 <S sid="250" ssid="1">Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.</S>
original cit marker offset is 0
new cit marker offset is 0



["250'"]
250'
['250']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="33">(Brin 98) ,describes a system for extracting (author, book-title) pairs from the World Wide Web using an approach that bootstraps from an initial seed set of examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["39'"]
39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="202" ssid="69">The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels, and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.</S>
original cit marker offset is 0
new cit marker offset is 0



["202'"]
202'
['202']
parsed_discourse_facet ['method_citation']
<S sid="61" ssid="15">The following features were used: full-string=x The full string (e.g., for Maury Cooper, full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word, this feature applies for any words that the string contains (e.g., Maury Cooper contributes two such features, contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g., IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods, and contains at least one period.</S>
original cit marker offset is 0
new cit marker offset is 0



["61'"]
61'
['61']
parsed_discourse_facet ['method_citation']
<S sid="176" ssid="43">(7) is at 0 when: 1) Vi : sign(gi (xi)) = sign(g2 (xi)); 2) Ig3(xi)l oo; and 3) sign(gi (xi)) = yi for i = 1, , m. In fact, Zco provides a bound on the sum of the classification error of the labeled examples and the number of disagreements between the two classifiers on the unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["176'"]
176'
['176']
parsed_discourse_facet ['method_citation']
 <S sid="108" ssid="41">In the cotraining case, (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples, and (2) must agree with each other on the unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["108'"]
108'
['108']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="21">The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).</S>
    <S sid="28" ssid="22">(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'", "'28'"]
27'
'28'
['27', '28']
parsed_discourse_facet ['method_citation']
<S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["7'"]
7'
['7']
parsed_discourse_facet ['method_citation']
 <S sid="172" ssid="39">To see this, note thai the first two terms in the above equation correspond to the function that AdaBoost attempts to minimize in the standard supervised setting (Equ.</S>
original cit marker offset is 0
new cit marker offset is 0



["172'"]
172'
['172']
parsed_discourse_facet ['method_citation']
<S sid="85" ssid="18">(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["85'"]
85'
['85']
parsed_discourse_facet ['method_citation']
 <S sid="214" ssid="81">This modification brings the method closer to the DL-CoTrain algorithm described earlier, and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples, preventing one label from dominating &#8212; this deserves more theoretical investigation.</S>
original cit marker offset is 0
new cit marker offset is 0



["214'"]
214'
['214']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'0'", "'179'"]
'137'
'0'
'179'
['137', '0', '179']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="33">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'33'", "'179'"]
'137'
'33'
'179'
['137', '33', '179']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S><S ssid="1" sid="33">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'0'", "'33'"]
'137'
'0'
'33'
['137', '0', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="79">2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'79'", "'0'"]
'137'
'79'
'0'
['137', '79', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'137'", "'0'"]
'179'
'137'
'0'
['179', '137', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="224">The model was parameterized such that the joint probability of a (label, feature-set) pair P(yi, xi) is written as The model assumes that (y, x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'224'"]
'137'
'179'
'224'
['137', '179', '224']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="222">The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'222'", "'179'"]
'137'
'222'
'179'
['137', '222', '179']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
['There are two differences between this method and the DL-CoTrain algorithm: spelling and contextual features, alternating between labeling and learning with the two types of features.']
['The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).', 'Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.']
['system', 'ROUGE-S*', 'Average_R:', '0.00285', '(95%-conf.int.', '0.00285', '-', '0.00285)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00466', '(95%-conf.int.', '0.00466', '-', '0.00466)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:78', 'F:1']
['The first method builds on results from (Yarowsky 95) and (Blum and Mitchell 98).', '(Yarowsky 95) describes an algorithm for word-sense disambiguation that exploits redundancy in contextual features, and gives impressive performance.']
['The model was parameterized such that the joint probability of a (label, feature-set) pair P(yi, xi) is written as The model assumes that (y, x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00314', '(95%-conf.int.', '0.00314', '-', '0.00314)']
['system', 'ROUGE-S*', 'Average_P:', '0.01732', '(95%-conf.int.', '0.01732', '-', '0.01732)']
['system', 'ROUGE-S*', 'Average_F:', '0.00531', '(95%-conf.int.', '0.00531', '-', '0.00531)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:231', 'F:4']
['In the cotraining case, (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples, and (2) must agree with each other on the unlabeled examples.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.01299', '(95%-conf.int.', '0.01299', '-', '0.01299)']
['system', 'ROUGE-S*', 'Average_P:', '0.01299', '(95%-conf.int.', '0.01299', '-', '0.01299)']
['system', 'ROUGE-S*', 'Average_F:', '0.01299', '(95%-conf.int.', '0.01299', '-', '0.01299)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:231', 'F:3']
['Thus at each iteration the algorithm is forced to pick features for the location, person and organization in turn for the classifier being trained.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00433', '(95%-conf.int.', '0.00433', '-', '0.00433)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00699', '(95%-conf.int.', '0.00699', '-', '0.00699)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:55', 'F:1']
['This modification brings the method closer to the DL-CoTrain algorithm described earlier, and is motivated by the intuition that all three labels should be kept healthily populated in the unlabeled examples, preventing one label from dominating &#8212; this deserves more theoretical investigation.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.02597', '(95%-conf.int.', '0.02597', '-', '0.02597)']
['system', 'ROUGE-S*', 'Average_P:', '0.02597', '(95%-conf.int.', '0.02597', '-', '0.02597)']
['system', 'ROUGE-S*', 'Average_F:', '0.02597', '(95%-conf.int.', '0.02597', '-', '0.02597)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:231', 'F:6']
['The CoBoost algorithm just described is for the case where there are two labels: for the named entity task there are three labels, and in general it will be useful to generalize the CoBoost algorithm to the multiclass case.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.06926', '(95%-conf.int.', '0.06926', '-', '0.06926)']
['system', 'ROUGE-S*', 'Average_P:', '0.17582', '(95%-conf.int.', '0.17582', '-', '0.17582)']
['system', 'ROUGE-S*', 'Average_F:', '0.09938', '(95%-conf.int.', '0.09938', '-', '0.09938)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:91', 'F:16']
['The following features were used: full-string=x The full string (e.g., for Maury Cooper, full- s tring=Maury_Cooper). contains(x) If the spelling contains more than one word, this feature applies for any words that the string contains (e.g., Maury Cooper contributes two such features, contains (Maury) and contains (Cooper) . allcapl This feature appears if the spelling is a single word which is all capitals (e.g., IBM would contribute this feature). allcap2 This feature appears if the spelling is a single word which is all capitals or full periods, and contains at least one period.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:903', 'F:0']
['The method uses a &quot;soft&quot; measure of the agreement between two classifiers as an objective function; we described an algorithm which directly optimizes this function.']
['The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00265', '(95%-conf.int.', '0.00265', '-', '0.00265)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00439', '(95%-conf.int.', '0.00439', '-', '0.00439)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:78', 'F:1']
['(If fewer than n rules have Precision greater than pin, we 3Note that taking tlie top n most frequent rules already makes the method robut to low count events, hence we do not use smoothing, allowing low-count high-precision features to be chosen on later iterations. keep only those rules which exceed the precision threshold.) pm,n was fixed at 0.95 in all experiments in this paper.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:630', 'F:0']
['Unlabeled examples in the named-entity classification problem can reduce the need for supervision to a handful of seed rules.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.01299', '(95%-conf.int.', '0.01299', '-', '0.01299)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.02098', '(95%-conf.int.', '0.02098', '-', '0.02098)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:55', 'F:3']
0.0330469996695 0.0134179998658 0.0180669998193





input/ref/Task1/P87-1015_vardha.csv
input/res/Task1/P87-1015.csv
parsing: input/ref/Task1/P87-1015_vardha.csv
<S sid="165" ssid="50">This class of formalisms have the properties that their derivation trees are local sets, and manipulate objects, using a finite number of composition operations that use a finite number of symbols.</S>
original cit marker offset is 0
new cit marker offset is 0



["'165'"]
'165'
['165']
parsed_discourse_facet ['method_citation']
 <S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'"]
'119'
['119']
parsed_discourse_facet ['method_citation']
    <S sid="3" ssid="1">Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.</S>
original cit marker offset is 0
new cit marker offset is 0



["'3'"]
'3'
['3']
parsed_discourse_facet ['method_citation']
    <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
    <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
    <S sid="133" ssid="18">To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'"]
'133'
['133']
parsed_discourse_facet ['method_citation']
 <S sid="138" ssid="23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
    <S sid="156" ssid="41">Giving a recognition algorithm for LCFRL's involves describing the substrings of the input that are spanned by the structures derived by the LCFRS's and how the composition operation combines these substrings.</S>
original cit marker offset is 0
new cit marker offset is 0



["'156'"]
'156'
['156']
parsed_discourse_facet ['method_citation']
 <S sid="118" ssid="3">In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows.</S>
original cit marker offset is 0
new cit marker offset is 0



["'118'"]
'118'
['118']
parsed_discourse_facet ['method_citation']
 <S sid="119" ssid="4">In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'119'"]
'119'
['119']
parsed_discourse_facet ['method_citation']
  <S sid="133" ssid="18">To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'133'"]
'133'
['133']
parsed_discourse_facet ['method_citation']
 <S sid="164" ssid="49">Although embedding this version of LCFRS's in the framework of ILFP developed by Rounds (1985) is straightforward, our motivation was to capture properties shared by a family of grammatical systems and generalize them defining a class of related formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'164'"]
'164'
['164']
parsed_discourse_facet ['method_citation']
 <S sid="204" ssid="10">The similarities become apparent when they are studied at the level of derivation structures: derivation nee sets of CFG's, HG's, TAG's, and MCTAG's are all local sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'204'"]
'204'
['204']
parsed_discourse_facet ['method_citation']
    <S sid="9" ssid="7">We examine both the complexity of the paths of trees in the tree sets, and the kinds of dependencies that the formalisms can impose between paths.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="28" ssid="13">A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'28'"]
'28'
['28']
parsed_discourse_facet ['method_citation']
 <S sid="138" ssid="23">We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments.</S>
original cit marker offset is 0
new cit marker offset is 0



["'138'"]
'138'
['138']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P87-1015.csv
<S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'92'", "'207'", "'125'"]
'92'
'207'
'125'
['92', '207', '125']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'125'", "'207'"]
'75'
'125'
'207'
['75', '125', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'75'", "'207'"]
'125'
'75'
'207'
['125', '75', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'125'", "'207'"]
'2'
'125'
'207'
['2', '125', '207']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'92'"]
'207'
'125'
'92'
['207', '125', '92']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'207'", "'75'"]
'2'
'207'
'75'
['2', '207', '75']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'92'"]
'207'
'125'
'92'
['207', '125', '92']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="151">We now turn our attention to the recognition of string languages generated by these formalisms (LCFRL's).</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'", "'207'", "'125'"]
'151'
'207'
'125'
['151', '207', '125']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'75'", "'207'"]
'2'
'75'
'207'
['2', '75', '207']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'75'", "'207'"]
'125'
'75'
'207'
['125', '75', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'", "'125'", "'207'"]
'75'
'125'
'207'
['75', '125', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'75'"]
'207'
'125'
'75'
['207', '125', '75']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S><S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'125'", "'75'", "'207'"]
'125'
'75'
'207'
['125', '75', '207']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="2">In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.</S><S ssid="1" sid="113">A formalism generating tree sets with complex path sets can still generate only semilinear languages if its tree sets have independent paths, and semilinear path sets.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'113'", "'75'"]
'2'
'113'
'75'
['2', '113', '75']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="92">We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'92'"]
'207'
'125'
'92'
['207', '125', '92']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="207">We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.</S><S ssid="1" sid="125">Each derivation of a grammar can be represented by a generalized context-free derivation tree.</S><S ssid="1" sid="75">A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'207'", "'125'", "'75'"]
'207'
'125'
'75'
['207', '125', '75']
parsed_discourse_facet ['result_citation']
["We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments."]
['We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:406', 'P:66', 'F:0']
['This class of formalisms have the properties that their derivation trees are local sets, and manipulate objects, using a finite number of composition operations that use a finite number of symbols.']
['We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.01478', '(95%-conf.int.', '0.01478', '-', '0.01478)']
['system', 'ROUGE-S*', 'Average_P:', '0.05000', '(95%-conf.int.', '0.05000', '-', '0.05000)']
['system', 'ROUGE-S*', 'Average_F:', '0.02281', '(95%-conf.int.', '0.02281', '-', '0.02281)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:406', 'P:120', 'F:6']
["In defining LCFRS's, we hope to generalize the definition of CFG's to formalisms manipulating any structure, e.g. strings, trees, or graphs."]
['A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.00806', '(95%-conf.int.', '0.00806', '-', '0.00806)']
['system', 'ROUGE-S*', 'Average_P:', '0.06061', '(95%-conf.int.', '0.06061', '-', '0.06061)']
['system', 'ROUGE-S*', 'Average_F:', '0.01423', '(95%-conf.int.', '0.01423', '-', '0.01423)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:66', 'F:4']
["We can show that languages generated by LCFRS's are semilinear as long as the composition operation does not remove any terminal symbols from its arguments."]
['A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:66', 'F:0']
['A TAG consists of a finite set of elementary trees that are either initial trees or auxiliary trees.']
['We loosely describe the class of all such systems as Linear Context-Free Rewriting Formalisms.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:406', 'P:45', 'F:0']
['We examine both the complexity of the paths of trees in the tree sets, and the kinds of dependencies that the formalisms can impose between paths.']
['A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', 'In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.', 'A formalism generating tree sets with complex path sets can still generate only semilinear languages if its tree sets have independent paths, and semilinear path sets.']
['system', 'ROUGE-S*', 'Average_R:', '0.00913', '(95%-conf.int.', '0.00913', '-', '0.00913)']
['system', 'ROUGE-S*', 'Average_P:', '0.43636', '(95%-conf.int.', '0.43636', '-', '0.43636)']
['system', 'ROUGE-S*', 'Average_F:', '0.01789', '(95%-conf.int.', '0.01789', '-', '0.01789)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2628', 'P:55', 'F:24']
['To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.']
['A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', 'In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.01141', '(95%-conf.int.', '0.01141', '-', '0.01141)']
['system', 'ROUGE-S*', 'Average_P:', '0.19118', '(95%-conf.int.', '0.19118', '-', '0.19118)']
['system', 'ROUGE-S*', 'Average_F:', '0.02154', '(95%-conf.int.', '0.02154', '-', '0.02154)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:136', 'F:26']
['To show that the derivation trees of any grammar in LCFRS is a local set, we can rewrite the annotated derivation trees such that every node is labelled by a pair to include the composition operations.']
['A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.03427', '(95%-conf.int.', '0.03427', '-', '0.03427)']
['system', 'ROUGE-S*', 'Average_P:', '0.12500', '(95%-conf.int.', '0.12500', '-', '0.12500)']
['system', 'ROUGE-S*', 'Average_F:', '0.05380', '(95%-conf.int.', '0.05380', '-', '0.05380)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:136', 'F:17']
['Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical formalism.']
['A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.00806', '(95%-conf.int.', '0.00806', '-', '0.00806)']
['system', 'ROUGE-S*', 'Average_P:', '0.07273', '(95%-conf.int.', '0.07273', '-', '0.07273)']
['system', 'ROUGE-S*', 'Average_F:', '0.01452', '(95%-conf.int.', '0.01452', '-', '0.01452)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:496', 'P:55', 'F:4']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
['In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.', 'Each derivation of a grammar can be represented by a generalized context-free derivation tree.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.01984', '(95%-conf.int.', '0.01984', '-', '0.01984)']
['system', 'ROUGE-S*', 'Average_P:', '0.33333', '(95%-conf.int.', '0.33333', '-', '0.33333)']
['system', 'ROUGE-S*', 'Average_F:', '0.03745', '(95%-conf.int.', '0.03745', '-', '0.03745)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2016', 'P:120', 'F:40']
["In the remainder of the paper, we outline how a class of Linear Context-Free Rewriting Systems (LCFRS's) may be defined and sketch how semilinearity and polynomial recognition of these systems follows."]
['A multicomponent Tree Adjoining Grammar (MCTAG) consists of a finite set of finite elementary tree sets.', 'In considering the relationship between formalisms, we show that it is useful to abstract away from the details of the formalism, and examine the nature of their derivation process as reflected by properties their trees. find that several of the formalisms considered can be seen as being closely related since they have derivation tree sets with the same structure as those produced by Context-Free Grammars On the basis of this observation, we describe a class of formalisms which we call Linear Context- Free Rewriting Systems, and show they are recognizable in polynomial time and generate only semilinear languages.', 'We outlined the definition of a family of constrained grammatical formalisms, called Linear Context-Free Rewriting Systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.01756', '(95%-conf.int.', '0.01756', '-', '0.01756)']
['system', 'ROUGE-S*', 'Average_P:', '0.33333', '(95%-conf.int.', '0.33333', '-', '0.33333)']
['system', 'ROUGE-S*', 'Average_F:', '0.03336', '(95%-conf.int.', '0.03336', '-', '0.03336)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2278', 'P:120', 'F:40']
0.145685453221 0.0111918180801 0.0195999998218





input/ref/Task1/W99-0623_vardha.csv
input/res/Task1/W99-0623.csv
parsing: input/ref/Task1/W99-0623_vardha.csv
    <S sid="85" ssid="14">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="117" ssid="46">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'117'"]
'117'
['117']
parsed_discourse_facet ['method_citation']
<S sid="72" ssid="1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
    <S sid="120" ssid="49">The precision and recall of similarity switching and constituent voting are both significantly better than the best individual parser, and constituent voting is significantly better than parser switching in precision.4 Constituent voting gives the highest accuracy for parsing the Penn Treebank reported to date.</S>
original cit marker offset is 0
new cit marker offset is 0



["'120'"]
'120'
['120']
parsed_discourse_facet ['method_citation']
  <S sid="139" ssid="1">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'139'"]
'139'
['139']
parsed_discourse_facet ['method_citation']
 <S sid="84" ssid="13">The first shows how constituent features and context do not help in deciding which parser to trust.</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'"]
'84'
['84']
parsed_discourse_facet ['method_citation']
    <S sid="76" ssid="5">The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.</S>
original cit marker offset is 0
new cit marker offset is 0



["'76'"]
'76'
['76']
parsed_discourse_facet ['method_citation']
 <S sid="38" ssid="24">Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'"]
'38'
['38']
parsed_discourse_facet ['method_citation']
  <S sid="25" ssid="11">In our particular case the majority requires the agreement of only two parsers because we have only three.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'"]
'25'
['25']
parsed_discourse_facet ['method_citation']
 <S sid="72" ssid="1">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
    <S sid="87" ssid="16">It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'"]
'87'
['87']
parsed_discourse_facet ['method_citation']
    <S sid="51" ssid="37">One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
    <S sid="79" ssid="8">Precision is the portion of hypothesized constituents that are correct and recall is the portion of the Treebank constituents that are hypothesized.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
  <S sid="27" ssid="13">Another technique for parse hybridization is to use a na&#239;ve Bayes classifier to determine which constituents to include in the parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'27'"]
'27'
['27']
parsed_discourse_facet ['method_citation']
 <S sid="77" ssid="6">Each parse is converted into a set of constituents represented as a tuples: (label, start, end).</S>
original cit marker offset is 0
new cit marker offset is 0



["'77'"]
'77'
['77']
parsed_discourse_facet ['method_citation']
  <S sid="11" ssid="7">Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
  <S sid="116" ssid="45">The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.</S>
original cit marker offset is 0
new cit marker offset is 0



["'116'"]
'116'
['116']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0623.csv
<S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'16'", "'23'"]
'58'
'16'
'23'
['58', '16', '23']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="21">One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'21'"]
'23'
'1'
'21'
['23', '1', '21']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'58'", "'16'"]
'23'
'58'
'16'
['23', '58', '16']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'58'", "'16'"]
'23'
'58'
'16'
['23', '58', '16']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="58">We call this approach parser switching.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="28">The development of a naïve Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'58'", "'23'", "'28'"]
'58'
'23'
'28'
['58', '23', '28']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="58">We call this approach parser switching.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'58'"]
'23'
'16'
'58'
['23', '16', '58']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'1'"]
'23'
'16'
'1'
['23', '16', '1']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="58">We call this approach parser switching.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'58'"]
'23'
'16'
'58'
['23', '16', '58']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="14">We used these three parsers to explore parser combination techniques.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'23'", "'14'"]
'1'
'23'
'14'
['1', '23', '14']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="58">We call this approach parser switching.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'16'", "'58'"]
'23'
'16'
'58'
['23', '16', '58']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'23'", "'1'"]
'16'
'23'
'1'
['16', '23', '1']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="28">The development of a naïve Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'", "'23'", "'28'"]
'16'
'23'
'28'
['16', '23', '28']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S><S ssid="1" sid="23">We call this technique constituent voting.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'16'", "'23'"]
'1'
'16'
'23'
['1', '16', '23']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="23">We call this technique constituent voting.</S><S ssid="1" sid="1">Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.</S><S ssid="1" sid="16">We call this approach parse hybridization.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'1'", "'16'"]
'23'
'1'
'16'
['23', '1', '16']
parsed_discourse_facet ['result_citation']
['The maximum precision row is the upper bound on accuracy if we could pick exactly the correct constituents from among the constituents suggested by the three parsers.']
['Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.', 'We call this approach parse hybridization.', 'We call this technique constituent voting.']
['system', 'ROUGE-S*', 'Average_R:', '0.01429', '(95%-conf.int.', '0.01429', '-', '0.01429)']
['system', 'ROUGE-S*', 'Average_P:', '0.04545', '(95%-conf.int.', '0.04545', '-', '0.04545)']
['system', 'ROUGE-S*', 'Average_F:', '0.02174', '(95%-conf.int.', '0.02174', '-', '0.02174)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:66', 'F:3']
['Each parse is converted into a set of constituents represented as a tuples: (label, start, end).']
['We call this technique constituent voting.', u'The development of a na\xefve Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.', 'We call this approach parse hybridization.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:171', 'P:36', 'F:0']
['One can trivially create situations in which strictly binary-branching trees are combined to create a tree with only the root node and the terminal nodes, a completely flat structure.']
['Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.', 'We call this approach parse hybridization.', 'We call this technique constituent voting.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:136', 'F:0']
['Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.']
['Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.', "One hybridization strategy is to let the parsers vote on constituents' membership in the hypothesized set.", 'We call this technique constituent voting.']
['system', 'ROUGE-S*', 'Average_R:', '0.00333', '(95%-conf.int.', '0.00333', '-', '0.00333)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00595', '(95%-conf.int.', '0.00595', '-', '0.00595)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:300', 'P:36', 'F:1']
['The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.']
['Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.', 'We call this approach parse hybridization.', 'We call this technique constituent voting.']
['system', 'ROUGE-S*', 'Average_R:', '0.01429', '(95%-conf.int.', '0.01429', '-', '0.01429)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.02264', '(95%-conf.int.', '0.02264', '-', '0.02264)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:55', 'F:3']
['Under certain conditions the constituent voting and na&#239;ve Bayes constituent combination techniques are guaranteed to produce sets of constituents with no crossing brackets.']
['Three state-of-the-art statistical parsers are combined to produce more accurate parses, as well as new bounds on achievable Treebank parsing accuracy.', 'We call this approach parse hybridization.', 'We call this technique constituent voting.']
['system', 'ROUGE-S*', 'Average_R:', '0.02857', '(95%-conf.int.', '0.02857', '-', '0.02857)']
['system', 'ROUGE-S*', 'Average_P:', '0.05000', '(95%-conf.int.', '0.05000', '-', '0.05000)']
['system', 'ROUGE-S*', 'Average_F:', '0.03636', '(95%-conf.int.', '0.03636', '-', '0.03636)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:120', 'F:6']
['The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.']
['We call this approach parse hybridization.', 'We call this approach parser switching.', 'We call this technique constituent voting.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:66', 'P:136', 'F:0']
['It is possible one could produce better models by introducing features describing constituents and their contexts because one parser could be much better than the majority of the others in particular situations.']
['We call this approach parse hybridization.', 'We call this approach parser switching.', 'We call this technique constituent voting.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:66', 'P:45', 'F:0']
0.0222224997222 0.0075599999055 0.0108362498645





input/ref/Task1/P08-1028_sweta.csv
input/res/Task1/P08-1028.csv
parsing: input/ref/Task1/P08-1028_sweta.csv
<S sid="65" ssid="13">So, if we assume that only the ith components of u and v contribute to the ith component of p, that these components are not dependent on i, and that the function is symmetric with regard to the interchange of u and v, we obtain a simpler instantiation of an additive model: Analogously, under the same assumptions, we obtain the following simpler multiplicative model: only the ith components of u and v contribute to the ith component of p. Another class of models can be derived by relaxing this constraint.</S>
original cit marker offset is 0
new cit marker offset is 0



["65'"]
65'
['65']
parsed_discourse_facet ['method_citation']
 <S sid="75" ssid="23">An extreme form of this differential in the contribution of constituents is where one of the vectors, say u, contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic, however it can serve as a simple baseline against which to compare more sophisticated models.</S>
original cit marker offset is 0
new cit marker offset is 0



["75'"]
75'
['75']
parsed_discourse_facet ['method_citation']
<S sid="42" ssid="15">This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.</S>
original cit marker offset is 0
new cit marker offset is 0



["42'"]
42'
['42']
parsed_discourse_facet ['method_citation']
 <S sid="21" ssid="17">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S>
original cit marker offset is 0
new cit marker offset is 0



["21'"]
21'
['21']
parsed_discourse_facet ['method_citation']
    <S sid="195" ssid="7">The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately.</S>
original cit marker offset is 0
new cit marker offset is 0



["195'"]
195'
['195']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["57'"]
57'
['57']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["190'"]
190'
['190']
parsed_discourse_facet ['method_citation']
 <S sid="29" ssid="2">While neural networks can readily represent single distinct objects, in the case of multiple objects there are fundamental difficulties in keeping track of which features are bound to which objects.</S>
original cit marker offset is 0
new cit marker offset is 0



["29'"]
29'
['29']
parsed_discourse_facet ['method_citation']
    <S sid="38" ssid="11">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["51'"]
51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="12">Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>
original cit marker offset is 0
new cit marker offset is 0



["64'"]
64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="163" ssid="76">We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).</S>
original cit marker offset is 0
new cit marker offset is 0



["163'"]
163'
['163']
parsed_discourse_facet ['method_citation']
<S sid="185" ssid="19">The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.</S>
original cit marker offset is 0
new cit marker offset is 0



["185'"]
185'
['185']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["190'"]
190'
['190']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1028.csv
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'24'"]
'0'
'51'
'24'
['0', '51', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'24'"]
'0'
'51'
'24'
['0', '51', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'52'"]
'0'
'51'
'52'
['0', '51', '52']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'142'", "'24'"]
'51'
'142'
'24'
['51', '142', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="25">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'25'"]
'0'
'51'
'25'
['0', '51', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'142'"]
'0'
'51'
'142'
['0', '51', '142']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'142'"]
'51'
'0'
'142'
['51', '0', '142']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'52'", "'24'"]
'51'
'52'
'24'
['51', '52', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'142'"]
'51'
'0'
'142'
['51', '0', '142']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'52'", "'51'"]
'0'
'52'
'51'
['0', '52', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="38">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'38'"]
'0'
'51'
'38'
['0', '51', '38']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="3">Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'3'"]
'0'
'51'
'3'
['0', '51', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'52'", "'24'"]
'0'
'52'
'24'
['0', '52', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'24'", "'51'"]
'0'
'24'
'51'
['0', '24', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'52'"]
'0'
'51'
'52'
['0', '51', '52']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'24'"]
'51'
'0'
'24'
['51', '0', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="25">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'25'"]
'51'
'0'
'25'
['51', '0', '25']
parsed_discourse_facet ['method_citation']
['An extreme form of this differential in the contribution of constituents is where one of the vectors, say u, contributes nothing at all to the combination: Admittedly the model in (8) is impoverished and rather simplistic, however it can serve as a simple baseline against which to compare more sophisticated models.']
['In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.', 'Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
['system', 'ROUGE-S*', 'Average_R:', '0.00920', '(95%-conf.int.', '0.00920', '-', '0.00920)']
['system', 'ROUGE-S*', 'Average_P:', '0.02339', '(95%-conf.int.', '0.02339', '-', '0.02339)']
['system', 'ROUGE-S*', 'Average_F:', '0.01320', '(95%-conf.int.', '0.01320', '-', '0.01320)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:171', 'F:4']
['The resulting vector is sparser but expresses more succinctly the meaning of the predicate-argument structure, and thus allows semantic similarity to be modelled more accurately.']
['In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.', 'We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
['system', 'ROUGE-S*', 'Average_R:', '0.00483', '(95%-conf.int.', '0.00483', '-', '0.00483)']
['system', 'ROUGE-S*', 'Average_P:', '0.06410', '(95%-conf.int.', '0.06410', '-', '0.06410)']
['system', 'ROUGE-S*', 'Average_F:', '0.00898', '(95%-conf.int.', '0.00898', '-', '0.00898)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:78', 'F:5']
['We expect better models to yield a pattern of similarity scores like those observed in the human ratings (see Figure 2).']
['In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.', 'Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
['system', 'ROUGE-S*', 'Average_R:', '0.00230', '(95%-conf.int.', '0.00230', '-', '0.00230)']
['system', 'ROUGE-S*', 'Average_P:', '0.01818', '(95%-conf.int.', '0.01818', '-', '0.01818)']
['system', 'ROUGE-S*', 'Average_F:', '0.00408', '(95%-conf.int.', '0.00408', '-', '0.00408)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:55', 'F:1']
['We present a general framework for vector-based composition which allows us to consider different classes of models.']
['Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).', 'We present a general framework for vector-based composition which allows us to consider different classes of models.']
['system', 'ROUGE-S*', 'Average_R:', '0.07407', '(95%-conf.int.', '0.07407', '-', '0.07407)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.13793', '(95%-conf.int.', '0.13793', '-', '0.13793)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:28', 'F:28']
['We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).', 'We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).']
['system', 'ROUGE-S*', 'Average_R:', '0.01463', '(95%-conf.int.', '0.01463', '-', '0.01463)']
['system', 'ROUGE-S*', 'Average_P:', '0.33333', '(95%-conf.int.', '0.33333', '-', '0.33333)']
['system', 'ROUGE-S*', 'Average_F:', '0.02804', '(95%-conf.int.', '0.02804', '-', '0.02804)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:36', 'F:12']
['We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).', 'We present a general framework for vector-based composition which allows us to consider different classes of models.']
['system', 'ROUGE-S*', 'Average_R:', '0.03704', '(95%-conf.int.', '0.03704', '-', '0.03704)']
['system', 'ROUGE-S*', 'Average_P:', '0.38889', '(95%-conf.int.', '0.38889', '-', '0.38889)']
['system', 'ROUGE-S*', 'Average_F:', '0.06763', '(95%-conf.int.', '0.06763', '-', '0.06763)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:36', 'F:14']
['The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.']
['Vector-based Models of Semantic Composition', 'The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
['system', 'ROUGE-S*', 'Average_R:', '0.11828', '(95%-conf.int.', '0.11828', '-', '0.11828)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.21154', '(95%-conf.int.', '0.21154', '-', '0.21154)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:55', 'F:55']
['This poses problems for modeling linguistic data which is typically represented by vectors with non-random structure.']
['In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.', 'Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
['system', 'ROUGE-S*', 'Average_R:', '0.00690', '(95%-conf.int.', '0.00690', '-', '0.00690)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01250', '(95%-conf.int.', '0.01250', '-', '0.01250)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:45', 'F:3']
['Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.']
['Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).', 'We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).']
['system', 'ROUGE-S*', 'Average_R:', '0.01220', '(95%-conf.int.', '0.01220', '-', '0.01220)']
['system', 'ROUGE-S*', 'Average_P:', '0.07353', '(95%-conf.int.', '0.07353', '-', '0.07353)']
['system', 'ROUGE-S*', 'Average_F:', '0.02092', '(95%-conf.int.', '0.02092', '-', '0.02092)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:136', 'F:10']
0.329787774113 0.031049999655 0.0560911104879





input/ref/Task1/P08-1028_aakansha.csv
input/res/Task1/P08-1028.csv
parsing: input/ref/Task1/P08-1028_aakansha.csv
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="185" ssid="19">The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.</S>
    <S sid="186" ssid="20">The combined model is best overall with &#961; = 0.19.</S>
    <S sid="187" ssid="21">However, the difference between the two models is not statistically significant.</S>
original cit marker offset is 0
new cit marker offset is 0



["'185'", "'186'"]
'185'
'186'
['185', '186']
parsed_discourse_facet ['result_citation']
<S sid="51" ssid="24">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'"]
'51'
['51']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="21">The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'"]
'48'
['48']
parsed_discourse_facet ['method_citation']
<S sid="53" ssid="1">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.</S><S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



["'53'", "'57'"]
'53'
'57'
['53', '57']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="16">Although the composition model in (5) is commonly used in the literature, from a linguistic perspective, the model in (6) is more appealing.</S>
    <S sid="69" ssid="17">Simply adding the vectors u and v lumps their contents together rather than allowing the content of one vector to pick out the relevant content of the other.</S>
    <S sid="70" ssid="18">Instead, it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v, and vice versa.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'", "'69'", "'70'"]
'68'
'69'
'70'
['68', '69', '70']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
    <S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



["'189'", "'190'"]
'189'
'190'
['189', '190']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
    <S sid="25" ssid="21">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'", "'25'"]
'24'
'25'
['24', '25']
parsed_discourse_facet ['method_citation']
<S sid="64" ssid="12">Now, if we assume that p lies in the same space as u and v, avoiding the issues of dimensionality associated with tensor products, and that f is a linear function, for simplicity, of the cartesian product of u and v, then we generate a class of additive models: where A and B are matrices which determine the contributions made by u and v to the product p. In contrast, if we assume that f is a linear function of the tensor product of u and v, then we obtain multiplicative models: where C is a tensor of rank 3, which projects the tensor product of u and v onto the space of p. Further constraints can be introduced to reduce the free parameters in these models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'64'"]
'64'
['64']
parsed_discourse_facet ['method_citation']
<S sid="176" ssid="10">The multiplicative and combined models yield means closer to the human ratings.</S>
    <S sid="177" ssid="11">The difference between High and Low similarity values estimated by these models are statistically significant (p &lt; 0.01 using the Wilcoxon rank sum test).</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'", "'177'"]
'176'
'177'
['176', '177']
parsed_discourse_facet ['method_citation']
<S sid="191" ssid="3">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S>
original cit marker offset is 0
new cit marker offset is 0



["'191'"]
'191'
['191']
parsed_discourse_facet ['method_citation']
<S sid="24" ssid="20">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P08-1028.csv
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'24'"]
'0'
'51'
'24'
['0', '51', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'24'"]
'0'
'51'
'24'
['0', '51', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'52'"]
'0'
'51'
'52'
['0', '51', '52']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'142'", "'24'"]
'51'
'142'
'24'
['51', '142', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="25">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'25'"]
'0'
'51'
'25'
['0', '51', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'142'"]
'0'
'51'
'142'
['0', '51', '142']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'142'"]
'51'
'0'
'142'
['51', '0', '142']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'52'", "'24'"]
'51'
'52'
'24'
['51', '52', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'142'"]
'51'
'0'
'142'
['51', '0', '142']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'52'", "'51'"]
'0'
'52'
'51'
['0', '52', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="38">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'38'"]
'0'
'51'
'38'
['0', '51', '38']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="3">Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'3'"]
'0'
'51'
'3'
['0', '51', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'52'", "'24'"]
'0'
'52'
'24'
['0', '52', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'24'", "'51'"]
'0'
'24'
'51'
['0', '24', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'52'"]
'0'
'51'
'52'
['0', '51', '52']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'24'"]
'51'
'0'
'24'
['51', '0', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="25">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'25'"]
'51'
'0'
'25'
['51', '0', '25']
parsed_discourse_facet ['method_citation']
['Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
['In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.', 'Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
['system', 'ROUGE-S*', 'Average_R:', '0.24138', '(95%-conf.int.', '0.24138', '-', '0.24138)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.38889', '(95%-conf.int.', '0.38889', '-', '0.38889)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:105', 'F:105']
['The combined model is best overall with &#961; = 0.19.', 'However, the difference between the two models is not statistically significant.', 'The multiplicative model yields a better fit with the experimental data, &#961; = 0.17.']
['In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.', 'We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
['system', 'ROUGE-S*', 'Average_R:', '0.00386', '(95%-conf.int.', '0.00386', '-', '0.00386)']
['system', 'ROUGE-S*', 'Average_P:', '0.02614', '(95%-conf.int.', '0.02614', '-', '0.02614)']
['system', 'ROUGE-S*', 'Average_F:', '0.00673', '(95%-conf.int.', '0.00673', '-', '0.00673)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:153', 'F:4']
['Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
['Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).', 'We present a general framework for vector-based composition which allows us to consider different classes of models.']
['system', 'ROUGE-S*', 'Average_R:', '0.27778', '(95%-conf.int.', '0.27778', '-', '0.27778)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.43478', '(95%-conf.int.', '0.43478', '-', '0.43478)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:105', 'F:105']
['Although the composition model in (5) is commonly used in the literature, from a linguistic perspective, the model in (6) is more appealing.', 'Simply adding the vectors u and v lumps their contents together rather than allowing the content of one vector to pick out the relevant content of the other.', 'Instead, it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v, and vice versa.']
['Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).', 'We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).']
['system', 'ROUGE-S*', 'Average_R:', '0.01707', '(95%-conf.int.', '0.01707', '-', '0.01707)']
['system', 'ROUGE-S*', 'Average_P:', '0.03448', '(95%-conf.int.', '0.03448', '-', '0.03448)']
['system', 'ROUGE-S*', 'Average_F:', '0.02284', '(95%-conf.int.', '0.02284', '-', '0.02284)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:406', 'F:14']
['In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.']
['Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).', 'We present a general framework for vector-based composition which allows us to consider different classes of models.']
['system', 'ROUGE-S*', 'Average_R:', '0.00794', '(95%-conf.int.', '0.00794', '-', '0.00794)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.01418', '(95%-conf.int.', '0.01418', '-', '0.01418)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:45', 'F:3']
['In this paper we presented a general framework for vector-based semantic composition.', 'We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['Vector-based Models of Semantic Composition', 'The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
['system', 'ROUGE-S*', 'Average_R:', '0.10538', '(95%-conf.int.', '0.10538', '-', '0.10538)']
['system', 'ROUGE-S*', 'Average_P:', '0.36029', '(95%-conf.int.', '0.36029', '-', '0.36029)']
['system', 'ROUGE-S*', 'Average_F:', '0.16306', '(95%-conf.int.', '0.16306', '-', '0.16306)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:136', 'F:49']
['The difference between High and Low similarity values estimated by these models are statistically significant (p &lt; 0.01 using the Wilcoxon rank sum test).', 'The multiplicative and combined models yield means closer to the human ratings.']
['In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.', 'Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
['system', 'ROUGE-S*', 'Average_R:', '0.01839', '(95%-conf.int.', '0.01839', '-', '0.01839)']
['system', 'ROUGE-S*', 'Average_P:', '0.02899', '(95%-conf.int.', '0.02899', '-', '0.02899)']
['system', 'ROUGE-S*', 'Average_F:', '0.02250', '(95%-conf.int.', '0.02250', '-', '0.02250)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:276', 'F:8']
['The idea is to add not only the vectors representing the predicate and its argument but also the neighbors associated with both of them.']
['Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).', 'We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:21', 'F:0']
0.314571246068 0.0839749989503 0.131622498355





input/ref/Task1/A00-2030_aakansha.csv
input/res/Task1/A00-2030.csv
parsing: input/ref/Task1/A00-2030_aakansha.csv
<S sid="6" ssid="4">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="49" ssid="9">By necessity, we adopted the strategy of hand marking only the semantics.</S>
    <S sid="50" ssid="10">Figure 4 shows an example of the semantic annotation, which was the only type of manual annotation we performed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'49'", "'50'"]
'49'
'50'
['49', '50']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="8">Instead, our parsing algorithm, trained on the UPenn TREEBANK, was run on the New York Times source to create unsupervised syntactic training which was constrained to be consistent with semantic annotation.</S
original cit marker offset is 0
new cit marker offset is 0



["'10'"]
'10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'"]
'16'
['16']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'"]
'23'
'24'
['23', '24']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="6">An integrated model can limit the propagation of errors by making all decisions jointly.</S>
    <S sid="24" ssid="7">For this reason, we focused on designing an integrated model in which tagging, namefinding, parsing, and semantic interpretation decisions all have the opportunity to mutually influence each other.</S><S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'23'", "'24'", "'33'", "'34'"]
'23'
'24'
'33'
'34'
['23', '24', '33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="60" ssid="1">In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).</S>
    <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'61'"]
'60'
'61'
['60', '61']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'"]
'33'
['33']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="33" ssid="1">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
    <S sid="34" ssid="2">In these trees, the standard TREEBANK structures are augmented to convey semantic information, that is, entities and relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'34'"]
'33'
'34'
['33', '34']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="1">We evaluated the new approach to information extraction on two of the tasks of the Seventh Message Understanding Conference (MUC-7) and reported in (Marsh, 1998).</S>
    <S sid="12" ssid="2">The Template Element (TE) task identifies organizations, persons, locations, and some artifacts (rocket and airplane-related artifacts).</S><S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'", "'12'", "'16'"]
'11'
'12'
'16'
['11', '12', '16']
parsed_discourse_facet ['method_citation']
<S sid="105" ssid="2">A single model proved capable of performing all necessary sentential processing, both syntactic and semantic.</S>
original cit marker offset is 0
new cit marker offset is 0



["'105'"]
'105'
['105']
parsed_discourse_facet ['result_citation']
<S sid="60" ssid="1">In our statistical model, trees are generated according to a process similar to that described in (Collins 1996, 1997).</S>
    <S sid="61" ssid="2">The detailed probability structure differs, however, in that it was designed to jointly perform part-of-speech tagging, name finding, syntactic parsing, and relation finding in a single process.</S>
original cit marker offset is 0
new cit marker offset is 0



["'60'", "'61'"]
'60'
'61'
['60', '61']
parsed_discourse_facet ['method_citation']
<S sid="16" ssid="6">For the following example, the The Template Relations (TR) task involves identifying instances of three relations in the text: TR builds on TE in that TR reports binary relations between elements of TE.</S>
original cit marker offset is 0
new cit marker offset is 0



["'16'"]
'16'
['16']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2030.csv
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'104'"]
'2'
'33'
'104'
['2', '33', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'6'"]
'2'
'33'
'6'
['2', '33', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="19">Currently, the prevailing architecture for dividing sentential processing is a four-stage pipeline consisting of: Since we were interested in exploiting recent advances in parsing, replacing the syntactic analysis stage of the standard pipeline with a modern statistical parser was an obvious possibility.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'19'"]
'2'
'33'
'19'
['2', '33', '19']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'6'", "'2'"]
'33'
'6'
'2'
['33', '6', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'2'", "'33'"]
'6'
'2'
'33'
['6', '2', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'6'"]
'2'
'33'
'6'
['2', '33', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="92">2.</S><S ssid="1" sid="69">8.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'92'", "'69'"]
'2'
'92'
'69'
['2', '92', '69']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'33'", "'2'", "'6'"]
'33'
'2'
'6'
['33', '2', '6']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'104'"]
'2'
'6'
'104'
['2', '6', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'6'", "'2'", "'104'"]
'6'
'2'
'104'
['6', '2', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'33'"]
'2'
'6'
'33'
['2', '6', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'104'"]
'2'
'6'
'104'
['2', '6', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'33'", "'104'"]
'2'
'33'
'104'
['2', '33', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="104">We have demonstrated, at least for one problem, that a lexicalized, probabilistic context-free parser with head rules (LPCFGHR) can be used effectively for information extraction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'104'"]
'2'
'6'
'104'
['2', '6', '104']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="33">Our integrated model represents syntax and semantics jointly using augmented parse trees.</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'33'"]
'2'
'6'
'33'
['2', '6', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="2">In this paper we report adapting a lexic al ized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.</S><S ssid="1" sid="6">In this paper, we report adapting a lexicalized, probabilistic context-free parser with head rules (LPCFG-HR) to information extraction.</S><S ssid="1" sid="0">A Novel Use of Statistical Parsing to Extract Information from Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'2'", "'6'", "'0'"]
'2'
'6'
'0'
['2', '6', '0']
parsed_discourse_facet ['method_citation']
Length 0 input/ref/Task1/A00-2030_aakansha.csv
0.0 0.0 0.0





input/ref/Task1/W06-3114_aakansha.csv
input/res/Task1/W06-3114.csv
parsing: input/ref/Task1/W06-3114_aakansha.csv
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



["'170'"]
'170'
['170']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="2">Training and testing is based on the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="9" ssid="2">Training and testing is based on the Europarl corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="36" ssid="2">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="102" ssid="18">Confidence Interval: To estimate confidence intervals for the average mean scores for the systems, we use standard significance testing.</S>
original cit marker offset is 0
new cit marker offset is 0



["'102'"]
'102'
['102']
parsed_discourse_facet ['method_citation']
<S sid="84" ssid="23">The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:</S>
original cit marker offset is 0
new cit marker offset is 0



["'84'"]
'84'
['84']
parsed_discourse_facet ['method_citation']
<S sid="11" ssid="4">To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources.</S>
original cit marker offset is 0
new cit marker offset is 0



["'11'"]
'11'
['11']
parsed_discourse_facet ['method_citation']
<S sid="140" ssid="33">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>
original cit marker offset is 0
new cit marker offset is 0



["'140'"]
'140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="19">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="8">Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="6">Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="3">&#8226; We evaluated translation from English, in addition to into English.</S>
    <S sid="6" ssid="4">English was again paired with German, French, and Spanish.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'", "'6'"]
'5'
'6'
['5', '6']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S><S ssid="1" sid="114">Pairwise comparison is done using the sign test.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'63'", "'114'"]
'1'
'63'
'114'
['1', '63', '114']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="26">Most of these groups follow a phrase-based statistical approach to machine translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'26'"]
'0'
'1'
'26'
['0', '1', '26']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="9">Training and testing is based on the Europarl corpus.</S><S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'0'", "'1'"]
'9'
'0'
'1'
['9', '0', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'0'"]
'1'
'103'
'0'
['1', '103', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="9">Training and testing is based on the Europarl corpus.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'1'", "'0'"]
'9'
'1'
'0'
['9', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="113">The confidence intervals are computed by bootstrap resampling for BLEU, and by standard significance testing for the manual scores, as described earlier in the paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["'103'", "'1'", "'113'"]
'103'
'1'
'113'
['103', '1', '113']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'103'", "'1'"]
'63'
'103'
'1'
['63', '103', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'103'", "'1'"]
'63'
'103'
'1'
['63', '103', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'2'", "'63'"]
'1'
'2'
'63'
['1', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="114">Pairwise comparison is done using the sign test.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'1'", "'103'"]
'114'
'1'
'103'
['114', '1', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'2'", "'103'"]
'1'
'2'
'103'
['1', '2', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="26">Most of these groups follow a phrase-based statistical approach to machine translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'26'"]
'1'
'103'
'26'
['1', '103', '26']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'63'"]
'1'
'103'
'63'
['1', '103', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="9">Training and testing is based on the Europarl corpus.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'1'", "'103'"]
'9'
'1'
'103'
['9', '1', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'2'"]
'1'
'103'
'2'
['1', '103', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'103'"]
'0'
'1'
'103'
['0', '1', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'2'", "'63'"]
'1'
'2'
'63'
['1', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="170">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'170'", "'1'"]
'0'
'170'
'1'
['0', '170', '1']
parsed_discourse_facet ['method_citation']
['&#8226; We evaluated translation from English, in addition to into English.', 'English was again paired with German, French, and Spanish.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'Manual and Automatic Evaluation of Machine Translation between European Languages', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00004', '(95%-conf.int.', '0.00004', '-', '0.00004)']
['system', 'ROUGE-S*', 'Average_P:', '0.60000', '(95%-conf.int.', '0.60000', '-', '0.60000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00007', '(95%-conf.int.', '0.00007', '-', '0.00007)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:940506', 'P:55', 'F:33']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'Pairwise comparison is done using the sign test.', 'Many human evaluation metrics have been proposed.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.07273', '(95%-conf.int.', '0.07273', '-', '0.07273)']
['system', 'ROUGE-S*', 'Average_F:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:926841', 'P:55', 'F:4']
['We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', u'0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 \u2022systran \u2022 ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency \u2022systran \u2022nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p \ufffd 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain \u2022upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv 23 24 25 26 27 28 29 30 31 32 \u2022upc-mr \u2022utd \u2022upc-jmc \u2022uedin-birch \u2022ntt \u2022rali \u2022uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy \u2022upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency \u2022ntt \u2022nrc \u2022rali \u2022uedin-birch -0.2 -0.3 -0.5 \u2022upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr \u2022 rali Fluency -0.4 \u2022upc-mr utd \u2022upc-jmc -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 \u2022upv 0.5 0.4 \u2022systran \u2022upc-mr \u2022 \u2022rali 0.3 \u2022ntt 0.2 0.1 -0.0 -0.1 \u2022systran \u2022upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022upv \u2022systran \u2022upc-mr \u2022 Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022systran \u2022ntt', 'Many human evaluation metrics have been proposed.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1599366', 'P:78', 'F:1']
['Training and testing is based on the Europarl corpus.']
['Training and testing is based on the Europarl corpus.', 'Manual and Automatic Evaluation of Machine Translation between European Languages', u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .']
['system', 'ROUGE-S*', 'Average_R:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00002', '(95%-conf.int.', '0.00002', '-', '0.00002)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:932295', 'P:10', 'F:10']
['The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', u'Given a set of n sentences, we can compute the sample mean x\ufffd and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x\u2212d, x+df can be computed by d = 1.96 \xb7\ufffdn (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.', 'Manual and Automatic Evaluation of Machine Translation between European Languages']
['system', 'ROUGE-S*', 'Average_R:', '0.00002', '(95%-conf.int.', '0.00002', '-', '0.00002)']
['system', 'ROUGE-S*', 'Average_P:', '0.20513', '(95%-conf.int.', '0.20513', '-', '0.20513)']
['system', 'ROUGE-S*', 'Average_F:', '0.00003', '(95%-conf.int.', '0.00003', '-', '0.00003)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:969528', 'P:78', 'F:16']
['We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', u'Given a set of n sentences, we can compute the sample mean x\ufffd and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x\u2212d, x+df can be computed by d = 1.96 \xb7\ufffdn (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.', 'Many human evaluation metrics have been proposed.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.01282', '(95%-conf.int.', '0.01282', '-', '0.01282)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:965355', 'P:78', 'F:1']
['Training and testing is based on the Europarl corpus.']
['Training and testing is based on the Europarl corpus.', u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'Manual and Automatic Evaluation of Machine Translation between European Languages']
['system', 'ROUGE-S*', 'Average_R:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Average_P:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00002', '(95%-conf.int.', '0.00002', '-', '0.00002)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:932295', 'P:10', 'F:10']
['To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', u'Given a set of n sentences, we can compute the sample mean x\ufffd and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x\u2212d, x+df can be computed by d = 1.96 \xb7\ufffdn (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.', 'Most of these groups follow a phrase-based statistical approach to machine translation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:970921', 'P:55', 'F:0']
['Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', u'Given a set of n sentences, we can compute the sample mean x\ufffd and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x\u2212d, x+df can be computed by d = 1.96 \xb7\ufffdn (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.', u'0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 \u2022systran \u2022 ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency \u2022systran \u2022nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p \ufffd 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain \u2022upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv 23 24 25 26 27 28 29 30 31 32 \u2022upc-mr \u2022utd \u2022upc-jmc \u2022uedin-birch \u2022ntt \u2022rali \u2022uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy \u2022upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency \u2022ntt \u2022nrc \u2022rali \u2022uedin-birch -0.2 -0.3 -0.5 \u2022upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr \u2022 rali Fluency -0.4 \u2022upc-mr utd \u2022upc-jmc -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 \u2022upv 0.5 0.4 \u2022systran \u2022upc-mr \u2022 \u2022rali 0.3 \u2022ntt 0.2 0.1 -0.0 -0.1 \u2022systran \u2022upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022upv \u2022systran \u2022upc-mr \u2022 Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022systran \u2022ntt']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.06667', '(95%-conf.int.', '0.06667', '-', '0.06667)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1649836', 'P:45', 'F:3']
['The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'Manual and Automatic Evaluation of Machine Translation between European Languages', u'Given a set of n sentences, we can compute the sample mean x\ufffd and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x\u2212d, x+df can be computed by d = 1.96 \xb7\ufffdn (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:969528', 'P:45', 'F:0']
['The human judges were presented with the following definition of adequacy and fluency, but no additional instructions:']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', u'Given a set of n sentences, we can compute the sample mean x\ufffd and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x\u2212d, x+df can be computed by d = 1.96 \xb7\ufffdn (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.', u'0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 \u2022systran \u2022 ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency \u2022systran \u2022nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p \ufffd 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain \u2022upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv 23 24 25 26 27 28 29 30 31 32 \u2022upc-mr \u2022utd \u2022upc-jmc \u2022uedin-birch \u2022ntt \u2022rali \u2022uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy \u2022upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency \u2022ntt \u2022nrc \u2022rali \u2022uedin-birch -0.2 -0.3 -0.5 \u2022upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr \u2022 rali Fluency -0.4 \u2022upc-mr utd \u2022upc-jmc -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 \u2022upv 0.5 0.4 \u2022systran \u2022upc-mr \u2022 \u2022rali 0.3 \u2022ntt 0.2 0.1 -0.0 -0.1 \u2022systran \u2022upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022upv \u2022systran \u2022upc-mr \u2022 Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022systran \u2022ntt']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.03571', '(95%-conf.int.', '0.03571', '-', '0.03571)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1649836', 'P:28', 'F:1']
['The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'Manual and Automatic Evaluation of Machine Translation between European Languages', 'Most of these groups follow a phrase-based statistical approach to machine translation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:936396', 'P:45', 'F:0']
['Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', u'0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 \u2022systran \u2022 ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency \u2022systran \u2022nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p \ufffd 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain \u2022upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv 23 24 25 26 27 28 29 30 31 32 \u2022upc-mr \u2022utd \u2022upc-jmc \u2022uedin-birch \u2022ntt \u2022rali \u2022uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy \u2022upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency \u2022ntt \u2022nrc \u2022rali \u2022uedin-birch -0.2 -0.3 -0.5 \u2022upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr \u2022 rali Fluency -0.4 \u2022upc-mr utd \u2022upc-jmc -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 \u2022upv 0.5 0.4 \u2022systran \u2022upc-mr \u2022 \u2022rali 0.3 \u2022ntt 0.2 0.1 -0.0 -0.1 \u2022systran \u2022upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022upv \u2022systran \u2022upc-mr \u2022 Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022systran \u2022ntt', 'Many human evaluation metrics have been proposed.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.04545', '(95%-conf.int.', '0.04545', '-', '0.04545)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1599366', 'P:66', 'F:3']
0.234717690502 6.15384610651e-06 1.15384614497e-05





input/ref/Task1/A97-1014_vardha.csv
input/res/Task1/A97-1014.csv
parsing: input/ref/Task1/A97-1014_vardha.csv
<S sid="72" ssid="17">In order to avoid inconsistencies, the corpus is annotated in two stages: basic annotation and nfirtellte714.</S>
original cit marker offset is 0
new cit marker offset is 0



["'72'"]
'72'
['72']
parsed_discourse_facet ['method_citation']
 <S sid="144" ssid="25">We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="14" ssid="4">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'14'"]
'14'
['14']
parsed_discourse_facet ['method_citation']
<S sid="15" ssid="5">Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
  <S sid="36" ssid="26">As for free word order languages, the following features may cause problems: sition between the two poles.</S>
original cit marker offset is 0
new cit marker offset is 0



["'36'"]
'36'
['36']
parsed_discourse_facet ['method_citation']
 <S sid="24" ssid="14">The typical treebank architecture is as follows: Structures: A context-free backbone is augmented with trace-filler representations of non-local dependencies.</S>
original cit marker offset is 0
new cit marker offset is 0



["'24'"]
'24'
['24']
parsed_discourse_facet ['method_citation']
 <S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'"]
'151'
['151']
parsed_discourse_facet ['method_citation']
 <S sid="4" ssid="1">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'"]
'4'
['4']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
 <S sid="167" ssid="9">In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.</S>
original cit marker offset is 0
new cit marker offset is 0



["'167'"]
'167'
['167']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="29">Consider the German sentence (1) daran wird ihn Anna erkennen, &amp;di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'"]
'39'
['39']
parsed_discourse_facet ['method_citation']
 <S sid="151" ssid="32">For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'151'"]
'151'
['151']
parsed_discourse_facet ['method_citation']
<S sid="71" ssid="16">However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'71'"]
'71'
['71']
parsed_discourse_facet ['method_citation']
 <S sid="144" ssid="25">We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.</S>
original cit marker offset is 0
new cit marker offset is 0



["'144'"]
'144'
['144']
parsed_discourse_facet ['method_citation']
<S sid="160" ssid="2">These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.</S>
original cit marker offset is 0
new cit marker offset is 0



["'160'"]
'160'
['160']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A97-1014.csv
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'0'"]
'100'
'12'
'0'
['100', '12', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'0'"]
'100'
'12'
'0'
['100', '12', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="47">Argument structure can be represented in terms of unordered trees (with crossing branches).</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'47'", "'12'"]
'100'
'47'
'12'
['100', '47', '12']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="74">During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'74'"]
'100'
'12'
'74'
['100', '12', '74']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="47">Argument structure can be represented in terms of unordered trees (with crossing branches).</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'", "'0'", "'100'"]
'47'
'0'
'100'
['47', '0', '100']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S ssid="1" sid="74">During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'4'", "'74'"]
'100'
'4'
'74'
['100', '4', '74']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="114">Fig.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'114'", "'12'"]
'100'
'114'
'12'
['100', '114', '12']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="14">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'0'", "'14'"]
'4'
'0'
'14'
['4', '0', '14']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'0'", "'4'"]
'100'
'0'
'4'
['100', '0', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="0">An Annotation Scheme for Free Word Order Languages</S><S ssid="1" sid="21">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'0'", "'21'"]
'100'
'0'
'21'
['100', '0', '21']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="4">The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.</S><S ssid="1" sid="14">Corpora annotated with syntactic structures are commonly referred to as trctbank.5.</S>
original cit marker offset is 0
new cit marker offset is 0



["'12'", "'4'", "'14'"]
'12'
'4'
'14'
['12', '4', '14']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="12">Realworld texts annotated with different strata of linguistic information can be used for grammar induction.</S><S ssid="1" sid="114">Fig.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'12'", "'114'"]
'100'
'12'
'114'
['100', '12', '114']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="170">Our annotation tool supplies efficient manipulation and immediate visualization of argument structures.</S><S ssid="1" sid="162">We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'170'", "'162'"]
'100'
'170'
'162'
['100', '170', '162']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="100">We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.</S><S ssid="1" sid="105">In addition, a. number of clear-cut NP cornponents can be defined outside that, juxtapositional kernel: pre- and postnominal genitives (GL, GR), relative clauses (RC), clausal and sentential complements (OC).</S><S ssid="1" sid="21">Disambiguation is based on human processing skills (cf.</S>
original cit marker offset is 0
new cit marker offset is 0



["'100'", "'105'", "'21'"]
'100'
'105'
'21'
['100', '105', '21']
parsed_discourse_facet ['method_citation']
['These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.']
['We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc.', "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction."]
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1176', 'P:21', 'F:0']
['For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).']
['Fig.', 'We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:136', 'P:55', 'F:0']
['These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.']
['An Annotation Scheme for Free Word Order Languages', 'We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction."]
['system', 'ROUGE-S*', 'Average_R:', '0.00308', '(95%-conf.int.', '0.00308', '-', '0.00308)']
['system', 'ROUGE-S*', 'Average_P:', '0.04762', '(95%-conf.int.', '0.04762', '-', '0.04762)']
['system', 'ROUGE-S*', 'Average_F:', '0.00578', '(95%-conf.int.', '0.00578', '-', '0.00578)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:325', 'P:21', 'F:1']
['For evaluation, the already annotated sentences were divided into two disjoint sets, one for training (90% of the corpus), the other one for testing (10%).']
['Fig.', 'We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:136', 'P:55', 'F:0']
['However, there is a trade-off between the granularity of information encoded in the labels and the speed and accuracy of annotation.']
['We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'We have argued that the selected approach is better suited for producing high quality interpreted corpora in languages exhibiting free constituent order.', 'Our annotation tool supplies efficient manipulation and immediate visualization of argument structures.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:28', 'F:0']
['These differences can be illustrated by a comparison with the Penn Treebank annotation scheme.']
['We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'Disambiguation is based on human processing skills (cf.', 'In addition, a. number of clear-cut NP cornponents can be defined outside that, juxtapositional kernel: pre- and postnominal genitives (GL, GR), relative clauses (RC), clausal and sentential complements (OC).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:595', 'P:21', 'F:0']
['Corpora annotated with syntactic structures are commonly referred to as trctbank.5.']
['Argument structure can be represented in terms of unordered trees (with crossing branches).', 'We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:276', 'P:28', 'F:0']
["Consider the German sentence (1) daran wird ihn Anna erkennen, &di er weint at-it will him Anna recognise that he cries 'Anna will recognise him at his cry' A sample constituent structure is given below: The fairly short sentence contains three non-local dependencies, marked by co-references between traces and the corresponding nodes."]
['Realworld texts annotated with different strata of linguistic information can be used for grammar induction.', "The work reported in this paper aims at providing syntactically annotated corpora (treebanks') for stochastic grammar induction.", 'Corpora annotated with syntactic structures are commonly referred to as trctbank.5.']
['system', 'ROUGE-S*', 'Average_R:', '0.00265', '(95%-conf.int.', '0.00265', '-', '0.00265)']
['system', 'ROUGE-S*', 'Average_P:', '0.00246', '(95%-conf.int.', '0.00246', '-', '0.00246)']
['system', 'ROUGE-S*', 'Average_F:', '0.00255', '(95%-conf.int.', '0.00255', '-', '0.00255)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:406', 'F:1']
['In the second phase of the project Verbmobil a. treebank for :30,000 German spoken sentences as well as for the same amount of English and Japanese sentences will be created.']
['An Annotation Scheme for Free Word Order Languages', 'We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'Disambiguation is based on human processing skills (cf.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:190', 'P:91', 'F:0']
['We distinguish five degrees of automation: So far, about 1100 sentences of our corpus have been annotated.']
['An Annotation Scheme for Free Word Order Languages', 'We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.']
['system', 'ROUGE-S*', 'Average_R:', '0.00433', '(95%-conf.int.', '0.00433', '-', '0.00433)']
['system', 'ROUGE-S*', 'Average_P:', '0.04762', '(95%-conf.int.', '0.04762', '-', '0.04762)']
['system', 'ROUGE-S*', 'Average_F:', '0.00794', '(95%-conf.int.', '0.00794', '-', '0.00794)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:21', 'F:1']
['Existing treebank annotation schemes exhibit a fairly uniform architecture, as they all have to meet the same basic requirements, namely: Descriptivity: Grammatical phenomena are to be described rather than explained.']
['We distinguish an NP kernel consisting of determiners, adjective phrases and nouns.', 'Realworld texts annotated with different strata of linguistic information can be used for grammar induction.', 'During the first phase, the focus is on annotating correct structures and a coarse-grained classification of grammatical functions, which represent the following areas of information: Dependency type: complements are further classified according to features such as category and case: clausal complements (OC), accusative objects (OA), datives (DA), etc.']
['system', 'ROUGE-S*', 'Average_R:', '0.00101', '(95%-conf.int.', '0.00101', '-', '0.00101)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00183', '(95%-conf.int.', '0.00183', '-', '0.00183)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:990', 'P:105', 'F:1']
0.00974727263866 0.00100636362721 0.0016454545305





input/ref/Task1/P11-1061_sweta.csv
input/res/Task1/P11-1061.csv
parsing: input/ref/Task1/P11-1061_sweta.csv
 <S sid="9" ssid="5">Unfortunately, the best completely unsupervised English POS tagger (that does not make use of a tagging dictionary) reaches only 76.1% accuracy (Christodoulopoulos et al., 2010), making its practical usability questionable at best.</S>
original cit marker offset is 0
new cit marker offset is 0



["9'"]
9'
['9']
parsed_discourse_facet ['method_citation']
<S sid="47" ssid="13">Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["47'"]
47'
['47']
parsed_discourse_facet ['method_citation']
<S sid="10" ssid="6">To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.</S>
original cit marker offset is 0
new cit marker offset is 0



["10'"]
10'
['10']
parsed_discourse_facet ['method_citation']
<S sid="70" ssid="1">Given the bilingual graph described in the previous section, we can use label propagation to project the English POS labels to the foreign language.</S>
original cit marker offset is 0
new cit marker offset is 0



["70'"]
70'
['70']
parsed_discourse_facet ['method_citation']
<S sid="52" ssid="18">This is similar to stacking the different feature instantiations into long (sparse) vectors and computing the cosine similarity between them.</S>
original cit marker offset is 0
new cit marker offset is 0



["52'"]
52'
['52']
parsed_discourse_facet ['method_citation']
 <S sid="83" ssid="14">We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value &#964;: We describe how we choose &#964; in &#167;6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["83'"]
83'
['83']
parsed_discourse_facet ['method_citation']
<S sid="113" ssid="13">For each language under consideration, Petrov et al. (2011) provide a mapping A from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["113'"]
113'
['113']
parsed_discourse_facet ['method_citation']
<S sid="3" ssid="3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
 <S sid="13" ssid="9">(2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available.</S>
original cit marker offset is 0
new cit marker offset is 0



["13'"]
13'
['13']
parsed_discourse_facet ['method_citation']
 <S sid="3" ssid="3">We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).</S>
original cit marker offset is 0
new cit marker offset is 0



["3'"]
3'
['3']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="20">We were intentionally lenient with our baselines: bilingual information by projecting POS tags directly across alignments in the parallel data.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="2" ssid="2">Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["2'"]
2'
['2']
parsed_discourse_facet ['method_citation']
<S sid="19" ssid="15">Syntactic universals are a well studied concept in linguistics (Carnie, 2002; Newmeyer, 2005), and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.</S>
original cit marker offset is 0
new cit marker offset is 0



["19'"]
19'
['19']
parsed_discourse_facet ['method_citation']
<S sid="153" ssid="16">Figure 2 shows an excerpt of a sentence from the Italian test set and the tags assigned by four different models, as well as the gold tags.</S>
original cit marker offset is 0
new cit marker offset is 0



["153'"]
153'
['153']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="14">To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).</S>
original cit marker offset is 0
new cit marker offset is 0



["18'"]
18'
['18']
parsed_discourse_facet ['method_citation']
<S sid="161" ssid="4">Our results outperform strong unsupervised baselines as well as approaches that rely on direct projections, and bridge the gap between purely supervised and unsupervised POS tagging models.</S>
original cit marker offset is 0
new cit marker offset is 0



["161'"]
161'
['161']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["23'"]
23'
['23']
parsed_discourse_facet ['method_citation']
<S sid="23" ssid="19">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["23'"]
23'
['23']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P11-1061.csv
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="74">This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'74'", "'25'"]
'0'
'74'
'25'
['0', '74', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="24">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'24'", "'25'"]
'0'
'24'
'25'
['0', '24', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'0'", "'23'"]
'25'
'0'
'23'
['25', '0', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="51">For each trigram type x2 x3 x4 in a sequence x1 x2 x3 x4 x5, we count how many times that trigram type co-occurs with the different instantiations of each concept, and compute the point-wise mutual information (PMI) between the two.5 The similarity between two trigram types is given by summing over the PMI values over feature instantiations that they have in common.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'51'"]
'0'
'25'
'51'
['0', '25', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="74">This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'74'"]
'0'
'25'
'74'
['0', '25', '74']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="81">Instead, we resort to an iterative update based method.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'81'", "'25'"]
'0'
'81'
'25'
['0', '81', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'23'"]
'0'
'25'
'23'
['0', '25', '23']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'23'", "'25'"]
'0'
'23'
'25'
['0', '23', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'23'", "'25'"]
'0'
'23'
'25'
['0', '23', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="74">This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf� to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x− x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and µ and ν are hyperparameters that we discuss in §6.4.</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'74'", "'16'"]
'0'
'74'
'16'
['0', '74', '16']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="29">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'29'"]
'0'
'25'
'29'
['0', '25', '29']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'16'", "'0'"]
'25'
'16'
'0'
['25', '16', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S ssid="1" sid="87">This locally normalized log-linear model can look at various aspects of the observation x, incorporating overlapping features of the observation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'25'", "'16'", "'87'"]
'25'
'16'
'87'
['25', '16', '87']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="1">We describe a novel approach for inducing unsupervised part-of-speech taggers for languages that have no labeled training data, but have translated text in a resource-rich language.</S><S ssid="1" sid="24">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'24'"]
'0'
'1'
'24'
['0', '1', '24']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="16">To this end, we construct a bilingual graph over word types to establish a connection between the two languages (§3), and then use graph label propagation to project syntactic information from English to the foreign language (§4).</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'16'", "'25'"]
'0'
'16'
'25'
['0', '16', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="24">The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.</S><S ssid="1" sid="23">Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.’s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'24'", "'23'"]
'0'
'24'
'23'
['0', '24', '23']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S><S ssid="1" sid="29">To establish a soft correspondence between the two languages, we use a second similarity function, which leverages standard unsupervised word alignment statistics (§3.3).3 Since we have no labeled foreign data, our goal is to project syntactic information from the English side to the foreign side.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'25'", "'29'"]
'0'
'25'
'29'
['0', '25', '29']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections</S><S ssid="1" sid="81">Instead, we resort to an iterative update based method.</S><S ssid="1" sid="25">Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'81'", "'25'"]
'0'
'81'
'25'
['0', '81', '25']
parsed_discourse_facet ['method_citation']
['To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.', u'To this end, we construct a bilingual graph over word types to establish a connection between the two languages (\xa73), and then use graph label propagation to project syntactic information from English to the foreign language (\xa74).']
['system', 'ROUGE-S*', 'Average_R:', '0.00385', '(95%-conf.int.', '0.00385', '-', '0.00385)']
['system', 'ROUGE-S*', 'Average_P:', '0.04545', '(95%-conf.int.', '0.04545', '-', '0.04545)']
['system', 'ROUGE-S*', 'Average_F:', '0.00709', '(95%-conf.int.', '0.00709', '-', '0.00709)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:66', 'F:3']
['Our monolingual similarity function (for connecting pairs of foreign trigram types) is the same as the one used by Subramanya et al. (2010).']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', u'This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf\ufffd to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x\u2212 x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and \xb5 and \u03bd are hyperparameters that we discuss in \xa76.4.', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00047', '(95%-conf.int.', '0.00047', '-', '0.00047)']
['system', 'ROUGE-S*', 'Average_P:', '0.03636', '(95%-conf.int.', '0.03636', '-', '0.03636)']
['system', 'ROUGE-S*', 'Average_F:', '0.00092', '(95%-conf.int.', '0.00092', '-', '0.00092)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4278', 'P:55', 'F:2']
['Syntactic universals are a well studied concept in linguistics (Carnie, 2002; Newmeyer, 2005), and were recently used in similar form by Naseem et al. (2010) for multilingual grammar induction.']
['Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.', 'This locally normalized log-linear model can look at various aspects of the observation x, incorporating overlapping features of the observation.', u'To this end, we construct a bilingual graph over word types to establish a connection between the two languages (\xa73), and then use graph label propagation to project syntactic information from English to the foreign language (\xa74).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:903', 'P:153', 'F:0']
['(2009) study related but different multilingual grammar and tagger induction tasks, where it is assumed that no labeled data at all is available.']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.', u'Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:55', 'F:0']
['Our method does not assume any knowledge about the target language (in particular no tagging dictionary is assumed), making it applicable to a wide array of resource-poor languages.']
['Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.', 'Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', u'To this end, we construct a bilingual graph over word types to establish a connection between the two languages (\xa73), and then use graph label propagation to project syntactic information from English to the foreign language (\xa74).']
['system', 'ROUGE-S*', 'Average_R:', '0.00256', '(95%-conf.int.', '0.00256', '-', '0.00256)']
['system', 'ROUGE-S*', 'Average_P:', '0.01905', '(95%-conf.int.', '0.01905', '-', '0.01905)']
['system', 'ROUGE-S*', 'Average_F:', '0.00452', '(95%-conf.int.', '0.00452', '-', '0.00452)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:780', 'P:105', 'F:2']
['We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', u'This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf\ufffd to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x\u2212 x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and \xb5 and \u03bd are hyperparameters that we discuss in \xa76.4.', u'To this end, we construct a bilingual graph over word types to establish a connection between the two languages (\xa73), and then use graph label propagation to project syntactic information from English to the foreign language (\xa74).']
['system', 'ROUGE-S*', 'Average_R:', '0.00297', '(95%-conf.int.', '0.00297', '-', '0.00297)']
['system', 'ROUGE-S*', 'Average_P:', '0.11029', '(95%-conf.int.', '0.11029', '-', '0.11029)']
['system', 'ROUGE-S*', 'Average_F:', '0.00578', '(95%-conf.int.', '0.00578', '-', '0.00578)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:5050', 'P:136', 'F:15']
['We then extract a set of possible tags tx(y) by eliminating labels whose probability is below a threshold value &#964;: We describe how we choose &#964; in &#167;6.4.']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', u'This stage of label propagation results in a tag distribution ri over labels y, which encodes the proportion of times the middle word of ui E Vf aligns to English words vy tagged with label y: The second stage consists of running traditional label propagation to propagate labels from these peripheral vertices Vf\ufffd to all foreign language vertices in the graph, optimizing the following objective: 5 POS Induction After running label propagation (LP), we compute tag probabilities for foreign word types x by marginalizing the POS tag distributions of foreign trigrams ui = x\u2212 x x+ over the left and right context words: where the qi (i = 1, ... , |Vf|) are the label distributions over the foreign language vertices and \xb5 and \u03bd are hyperparameters that we discuss in \xa76.4.', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00234', '(95%-conf.int.', '0.00234', '-', '0.00234)']
['system', 'ROUGE-S*', 'Average_P:', '0.09524', '(95%-conf.int.', '0.09524', '-', '0.09524)']
['system', 'ROUGE-S*', 'Average_F:', '0.00456', '(95%-conf.int.', '0.00456', '-', '0.00456)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:4278', 'P:105', 'F:10']
['To bridge this gap, we consider a practically motivated scenario, in which we want to leverage existing resources from a resource-rich language (like English) when building tools for resource-poor foreign languages.1 We assume that absolutely no labeled training data is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.']
['The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages.', 'Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.04813', '(95%-conf.int.', '0.04813', '-', '0.04813)']
['system', 'ROUGE-S*', 'Average_P:', '0.05114', '(95%-conf.int.', '0.05114', '-', '0.05114)']
['system', 'ROUGE-S*', 'Average_F:', '0.04959', '(95%-conf.int.', '0.04959', '-', '0.04959)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:528', 'F:27']
['Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.&#8217;s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', 'Instead, we resort to an iterative update based method.', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.00333', '(95%-conf.int.', '0.00333', '-', '0.00333)']
['system', 'ROUGE-S*', 'Average_P:', '0.00202', '(95%-conf.int.', '0.00202', '-', '0.00202)']
['system', 'ROUGE-S*', 'Average_F:', '0.00251', '(95%-conf.int.', '0.00251', '-', '0.00251)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:300', 'P:496', 'F:1']
['To make the projection practical, we rely on the twelve universal part-of-speech tags of Petrov et al. (2011).']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.', u'Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).']
['system', 'ROUGE-S*', 'Average_R:', '0.00627', '(95%-conf.int.', '0.00627', '-', '0.00627)']
['system', 'ROUGE-S*', 'Average_P:', '0.12121', '(95%-conf.int.', '0.12121', '-', '0.12121)']
['system', 'ROUGE-S*', 'Average_F:', '0.01193', '(95%-conf.int.', '0.01193', '-', '0.01193)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:66', 'F:8']
['We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg- Kirkpatrick et al., 2010).']
['Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Projections', u'Our final average POS tagging accuracy of 83.4% compares very favorably to the average accuracy of Berg-Kirkpatrick et al.\u2019s monolingual unsupervised state-of-the-art model (73.0%), and considerably bridges the gap to fully supervised POS tagging performance (96.6%).', 'Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned parallel corpus.']
['system', 'ROUGE-S*', 'Average_R:', '0.01961', '(95%-conf.int.', '0.01961', '-', '0.01961)']
['system', 'ROUGE-S*', 'Average_P:', '0.18382', '(95%-conf.int.', '0.18382', '-', '0.18382)']
['system', 'ROUGE-S*', 'Average_F:', '0.03544', '(95%-conf.int.', '0.03544', '-', '0.03544)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:136', 'F:25']
0.0604163630871 0.0081390908351 0.0111218180807





input/ref/Task1/P08-1028_swastika.csv
input/res/Task1/P08-1028.csv
parsing: input/ref/Task1/P08-1028_swastika.csv
<S sid="21" ssid="17">Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.</S>
original cit marker offset is 0
new cit marker offset is 0



['21']
21
['21']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="23">Our results show that the multiplicative models are superior and correlate significantly with behavioral data.</S>
original cit marker offset is 0
new cit marker offset is 0



['27']
27
['27']
parsed_discourse_facet ['result_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['189']
189
['189']
parsed_discourse_facet ['aim_citation']
<S sid="53" ssid="1">We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.</S>
original cit marker offset is 0
new cit marker offset is 0



['53']
53
['53']
parsed_discourse_facet ['method_citation']
<S sid="76" ssid="24">The models considered so far assume that components do not &#8216;interfere&#8217; with each other, i.e., that It is also possible to re-introduce the dependence on K into the model of vector composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['76']
76
['76']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['57']
57
['57']
parsed_discourse_facet ['method_citation']
<S sid="57" ssid="5">Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['57']
57
['57']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="60" ssid="8">To derive specific models from this general framework requires the identification of appropriate constraints to narrow the space of functions being considered.</S>
original cit marker offset is 0
new cit marker offset is 0



['60']
60
['60']
parsed_discourse_facet ['method_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
<S sid="73" ssid="21">Relaxing the assumption of symmetry in the case of the simple additive model produces a model which weighs the contribution of the two components differently: This allows additive models to become more syntax aware, since semantically important constituents can participate more actively in the composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['73']
73
['73']
parsed_discourse_facet ['result_citation']
<S sid="99" ssid="12">In order to establish an independent measure of sentence similarity, we assembled a set of experimental materials and elicited similarity ratings from human subjects.</S>
original cit marker offset is 0
new cit marker offset is 0



['99']
99
['99']
parsed_discourse_facet ['method_citation']
<S sid="189" ssid="1">In this paper we presented a general framework for vector-based semantic composition.</S>
original cit marker offset is 0
new cit marker offset is 0



['189']
189
['189']
parsed_discourse_facet ['aim_citation']
<S sid="191" ssid="3">Despite the popularity of additive models, our experimental results showed the superiority of models utilizing multiplicative combinations, at least for the sentence similarity task attempted here.</S>
original cit marker offset is 0
new cit marker offset is 0



['191']
191
['191']
parsed_discourse_facet ['result_citation']
<S sid="190" ssid="2">We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.</S>
original cit marker offset is 0
new cit marker offset is 0



['190']
190
['190']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/P08-1028.csv
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'24'"]
'0'
'51'
'24'
['0', '51', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'24'"]
'0'
'51'
'24'
['0', '51', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'52'"]
'0'
'51'
'52'
['0', '51', '52']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'142'", "'24'"]
'51'
'142'
'24'
['51', '142', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="25">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'25'"]
'0'
'51'
'25'
['0', '51', '25']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'142'"]
'0'
'51'
'142'
['0', '51', '142']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'142'"]
'51'
'0'
'142'
['51', '0', '142']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'52'", "'24'"]
'51'
'52'
'24'
['51', '52', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="142">We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'142'"]
'51'
'0'
'142'
['51', '0', '142']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'52'", "'51'"]
'0'
'52'
'51'
['0', '52', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="38">The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'38'"]
'0'
'51'
'38'
['0', '51', '38']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="3">Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'3'"]
'0'
'51'
'3'
['0', '51', '3']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'52'", "'24'"]
'0'
'52'
'24'
['0', '52', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'24'", "'51'"]
'0'
'24'
'51'
['0', '24', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="52">Under this framework, we introduce novel composition models which we compare empirically against previous work using a rigorous evaluation methodology.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'52'"]
'0'
'51'
'52'
['0', '51', '52']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="24">In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'24'"]
'51'
'0'
'24'
['51', '0', '24']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="51">Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).</S><S ssid="1" sid="0">Vector-based Models of Semantic Composition</S><S ssid="1" sid="25">We present a general framework for vector-based composition which allows us to consider different classes of models.</S>
original cit marker offset is 0
new cit marker offset is 0



["'51'", "'0'", "'25'"]
'51'
'0'
'25'
['51', '0', '25']
parsed_discourse_facet ['method_citation']
['Our results show that the multiplicative models are superior and correlate significantly with behavioral data.']
['In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.', 'Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:36', 'F:0']
['We formulate semantic composition as a function of two vectors, u and v. We assume that individual words are represented by vectors acquired from a corpus following any of the parametrisations that have been suggested in the literature.1 We briefly note here that a word&#8217;s vector typically represents its co-occurrence with neighboring words.']
['In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.', 'We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
['system', 'ROUGE-S*', 'Average_R:', '0.00966', '(95%-conf.int.', '0.00966', '-', '0.00966)']
['system', 'ROUGE-S*', 'Average_P:', '0.03077', '(95%-conf.int.', '0.03077', '-', '0.03077)']
['system', 'ROUGE-S*', 'Average_F:', '0.01471', '(95%-conf.int.', '0.01471', '-', '0.01471)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1035', 'P:325', 'F:10']
['In order to establish an independent measure of sentence similarity, we assembled a set of experimental materials and elicited similarity ratings from human subjects.']
['In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.', 'Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:105', 'F:0']
['The models considered so far assume that components do not &#8216;interfere&#8217; with each other, i.e., that It is also possible to re-introduce the dependence on K into the model of vector composition.']
['Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).', 'We present a general framework for vector-based composition which allows us to consider different classes of models.']
['system', 'ROUGE-S*', 'Average_R:', '0.01587', '(95%-conf.int.', '0.01587', '-', '0.01587)']
['system', 'ROUGE-S*', 'Average_P:', '0.09091', '(95%-conf.int.', '0.09091', '-', '0.09091)']
['system', 'ROUGE-S*', 'Average_F:', '0.02703', '(95%-conf.int.', '0.02703', '-', '0.02703)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:66', 'F:6']
['We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).', 'We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).']
['system', 'ROUGE-S*', 'Average_R:', '0.01463', '(95%-conf.int.', '0.01463', '-', '0.01463)']
['system', 'ROUGE-S*', 'Average_P:', '0.33333', '(95%-conf.int.', '0.33333', '-', '0.33333)']
['system', 'ROUGE-S*', 'Average_F:', '0.02804', '(95%-conf.int.', '0.02804', '-', '0.02804)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:36', 'F:12']
['We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).', 'We present a general framework for vector-based composition which allows us to consider different classes of models.']
['system', 'ROUGE-S*', 'Average_R:', '0.03704', '(95%-conf.int.', '0.03704', '-', '0.03704)']
['system', 'ROUGE-S*', 'Average_P:', '0.38889', '(95%-conf.int.', '0.38889', '-', '0.38889)']
['system', 'ROUGE-S*', 'Average_F:', '0.06763', '(95%-conf.int.', '0.06763', '-', '0.06763)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:36', 'F:14']
['We formulated composition as a function of two vectors and introduced several models based on addition and multiplication.']
['Vector-based Models of Semantic Composition', 'The projection is defined in terms of circular convolution a mathematical function that compresses the tensor product of two vectors.', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
['system', 'ROUGE-S*', 'Average_R:', '0.03656', '(95%-conf.int.', '0.03656', '-', '0.03656)']
['system', 'ROUGE-S*', 'Average_P:', '0.47222', '(95%-conf.int.', '0.47222', '-', '0.47222)']
['system', 'ROUGE-S*', 'Average_F:', '0.06786', '(95%-conf.int.', '0.06786', '-', '0.06786)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:465', 'P:36', 'F:17']
['In this paper we presented a general framework for vector-based semantic composition.']
['In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.', 'Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).']
['system', 'ROUGE-S*', 'Average_R:', '0.02989', '(95%-conf.int.', '0.02989', '-', '0.02989)']
['system', 'ROUGE-S*', 'Average_P:', '0.46429', '(95%-conf.int.', '0.46429', '-', '0.46429)']
['system', 'ROUGE-S*', 'Average_F:', '0.05616', '(95%-conf.int.', '0.05616', '-', '0.05616)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:28', 'F:13']
['Let p denote the composition of two vectors u and v, representing a pair of constituents which stand in some syntactic relation R. Let K stand for any additional knowledge or information which is needed to construct the semantics of their composition.']
['Vector-based Models of Semantic Composition', 'Our work proposes a framework for vector composition which allows the derivation of different types of models and licenses two fundamental composition operations, multiplication and addition (and their combination).', 'We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).']
['system', 'ROUGE-S*', 'Average_R:', '0.01220', '(95%-conf.int.', '0.01220', '-', '0.01220)']
['system', 'ROUGE-S*', 'Average_P:', '0.07353', '(95%-conf.int.', '0.07353', '-', '0.07353)']
['system', 'ROUGE-S*', 'Average_F:', '0.02092', '(95%-conf.int.', '0.02092', '-', '0.02092)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:820', 'P:136', 'F:10']
0.205993331045 0.0173166664743 0.0313722218736





input/ref/Task1/W06-3114_swastika.csv
input/res/Task1/W06-3114.csv
parsing: input/ref/Task1/W06-3114_swastika.csv
<S sid="47" ssid="13">Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data.</S>
original cit marker offset is 0
new cit marker offset is 0



['47']
47
['47']
parsed_discourse_facet ['method_citation']
    <S sid="8" ssid="1">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['8']
8
['8']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="144" ssid="37">Our suspicion is that BLEU is very sensitive to jargon, to selecting exactly the right words, and not synonyms that human judges may appreciate as equally good.</S>
original cit marker offset is 0
new cit marker offset is 0



['144']
144
['144']
parsed_discourse_facet ['result_citation']
<S sid="145" ssid="38">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S>
original cit marker offset is 0
new cit marker offset is 0



['145']
145
['145']
parsed_discourse_facet ['result_citation']
    <S sid="103" ssid="19">Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



['103']
103
['103']
parsed_discourse_facet ['method_citation']
<S sid="50" ssid="16">Following this method, we repeatedly &#8212; say, 1000 times &#8212; sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.</S>
original cit marker offset is 0
new cit marker offset is 0



['50']
50
['50']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="7">We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.</S>
original cit marker offset is 0
new cit marker offset is 0



['68']
68
['68']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="18" ssid="11">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>
original cit marker offset is 0
new cit marker offset is 0



['18']
18
['18']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="170" ssid="1">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S>
original cit marker offset is 0
new cit marker offset is 0



['170']
170
['170']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="8">The way judgements are collected, human judges tend to use the scores to rank systems against each other.</S>
original cit marker offset is 0
new cit marker offset is 0



['92']
92
['92']
parsed_discourse_facet ['result_citation']
<S sid="145" ssid="38">This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.</S>
original cit marker offset is 0
new cit marker offset is 0



['145']
145
['145']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W06-3114.csv
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S><S ssid="1" sid="114">Pairwise comparison is done using the sign test.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'63'", "'114'"]
'1'
'63'
'114'
['1', '63', '114']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="26">Most of these groups follow a phrase-based statistical approach to machine translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'26'"]
'0'
'1'
'26'
['0', '1', '26']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="9">Training and testing is based on the Europarl corpus.</S><S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'0'", "'1'"]
'9'
'0'
'1'
['9', '0', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'0'"]
'1'
'103'
'0'
['1', '103', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="9">Training and testing is based on the Europarl corpus.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'1'", "'0'"]
'9'
'1'
'0'
['9', '1', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="113">The confidence intervals are computed by bootstrap resampling for BLEU, and by standard significance testing for the manual scores, as described earlier in the paper.</S>
original cit marker offset is 0
new cit marker offset is 0



["'103'", "'1'", "'113'"]
'103'
'1'
'113'
['103', '1', '113']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'103'", "'1'"]
'63'
'103'
'1'
['63', '103', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'103'", "'1'"]
'63'
'103'
'1'
['63', '103', '1']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'2'", "'63'"]
'1'
'2'
'63'
['1', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="114">Pairwise comparison is done using the sign test.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'114'", "'1'", "'103'"]
'114'
'1'
'103'
['114', '1', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'2'", "'103'"]
'1'
'2'
'103'
['1', '2', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="26">Most of these groups follow a phrase-based statistical approach to machine translation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'26'"]
'1'
'103'
'26'
['1', '103', '26']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'63'"]
'1'
'103'
'63'
['1', '103', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="9">Training and testing is based on the Europarl corpus.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'", "'1'", "'103'"]
'9'
'1'
'103'
['9', '1', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'103'", "'2'"]
'1'
'103'
'2'
['1', '103', '2']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="103">Given a set of n sentences, we can compute the sample mean x� and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x−d, x+df can be computed by d = 1.96 ·�n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'1'", "'103'"]
'0'
'1'
'103'
['0', '1', '103']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S><S ssid="1" sid="2">0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 •systran • ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency •systran •nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p � 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain •upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv 23 24 25 26 27 28 29 30 31 32 •upc-mr •utd •upc-jmc •uedin-birch •ntt •rali •uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy •upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency •ntt •nrc •rali •uedin-birch -0.2 -0.3 -0.5 •upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr • rali Fluency -0.4 •upc-mr utd •upc-jmc -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 •upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 •upv 0.5 0.4 •systran •upc-mr • •rali 0.3 •ntt 0.2 0.1 -0.0 -0.1 •systran •upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •upv •systran •upc-mr • Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 •systran •ntt</S><S ssid="1" sid="63">Many human evaluation metrics have been proposed.</S>
original cit marker offset is 0
new cit marker offset is 0



["'1'", "'2'", "'63'"]
'1'
'2'
'63'
['1', '2', '63']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">Manual and Automatic Evaluation of Machine Translation between European Languages</S><S ssid="1" sid="170">We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.</S><S ssid="1" sid="1">Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 • 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 •upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 •upv •systran upcntt • rali upc-jmc • cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upv -0.5 •systran •upv upc -jmc • Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 • • • td t cc upc- • rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 •upv -0.4 •upv -0.3 In Domain •upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain •upc-jmc •nrc •ntt Adequacy upc-jmc • • •lcc • rali • •rali -0.7 -0.5 -0.6 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 • •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt • upc-mr •lcc •utd •upc-jg •rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •upc-jmc • uedin-birch -0.5 -0.5 •upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc • upc-jmc •systran •upv Fluency •ula •upc-mr •lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 •systran •upv •uedin-phi -jmc •rali •systran -0.3 -0.4 -0.5 -0.6 •upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi • • •utd •upc-jmc •upc-mr 0.4 •rali -0.3 -0.4 -0.5 •upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'170'", "'1'"]
'0'
'170'
'1'
['0', '170', '1']
parsed_discourse_facet ['method_citation']
['This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'Manual and Automatic Evaluation of Machine Translation between European Languages', 'We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
['system', 'ROUGE-S*', 'Average_R:', '0.00004', '(95%-conf.int.', '0.00004', '-', '0.00004)']
['system', 'ROUGE-S*', 'Average_P:', '0.25735', '(95%-conf.int.', '0.25735', '-', '0.25735)']
['system', 'ROUGE-S*', 'Average_F:', '0.00007', '(95%-conf.int.', '0.00007', '-', '0.00007)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:940506', 'P:136', 'F:35']
['Because of this, we retokenized and lowercased submitted output with our own tokenizer, which was also used to prepare the training and test data.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'Pairwise comparison is done using the sign test.', 'Many human evaluation metrics have been proposed.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.02778', '(95%-conf.int.', '0.02778', '-', '0.02778)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:926841', 'P:36', 'F:1']
['Given a set of n sentences, we can compute the sample mean x&#65533; and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x&#8722;d, x+df can be computed by d = 1.96 &#183;&#65533;n (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', u'0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 \u2022systran \u2022 ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency \u2022systran \u2022nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p \ufffd 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain \u2022upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv 23 24 25 26 27 28 29 30 31 32 \u2022upc-mr \u2022utd \u2022upc-jmc \u2022uedin-birch \u2022ntt \u2022rali \u2022uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy \u2022upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency \u2022ntt \u2022nrc \u2022rali \u2022uedin-birch -0.2 -0.3 -0.5 \u2022upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr \u2022 rali Fluency -0.4 \u2022upc-mr utd \u2022upc-jmc -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 \u2022upv 0.5 0.4 \u2022systran \u2022upc-mr \u2022 \u2022rali 0.3 \u2022ntt 0.2 0.1 -0.0 -0.1 \u2022systran \u2022upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022upv \u2022systran \u2022upc-mr \u2022 Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022systran \u2022ntt', 'Many human evaluation metrics have been proposed.']
['system', 'ROUGE-S*', 'Average_R:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Average_P:', '0.02063', '(95%-conf.int.', '0.02063', '-', '0.02063)']
['system', 'ROUGE-S*', 'Average_F:', '0.00002', '(95%-conf.int.', '0.00002', '-', '0.00002)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1599366', 'P:630', 'F:13']
['In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.']
['Training and testing is based on the Europarl corpus.', 'Manual and Automatic Evaluation of Machine Translation between European Languages', u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .']
['system', 'ROUGE-S*', 'Average_R:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Average_P:', '0.05495', '(95%-conf.int.', '0.05495', '-', '0.05495)']
['system', 'ROUGE-S*', 'Average_F:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:932295', 'P:91', 'F:5']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', u'Given a set of n sentences, we can compute the sample mean x\ufffd and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x\u2212d, x+df can be computed by d = 1.96 \xb7\ufffdn (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.', u'0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 \u2022systran \u2022 ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency \u2022systran \u2022nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p \ufffd 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain \u2022upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv 23 24 25 26 27 28 29 30 31 32 \u2022upc-mr \u2022utd \u2022upc-jmc \u2022uedin-birch \u2022ntt \u2022rali \u2022uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy \u2022upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency \u2022ntt \u2022nrc \u2022rali \u2022uedin-birch -0.2 -0.3 -0.5 \u2022upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr \u2022 rali Fluency -0.4 \u2022upc-mr utd \u2022upc-jmc -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 \u2022upv 0.5 0.4 \u2022systran \u2022upc-mr \u2022 \u2022rali 0.3 \u2022ntt 0.2 0.1 -0.0 -0.1 \u2022systran \u2022upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022upv \u2022systran \u2022upc-mr \u2022 Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022systran \u2022ntt']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.14545', '(95%-conf.int.', '0.14545', '-', '0.14545)']
['system', 'ROUGE-S*', 'Average_F:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1649836', 'P:55', 'F:8']
['In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.']
['Training and testing is based on the Europarl corpus.', u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'Manual and Automatic Evaluation of Machine Translation between European Languages']
['system', 'ROUGE-S*', 'Average_R:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Average_P:', '0.06593', '(95%-conf.int.', '0.06593', '-', '0.06593)']
['system', 'ROUGE-S*', 'Average_F:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:932295', 'P:91', 'F:6']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', u'Given a set of n sentences, we can compute the sample mean x\ufffd and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x\u2212d, x+df can be computed by d = 1.96 \xb7\ufffdn (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.', 'Most of these groups follow a phrase-based statistical approach to machine translation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Average_P:', '0.25455', '(95%-conf.int.', '0.25455', '-', '0.25455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00003', '(95%-conf.int.', '0.00003', '-', '0.00003)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:970921', 'P:55', 'F:14']
['We carried out an extensive manual and automatic evaluation of machine translation performance on European language pairs.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'Manual and Automatic Evaluation of Machine Translation between European Languages', u'Given a set of n sentences, we can compute the sample mean x\ufffd and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x\u2212d, x+df can be computed by d = 1.96 \xb7\ufffdn (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.']
['system', 'ROUGE-S*', 'Average_R:', '0.00003', '(95%-conf.int.', '0.00003', '-', '0.00003)']
['system', 'ROUGE-S*', 'Average_P:', '0.50909', '(95%-conf.int.', '0.50909', '-', '0.50909)']
['system', 'ROUGE-S*', 'Average_F:', '0.00006', '(95%-conf.int.', '0.00006', '-', '0.00006)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:969528', 'P:55', 'F:28']
['We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', u'Given a set of n sentences, we can compute the sample mean x\ufffd and sample variance s2 of the individual sentence judgements xi: The extend of the confidence interval [x\u2212d, x+df can be computed by d = 1.96 \xb7\ufffdn (6) Pairwise Comparison: As for the automatic evaluation metric, we want to be able to rank different systems against each other, for which we need assessments of statistical significance on the differences between a pair of systems.', u'0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 \u2022systran \u2022 ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency \u2022systran \u2022nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p \ufffd 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain \u2022upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv 23 24 25 26 27 28 29 30 31 32 \u2022upc-mr \u2022utd \u2022upc-jmc \u2022uedin-birch \u2022ntt \u2022rali \u2022uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy \u2022upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency \u2022ntt \u2022nrc \u2022rali \u2022uedin-birch -0.2 -0.3 -0.5 \u2022upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr \u2022 rali Fluency -0.4 \u2022upc-mr utd \u2022upc-jmc -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 \u2022upv 0.5 0.4 \u2022systran \u2022upc-mr \u2022 \u2022rali 0.3 \u2022ntt 0.2 0.1 -0.0 -0.1 \u2022systran \u2022upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022upv \u2022systran \u2022upc-mr \u2022 Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022systran \u2022ntt']
['system', 'ROUGE-S*', 'Average_R:', '0.00001', '(95%-conf.int.', '0.00001', '-', '0.00001)']
['system', 'ROUGE-S*', 'Average_P:', '0.16484', '(95%-conf.int.', '0.16484', '-', '0.16484)']
['system', 'ROUGE-S*', 'Average_F:', '0.00002', '(95%-conf.int.', '0.00002', '-', '0.00002)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1649836', 'P:91', 'F:15']
['The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', 'Manual and Automatic Evaluation of Machine Translation between European Languages', 'Most of these groups follow a phrase-based statistical approach to machine translation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:936396', 'P:45', 'F:0']
['The way judgements are collected, human judges tend to use the scores to rank systems against each other.']
[u'Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-8) (1-6) lcc (1-6) (1-7) (1-4) utd (1-7) (1-6) (2-7) upc-mr (1-8) (1-6) (1-7) nrc (1-7) (2-6) (8) ntt (1-8) (2-8) (1-7) cmu (3-7) (4-8) (2-7) rali (5-8) (3-9) (3-7) systran (9) (8-9) (10) upv (10) (10) (9) Spanish-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-7) (1-6) (1-5) ntt (1-7) (1-8) (1-5) lcc (1-8) (2-8) (1-4) utd (1-8) (2-7) (1-5) nrc (2-8) (1-9) (6) upc-mr (1-8) (1-6) (7) uedin-birch (1-8) (2-10) (8) rali (3-9) (3-9) (2-5) upc-jg (7-9) (6-9) (9) upv (10) (9-10) (10) German-English (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) uedin-phi (1-2) (1) (1) lcc (2-7) (2-7) (2) nrc (2-7) (2-6) (5-7) utd (3-7) (2-8) (3-4) ntt (2-9) (2-8) (3-4) upc-mr (3-9) (6-9) (8) rali (4-9) (3-9) (5-7) upc-jmc (2-9) (3-9) (5-7) systran (3-9) (3-9) (10) upv (10) (10) (9) Figure 7: Evaluation of translation to English on in-domain test data 112 English-French (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) nrc (1-5) (1-5) (1-6) upc-mr (1-4) (1-5) (1-6) upc-jmc (1-6) (1-6) (1-5) systran (2-7) (1-6) (7) utd (3-7) (3-7) (3-6) rali (1-7) (2-7) (1-6) ntt (4-7) (4-7) (1-5) English-Spanish (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) ms (1-5) (1-7) (7-8) upc-mr (1-4) (1-5) (1-4) utd (1-5) (1-6) (1-4) nrc (2-7) (1-6) (5-6) ntt (3-7) (1-6) (1-4) upc-jmc (2-7) (2-7) (1-4) rali (5-8) (6-8) (5-6) uedin-birch (6-9) (6-10) (7-8) upc-jg (9) (8-10) (9) upv (9-10) (8-10) (10) English-German (In Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-5) (3-5) ntt (1-5) (2-6) (1-3) upc-jmc (1-5) (1-4) (1-3) nrc (2-4) (1-5) (4-5) rali (3-6) (2-6) (1-4) systran (5-6) (3-6) (7) upv (7) (7) (6) Figure 8: Evaluation of translation from English on in-domain test data 113 French-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-5) (1-8) (1-4) cmu (1-8) (1-9) (4-7) systran (1-8) (1-7) (9) lcc (1-9) (1-9) (1-5) upc-mr (2-8) (1-7) (1-3) utd (1-9) (1-8) (3-7) ntt (3-9) (1-9) (3-7) nrc (3-8) (3-9) (3-7) rali (4-9) (5-9) (8) upv (10) (10) (10) Spanish-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-jmc (1-2) (1-6) (1-3) uedin-birch (1-7) (1-6) (5-8) nrc (2-8) (1-8) (5-7) ntt (2-7) (2-6) (3-4) upc-mr (2-8) (1-7) (5-8) lcc (4-9) (3-7) (1-4) utd (2-9) (2-8) (1-3) upc-jg (4-9) (7-9) (9) rali (4-9) (6-9) (6-8) upv (10) (10) (10) German-English (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1-4) (1-4) (7-9) uedin-phi (1-6) (1-7) (1) lcc (1-6) (1-7) (2-3) utd (2-7) (2-6) (4-6) ntt (1-9) (1-7) (3-5) nrc (3-8) (2-8) (7-8) upc-mr (4-8) (6-8) (4-6) upc-jmc (4-8) (3-9) (2-5) rali (8-9) (8-9) (8-9) upv (10) (10) (10) Figure 9: Evaluation of translation to English on out-of-domain test data 114 English-French (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1) (1) upc-jmc (2-5) (2-4) (2-6) upc-mr (2-4) (2-4) (2-6) utd (2-6) (2-6) (7) rali (4-7) (5-7) (2-6) nrc (4-7) (4-7) (2-5) ntt (4-7) (4-7) (3-6) English-Spanish (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) upc-mr (1-3) (1-6) (1-2) ms (1-7) (1-8) (6-7) utd (2-6) (1-7) (3-5) nrc (1-6) (2-7) (3-5) upc-jmc (2-7) (1-6) (3-5) ntt (2-7) (1-7) (1-2) rali (6-8) (4-8) (6-8) uedin-birch (6-10) (5-9) (7-8) upc-jg (8-9) (9-10) (9) upv (9) (8-9) (10) English-German (Out of Domain) Adequacy (rank) Fluency (rank) BLEU (rank) systran (1) (1-2) (1-6) upc-mr (2-3) (1-3) (1-5) upc-jmc (2-3) (3-6) (1-6) rali (4-6) (4-6) (1-6) nrc (4-6) (2-6) (2-6) ntt (4-6) (3-5) (1-6) upv (7) (7) (7) Figure 10: Evaluation of translation from English on out-of-domain test data 115 French-English In domain Out of Domain Adequacy Adequacy 0.3 0.3 \u2022 0.2 0.2 0.1 0.1 -0.0 -0.0 -0.1 -0.1 -0.2 -0.2 -0.3 -0.3 -0.4 -0.4 -0.5 -0.5 -0.6 -0.6 -0.7 -0.7 \u2022upv -0.8 -0.8 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 \u2022upv \u2022systran upcntt \u2022 rali upc-jmc \u2022 cc Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022systran \u2022upv upc -jmc \u2022 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022 \u2022 \u2022 td t cc upc- \u2022 rali 21 22 23 24 25 26 27 28 29 30 31 15 16 17 18 19 20 21 22 Figure 11: Correlation between manual and automatic scores for French-English 116 Spanish-English Figure 12: Correlation between manual and automatic scores for Spanish-English -0.3 -0.4 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 \u2022upv -0.4 \u2022upv -0.3 In Domain \u2022upc-jg Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 Out of Domain \u2022upc-jmc \u2022nrc \u2022ntt Adequacy upc-jmc \u2022 \u2022 \u2022lcc \u2022 rali \u2022 \u2022rali -0.7 -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 \u2022 \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 ntt \u2022 upc-mr \u2022lcc \u2022utd \u2022upc-jg \u2022rali Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upc-jmc \u2022 uedin-birch -0.5 -0.5 \u2022upv 23 24 25 26 27 28 29 30 31 32 19 20 21 22 23 24 25 26 27 28 117 In Domain Out of Domain Adequacy Adequacy German-English 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 lcc \u2022 upc-jmc \u2022systran \u2022upv Fluency \u2022ula \u2022upc-mr \u2022lcc 15 16 17 18 19 20 21 22 23 24 25 26 27 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022systran \u2022upv \u2022uedin-phi -jmc \u2022rali \u2022systran -0.3 -0.4 -0.5 -0.6 \u2022upv 12 13 14 15 16 17 18 19 20 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 Fluency uedin-phi \u2022 \u2022 \u2022utd \u2022upc-jmc \u2022upc-mr 0.4 \u2022rali -0.3 -0.4 -0.5 \u2022upv 12 13 14 15 16 17 18 19 20 0.3 0.2 0.1 -0.0 -0.1 -0.2 English-French In Domain Out of Domain Adequacy Adequacy .', u'0.2 0.1 0.0 -0.1 25 26 27 28 29 30 31 32 -0.2 -0.3 \u2022systran \u2022 ntt 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 20 21 22 23 24 25 26 Fluency Fluency \u2022systran \u2022nrc rali 25 26 27 28 29 30 31 32 0.2 0.1 0.0 -0.1 -0.2 -0.3 cme p \ufffd 20 21 22 23 24 25 26 0.5 0.4 0.3 0.2 0.1 0.0 -0.1 -0.2 -0.3 Figure 14: Correlation between manual and automatic scores for English-French 119 In Domain Out of Domain \u2022upv Adequacy -0.9 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv 23 24 25 26 27 28 29 30 31 32 \u2022upc-mr \u2022utd \u2022upc-jmc \u2022uedin-birch \u2022ntt \u2022rali \u2022uedin-birch 16 17 18 19 20 21 22 23 24 25 26 27 Adequacy \u2022upc-mr 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 -1.0 -1.1 English-Spanish Fluency \u2022ntt \u2022nrc \u2022rali \u2022uedin-birch -0.2 -0.3 -0.5 \u2022upv 16 17 18 19 20 21 22 23 24 25 26 27 -0.4 nr \u2022 rali Fluency -0.4 \u2022upc-mr utd \u2022upc-jmc -0.5 -0.6 \u2022upv 23 24 25 26 27 28 29 30 31 32 0.2 0.1 -0.0 -0.1 -0.2 -0.3 0.3 0.2 0.1 -0.0 -0.1 -0.6 -0.7 Figure 15: Correlation between manual and automatic scores for English-Spanish 120 English-German In Domain Out of Domain Adequacy Adequacy 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 \u2022upv -0.2 -0.3 -0.4 -0.5 -0.6 -0.7 -0.8 -0.9 \u2022upv 0.5 0.4 \u2022systran \u2022upc-mr \u2022 \u2022rali 0.3 \u2022ntt 0.2 0.1 -0.0 -0.1 \u2022systran \u2022upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 \u2022upv -0.5 \u2022upv \u2022systran \u2022upc-mr \u2022 Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 \u2022systran \u2022ntt', 'Many human evaluation metrics have been proposed.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1599366', 'P:28', 'F:0']
0.136415453305 1.09090908099e-05 2.0909090719e-05





input/ref/Task1/W99-0613_vardha.csv
input/res/Task1/W99-0613.csv
parsing: input/ref/Task1/W99-0613_vardha.csv
    <S sid="9" ssid="3">This paper discusses the use of unlabeled examples for the problem of named entity classification.</S>
original cit marker offset is 0
new cit marker offset is 0



["'9'"]
'9'
['9']
parsed_discourse_facet ['method_citation']
    <S sid="35" ssid="29">AdaBoost finds a weighted combination of simple (weak) classifiers, where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'35'"]
'35'
['35']
parsed_discourse_facet ['method_citation']
    <S sid="134" ssid="1">This section describes an algorithm based on boosting algorithms, which were previously developed for supervised machine learning problems.</S>
original cit marker offset is 0
new cit marker offset is 0



["'134'"]
'134'
['134']
parsed_discourse_facet ['method_citation']
    <S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'"]
'236'
['236']
parsed_discourse_facet ['method_citation']
    <S sid="8" ssid="2">Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
    <S sid="42" ssid="36">(Riloff and Shepherd 97) describe a bootstrapping approach for acquiring nouns in particular categories (such as &amp;quot;vehicle&amp;quot; or &amp;quot;weapon&amp;quot; categories).</S>
original cit marker offset is 0
new cit marker offset is 0



["'42'"]
'42'
['42']
parsed_discourse_facet ['method_citation']
    <S sid="236" ssid="3">We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.</S>
original cit marker offset is 0
new cit marker offset is 0



["'236'"]
'236'
['236']
parsed_discourse_facet ['method_citation']
    <S sid="222" ssid="1">The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.</S>
original cit marker offset is 0
new cit marker offset is 0



["'222'"]
'222'
['222']
parsed_discourse_facet ['method_citation']
    <S sid="30" ssid="24">(Blum and Mitchell 98) offer a promising formulation of redundancy, also prove some results about how the use of can help classification, and suggest an objective function when training with unlabeled examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'30'"]
'30'
['30']
parsed_discourse_facet ['method_citation']
 <S sid="26" ssid="20">We present two algorithms.</S>
original cit marker offset is 0
new cit marker offset is 0



["'26'"]
'26'
['26']
parsed_discourse_facet ['method_citation']
    <S sid="7" ssid="1">Many statistical or machine-learning approaches for natural language problems require a relatively large amount of supervision, in the form of labeled training examples.</S>
original cit marker offset is 0
new cit marker offset is 0



["'7'"]
'7'
['7']
parsed_discourse_facet ['method_citation']
    <S sid="32" ssid="26">The algorithm can be viewed as heuristically optimizing an objective function suggested by (Blum and Mitchell 98); empirically it is shown to be quite successful in optimizing this criterion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'32'"]
'32'
['32']
parsed_discourse_facet ['method_citation']
    <S sid="47" ssid="1">971,746 sentences of New York Times text were parsed using the parser of (Collins 96).1 Word sequences that met the following criteria were then extracted as named entity examples: whose head is a singular noun (tagged NN).</S>
original cit marker offset is 0
new cit marker offset is 0



["'47'"]
'47'
['47']
parsed_discourse_facet ['method_citation']
    <S sid="127" ssid="60">The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.</S>
original cit marker offset is 0
new cit marker offset is 0



["'127'"]
'127'
['127']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/W99-0613.csv
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'0'", "'179'"]
'137'
'0'
'179'
['137', '0', '179']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="33">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'33'", "'179'"]
'137'
'33'
'179'
['137', '33', '179']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S><S ssid="1" sid="33">The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'0'", "'33'"]
'137'
'0'
'33'
['137', '0', '33']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="79">2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'79'", "'0'"]
'137'
'79'
'0'
['137', '79', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'137'", "'0'"]
'179'
'137'
'0'
['179', '137', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="224">The model was parameterized such that the joint probability of a (label, feature-set) pair P(yi, xi) is written as The model assumes that (y, x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'224'"]
'137'
'179'
'224'
['137', '179', '224']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="222">The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'222'", "'179'"]
'137'
'222'
'179'
['137', '222', '179']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
<S ssid="1" sid="137">The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.</S><S ssid="1" sid="179">The algorithm builds two classifiers in parallel from labeled and unlabeled data.</S><S ssid="1" sid="0">Unsupervised Models for Named Entity Classification Collins</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'", "'179'", "'0'"]
'137'
'179'
'0'
['137', '179', '0']
parsed_discourse_facet ['aim_citation', 'method_citation']
['This section describes an algorithm based on boosting algorithms, which were previously developed for supervised machine learning problems.']
['The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).', 'Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.']
['system', 'ROUGE-S*', 'Average_R:', '0.00855', '(95%-conf.int.', '0.00855', '-', '0.00855)']
['system', 'ROUGE-S*', 'Average_P:', '0.04545', '(95%-conf.int.', '0.04545', '-', '0.04545)']
['system', 'ROUGE-S*', 'Average_F:', '0.01439', '(95%-conf.int.', '0.01439', '-', '0.01439)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:66', 'F:3']
['We present two algorithms.']
['The model was parameterized such that the joint probability of a (label, feature-set) pair P(yi, xi) is written as The model assumes that (y, x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1275', 'P:1', 'F:0']
['We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:45', 'F:0']
['The DL-CoTrain algorithm can be motivated as being a greedy method of satisfying the above 2 constraints.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:36', 'F:0']
['We chose one of four labels for each example: location, person, organization, or noise where the noise category was used for items that were outside the three categories.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:45', 'F:0']
['The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.01299', '(95%-conf.int.', '0.01299', '-', '0.01299)']
['system', 'ROUGE-S*', 'Average_P:', '0.01961', '(95%-conf.int.', '0.01961', '-', '0.01961)']
['system', 'ROUGE-S*', 'Average_F:', '0.01562', '(95%-conf.int.', '0.01562', '-', '0.01562)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:153', 'F:3']
['AdaBoost finds a weighted combination of simple (weak) classifiers, where the weights are chosen to minimize a function that bounds the classification error on a set of training examples.']
['The second algorithm builds on a boosting algorithm called AdaBoost (Freund and Schapire 97; Schapire and Singer 98).', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00265', '(95%-conf.int.', '0.00265', '-', '0.00265)']
['system', 'ROUGE-S*', 'Average_P:', '0.00735', '(95%-conf.int.', '0.00735', '-', '0.00735)']
['system', 'ROUGE-S*', 'Average_F:', '0.00389', '(95%-conf.int.', '0.00389', '-', '0.00389)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:136', 'F:1']
['The algorithm can be viewed as heuristically optimizing an objective function suggested by (Blum and Mitchell 98); empirically it is shown to be quite successful in optimizing this criterion.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:105', 'F:0']
['Recent results (e.g., (Yarowsky 95; Brill 95; Blum and Mitchell 98)) have suggested that unlabeled data can be used quite profitably in reducing the need for supervision.']
['Unsupervised Models for Named Entity Classification Collins', 'The new algorithm, which we call CoBoost, uses labeled and unlabeled data and builds two classifiers in parallel.', 'The algorithm builds two classifiers in parallel from labeled and unlabeled data.']
['system', 'ROUGE-S*', 'Average_R:', '0.00433', '(95%-conf.int.', '0.00433', '-', '0.00433)']
['system', 'ROUGE-S*', 'Average_P:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_F:', '0.00595', '(95%-conf.int.', '0.00595', '-', '0.00595)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:231', 'P:105', 'F:1']
0.00910333323219 0.00316888885368 0.00442777772858





input/ref/Task1/A00-2018_akanksha.csv
input/res/Task1/A00-2018.csv
parsing: input/ref/Task1/A00-2018_akanksha.csv
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'"]
'90'
'91'
['90', '91']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="48" ssid="17">Maximum-entropy models have two benefits for a parser builder.</S>
    <S sid="49" ssid="18">First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &amp;quot;features&amp;quot; suggests that the probability model should be easily changeable &#8212; just change the set of features used.</S>
    <S sid="51" ssid="20">Second, and this is a point we have not yet mentioned, the features used in these models need have no particular independence of one another.</S>
original cit marker offset is 0
new cit marker offset is 0



["'48'", "'49'", "'51'"]
'48'
'49'
'51'
['48', '49', '51']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
    <S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
    <S sid="93" ssid="4">This allows the second pass to see expansions not present in the training corpus.</S>
    <S sid="94" ssid="5">We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'", "'92'", "'93'", "'94'"]
'90'
'91'
'92'
'93'
'94'
['90', '91', '92', '93', '94']
parsed_discourse_facet ['method_citation']
“NA”
original cit marker offset is 0
new cit marker offset is 0



['0']
0
['0']
Error in Reference Offset
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
    <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
    <S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
    <S sid="93" ssid="4">This allows the second pass to see expansions not present in the training corpus.</S>
    <S sid="94" ssid="5">We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'91'", "'92'", "'93'", "'94'"]
'90'
'91'
'92'
'93'
'94'
['90', '91', '92', '93', '94']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="7">To compute a probability in a log-linear model one first defines a set of &amp;quot;features&amp;quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.</S>
    <S sid="39" ssid="8">In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.</S>
    <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'38'", "'39'", "'40'"]
'38'
'39'
'40'
['38', '39', '40']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['result_citation']
<S sid="85" ssid="54">As partition-function calculation is typically the major on-line computational problem for maximum-entropy models, this simplifies the model significantly.</S>
original cit marker offset is 0
new cit marker offset is 0



["'85'"]
'85'
['85']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="32">As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &amp;quot;before&amp;quot;), and the label of the grandparent of c (la).</S><S sid="143" ssid="34">The first is simply that if we first guess the pre-terminal, when we go to guess the head the first thing we can condition upon is the pre-terminal, i.e., we compute p(h I t).</S><S sid="146" ssid="37">The second major reason why first guessing the pre-terminal makes so much difference is that it can be used when backing off the lexical head in computing the probability of the rule expansion.</S>
original cit marker offset is 0
new cit marker offset is 0



["'63'", "'143'", "'146'"]
'63'
'143'
'146'
['63', '143', '146']
parsed_discourse_facet ['method_citation']
<S sid="78" ssid="47">With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.</S>                                        <S sid="79" ssid="48">We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.</S>
original cit marker offset is 0
new cit marker offset is 0



["'78'", "'79'"]
'78'
'79'
['78', '79']
parsed_discourse_facet ['method_citation']
<S sid="90" ssid="1">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'"]
'90'
['90']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['result_citation']
parsing: input/res/Task1/A00-2018.csv
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="7">Following [5,10], our parser is based upon a probabilistic generative model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'0'", "'7'"]
'90'
'0'
'7'
['90', '0', '7']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="55">This method is known as &quot;deleted interpolation&quot; smoothing.</S><S ssid="1" sid="130">This parser achieves an average precision/recall of 86.2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'55'", "'130'"]
'0'
'55'
'130'
['0', '55', '130']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S><S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'90'", "'174'"]
'0'
'90'
'174'
['0', '90', '174']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'174'", "'90'"]
'0'
'174'
'90'
['0', '174', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S ssid="1" sid="20">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar, and a right-hand side that is a sequence of one or more such symbols.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'91'", "'20'"]
'174'
'91'
'20'
['174', '91', '20']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="130">This parser achieves an average precision/recall of 86.2%.</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'130'", "'91'"]
'174'
'130'
'91'
['174', '130', '91']
parsed_discourse_facet ['method_citation']
['As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.', 'We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.02857', '(95%-conf.int.', '0.02857', '-', '0.02857)']
['system', 'ROUGE-S*', 'Average_P:', '0.01070', '(95%-conf.int.', '0.01070', '-', '0.01070)']
['system', 'ROUGE-S*', 'Average_F:', '0.01556', '(95%-conf.int.', '0.01556', '-', '0.01556)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:561', 'F:6']
['In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.', 'To compute a probability in a log-linear model one first defines a set of &quot;features&quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.', 'In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.']
['A Maximum-Entropy-Inspired Parser *', 'We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ', 'We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.']
['system', 'ROUGE-S*', 'Average_R:', '0.00450', '(95%-conf.int.', '0.00450', '-', '0.00450)']
['system', 'ROUGE-S*', 'Average_P:', '0.00317', '(95%-conf.int.', '0.00317', '-', '0.00317)']
['system', 'ROUGE-S*', 'Average_F:', '0.00372', '(95%-conf.int.', '0.00372', '-', '0.00372)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:946', 'F:3']
['Second, and this is a point we have not yet mentioned, the features used in these models need have no particular independence of one another.', 'Maximum-entropy models have two benefits for a parser builder.', 'First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &quot;features&quot; suggests that the probability model should be easily changeable &#8212; just change the set of features used.']
['We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.', 'Following [5,10], our parser is based upon a probabilistic generative model.', 'A Maximum-Entropy-Inspired Parser *']
['system', 'ROUGE-S*', 'Average_R:', '0.03557', '(95%-conf.int.', '0.03557', '-', '0.03557)']
['system', 'ROUGE-S*', 'Average_P:', '0.01935', '(95%-conf.int.', '0.01935', '-', '0.01935)']
['system', 'ROUGE-S*', 'Average_F:', '0.02507', '(95%-conf.int.', '0.02507', '-', '0.02507)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:253', 'P:465', 'F:9']
['We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_P:', '0.03030', '(95%-conf.int.', '0.03030', '-', '0.03030)']
['system', 'ROUGE-S*', 'Average_F:', '0.01449', '(95%-conf.int.', '0.01449', '-', '0.01449)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:66', 'F:2']
['We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_P:', '0.03030', '(95%-conf.int.', '0.03030', '-', '0.03030)']
['system', 'ROUGE-S*', 'Average_F:', '0.01449', '(95%-conf.int.', '0.01449', '-', '0.01449)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:66', 'F:2']
no Reference Text in gold A00-2018
['We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_P:', '0.03030', '(95%-conf.int.', '0.03030', '-', '0.03030)']
['system', 'ROUGE-S*', 'Average_F:', '0.01449', '(95%-conf.int.', '0.01449', '-', '0.01449)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:66', 'F:2']
['As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.', 'We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.', 'This allows the second pass to see expansions not present in the training corpus.', 'For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.', 'We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.08095', '(95%-conf.int.', '0.08095', '-', '0.08095)']
['system', 'ROUGE-S*', 'Average_P:', '0.00769', '(95%-conf.int.', '0.00769', '-', '0.00769)']
['system', 'ROUGE-S*', 'Average_F:', '0.01404', '(95%-conf.int.', '0.01404', '-', '0.01404)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:2211', 'F:17']
['We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.27143', '(95%-conf.int.', '0.27143', '-', '0.27143)']
['system', 'ROUGE-S*', 'Average_P:', '0.06025', '(95%-conf.int.', '0.06025', '-', '0.06025)']
['system', 'ROUGE-S*', 'Average_F:', '0.09862', '(95%-conf.int.', '0.09862', '-', '0.09862)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:946', 'F:57']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.51724', '(95%-conf.int.', '0.51724', '-', '0.51724)']
['system', 'ROUGE-S*', 'Average_F:', '0.68182', '(95%-conf.int.', '0.68182', '-', '0.68182)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:406', 'F:210']
['As partition-function calculation is typically the major on-line computational problem for maximum-entropy models, this simplifies the model significantly.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:91', 'F:0']
['We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.', 'With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:465', 'F:0']
0.0644818175956 0.131779998802 0.0802090901799





input/ref/Task1/E03-1005_swastika.csv
input/res/Task1/E03-1005.csv
parsing: input/ref/Task1/E03-1005_swastika.csv
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
    <S sid="41" ssid="38">This paper presents the first published results with Goodman's PCFG-reductions of both Bonnema et al. 's (1999) and Bod's (2001) estimators on the WSJ.</S>
original cit marker offset is 0
new cit marker offset is 0



['41']
41
['41']
parsed_discourse_facet ['aim_citation']
<S sid="80" ssid="32">DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).</S>
original cit marker offset is 0
new cit marker offset is 0



['80']
80
['80']
parsed_discourse_facet ['result_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



['143']
143
['143']
parsed_discourse_facet ['method_citation']
<S sid="146" ssid="11">This paper also re-affirmed that the coarsegrained approach of using all subtrees from a treebank outperforms the fine-grained approach of specifically modeling lexical-syntactic depen dencies (as e.g. in Collins 1999 and Charniak 2000).</S>
original cit marker offset is 0
new cit marker offset is 0



['146']
146
['146']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
<S sid="105" ssid="8">The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.</S>
original cit marker offset is 0
new cit marker offset is 0



['105']
105
['105']
parsed_discourse_facet ['result_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
<S sid="100" ssid="3">In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.</S>
original cit marker offset is 0
new cit marker offset is 0



['100']
100
['100']
parsed_discourse_facet ['result_citation']
    <S sid="30" ssid="27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S>
original cit marker offset is 0
new cit marker offset is 0



['30']
30
['30']
parsed_discourse_facet ['method_citation']
    <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



['140']
140
['140']
parsed_discourse_facet ['result_citation']
    <S sid="30" ssid="27">Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.</S>
original cit marker offset is 0
new cit marker offset is 0



['30']
30
['30']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="74">Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'74'", "'0'"]
'97'
'74'
'0'
['97', '74', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'31'", "'97'"]
'0'
'31'
'97'
['0', '31', '97']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'108'", "'51'"]
'97'
'108'
'51'
['97', '108', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'97'"]
'0'
'51'
'97'
['0', '51', '97']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'108'"]
'97'
'0'
'108'
['97', '0', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'51'"]
'97'
'0'
'51'
['97', '0', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'108'"]
'97'
'0'
'108'
['97', '0', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="101">We will refer to this model as Simplicity-DOP.</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'31'", "'97'"]
'101'
'31'
'97'
['101', '31', '97']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'108'"]
'97'
'0'
'108'
['97', '0', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'51'", "'0'"]
'97'
'51'
'0'
['97', '51', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="101">We will refer to this model as Simplicity-DOP.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'101'", "'108'"]
'0'
'101'
'108'
['0', '101', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'31'", "'108'"]
'0'
'31'
'108'
['0', '31', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'31'", "'108'"]
'0'
'31'
'108'
['0', '31', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'108'", "'51'"]
'97'
'108'
'51'
['97', '108', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="67">We can create a subtree by choosing any possible left subtree and any possible right subtree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'67'"]
'97'
'0'
'67'
['97', '0', '67']
parsed_discourse_facet ['method_citation']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['An Efficient Implementation of a New DOP Model', "That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.", 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:78', 'F:0']
['The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.']
['An Efficient Implementation of a New DOP Model', "Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.", 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.01961', '(95%-conf.int.', '0.01961', '-', '0.01961)']
['system', 'ROUGE-S*', 'Average_P:', '0.03667', '(95%-conf.int.', '0.03667', '-', '0.03667)']
['system', 'ROUGE-S*', 'Average_F:', '0.02555', '(95%-conf.int.', '0.02555', '-', '0.02555)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:300', 'F:11']
['Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.']
['An Efficient Implementation of a New DOP Model', 'This resulted in a statistically consistent model dubbed ML-DOP.', 'The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:300', 'P:78', 'F:0']
['While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.']
['An Efficient Implementation of a New DOP Model', "That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.", 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.00325', '(95%-conf.int.', '0.00325', '-', '0.00325)']
['system', 'ROUGE-S*', 'Average_P:', '0.01425', '(95%-conf.int.', '0.01425', '-', '0.01425)']
['system', 'ROUGE-S*', 'Average_F:', '0.00529', '(95%-conf.int.', '0.00529', '-', '0.00529)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:351', 'F:5']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.', "That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.", 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:78', 'F:0']
['Bod (2000a) solved this problem by training the subtree probabilities by a maximum likelihood procedure based on Expectation-Maximization.']
['An Efficient Implementation of a New DOP Model', 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.', 'We can create a subtree by choosing any possible left subtree and any possible right subtree.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:78', 'F:0']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['An Efficient Implementation of a New DOP Model', "That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.", 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:78', 'F:0']
['DOP1 has a serious bias: its subtree estimator provides more probability to nodes with more subtrees (Bonnema et al. 1999).']
['The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.', "That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.", 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.00326', '(95%-conf.int.', '0.00326', '-', '0.00326)']
['system', 'ROUGE-S*', 'Average_P:', '0.15556', '(95%-conf.int.', '0.15556', '-', '0.15556)']
['system', 'ROUGE-S*', 'Average_F:', '0.00639', '(95%-conf.int.', '0.00639', '-', '0.00639)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:45', 'F:7']
['The derivation with the smallest sum, or highest rank, is taken as the final best derivation producing the best parse tree in Simplicity-DOP.3 Although Bod (2000b) reports that Simplicity DOP is outperformed by Likelihood-DOP, its results are still rather impressive for such a simple model.']
['An Efficient Implementation of a New DOP Model', 'The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.', 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.01587', '(95%-conf.int.', '0.01587', '-', '0.01587)']
['system', 'ROUGE-S*', 'Average_P:', '0.03333', '(95%-conf.int.', '0.03333', '-', '0.03333)']
['system', 'ROUGE-S*', 'Average_F:', '0.02151', '(95%-conf.int.', '0.02151', '-', '0.02151)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:300', 'F:10']
0.0266455552595 0.00466555550372 0.00652666659415





input/ref/Task1/P04-1036_vardha.csv
input/res/Task1/P04-1036.csv
parsing: input/ref/Task1/P04-1036_vardha.csv
    <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
    <S sid="15" ssid="8">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>
original cit marker offset is 0
new cit marker offset is 0



["'15'"]
'15'
['15']
parsed_discourse_facet ['method_citation']
<S sid="45" ssid="1">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>
original cit marker offset is 0
new cit marker offset is 0



["'45'"]
'45'
['45']
parsed_discourse_facet ['method_citation']
<S sid="66" ssid="22">It uses the glosses of semantically related (according to WordNet) senses too. jcn (Jiang and Conrath, 1997) This score uses corpus data to populate classes (synsets) in the WordNet hierarchy with frequency counts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'66'"]
'66'
['66']
parsed_discourse_facet ['method_citation']
 <S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="101" ssid="30">Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'"]
'101'
['101']
parsed_discourse_facet ['method_citation']
 <S sid="126" ssid="3">We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking.</S>
original cit marker offset is 0
new cit marker offset is 0



["'126'"]
'126'
['126']
parsed_discourse_facet ['method_citation']
 <S sid="115" ssid="13">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.</S>
original cit marker offset is 0
new cit marker offset is 0



["'115'"]
'115'
['115']
parsed_discourse_facet ['method_citation']
  <S sid="89" ssid="18">Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.</S>
original cit marker offset is 0
new cit marker offset is 0



["'89'"]
'89'
['89']
parsed_discourse_facet ['method_citation']
 <S sid="137" ssid="14">Additionally, we evaluated our method quantitatively using the Subject Field Codes (SFC) resource (Magnini and Cavagli`a, 2000) which annotates WordNet synsets with domain labels.</S>
original cit marker offset is 0
new cit marker offset is 0



["'137'"]
'137'
['137']
parsed_discourse_facet ['method_citation']
75 ssid="4">We generated a thesaurus entry for all polysemous nouns which occurred in SemCor with a frequency 2, and in the BNC with a frequency 10 in the grammatical relations listed in section 2.1 above.</S>
original cit marker offset is 0
new cit marker offset is 0



["'75'"]
'75'
['75']
Error in Reference Offset
 <S sid="155" ssid="3">A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
  <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
 <S sid="155" ssid="3">A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
<S sid="68" ssid="24">We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'68'"]
'68'
['68']
parsed_discourse_facet ['method_citation']
    <S sid="13" ssid="6">The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.</S>
original cit marker offset is 0
new cit marker offset is 0



["'13'"]
'13'
['13']
parsed_discourse_facet ['method_citation']
  <S sid="8" ssid="1">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>
original cit marker offset is 0
new cit marker offset is 0



["'8'"]
'8'
['8']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/P04-1036.csv
<S ssid="1" sid="107">To disambiguate senses a system should take context into account.</S><S ssid="1" sid="1">word sense disambiguation the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'107'", "'1'", "'153'"]
'107'
'1'
'153'
['107', '1', '153']
parsed_discourse_facet ['result_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="162">It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'162'"]
'179'
'178'
'162'
['179', '178', '162']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="34">In these each target word is entered with an ordered list of “nearest neighbours”.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'34'"]
'179'
'153'
'34'
['179', '153', '34']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="0">Finding Predominant Word Senses in Untagged Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'0'"]
'179'
'4'
'0'
['179', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'153'"]
'179'
'4'
'153'
['179', '4', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="169">This method obtains precision of 61% and recall 51%.</S><S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'169'", "'179'", "'4'"]
'169'
'179'
'4'
['169', '179', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'153'"]
'179'
'4'
'153'
['179', '4', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="0">Finding Predominant Word Senses in Untagged Text</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'0'"]
'179'
'4'
'0'
['179', '4', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="62">2 The WordNet Similarity package supports a range of WordNet similarity scores.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'62'", "'4'"]
'179'
'62'
'4'
['179', '62', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'179'", "'178'"]
'4'
'179'
'178'
['4', '179', '178']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'4'", "'36'"]
'179'
'4'
'36'
['179', '4', '36']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'153'"]
'179'
'178'
'153'
['179', '178', '153']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'178'", "'36'"]
'179'
'178'
'36'
['179', '178', '36']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S><S ssid="1" sid="162">It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text.</S><S ssid="1" sid="178">We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.</S>
original cit marker offset is 0
new cit marker offset is 0



["'4'", "'162'", "'178'"]
'4'
'162'
'178'
['4', '162', '178']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'4'"]
'179'
'153'
'4'
['179', '153', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="4">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'4'"]
'179'
'153'
'4'
['179', '153', '4']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="179">We use an automatically acquired thesaurus and a WordNet Similarity measure.</S><S ssid="1" sid="153">Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.</S><S ssid="1" sid="36">Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.</S>
original cit marker offset is 0
new cit marker offset is 0



["'179'", "'153'", "'36'"]
'179'
'153'
'36'
['179', '153', '36']
parsed_discourse_facet ['method_citation']
['In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).']
['Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.', u'In these each target word is entered with an ordered list of \u201cnearest neighbours\u201d.']
['system', 'ROUGE-S*', 'Average_R:', '0.03692', '(95%-conf.int.', '0.03692', '-', '0.03692)']
['system', 'ROUGE-S*', 'Average_P:', '0.11429', '(95%-conf.int.', '0.11429', '-', '0.11429)']
['system', 'ROUGE-S*', 'Average_F:', '0.05581', '(95%-conf.int.', '0.05581', '-', '0.05581)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:325', 'P:105', 'F:12']
['The high performance of the first sense baseline is due to the skewed frequency distribution of word senses.']
['Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.', 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.']
['system', 'ROUGE-S*', 'Average_R:', '0.00535', '(95%-conf.int.', '0.00535', '-', '0.00535)']
['system', 'ROUGE-S*', 'Average_P:', '0.05455', '(95%-conf.int.', '0.05455', '-', '0.05455)']
['system', 'ROUGE-S*', 'Average_F:', '0.00974', '(95%-conf.int.', '0.00974', '-', '0.00974)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:55', 'F:3']
['Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.']
['Finding Predominant Word Senses in Untagged Text', 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.']
['system', 'ROUGE-S*', 'Average_R:', '0.03134', '(95%-conf.int.', '0.03134', '-', '0.03134)']
['system', 'ROUGE-S*', 'Average_P:', '0.10476', '(95%-conf.int.', '0.10476', '-', '0.10476)']
['system', 'ROUGE-S*', 'Average_F:', '0.04825', '(95%-conf.int.', '0.04825', '-', '0.04825)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:105', 'F:11']
['A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.']
['We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.', 'It only requires raw text from the given domain and because of this it can easily be applied to a new domain, or sense inventory, given sufficient text.', 'We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.']
['system', 'ROUGE-S*', 'Average_R:', '0.02853', '(95%-conf.int.', '0.02853', '-', '0.02853)']
['system', 'ROUGE-S*', 'Average_P:', '0.12418', '(95%-conf.int.', '0.12418', '-', '0.12418)']
['system', 'ROUGE-S*', 'Average_F:', '0.04640', '(95%-conf.int.', '0.04640', '-', '0.04640)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:153', 'F:19']
['We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.']
['Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.', 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:105', 'F:0']
['Thus, if we used the sense ranking as a heuristic for an &#8220;all nouns&#8221; task we would expect to get precision in the region of 60%.']
['This method obtains precision of 61% and recall 51%.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.', 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:351', 'P:55', 'F:0']
['Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense, and is much more efficient than lesk, given the precompilation of the IC files.']
['2 The WordNet Similarity package supports a range of WordNet similarity scores.', 'We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.']
['system', 'ROUGE-S*', 'Average_R:', '0.00690', '(95%-conf.int.', '0.00690', '-', '0.00690)']
['system', 'ROUGE-S*', 'Average_P:', '0.01754', '(95%-conf.int.', '0.01754', '-', '0.01754)']
['system', 'ROUGE-S*', 'Average_F:', '0.00990', '(95%-conf.int.', '0.00990', '-', '0.00990)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:171', 'F:3']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.', 'Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.']
['system', 'ROUGE-S*', 'Average_R:', '0.00460', '(95%-conf.int.', '0.00460', '-', '0.00460)']
['system', 'ROUGE-S*', 'Average_P:', '0.03030', '(95%-conf.int.', '0.03030', '-', '0.03030)']
['system', 'ROUGE-S*', 'Average_F:', '0.00798', '(95%-conf.int.', '0.00798', '-', '0.00798)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:66', 'F:2']
['The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.']
['Distributional similarity is a measure indicating the degree that two words, a word and its neighbour, occur in similar contexts.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.', 'We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:66', 'F:0']
['A major benefit of our work, rather than reliance on hand-tagged training data such as SemCor, is that this method permits us to produce predominant senses for the domain and text type required.']
['Most research in WSD concentrates on using contextual features, typically neighbouring words, to help determine the correct sense of a target word.', 'We use an automatically acquired thesaurus and a WordNet Similarity measure.', 'We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet.']
['system', 'ROUGE-S*', 'Average_R:', '0.01149', '(95%-conf.int.', '0.01149', '-', '0.01149)']
['system', 'ROUGE-S*', 'Average_P:', '0.03268', '(95%-conf.int.', '0.03268', '-', '0.03268)']
['system', 'ROUGE-S*', 'Average_F:', '0.01701', '(95%-conf.int.', '0.01701', '-', '0.01701)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:435', 'P:153', 'F:5']
0.0478299995217 0.0125129998749 0.0195089998049





input/ref/Task1/A00-2018_sweta.csv
input/res/Task1/A00-2018.csv
parsing: input/ref/Task1/A00-2018_sweta.csv
 <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["17'"]
17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["5'"]
5'
['5']
parsed_discourse_facet ['method_citation']
 <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["17'"]
17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="120" ssid="11">Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &amp;quot;correct&amp;quot;, and statistics were collected on the resulting parses.</S>
original cit marker offset is 0
new cit marker offset is 0



["120'"]
120'
['120']
parsed_discourse_facet ['method_citation']
<S sid="119" ssid="10">(It is &amp;quot;soft&amp;quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.)</S>
original cit marker offset is 0
new cit marker offset is 0



["119'"]
119'
['119']
parsed_discourse_facet ['method_citation']
<S sid="95" ssid="6">We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.</S>
original cit marker offset is 0
new cit marker offset is 0



["95'"]
95'
['95']
parsed_discourse_facet ['method_citation']
 <S sid="175" ssid="2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["175'"]
175'
['175']
parsed_discourse_facet ['method_citation']
<S sid="92" ssid="3">For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.</S>
original cit marker offset is 0
new cit marker offset is 0



["92'"]
92'
['92']
parsed_discourse_facet ['method_citation']
<S sid="1" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["1'"]
1'
['1']
parsed_discourse_facet ['method_citation']
<S sid="126" ssid="17">This is indicated in Figure 2, where the model labeled &amp;quot;Best&amp;quot; has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["126'"]
126'
['126']
parsed_discourse_facet ['method_citation']
<S sid="12" ssid="1">The model assigns a probability to a parse by a top-down process of considering each constituent c in Ir and for each c first guessing the pre-terminal of c, t(c) (t for &amp;quot;tag&amp;quot;), then the lexical head of c, h(c), and then the expansion of c into further constituents e(c).</S>
original cit marker offset is 0
new cit marker offset is 0



["12'"]
12'
['12']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["174'"]
174'
['174']
parsed_discourse_facet ['method_citation']
<S sid="63" ssid="32">As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre-terminal and head of the parent of c (denoted as lp, tp, hp), the label of c's left sibling (lb for &amp;quot;before&amp;quot;), and the label of the grandparent of c (la).</S>
original cit marker offset is 0
new cit marker offset is 0



["63'"]
63'
['63']
parsed_discourse_facet ['method_citation']
<S sid="78" ssid="47">With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.</S>
original cit marker offset is 0
new cit marker offset is 0



["78'"]
78'
['78']
parsed_discourse_facet ['method_citation']
 <S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["91'"]
91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="180" ssid="7">From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head.</S>
original cit marker offset is 0
new cit marker offset is 0



["180'"]
180'
['180']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2018.csv
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="7">Following [5,10], our parser is based upon a probabilistic generative model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'0'", "'7'"]
'90'
'0'
'7'
['90', '0', '7']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="55">This method is known as &quot;deleted interpolation&quot; smoothing.</S><S ssid="1" sid="130">This parser achieves an average precision/recall of 86.2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'55'", "'130'"]
'0'
'55'
'130'
['0', '55', '130']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S><S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'90'", "'174'"]
'0'
'90'
'174'
['0', '90', '174']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'174'", "'90'"]
'0'
'174'
'90'
['0', '174', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S ssid="1" sid="20">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar, and a right-hand side that is a sequence of one or more such symbols.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'91'", "'20'"]
'174'
'91'
'20'
['174', '91', '20']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="130">This parser achieves an average precision/recall of 86.2%.</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'130'", "'91'"]
'174'
'130'
'91'
['174', '130', '91']
parsed_discourse_facet ['method_citation']
['In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.01429', '(95%-conf.int.', '0.01429', '-', '0.01429)']
['system', 'ROUGE-S*', 'Average_P:', '0.02857', '(95%-conf.int.', '0.02857', '-', '0.02857)']
['system', 'ROUGE-S*', 'Average_F:', '0.01905', '(95%-conf.int.', '0.01905', '-', '0.01905)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:105', 'F:3']
['We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal treebank.']
['A Maximum-Entropy-Inspired Parser *', 'We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ', 'We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.']
['system', 'ROUGE-S*', 'Average_R:', '0.14715', '(95%-conf.int.', '0.14715', '-', '0.14715)']
['system', 'ROUGE-S*', 'Average_P:', '0.15556', '(95%-conf.int.', '0.15556', '-', '0.15556)']
['system', 'ROUGE-S*', 'Average_F:', '0.15123', '(95%-conf.int.', '0.15123', '-', '0.15123)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:630', 'F:98']
['Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as &quot;correct&quot;, and statistics were collected on the resulting parses.']
['We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.', 'Following [5,10], our parser is based upon a probabilistic generative model.', 'A Maximum-Entropy-Inspired Parser *']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:253', 'P:171', 'F:0']
['This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:78', 'F:0']
['As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_P:', '0.00866', '(95%-conf.int.', '0.00866', '-', '0.00866)']
['system', 'ROUGE-S*', 'Average_F:', '0.00907', '(95%-conf.int.', '0.00907', '-', '0.00907)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:231', 'F:2']
['We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre-terminal is realized using a previously unobserved word.']
['This method is known as &quot;deleted interpolation&quot; smoothing.', 'A Maximum-Entropy-Inspired Parser *', 'This parser achieves an average precision/recall of 86.2%.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:136', 'P:190', 'F:0']
['In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.01429', '(95%-conf.int.', '0.01429', '-', '0.01429)']
['system', 'ROUGE-S*', 'Average_P:', '0.02857', '(95%-conf.int.', '0.02857', '-', '0.02857)']
['system', 'ROUGE-S*', 'Average_F:', '0.01905', '(95%-conf.int.', '0.01905', '-', '0.01905)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:105', 'F:3']
['(It is &quot;soft&quot; clustering in that a word can belong to more than one cluster with different weights - the weights express the probability of producing the word given that one is going to produce a word from that cluster.)']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:120', 'F:0']
['We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.27143', '(95%-conf.int.', '0.27143', '-', '0.27143)']
['system', 'ROUGE-S*', 'Average_P:', '0.06025', '(95%-conf.int.', '0.06025', '-', '0.06025)']
['system', 'ROUGE-S*', 'Average_F:', '0.09862', '(95%-conf.int.', '0.09862', '-', '0.09862)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:946', 'F:57']
["From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre-terminal before guessing the head."]
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.00476', '(95%-conf.int.', '0.00476', '-', '0.00476)']
['system', 'ROUGE-S*', 'Average_P:', '0.00362', '(95%-conf.int.', '0.00362', '-', '0.00362)']
['system', 'ROUGE-S*', 'Average_F:', '0.00412', '(95%-conf.int.', '0.00412', '-', '0.00412)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:276', 'F:1']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '1.00000', '(95%-conf.int.', '1.00000', '-', '1.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.51724', '(95%-conf.int.', '0.51724', '-', '0.51724)']
['system', 'ROUGE-S*', 'Average_F:', '0.68182', '(95%-conf.int.', '0.68182', '-', '0.68182)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:406', 'F:210']
['With some prior knowledge, non-zero values can greatly speed up this process because fewer iterations are required for convergence.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:45', 'F:0']
0.0668724994427 0.121786665652 0.0819133326507





input/ref/Task1/E03-1005_sweta.csv
input/res/Task1/E03-1005.csv
parsing: input/ref/Task1/E03-1005_sweta.csv
<S sid="20" ssid="17">Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).</S>
original cit marker offset is 0
new cit marker offset is 0



["20'"]
20'
['20']
parsed_discourse_facet ['method_citation']
 <S sid="74" ssid="26">Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.</S>
original cit marker offset is 0
new cit marker offset is 0



["74'"]
74'
['74']
parsed_discourse_facet ['method_citation']
<S sid="44" ssid="41">But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["44'"]
44'
['44']
parsed_discourse_facet ['method_citation']
<S sid="143" ssid="8">While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.</S>
original cit marker offset is 0
new cit marker offset is 0



["143'"]
143'
['143']
parsed_discourse_facet ['method_citation']
<S sid="145" ssid="10">This paper showed that a PCFG-reduction of DOP in combination with a new notion of the best parse tree results in fast processing times and very competitive accuracy on the Wall Street Journal treebank.</S>
original cit marker offset is 0
new cit marker offset is 0



["145'"]
145'
['145']
parsed_discourse_facet ['method_citation']
<S sid="134" ssid="15">This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.</S>
original cit marker offset is 0
new cit marker offset is 0



["134'"]
134'
['134']
parsed_discourse_facet ['method_citation']
<S sid="22" ssid="19">DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).</S>
original cit marker offset is 0
new cit marker offset is 0



["22'"]
22'
['22']
parsed_discourse_facet ['method_citation']
<S sid="133" ssid="14">It should be mentioned that the best precision and recall scores reported in Bod (2001) are slightly better than the ones reported here (the difference is only 0.2% for sentences 100 words).</S>
original cit marker offset is 0
new cit marker offset is 0



["133'"]
133'
['133']
parsed_discourse_facet ['method_citation']
<S sid="25" ssid="22">Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier &amp; Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002).</S>
original cit marker offset is 0
new cit marker offset is 0



["25'"]
25'
['25']
parsed_discourse_facet ['method_citation']
<S sid="38" ssid="35">Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003).</S>
original cit marker offset is 0
new cit marker offset is 0



["38'"]
38'
['38']
parsed_discourse_facet ['method_citation']
 <S sid="100" ssid="3">In Bod (2000b), an alternative notion for the best parse tree was proposed based on a simplicity criterion: instead of producing the most probable tree, this model produced the tree generated by the shortest derivation with the fewest training subtrees.</S>
original cit marker offset is 0
new cit marker offset is 0



["100'"]
100'
['100']
parsed_discourse_facet ['method_citation']
<S sid="32" ssid="29">However, ML-DOP suffers from overlearning if the subtrees are trained on the same treebank trees as they are derived from.</S>
original cit marker offset is 0
new cit marker offset is 0



["32'"]
32'
['32']
parsed_discourse_facet ['method_citation']
 <S sid="130" ssid="11">While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996).</S>
original cit marker offset is 0
new cit marker offset is 0



["130'"]
130'
['130']
parsed_discourse_facet ['method_citation']
 <S sid="140" ssid="5">The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.</S>
original cit marker offset is 0
new cit marker offset is 0



["140'"]
140'
['140']
parsed_discourse_facet ['method_citation']
<S sid="27" ssid="24">Goodman (1996, 1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.</S>
original cit marker offset is 0
new cit marker offset is 0



["27'"]
27'
['27']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/E03-1005.csv
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="74">Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'74'", "'0'"]
'97'
'74'
'0'
['97', '74', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'31'", "'97'"]
'0'
'31'
'97'
['0', '31', '97']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'108'", "'51'"]
'97'
'108'
'51'
['97', '108', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'51'", "'97'"]
'0'
'51'
'97'
['0', '51', '97']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'108'"]
'97'
'0'
'108'
['97', '0', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'51'"]
'97'
'0'
'51'
['97', '0', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'108'"]
'97'
'0'
'108'
['97', '0', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="101">We will refer to this model as Simplicity-DOP.</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'", "'31'", "'97'"]
'101'
'31'
'97'
['101', '31', '97']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'108'"]
'97'
'0'
'108'
['97', '0', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'51'", "'0'"]
'97'
'51'
'0'
['97', '51', '0']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="101">We will refer to this model as Simplicity-DOP.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'101'", "'108'"]
'0'
'101'
'108'
['0', '101', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'31'", "'108'"]
'0'
'31'
'108'
['0', '31', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="31">This resulted in a statistically consistent model dubbed ML-DOP.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'31'", "'108'"]
'0'
'31'
'108'
['0', '31', '108']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="108">The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.</S><S ssid="1" sid="51">That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'108'", "'51'"]
'97'
'108'
'51'
['97', '108', '51']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="97">In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.</S><S ssid="1" sid="0">An Efficient Implementation of a New DOP Model</S><S ssid="1" sid="67">We can create a subtree by choosing any possible left subtree and any possible right subtree.</S>
original cit marker offset is 0
new cit marker offset is 0



["'97'", "'0'", "'67'"]
'97'
'0'
'67'
['97', '0', '67']
parsed_discourse_facet ['method_citation']
['This may be explained by the fact our best results in Bod (2001) were obtained by testing various subtree restrictions until the highest accuracy was obtained, while in the current experiment we used all subtrees as given by the PCFG-reduction.']
['An Efficient Implementation of a New DOP Model', "That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.", 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.00065', '(95%-conf.int.', '0.00065', '-', '0.00065)']
['system', 'ROUGE-S*', 'Average_P:', '0.00735', '(95%-conf.int.', '0.00735', '-', '0.00735)']
['system', 'ROUGE-S*', 'Average_F:', '0.00119', '(95%-conf.int.', '0.00119', '-', '0.00119)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:136', 'F:1']
['Thus the major innovations of DOP are: 2. the use of arbitrarily large fragments rather than restricted ones Both have gained or are gaining wide usage, and are also becoming relevant for theoretical linguistics (see Bod et al. 2003a).']
['An Efficient Implementation of a New DOP Model', "Goodman's main theorem is that this construction produces PCFG derivations isomorphic to DOP derivations with equal probability.", 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:561', 'P:153', 'F:0']
["While the PCFG reduction of Bod (2001) obtains state-of-the-art results on the WSJ, comparable to Charniak (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996)."]
['An Efficient Implementation of a New DOP Model', 'This resulted in a statistically consistent model dubbed ML-DOP.', 'The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:300', 'P:190', 'F:0']
['While SL-DOP and LS-DOP have been compared before in Bod (2002), especially in the context of musical parsing, this paper presents the The DOP approach is based on two distinctive features: (1) the use of corpus fragments rather than grammar rules, and (2) the use of arbitrarily large fragments rather than restricted ones.']
['An Efficient Implementation of a New DOP Model', "That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.", 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.00325', '(95%-conf.int.', '0.00325', '-', '0.00325)']
['system', 'ROUGE-S*', 'Average_P:', '0.01425', '(95%-conf.int.', '0.01425', '-', '0.01425)']
['system', 'ROUGE-S*', 'Average_F:', '0.00529', '(95%-conf.int.', '0.00529', '-', '0.00529)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:351', 'F:5']
['The highest accuracy is obtained by SL-DOP at 12 n 14: an LP of 90.8% and an LR of 90.7%.']
['The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.', "That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.", 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:78', 'F:0']
['Goodman (1996, 1998) developed a polynomial time PCFG-reduction of DOP1 whose size is linear in the size of the training set, thus converting the exponential number of subtrees to a compact grammar.']
['An Efficient Implementation of a New DOP Model', 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.', 'We can create a subtree by choosing any possible left subtree and any possible right subtree.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:378', 'P:190', 'F:0']
["Although Bod's method obtains very competitive results on the Wall Street Journal (WSJ) task, the parsing time was reported to be over 200 seconds per sentence (Bod 2003)."]
['An Efficient Implementation of a New DOP Model', "That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.", 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:1540', 'P:153', 'F:0']
["But while Bod's estimator obtains state-of-the-art results on the WSJ, comparable to Charniak (2000) and Collins (2000), Bonnema et al. 's estimator performs worse and is comparable to Collins (1996)."]
['The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.', "That is, the probability of a subtree t is taken as the number of occurrences of t in the training set, I t I, divided by the total number of occurrences of all subtrees t' with the same root label as t. Let r(t) return the root label of t: The probability of a derivation ti0...0tn is computed by the product of the probabilities of its subtrees ti: = HP (t1) An important feature of DOP1 is that there may be several derivations that generate the same parse tree.", 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.00047', '(95%-conf.int.', '0.00047', '-', '0.00047)']
['system', 'ROUGE-S*', 'Average_P:', '0.00526', '(95%-conf.int.', '0.00526', '-', '0.00526)']
['system', 'ROUGE-S*', 'Average_F:', '0.00086', '(95%-conf.int.', '0.00086', '-', '0.00086)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:2145', 'P:190', 'F:1']
['DOP1 combines subtrees from a treebank by means of node-substitution and computes the probability of a tree from the normalized frequencies of the subtrees (see Section 2 for a full definition).']
['An Efficient Implementation of a New DOP Model', 'The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.', 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.01111', '(95%-conf.int.', '0.01111', '-', '0.01111)']
['system', 'ROUGE-S*', 'Average_P:', '0.05147', '(95%-conf.int.', '0.05147', '-', '0.05147)']
['system', 'ROUGE-S*', 'Average_F:', '0.01828', '(95%-conf.int.', '0.01828', '-', '0.01828)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:136', 'F:7']
["Many implementations of DOP1 therefore estimate the most probable parse by Monte Carlo techniques (Bod 1998; Chappelier & Rajman 2000), or by Viterbi n-best search (Bod 2001), or by restricting the set of subtrees (Sima'an 1999; Chappelier et al. 2002)."]
['An Efficient Implementation of a New DOP Model', 'The underlying idea of combining LikelihoodDOP and Simplicity-DOP is that the parser selects the simplest tree from among the n most probable trees, where n is a free parameter.', 'In this paper, we will estimate the most probable parse by computing the 10,000 most probable derivations by means of Viterbi n-best, from which the most likely parse is estimated by summing up the probabilities of the derivations that generate the same parse.']
['system', 'ROUGE-S*', 'Average_R:', '0.01587', '(95%-conf.int.', '0.01587', '-', '0.01587)']
['system', 'ROUGE-S*', 'Average_P:', '0.03333', '(95%-conf.int.', '0.03333', '-', '0.03333)']
['system', 'ROUGE-S*', 'Average_F:', '0.02151', '(95%-conf.int.', '0.02151', '-', '0.02151)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:630', 'P:300', 'F:10']
0.0111659998883 0.00313499996865 0.00471299995287





input/ref/Task1/A00-2018_vardha.csv
input/res/Task1/A00-2018.csv
parsing: input/ref/Task1/A00-2018_vardha.csv
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="5" ssid="1">We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank.</S>
original cit marker offset is 0
new cit marker offset is 0



["'5'"]
'5'
['5']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'91'"]
'91'
['91']
parsed_discourse_facet ['method_citation']
<S sid="39" ssid="8">In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.</S>
original cit marker offset is 0
new cit marker offset is 0



["'39'"]
'39'
['39']
parsed_discourse_facet ['method_citation']
<S sid="162" ssid="53">Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'"]
'162'
['162']
parsed_discourse_facet ['method_citation']
 <S sid="87" ssid="56">In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17].</S>
original cit marker offset is 0
new cit marker offset is 0



["'87'"]
'87'
['87']
parsed_discourse_facet ['method_citation']
<S sid="91" ssid="2">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'91'"]
'91'
['91']
parsed_discourse_facet ['method_citation']
 <S sid="176" ssid="3">That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.</S>
original cit marker offset is 0
new cit marker offset is 0



["'176'"]
'176'
['176']
parsed_discourse_facet ['method_citation']
<S sid="174" ssid="1">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'"]
'174'
['174']
parsed_discourse_facet ['method_citation']
 <S sid="101" ssid="12">In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning).</S>
original cit marker offset is 0
new cit marker offset is 0



["'101'"]
'101'
['101']
parsed_discourse_facet ['method_citation']
<S sid="155" ssid="46">For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.</S>
original cit marker offset is 0
new cit marker offset is 0



["'155'"]
'155'
['155']
parsed_discourse_facet ['method_citation']
  <S sid="17" ssid="6">In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus.</S>
original cit marker offset is 0
new cit marker offset is 0



["'17'"]
'17'
['17']
parsed_discourse_facet ['method_citation']
<S sid="162" ssid="53">Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.</S>
original cit marker offset is 0
new cit marker offset is 0



["'162'"]
'162'
['162']
parsed_discourse_facet ['method_citation']
 <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
 <S sid="79" ssid="48">We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.</S>
original cit marker offset is 0
new cit marker offset is 0



["'79'"]
'79'
['79']
parsed_discourse_facet ['method_citation']
    <S sid="40" ssid="9">In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.</S>
original cit marker offset is 0
new cit marker offset is 0



["'40'"]
'40'
['40']
parsed_discourse_facet ['method_citation']
    <S sid="175" ssid="2">This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].</S>
original cit marker offset is 0
new cit marker offset is 0



["'175'"]
'175'
['175']
parsed_discourse_facet ['method_citation']
parsing: input/res/Task1/A00-2018.csv
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="7">Following [5,10], our parser is based upon a probabilistic generative model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'90'", "'0'", "'7'"]
'90'
'0'
'7'
['90', '0', '7']
parsed_discourse_facet ['aim_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="55">This method is known as &quot;deleted interpolation&quot; smoothing.</S><S ssid="1" sid="130">This parser achieves an average precision/recall of 86.2%.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'55'", "'130'"]
'0'
'55'
'130'
['0', '55', '130']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S><S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'90'", "'174'"]
'0'
'90'
'174'
['0', '90', '174']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'0'", "'174'", "'90'"]
'0'
'174'
'90'
['0', '174', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="90">We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'90'"]
'174'
'0'
'90'
['174', '0', '90']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="0">A Maximum-Entropy-Inspired Parser *</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'0'", "'91'"]
'174'
'0'
'91'
['174', '0', '91']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S><S ssid="1" sid="20">In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar, and a right-hand side that is a sequence of one or more such symbols.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'91'", "'20'"]
'174'
'91'
'20'
['174', '91', '20']
parsed_discourse_facet ['method_citation']
<S ssid="1" sid="174">We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.</S><S ssid="1" sid="130">This parser achieves an average precision/recall of 86.2%.</S><S ssid="1" sid="91">As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.</S>
original cit marker offset is 0
new cit marker offset is 0



["'174'", "'130'", "'91'"]
'174'
'130'
'91'
['174', '130', '91']
parsed_discourse_facet ['method_citation']
['We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.27143', '(95%-conf.int.', '0.27143', '-', '0.27143)']
['system', 'ROUGE-S*', 'Average_P:', '0.06025', '(95%-conf.int.', '0.06025', '-', '0.06025)']
['system', 'ROUGE-S*', 'Average_F:', '0.09862', '(95%-conf.int.', '0.09862', '-', '0.09862)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:946', 'F:57']
['In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does.']
['We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.', 'Following [5,10], our parser is based upon a probabilistic generative model.', 'A Maximum-Entropy-Inspired Parser *']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:253', 'P:55', 'F:0']
["In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi's maximum-entropy parser [17]."]
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:55', 'F:0']
['In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub-features.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:45', 'F:0']
['Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.']
['This method is known as &quot;deleted interpolation&quot; smoothing.', 'A Maximum-Entropy-Inspired Parser *', 'This parser achieves an average precision/recall of 86.2%.']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:136', 'P:28', 'F:0']
['As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.00952', '(95%-conf.int.', '0.00952', '-', '0.00952)']
['system', 'ROUGE-S*', 'Average_P:', '0.00866', '(95%-conf.int.', '0.00866', '-', '0.00866)']
['system', 'ROUGE-S*', 'Average_F:', '0.00907', '(95%-conf.int.', '0.00907', '-', '0.00907)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:231', 'F:2']
['That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.']
['A Maximum-Entropy-Inspired Parser *', 'We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ', 'We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation.']
['system', 'ROUGE-S*', 'Average_R:', '0.01502', '(95%-conf.int.', '0.01502', '-', '0.01502)']
['system', 'ROUGE-S*', 'Average_P:', '0.04762', '(95%-conf.int.', '0.04762', '-', '0.04762)']
['system', 'ROUGE-S*', 'Average_F:', '0.02283', '(95%-conf.int.', '0.02283', '-', '0.02283)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:666', 'P:210', 'F:10']
['We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &quot;standard&quot; sections of the Wall Street Journal tree-bank.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.27143', '(95%-conf.int.', '0.27143', '-', '0.27143)']
['system', 'ROUGE-S*', 'Average_P:', '0.06025', '(95%-conf.int.', '0.06025', '-', '0.06025)']
['system', 'ROUGE-S*', 'Average_F:', '0.09862', '(95%-conf.int.', '0.09862', '-', '0.09862)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:946', 'F:57']
['This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:78', 'F:0']
['Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:28', 'F:0']
['We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum-entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.']
['We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length ']
['system', 'ROUGE-S*', 'Average_R:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_P:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Average_F:', '0.00000', '(95%-conf.int.', '0.00000', '-', '0.00000)']
['system', 'ROUGE-S*', 'Eval', 'test.system', 'R:210', 'P:210', 'F:0']
0.0160709089448 0.0515818177129 0.0208309089015
