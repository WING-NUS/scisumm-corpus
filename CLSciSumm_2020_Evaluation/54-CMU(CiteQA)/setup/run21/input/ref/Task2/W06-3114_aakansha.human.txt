The paper'Manual and Automatic Evaluation of Machine Translation between European Languages'by Philipp Koehn and Christof Monz talks about a shared task to evaluate machine translation performance.They assembled various forms of data and resources: a baseline MT system,language models, prepared training and test sets,resulting in actual machine translation output from several state-of-the-art systems and manual evaluations.Training and testing is based on the Europarl corpus.For translation they used automatic evaluation and manual evaluation.For automatic evaluation they used BLEU.They were not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.
