Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W11-2123,W11-2138,0,"Heafield, 2011",0,"We used common tools for phrase-based translation? Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","We used common tools for phrase-based translation Moses (Koehn et al, 2007) decoder and tools, SRILM (Stolcke, 2002) and KenLM (Heafield, 2011) for language modelling and GIZA++ (Och and Ney, 2000) for word alignments","['199', '264', '21', '268', '98']","['    <S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>\n', '    <S sid=""264"" ssid=""6"">For even larger models, storing counts (Talbot and Osborne, 2007; Pauls and Klein, 2011; Guthrie and Hepple, 2010) is a possibility.</S>\n', '    <S sid=""21"" ssid=""16"">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S>\n', '    <S sid=""268"" ssid=""10"">For example, syntactic decoders (Koehn et al., 2007; Dyer et al., 2010; Li et al., 2009) perform dynamic programming parametrized by both backward- and forward-looking state.</S>\n', '    <S sid=""98"" ssid=""2"">It is generally considered to be fast (Pauls 29 &#8722; 1 probabilities and 2\' &#8722; 2 non-zero backoffs. and Klein, 2011), with a default implementation based on hash tables within each trie node.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Method_Citation', 'Result_Citation']"
2,W11-2123,P14-2022,0,"Heafield, 2011",0,"The language model was com piled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run","['223', '76', '124', '232', '268']","['    <S sid=""223"" ssid=""42"">The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run.</S>\n', '    <S sid=""76"" ssid=""54"">This technique was introduced by Clarkson and Rosenfeld (1997) and is also implemented by IRSTLM and BerkeleyLM&#8217;s compressed option.</S>\n', '    <S sid=""124"" ssid=""28"">Lossy compressed models RandLM (Talbot and Osborne, 2007) and Sheffield (Guthrie and Hepple, 2010) offer better memory consumption at the expense of CPU and accuracy.</S>\n', '    <S sid=""232"" ssid=""51"">Memory usage is likely much lower than ours. fThe original paper (Germann et al., 2009) provided only 2s of query timing and compared with SRI when it exceeded available RAM.</S>\n', '    <S sid=""268"" ssid=""10"">For example, syntactic decoders (Koehn et al., 2007; Dyer et al., 2010; Li et al., 2009) perform dynamic programming parametrized by both backward- and forward-looking state.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Method_Citation']"
3,W11-2123,W12-3145,0,"Heafield, 2011",0,"Thus given afragment tf consisting of a sequence of target to kens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning.2 While this increases the number ofLM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","Thus given a fragment tf consisting of a sequence of target tokens, we compute LM scores for (i) < s& gt ;tf, (ii )tf and (iii )tf < /s& gt; and use the best score (only) for pruning. While this increases the number of LM queries, we exploit the language model state in formation in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states","['1', '93', '66', '152', '273']","['    <S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>\n', '    <S sid=""93"" ssid=""71"">The cost of storing these averages, in bits, is Because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster.</S>\n', '    <S sid=""66"" ssid=""44"">This has the effect of randomly permuting vocabulary identifiers, meeting the requirements of interpolation search when vocabulary identifiers are used as keys.</S>\n', '    <S sid=""152"" ssid=""24"">All language model queries issued by machine translation decoders follow a left-to-right pattern, starting with either the begin of sentence token or null context for mid-sentence fragments.</S>\n', '    <S sid=""273"" ssid=""15"">This would result in better rest cost estimation and better pruning.10 In general, tighter, but well factored, integration between the decoder and language model should produce a significant speed improvement.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
4,W11-2123,W12-3131,0,"Heafield, 2011",0,"Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","Our translation system uses cdec (Dyer et al,2010), an implementation of the hierarchical phrase based translation model (Chiang, 2007) that uses the KenLM library (Heafield, 2011) for language model inference","['21', '76', '113', '51', '98']","['    <S sid=""21"" ssid=""16"">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S>\n', '    <S sid=""76"" ssid=""54"">This technique was introduced by Clarkson and Rosenfeld (1997) and is also implemented by IRSTLM and BerkeleyLM&#8217;s compressed option.</S>\n', '    <S sid=""113"" ssid=""17"">Our code has been publicly available and intergrated into Moses since October 2010.</S>\n', '    <S sid=""51"" ssid=""29"">This differs from other implementations (Stolcke, 2002; Pauls and Klein, 2011) that use hash tables as nodes in a trie, as explained in the next section.</S>\n', '    <S sid=""98"" ssid=""2"">It is generally considered to be fast (Pauls 29 &#8722; 1 probabilities and 2\' &#8722; 2 non-zero backoffs. and Klein, 2011), with a default implementation based on hash tables within each trie node.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
5,W11-2123,W12-3154,0,"Heafield, 2011",0,"The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","The three data sets in use in this paper are summarised in Table 1.The translation systems consisted of phrase tables and lexicalised reordering tables estimated using the standard Moses (Koehn et al, 2007) training pipeline, and 5-gram Kneser-Ney smoothed language models estimated using the SRILM toolkit (Stolcke, 2002), with KenLM (Heafield, 2011) used at runtime","['51', '262', '1', '224', '21']","['    <S sid=""51"" ssid=""29"">This differs from other implementations (Stolcke, 2002; Pauls and Klein, 2011) that use hash tables as nodes in a trie, as explained in the next section.</S>\n', '    <S sid=""262"" ssid=""4"">Raj and Whittaker (2003) show that integers in a trie implementation can be compressed substantially.</S>\n', '    <S sid=""1"" ssid=""1"">We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.</S>\n', '    <S sid=""224"" ssid=""43"">Time starts when Moses is launched and therefore includes model loading time.</S>\n', '    <S sid=""21"" ssid=""16"">Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
6,W11-2123,P12-2058,0,"Heafield, 2011",0,"The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","The features used are basic lexical features, word penalty and a 3-gram Language Model (Heafield, 2011)","['103', '223', '216', '112', '246']","['    <S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S>\n', '    <S sid=""223"" ssid=""42"">The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run.</S>\n', '    <S sid=""216"" ssid=""35"">Though we are not able to calculate their memory usage on our model, results reported in their paper suggest lower memory consumption than TRIE on large-scale models, at the expense of CPU time.</S>\n', '    <S sid=""112"" ssid=""16"">The developers suggested some changes, such as building the model from scratch with IRSTLM, but these did not resolve the problem.</S>\n', '    <S sid=""246"" ssid=""65"">For RandLM and IRSTLM, the effect of caching can be seen on speed and memory usage.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
7,W11-2123,W11-2139,0,2011,0,Inference was carried out using the language modeling library described by Heafield (2011),Inference was carried out using the language modeling library described by Heafield (2011),"['87', '223', '273', '232', '56']","['    <S sid=""87"" ssid=""65"">Queries detect the invalid probability, using the node only if it leads to a longer match.</S>\n', '    <S sid=""223"" ssid=""42"">The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run.</S>\n', '    <S sid=""273"" ssid=""15"">This would result in better rest cost estimation and better pruning.10 In general, tighter, but well factored, integration between the decoder and language model should produce a significant speed improvement.</S>\n', '    <S sid=""232"" ssid=""51"">Memory usage is likely much lower than ours. fThe original paper (Germann et al., 2009) provided only 2s of query timing and compared with SRI when it exceeded available RAM.</S>\n', '    <S sid=""56"" ssid=""34"">We reduce this to O(log log |A|) time by evenly distributing keys over their range then using interpolation search4 (Perl et al., 1978).</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
8,W11-2123,P13-2003,0,"Heafield, 2011",0,"We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","We used the MADA ATB segmentation for Arabic (Roth et al, 2008) and true casing for English, phrases of maximal length 7, KneserNey smoothing, and lexicalized reordering (Koehn et al, 2005), and a 5-gram language model, trained on GigaWordv.5 using KenLM (Heafield, 2011)","['56', '103', '200', '98', '199']","['    <S sid=""56"" ssid=""34"">We reduce this to O(log log |A|) time by evenly distributing keys over their range then using interpolation search4 (Perl et al., 1978).</S>\n', '    <S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S>\n', '    <S sid=""200"" ssid=""19"">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>\n', '    <S sid=""98"" ssid=""2"">It is generally considered to be fast (Pauls 29 &#8722; 1 probabilities and 2\' &#8722; 2 non-zero backoffs. and Klein, 2011), with a default implementation based on hash tables within each trie node.</S>\n', '    <S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
9,W11-2123,W12-3134,0,2011,0,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language mod els by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,The approach we take is similar to work on efficiently storing large phrase tables by Zens and Ney (2007) and language models by Heafield (2011) and Pauls and Klein (2011)? both language model implementations are now integrated with Joshua,"['98', '199', '51', '6', '103']","['    <S sid=""98"" ssid=""2"">It is generally considered to be fast (Pauls 29 &#8722; 1 probabilities and 2\' &#8722; 2 non-zero backoffs. and Klein, 2011), with a default implementation based on hash tables within each trie node.</S>\n', '    <S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>\n', '    <S sid=""51"" ssid=""29"">This differs from other implementations (Stolcke, 2002; Pauls and Klein, 2011) that use hash tables as nodes in a trie, as explained in the next section.</S>\n', '    <S sid=""6"" ssid=""1"">Language models are widely applied in natural language processing, and applications such as machine translation make very frequent queries.</S>\n', '    <S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
10,W11-2123,W12-3134,0,2011,0,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,Our quantization approach follows Federico and Bertoldi (2006) and Heafield (2011) in partitioning the value histogram into 256 equal-sized buckets,"['98', '264', '253', '103', '199']","['    <S sid=""98"" ssid=""2"">It is generally considered to be fast (Pauls 29 &#8722; 1 probabilities and 2\' &#8722; 2 non-zero backoffs. and Klein, 2011), with a default implementation based on hash tables within each trie node.</S>\n', '    <S sid=""264"" ssid=""6"">For even larger models, storing counts (Talbot and Osborne, 2007; Pauls and Klein, 2011; Guthrie and Hepple, 2010) is a possibility.</S>\n', '    <S sid=""253"" ssid=""72"">Using RandLM and the documented settings (8-bit values and 1 256 false-positive probability), we built a stupid backoff model on the same data as in Section 5.2.</S>\n', '    <S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S>\n', '    <S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
11,W11-2123,W12-3134,0,2011,0,"With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","With the help of the respective original authors, the language model implementations by Heafield (2011) and Pauls and Klein (2011) have been integrated with Joshua, dropping support for the slower and more difficult to compile SRILM toolkit (Stolcke, 2002)","['199', '205', '264', '103', '98']","['    <S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>\n', '    <S sid=""205"" ssid=""24"">We evaluate the time and memory consumption of each data structure by computing perplexity on 4 billion tokens from the English Gigaword corpus (Parker et al., 2009).</S>\n', '    <S sid=""264"" ssid=""6"">For even larger models, storing counts (Talbot and Osborne, 2007; Pauls and Klein, 2011; Guthrie and Hepple, 2010) is a possibility.</S>\n', '    <S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S>\n', '    <S sid=""98"" ssid=""2"">It is generally considered to be fast (Pauls 29 &#8722; 1 probabilities and 2\' &#8722; 2 non-zero backoffs. and Klein, 2011), with a default implementation based on hash tables within each trie node.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
12,W11-2123,W12-3160,0,"Heafield, 2011",0,"This was used to create a KenLM (Heafield, 2011)","This was used to create a KenLM (Heafield, 2011)","['114', '247', '17', '51', '260']","['    <S sid=""114"" ssid=""18"">Later, BerkeleyLM (Pauls and Klein, 2011) described ideas similar to ours.</S>\n', '    <S sid=""247"" ssid=""66"">This is most severe with RandLM in the multi-threaded case, where each thread keeps a separate cache, exceeding the original model size.</S>\n', '    <S sid=""17"" ssid=""12"">Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques, but did not release code.</S>\n', '    <S sid=""51"" ssid=""29"">This differs from other implementations (Stolcke, 2002; Pauls and Klein, 2011) that use hash tables as nodes in a trie, as explained in the next section.</S>\n', '    <S sid=""260"" ssid=""2"">For speed, we plan to implement the direct-mapped cache from BerkeleyLM.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Method_Citation']"
13,W11-2123,W12-3706,0,"Heafield, 2011",0,"In the Opinum system we query the M p, M n mod els with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable fora web application","In the Opinum system we query the M p, M n models with the KenLM (Heafield, 2011) open-source library because it answers the queries very quickly and has a short loading time, which is suitable for a web application","['6', '3', '98', '199', '103']","['    <S sid=""6"" ssid=""1"">Language models are widely applied in natural language processing, and applications such as machine translation make very frequent queries.</S>\n', '    <S sid=""3"" ssid=""3"">Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.</S>\n', '    <S sid=""98"" ssid=""2"">It is generally considered to be fast (Pauls 29 &#8722; 1 probabilities and 2\' &#8722; 2 non-zero backoffs. and Klein, 2011), with a default implementation based on hash tables within each trie node.</S>\n', '    <S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>\n', '    <S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
14,W11-2123,W11-2147,0,"Heafield, 2011",0,"Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","Our base line is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al, 2007) for translation model training and decoding, GIZA++ (Ochand Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights","['98', '13', '103', '199', '200']","['    <S sid=""98"" ssid=""2"">It is generally considered to be fast (Pauls 29 &#8722; 1 probabilities and 2\' &#8722; 2 non-zero backoffs. and Klein, 2011), with a default implementation based on hash tables within each trie node.</S>\n', '    <S sid=""13"" ssid=""8"">IRSTLM 5.60.02 (Federico et al., 2008) is a sorted trie implementation designed for lower memory consumption.</S>\n', '    <S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S>\n', '    <S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>\n', '    <S sid=""200"" ssid=""19"">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
15,W11-2123,E12-1083,0,"Heafield, 2011",0,"For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","For language modeling, we computed 5-gram models using IRSTLM7 (Federico et al., 2008) and queried the model with KenLM (Heafield, 2011)","['103', '232', '13', '76', '244']","['    <S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S>\n', '    <S sid=""232"" ssid=""51"">Memory usage is likely much lower than ours. fThe original paper (Germann et al., 2009) provided only 2s of query timing and compared with SRI when it exceeded available RAM.</S>\n', '    <S sid=""13"" ssid=""8"">IRSTLM 5.60.02 (Federico et al., 2008) is a sorted trie implementation designed for lower memory consumption.</S>\n', '    <S sid=""76"" ssid=""54"">This technique was introduced by Clarkson and Rosenfeld (1997) and is also implemented by IRSTLM and BerkeleyLM&#8217;s compressed option.</S>\n', '    <S sid=""244"" ssid=""63"">Time for Moses itself to load, including loading the language model and phrase table, is included.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
16,W11-2123,P12-1002,0,"Heafield, 2011",0,"Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Sima ?an,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","Furthermore, the extraction of grammars for training is done in a leave-one-out fashion (Zollmann and Simaan,2005) where rules are extracted for a parallel sentence pair only if the same rules are found in other sentences of the corpus as well.3-gram (news-commentary) and 5-gram (Europarl) language models are trained on the data described in Table 1, using the SRILM toolkit (Stolcke, 2002) and binarized for efficient querying using kenlm (Heafield, 2011)","['200', '67', '223', '103', '56']","['    <S sid=""200"" ssid=""19"">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>\n', '    <S sid=""67"" ssid=""45"">While sorted arrays could be used to implement the same data structure as PROBING, effectively making m = 1, we abandoned this implementation because it is slower and larger than a trie implementation.</S>\n', '    <S sid=""223"" ssid=""42"">The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run.</S>\n', '    <S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S>\n', '    <S sid=""56"" ssid=""34"">We reduce this to O(log log |A|) time by evenly distributing keys over their range then using interpolation search4 (Perl et al., 1978).</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
17,W11-2123,D12-1108,0,"Heafield, 2011",0,"n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","n-gram language model scores implemented with the KenLM toolkit (Heafield, 2011), 3","['76', '201', '103', '98', '51']","['    <S sid=""76"" ssid=""54"">This technique was introduced by Clarkson and Rosenfeld (1997) and is also implemented by IRSTLM and BerkeleyLM&#8217;s compressed option.</S>\n', '    <S sid=""201"" ssid=""20"">Unlike Germann et al. (2009), we chose a model size so that all benchmarks fit comfortably in main memory.</S>\n', '    <S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S>\n', '    <S sid=""98"" ssid=""2"">It is generally considered to be fast (Pauls 29 &#8722; 1 probabilities and 2\' &#8722; 2 non-zero backoffs. and Klein, 2011), with a default implementation based on hash tables within each trie node.</S>\n', '    <S sid=""51"" ssid=""29"">This differs from other implementations (Stolcke, 2002; Pauls and Klein, 2011) that use hash tables as nodes in a trie, as explained in the next section.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
18,W11-2123,P12-2006,0,"Heafield, 2011",0,"Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","Research efforts to increase search efficiency for phrase-based MT (Koehn et al, 2003) have explored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al, 2006) to additional early pruning techniques (Delaney et al, 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011)","['223', '103', '56', '232', '98']","['    <S sid=""223"" ssid=""42"">The binary language model from Section 5.2 and text phrase table were forced into disk cache before each run.</S>\n', '    <S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S>\n', '    <S sid=""56"" ssid=""34"">We reduce this to O(log log |A|) time by evenly distributing keys over their range then using interpolation search4 (Perl et al., 1978).</S>\n', '    <S sid=""232"" ssid=""51"">Memory usage is likely much lower than ours. fThe original paper (Germann et al., 2009) provided only 2s of query timing and compared with SRI when it exceeded available RAM.</S>\n', '    <S sid=""98"" ssid=""2"">It is generally considered to be fast (Pauls 29 &#8722; 1 probabilities and 2\' &#8722; 2 non-zero backoffs. and Klein, 2011), with a default implementation based on hash tables within each trie node.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
19,W11-2123,P13-2073,0,"Heafield, 2011",0,"For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","For English language modeling, we use English Giga word Corpus with 5-gram LM using the KenLM toolkit (Heafield, 2011)","['199', '103', '244', '102', '42']","['    <S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>\n', '    <S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S>\n', '    <S sid=""244"" ssid=""63"">Time for Moses itself to load, including loading the language model and phrase table, is included.</S>\n', '    <S sid=""102"" ssid=""6"">The PROBING model was designed to improve upon SRILM by using linear probing hash tables (though not arranged in a trie), allocating memory all at once (eliminating the need for full pointers), and being easy to compile.</S>\n', '    <S sid=""42"" ssid=""20"">With a good hash function, collisions of the full 64bit hash are exceedingly rare: one in 266 billion queries for our baseline model will falsely find a key not present.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
20,W11-2123,P13-1109,0,"Heafield, 2011",0,"For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with KneserNey smoothing","['76', '56', '199', '200', '103']","['    <S sid=""76"" ssid=""54"">This technique was introduced by Clarkson and Rosenfeld (1997) and is also implemented by IRSTLM and BerkeleyLM&#8217;s compressed option.</S>\n', '    <S sid=""56"" ssid=""34"">We reduce this to O(log log |A|) time by evenly distributing keys over their range then using interpolation search4 (Perl et al., 1978).</S>\n', '    <S sid=""199"" ssid=""18"">For the perplexity and translation tasks, we used SRILM to build a 5-gram English language model on 834 million tokens from Europarl v6 (Koehn, 2005) and the 2011 Workshop on Machine Translation News Crawl corpus with duplicate lines removed.</S>\n', '    <S sid=""200"" ssid=""19"">The model was built with open vocabulary, modified Kneser-Ney smoothing, and default pruning settings that remove singletons of order 3 and higher.</S>\n', '    <S sid=""103"" ssid=""7"">IRSTLM (Federico et al., 2008) is an open-source toolkit for building and querying language models.</S>\n']","['Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation', 'Result_Citation']"
