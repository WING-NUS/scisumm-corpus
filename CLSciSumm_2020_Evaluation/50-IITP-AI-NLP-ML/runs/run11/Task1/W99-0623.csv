Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Reference Citation
1,W99-0623,A00-2005,0,1999,0,1 Introduct ion Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy,Henderson and Brill (1999) showed that independent human research efforts produce parsers that can be combined for an overall boost in accuracy,"['143', '64', '136', '63', '115']","['    <S sid=""143"" ssid=""5"">Through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result.</S>\n', '    <S sid=""64"" ssid=""50"">Furthermore, we know one of the original parses will be the hypothesized parse, so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in Section 2.1.</S>\n', '    <S sid=""136"" ssid=""65"">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S>\n', '    <S sid=""63"" ssid=""49"">The probabilistic version of this procedure is straightforward: We once again assume independence among our various member parsers.</S>\n', '    <S sid=""115"" ssid=""44"">It is the performance we could achieve if an omniscient observer told us which parser to pick for each of the sentences.</S>\n']","['Result', 'Hypothesis', 'Hypothesis', 'Hypothesis', 'Result']"
2,W99-0623,A00-2005,0,1999,0,the collection of hypotheses ti =fi (Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999),"Given a novel sentence Stest E Ctest, combine the collection of hypotheses ti = fi(Stest) using the unweighted constituent voting scheme of Henderson and Brill (1999)","['73', '105', '19', '125', '39']","['    <S sid=""73"" ssid=""2"">We used section 23 as the development set for our combining techniques, and section 22 only for final testing.</S>\n', '    <S sid=""105"" ssid=""34"">Similarly Figures 1 and 2 show how the isolated constituent precision varies by sentence length and the size of the span of the hypothesized constituent.</S>\n', '    <S sid=""19"" ssid=""5"">The precision and recall measures (described in more detail in Section 3) used in evaluating Treebank parsing treat each constituent as a separate entity, a minimal unit of correctness.</S>\n', '    <S sid=""125"" ssid=""54"">The constituent voting and na&#239;ve Bayes techniques are equivalent because the parameters learned in the training set did not sufficiently discriminate between the three parsers.</S>\n', '    <S sid=""39"" ssid=""25"">There are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.</S>\n']","['Hypothesis', 'Hypothesis', 'Result', 'Result', 'Hypothesis']"
4,W99-0623,N10-1091,0,"Henderson and Brill, 1999",0,"5 (Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers","(Henderson and Brill, 1999) used a similar framework in the context of constituent parsing and only three base parsers","['13', '129', '100', '7', '101']","['    <S sid=""13"" ssid=""9"">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993).</S>\n', '    <S sid=""129"" ssid=""58"">In the interest of testing the robustness of these combining techniques, we added a fourth, simple nonlexicalized PCFG parser.</S>\n', '    <S sid=""100"" ssid=""29"">When this metric is less than 0.5, we expect to incur more errors\' than we will remove by adding those constituents to the parse.</S>\n', '    <S sid=""7"" ssid=""3"">Their theoretical finding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent, accurate classifiers.</S>\n', '    <S sid=""101"" ssid=""30"">We show the results of three of the experiments we conducted to measure isolated constituent precision under various partitioning schemes.</S>\n']","['Result', 'Hypothesis', 'Result', 'Result', 'Result']"
5,W99-0623,W05-1518,0,"Henderson and Brill, 1999",0,"A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)","A successful application of voting and of a stacked classifier to constituent parsing followed in (Henderson and Brill, 1999)","['98', '32', '71', '12', '104']","['    <S sid=""98"" ssid=""27"">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S>\n', '    <S sid=""32"" ssid=""18"">In Equations 1 through 3 we develop the model for constructing our parse using na&#239;ve Bayes classification.</S>\n', '    <S sid=""71"" ssid=""57"">It is chosen such that the decisions it made in including or excluding constituents are most probable under the models for all of the parsers.</S>\n', '    <S sid=""12"" ssid=""8"">The corpus-based statistical parsing community has many fast and accurate automated parsing systems, including systems produced by Collins (1997), Charniak (1997) and Ratnaparkhi (1997).</S>\n', '    <S sid=""104"" ssid=""33"">In the cases where isolated constituent precision is larger than 0.5 the affected portion of the hypotheses is negligible.</S>\n']","['Hypothesis', 'Hypothesis', 'Hypothesis', 'Implication', 'Hypothesis']"
6,W99-0623,W05-1518,0,"Henderson and Brill, 1999",0,"This approach roughly corresponds to (Henderson and Brill, 1999)? s Na ?ve Bayes parse hybridization","This approach roughly corresponds to (Henderson and Brill, 1999)'s Naive Bayes parse hybridization","['139', '118', '29', '106', '64']","['    <S sid=""139"" ssid=""1"">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>\n', '    <S sid=""118"" ssid=""47"">The maximum precision oracle is an upper bound on the possible gain we can achieve by parse hybridization.</S>\n', '    <S sid=""29"" ssid=""15"">Our original hope in combining these parsers is that their errors are independently distributed.</S>\n', '    <S sid=""106"" ssid=""35"">In each figure the upper graph shows the isolated constituent precision and the bottom graph shows the corresponding number of hypothesized constituents.</S>\n', '    <S sid=""64"" ssid=""50"">Furthermore, we know one of the original parses will be the hypothesized parse, so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in Section 2.1.</S>\n']","['Hypothesis', 'Implication', 'Hypothesis', 'Hypothesis', 'Hypothesis']"
7,W99-0623,W05-1518,0,1999,0,Henderson and Brill (1999) also reported that context did not help them to outperform simple voting,Henderson and Brill (1999) also reported that context did not help them to outperform simple voting,"['145', '64', '72', '103', '86']","['    <S sid=""145"" ssid=""7"">We plan to explore more powerful techniques for exploiting the diversity of parsing methods.</S>\n', '    <S sid=""64"" ssid=""50"">Furthermore, we know one of the original parses will be the hypothesized parse, so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in Section 2.1.</S>\n', '    <S sid=""72"" ssid=""1"">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>\n', '    <S sid=""103"" ssid=""32"">The counts represent portions of the approximately 44000 constituents hypothesized by the parsers in the development set.</S>\n', '    <S sid=""86"" ssid=""15"">Finally we show the combining techniques degrade very little when a poor parser is added to the set.</S>\n']","['Hypothesis', 'Hypothesis', 'Result', 'Hypothesis', 'Hypothesis']"
8,W99-0623,W05-1518,0,"Henderson and Brill, 1999",0,"(Henderson and Brill, 1999) improved their best parser? s F-measure of 89.7 to 91.3, using their na ?ve Bayes voting on the Penn TreeBank constituent structures (16% error reduction)","(Henderson and Brill, 1999) improved their best parser's F-measure of 89.7 to 91.3, using their naive Bayes voting on the Penn TreeBank constituent structures (16% error reduction)","['72', '9', '11', '76', '13']","['    <S sid=""72"" ssid=""1"">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>\n', '    <S sid=""9"" ssid=""5"">Recently, combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al., 1998; Brill and Wu, 1998).</S>\n', '    <S sid=""11"" ssid=""7"">Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).</S>\n', '    <S sid=""76"" ssid=""5"">The standard measures for evaluating Penn Treebank parsing performance are precision and recall of the predicted constituents.</S>\n', '    <S sid=""13"" ssid=""9"">These three parsers have given the best reported parsing results on the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993).</S>\n']","['Result', 'Result', 'Hypothesis', 'Result', 'Result']"
10,W99-0623,P01-1005,0,"Henderson and Brill, 1999",0,"Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000)","Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000)","['32', '11', '9', '85', '136']","['    <S sid=""32"" ssid=""18"">In Equations 1 through 3 we develop the model for constructing our parse using na&#239;ve Bayes classification.</S>\n', '    <S sid=""11"" ssid=""7"">Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).</S>\n', '    <S sid=""9"" ssid=""5"">Recently, combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al., 1998; Brill and Wu, 1998).</S>\n', '    <S sid=""85"" ssid=""14"">We then show that the combining techniques presented above give better parsing accuracy than any of the individual parsers.</S>\n', '    <S sid=""136"" ssid=""65"">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S>\n']","['Hypothesis', 'Hypothesis', 'Result', 'Hypothesis', 'Hypothesis']"
11,W99-0623,D09-1161,0,1999,0,"Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees","Regarding the system combination study, Henderson and Brill (1999) propose two parser combination schemes, one that selects an entire tree from one of the parsers, and one that builds a new tree by selecting constituents suggested by the initial trees","['117', '19', '39', '101', '7']","['    <S sid=""117"" ssid=""46"">Another way to interpret this is that less than 5% of the correct constituents are missing from the hypotheses generated by the union of the three parsers.</S>\n', '    <S sid=""19"" ssid=""5"">The precision and recall measures (described in more detail in Section 3) used in evaluating Treebank parsing treat each constituent as a separate entity, a minimal unit of correctness.</S>\n', '    <S sid=""39"" ssid=""25"">There are simply not enough votes remaining to allow any of the crossing structures to enter the hypothesized constituent set.</S>\n', '    <S sid=""101"" ssid=""30"">We show the results of three of the experiments we conducted to measure isolated constituent precision under various partitioning schemes.</S>\n', '    <S sid=""7"" ssid=""3"">Their theoretical finding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent, accurate classifiers.</S>\n']","['Result', 'Result', 'Hypothesis', 'Result', 'Result']"
12,W99-0623,D09-1161,0,1999,0,"Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper","Henderson and Brill (1999) combine three parsers and obtained an F1 score of 90.6, which is better than the score of 88.6 obtained by the best individual parser as reported in their paper","['143', '64', '72', '133', '100']","['    <S sid=""143"" ssid=""5"">Through parser combination we have reduced the precision error rate by 30% and the recall error rate by 6% compared to the best previously published result.</S>\n', '    <S sid=""64"" ssid=""50"">Furthermore, we know one of the original parses will be the hypothesized parse, so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in Section 2.1.</S>\n', '    <S sid=""72"" ssid=""1"">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>\n', '    <S sid=""133"" ssid=""62"">The entries in this table can be compared with those of Table 3 to see how the performance of the combining techniques degrades in the presence of an inferior parser.</S>\n', '    <S sid=""100"" ssid=""29"">When this metric is less than 0.5, we expect to incur more errors\' than we will remove by adding those constituents to the parse.</S>\n']","['Result', 'Hypothesis', 'Result', 'Result', 'Result']"
13,W99-0623,D09-1161,0,"Henderson and Brill, 1999",0,"Besides the two model scores, we also adopt constituent count as an additional feature in spired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)","Besides the two model scores, we also adopt constituent count as an additional feature in spired by (Henderson and Brill 1999) and (Sagae and Lavie 2006)","['98', '86', '64', '104', '9']","['    <S sid=""98"" ssid=""27"">Adding the isolated constituents to our hypothesis parse could increase our expected recall, but in the cases we investigated it would invariably hurt our precision more than we would gain on recall.</S>\n', '    <S sid=""86"" ssid=""15"">Finally we show the combining techniques degrade very little when a poor parser is added to the set.</S>\n', '    <S sid=""64"" ssid=""50"">Furthermore, we know one of the original parses will be the hypothesized parse, so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in Section 2.1.</S>\n', '    <S sid=""104"" ssid=""33"">In the cases where isolated constituent precision is larger than 0.5 the affected portion of the hypotheses is negligible.</S>\n', '    <S sid=""9"" ssid=""5"">Recently, combination techniques have been investigated for part of speech tagging with positive results (van Halteren et al., 1998; Brill and Wu, 1998).</S>\n']","['Hypothesis', 'Hypothesis', 'Hypothesis', 'Hypothesis', 'Result']"
14,W99-0623,N06-2033,0,"Henderson and Brill, 1999",0,"Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees","Henderson and Brill (1999) proposed two parser combination schemes, one that picks an entire tree from one of the parsers, and one that, like ours, builds a new tree from constituents from the initial trees","['72', '7', '19', '115', '104']","['    <S sid=""72"" ssid=""1"">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>\n', '    <S sid=""7"" ssid=""3"">Their theoretical finding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent, accurate classifiers.</S>\n', '    <S sid=""19"" ssid=""5"">The precision and recall measures (described in more detail in Section 3) used in evaluating Treebank parsing treat each constituent as a separate entity, a minimal unit of correctness.</S>\n', '    <S sid=""115"" ssid=""44"">It is the performance we could achieve if an omniscient observer told us which parser to pick for each of the sentences.</S>\n', '    <S sid=""104"" ssid=""33"">In the cases where isolated constituent precision is larger than 0.5 the affected portion of the hypotheses is negligible.</S>\n']","['Result', 'Result', 'Result', 'Result', 'Hypothesis']"
15,W99-0623,N09-2064,0,"Henderson and Brill, 1999",0,"(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined","(Henderson and Brill, 1999) perform parse selection by maximizing the expected precision of the selected parse with respect to the set of parses being combined","['133', '80', '14', '88', '89']","['    <S sid=""133"" ssid=""62"">The entries in this table can be compared with those of Table 3 to see how the performance of the combining techniques degrades in the presence of an inferior parser.</S>\n', '    <S sid=""80"" ssid=""9"">For our experiments we also report the mean of precision and recall, which we denote by (P + R)I2 and F-measure.</S>\n', '    <S sid=""14"" ssid=""10"">We used these three parsers to explore parser combination techniques.</S>\n', '    <S sid=""88"" ssid=""17"">For example, one parser could be more accurate at predicting noun phrases than the other parsers.</S>\n', '    <S sid=""89"" ssid=""18"">None of the models we have presented utilize features associated with a particular constituent (i.e. the label, span, parent label, etc.) to influence parser preference.</S>\n']","['Result', 'Result', 'Result', 'Result', 'Result']"
16,W99-0623,N09-2064,0,"Henderson and Brill, 1999",0,"(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents","(Henderson and Brill, 1999) and (Sagae and Lavie, 2006) propose methods for parse hybridization by recombining constituents","['64', '11', '139', '136', '29']","['    <S sid=""64"" ssid=""50"">Furthermore, we know one of the original parses will be the hypothesized parse, so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in Section 2.1.</S>\n', '    <S sid=""11"" ssid=""7"">Similar advances have been made in machine translation (Frederking and Nirenburg, 1994), speech recognition (Fiscus, 1997) and named entity recognition (Borthwick et al., 1998).</S>\n', '    <S sid=""139"" ssid=""1"">We have presented two general approaches to studying parser combination: parser switching and parse hybridization.</S>\n', '    <S sid=""136"" ssid=""65"">The Bayes models were able to achieve significantly higher precision than their non-parametric counterparts.</S>\n', '    <S sid=""29"" ssid=""15"">Our original hope in combining these parsers is that their errors are independently distributed.</S>\n']","['Hypothesis', 'Hypothesis', 'Hypothesis', 'Hypothesis', 'Hypothesis']"
17,W99-0623,N09-2064,0,"Henderson and Brill, 1999",0,"output (Figure 3) .Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Mini mum Bayes Risk (MBR) framework. Third, we extend these parser combination methods from 1-best outputs to n-best outputs","Second, the parse selection method of (Henderson and Brill, 1999) selects the parse with maximum expected precision; here, we present an efficient, linear-time algorithm for selecting the parse with maximum expected f-score within the Mini mum Bayes Risk (MBR) framework","['18', '7', '64', '19', '104']","['    <S sid=""18"" ssid=""4"">These two principles guide experimentation in this framework, and together with the evaluation measures help us decide which specific type of substructure to combine.</S>\n', '    <S sid=""7"" ssid=""3"">Their theoretical finding is simply stated: classification error rate decreases toward the noise rate exponentially in the number of independent, accurate classifiers.</S>\n', '    <S sid=""64"" ssid=""50"">Furthermore, we know one of the original parses will be the hypothesized parse, so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in Section 2.1.</S>\n', '    <S sid=""19"" ssid=""5"">The precision and recall measures (described in more detail in Section 3) used in evaluating Treebank parsing treat each constituent as a separate entity, a minimal unit of correctness.</S>\n', '    <S sid=""104"" ssid=""33"">In the cases where isolated constituent precision is larger than 0.5 the affected portion of the hypotheses is negligible.</S>\n']","['Result', 'Result', 'Hypothesis', 'Result', 'Hypothesis']"
18,W99-0623,P09-1065,0,"Henderson and Brill, 1999",0,"System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999))","System combination has benefited various NLP tasks in recent years, such as products-of-experts (e.g., (Smith and Eisner, 2005)) and ensemble based parsing (e.g., (Henderson and Brill, 1999))","['100', '64', '72', '56', '28']","['    <S sid=""100"" ssid=""29"">When this metric is less than 0.5, we expect to incur more errors\' than we will remove by adding those constituents to the parse.</S>\n', '    <S sid=""64"" ssid=""50"">Furthermore, we know one of the original parses will be the hypothesized parse, so the direct method of determining which one is best is to compute the probability of each of the candidate parses using the probabilistic model we developed in Section 2.1.</S>\n', '    <S sid=""72"" ssid=""1"">The three parsers were trained and tuned by their creators on various sections of the WSJ portion of the Penn Treebank, leaving only sections 22 and 23 completely untouched during the development of any of the parsers.</S>\n', '    <S sid=""56"" ssid=""42"">The combining algorithm is presented with the candidate parses and asked to choose which one is best.</S>\n', '    <S sid=""28"" ssid=""14"">The development of a na&#239;ve Bayes classifier involves learning how much each parser should be trusted for the decisions it makes.</S>\n']","['Result', 'Hypothesis', 'Result', 'Hypothesis', 'Result']"
20,W99-0623,C10-1151,0,1999,0,Henderson and Brill (1999) performs parse selection by maximizing the expected precision of selected parse with respect to the set of parses to be combined,Henderson and Brill (1999) performs parse selection by maximizing the expected precision of selected parse with respect to the set of parses to be combined,"['133', '14', '80', '88', '100']","['    <S sid=""133"" ssid=""62"">The entries in this table can be compared with those of Table 3 to see how the performance of the combining techniques degrades in the presence of an inferior parser.</S>\n', '    <S sid=""14"" ssid=""10"">We used these three parsers to explore parser combination techniques.</S>\n', '    <S sid=""80"" ssid=""9"">For our experiments we also report the mean of precision and recall, which we denote by (P + R)I2 and F-measure.</S>\n', '    <S sid=""88"" ssid=""17"">For example, one parser could be more accurate at predicting noun phrases than the other parsers.</S>\n', '    <S sid=""100"" ssid=""29"">When this metric is less than 0.5, we expect to incur more errors\' than we will remove by adding those constituents to the parse.</S>\n']","['Result', 'Result', 'Result', 'Result', 'Result']"
