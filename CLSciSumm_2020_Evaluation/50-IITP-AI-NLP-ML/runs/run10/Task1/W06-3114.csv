Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,W06-3114,W06-3120,0,"Koehn and Monz, 2006",0,"The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","The official results were slightly better because a lowercase evaluation was used, see (Koehn and Monz, 2006)","['50', '70', '60', '18', '39']","['    <S sid=""50"" ssid=""16"">Following this method, we repeatedly &#8212; say, 1000 times &#8212; sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.</S>\n', '    <S sid=""70"" ssid=""9"">See Figure 3 for a screenshot of the evaluation tool.</S>\n', '    <S sid=""60"" ssid=""26"">Let say, if we find one system doing better on 20 of the blocks, and worse on 80 of the blocks, is it significantly worse?</S>\n', '    <S sid=""18"" ssid=""11"">In addition to the Europarl test set, we also collected 29 editorials from the Project Syndicate website2, which are published in all the four languages of the shared task.</S>\n', '    <S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S>\n']","['Result', 'Result', 'Hypothesis', 'Result', 'Hypothesis']"
2,W06-3114,D07-1092,0,"Koehn and Monz, 2006",0,"We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","We are further focusing on the shared task of the workshop on Statistical Machine Translation, which took place last year (Koehn and Monz, 2006) and consisted in translating Spanish, German, and French texts from and to English","['64', '26', '122', '60', '139']","['    <S sid=""64"" ssid=""3"">Also, the argument has been made that machine translation performance should be evaluated via task-based evaluation metrics, i.e. how much it assists performing a useful task, such as supporting human translators or aiding the analysis of texts.</S>\n', '    <S sid=""26"" ssid=""19"">Most of these groups follow a phrase-based statistical approach to machine translation.</S>\n', '    <S sid=""122"" ssid=""15"">While the Bootstrap method is slightly more sensitive, it is very much in line with the sign test on text blocks.</S>\n', '    <S sid=""60"" ssid=""26"">Let say, if we find one system doing better on 20 of the blocks, and worse on 80 of the blocks, is it significantly worse?</S>\n', '    <S sid=""139"" ssid=""32"">Given the closeness of most systems and the wide over-lapping confidence intervals it is hard to make strong statements about the correlation between human judgements and automatic scoring methods such as BLEU.</S>\n']","['Hypothesis', 'Hypothesis', 'Hypothesis', 'Hypothesis', 'Hypothesis']"
3,W06-3114,C08-1074,0,"Koehn and Monz, 2006",0,"For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","For our training and test data we used the English-French subset of the Europarl corpus provided for the shared task (Koehn and Monz, 2006) at the Statistical Machine Translation workshop held in conjunction with the 2006 HLT-NAACL conference","['38', '39', '66', '139', '140']","['    <S sid=""38"" ssid=""4"">The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).</S>\n', '    <S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S>\n', '    <S sid=""66"" ssid=""5"">In this shared task, we were also confronted with this problem, and since we had no funding for paying human judgements, we asked participants in the evaluation to share the burden.</S>\n', '    <S sid=""139"" ssid=""32"">Given the closeness of most systems and the wide over-lapping confidence intervals it is hard to make strong statements about the correlation between human judgements and automatic scoring methods such as BLEU.</S>\n', '    <S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>\n']","['Hypothesis', 'Hypothesis', 'Hypothesis', 'Hypothesis', 'Result']"
4,W06-3114,W07-0718,0,"Koehn and Monz, 2006",0,"The results of last year? s workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","The results of last year's workshop further suggested that Bleu systematically underestimated the quality of rule-based machine translation systems (Koehn and Monz, 2006)","['90', '36', '125', '71', '50']","['    <S sid=""90"" ssid=""6"">Another way to view the judgements is that they are less quality judgements of machine translation systems per se, but rankings of machine translation systems.</S>\n', '    <S sid=""36"" ssid=""2"">The BLEU metric, as all currently proposed automatic metrics, is occasionally suspected to be biased towards statistical systems, especially the phrase-based systems currently in use.</S>\n', '    <S sid=""125"" ssid=""18"">We can check, what the consequences of less manual annotation of results would have been: With half the number of manual judgements, we can distinguish about 40% of the systems, 10% less.</S>\n', '    <S sid=""71"" ssid=""10"">Presenting the output of several system allows the human judge to make more informed judgements, contrasting the quality of the different systems.</S>\n', '    <S sid=""50"" ssid=""16"">Following this method, we repeatedly &#8212; say, 1000 times &#8212; sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.</S>\n']","['Hypothesis', 'Hypothesis', 'Result', 'Result', 'Result']"
5,W06-3114,P07-1083,0,"Koehn and Monz, 2006",0,"For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and GermanEnglish (De) (Koehn and Monz, 2006)","For the bi text-based annotation, we use publicly available word alignments from the Europarl corpus, automatically generated by GIZA++ for FrenchEnglish (Fr), Spanish-English (Es) and German-English (De) (Koehn and Monz, 2006)","['152', '33', '144', '151', '66']","['    <S sid=""152"" ssid=""45"">The predominate focus of building systems that translate into English has ignored so far the difficult issues of generating rich morphology which may not be determined solely by local context.</S>\n', '    <S sid=""33"" ssid=""26"">While building a machine translation system is a serious undertaking, in future we hope to attract more newcomers to the field by keeping the barrier of entry as low as possible.</S>\n', '    <S sid=""144"" ssid=""37"">Our suspicion is that BLEU is very sensitive to jargon, to selecting exactly the right words, and not synonyms that human judges may appreciate as equally good.</S>\n', '    <S sid=""151"" ssid=""44"">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages, as demonstrated by the results for English-German and English-French.</S>\n', '    <S sid=""66"" ssid=""5"">In this shared task, we were also confronted with this problem, and since we had no funding for paying human judgements, we asked participants in the evaluation to share the burden.</S>\n']","['Hypothesis', 'Result', 'Hypothesis', 'Hypothesis', 'Hypothesis']"
6,W06-3114,W07-0738,0,2006,0,"Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","Evaluation results recently reported by Callison-Burch et al (2006) and Koehn and Monz (2006), revealed that, in certain cases, the BLEU metric may not be a reliable MTquality indicator","['39', '140', '50', '38', '4']","['    <S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S>\n', '    <S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>\n', '    <S sid=""50"" ssid=""16"">Following this method, we repeatedly &#8212; say, 1000 times &#8212; sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.</S>\n', '    <S sid=""38"" ssid=""4"">The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).</S>\n', '    <S sid=""4"" ssid=""2"">This revealed interesting clues about the properties of automatic and manual scoring.</S>\n']","['Hypothesis', 'Result', 'Result', 'Hypothesis', 'Result']"
7,W06-3114,W07-0738,0,2006,0,"For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","For instance, Callison-Burch et al (2006) and Koehn and Monz (2006) reported and analyzed several cases of strong disagreement between system rankings provided by human assessors and those produced by the BLEU metric (Papineni et al, 2001)","['49', '38', '68', '174', '139']","['    <S sid=""49"" ssid=""15"">Hence, we use the bootstrap resampling method described by Koehn (2004).</S>\n', '    <S sid=""38"" ssid=""4"">The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).</S>\n', '    <S sid=""68"" ssid=""7"">We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.</S>\n', '    <S sid=""174"" ssid=""5"">The manual evaluation of scoring translation on a graded scale from 1&#8211;5 seems to be very hard to perform.</S>\n', '    <S sid=""139"" ssid=""32"">Given the closeness of most systems and the wide over-lapping confidence intervals it is hard to make strong statements about the correlation between human judgements and automatic scoring methods such as BLEU.</S>\n']","['Method', 'Hypothesis', 'Result', 'Result', 'Hypothesis']"
8,W06-3114,W07-0738,0,2006,0,Wepresenta comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),We present a comparative study on the behavior of several metric representatives from each linguistic level in the context of some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006) (see Section 3),"['11', '4', '50', '48', '165']","['    <S sid=""11"" ssid=""4"">To lower the barrier of entrance to the competition, we provided a complete baseline MT system, along with data resources.</S>\n', '    <S sid=""4"" ssid=""2"">This revealed interesting clues about the properties of automatic and manual scoring.</S>\n', '    <S sid=""50"" ssid=""16"">Following this method, we repeatedly &#8212; say, 1000 times &#8212; sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.</S>\n', '    <S sid=""48"" ssid=""14"">Confidence Interval: Since BLEU scores are not computed on the sentence level, traditional methods to compute statistical significance and confidence intervals do not apply.</S>\n', '    <S sid=""165"" ssid=""58"">However, ince we extracted the test corpus automatically from web sources, the reference translation was not always accurate &#8212; due to sentence alignment errors, or because translators did not adhere to a strict sentence-by-sentence translation (say, using pronouns when referring to entities mentioned in the previous sentence).</S>\n']","['Result', 'Result', 'Result', 'Implication', 'Result']"
9,W06-3114,W07-0738,0,2006,0,Weanalyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),We analyze some of the cases reported by Koehn and Monz (2006) and Callison-Burch et al (2006),"['38', '39', '49', '140', '126']","['    <S sid=""38"" ssid=""4"">The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).</S>\n', '    <S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S>\n', '    <S sid=""49"" ssid=""15"">Hence, we use the bootstrap resampling method described by Koehn (2004).</S>\n', '    <S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>\n', '    <S sid=""126"" ssid=""19"">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>\n']","['Hypothesis', 'Hypothesis', 'Method', 'Result', 'Result']"
10,W06-3114,D07-1030,0,"Koehn and Monz, 2006",0,"We use the same method described in (Koehn and Monz, 2006) to perform the significance test","We use the same method described in (Koehn and Monz, 2006) to perform the significance test","['49', '38', '41', '140', '139']","['    <S sid=""49"" ssid=""15"">Hence, we use the bootstrap resampling method described by Koehn (2004).</S>\n', '    <S sid=""38"" ssid=""4"">The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).</S>\n', '    <S sid=""41"" ssid=""7"">The development of automatic scoring methods is an open field of research.</S>\n', '    <S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>\n', '    <S sid=""139"" ssid=""32"">Given the closeness of most systems and the wide over-lapping confidence intervals it is hard to make strong statements about the correlation between human judgements and automatic scoring methods such as BLEU.</S>\n']","['Method', 'Hypothesis', 'Method', 'Result', 'Hypothesis']"
11,W06-3114,D07-1030,0,"Koehn and Monz, 2016",0,"We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","We also manually evaluated the RBMT systems and SMT systems in terms of both adequacy and fluency as defined in (Koehn and Monz, 2006)","['68', '118', '140', '71', '79']","['    <S sid=""68"" ssid=""7"">We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.</S>\n', '    <S sid=""118"" ssid=""11"">At first glance, we quickly recognize that many systems are scored very similar, both in terms of manual judgement and BLEU.</S>\n', '    <S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>\n', '    <S sid=""71"" ssid=""10"">Presenting the output of several system allows the human judge to make more informed judgements, contrasting the quality of the different systems.</S>\n', '    <S sid=""79"" ssid=""18"">Sentences and systems were randomly selected and randomly shuffled for presentation.</S>\n']","['Result', 'Result', 'Result', 'Result', 'Result']"
12,W06-3114,W08-0406,0,"Koehn and Monz, 2017",0,"The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","The baseline is the PSMT system used for the 2006 NAACL SMT workshop (Koehn and Monz, 2006) with phrase length 3 and a trigram language model (Stolcke, 2002)","['38', '61', '49', '60', '151']","['    <S sid=""38"" ssid=""4"">The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).</S>\n', '    <S sid=""61"" ssid=""27"">We check, how likely only up to k = 20 better scores out of n = 100 would have been generated by two equal systems, using the binomial distribution: If p(0..k; n, p) &lt; 0.05, or p(0..k; n, p) &gt; 0.95 then we have a statistically significant difference between the systems.</S>\n', '    <S sid=""49"" ssid=""15"">Hence, we use the bootstrap resampling method described by Koehn (2004).</S>\n', '    <S sid=""60"" ssid=""26"">Let say, if we find one system doing better on 20 of the blocks, and worse on 80 of the blocks, is it significantly worse?</S>\n', '    <S sid=""151"" ssid=""44"">The statistical systems seem to still lag behind the commercial rule-based competition when translating into morphological rich languages, as demonstrated by the results for English-German and English-French.</S>\n']","['Hypothesis', 'Hypothesis', 'Method', 'Hypothesis', 'Hypothesis']"
13,W06-3114,W11-1002,0,2006,0,"Callison-Burch et al (2006 )andKoehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality","Callison-Burch et al (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality","['38', '62', '125', '68', '172']","['    <S sid=""38"" ssid=""4"">The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).</S>\n', '    <S sid=""62"" ssid=""1"">While automatic measures are an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.</S>\n', '    <S sid=""125"" ssid=""18"">We can check, what the consequences of less manual annotation of results would have been: With half the number of manual judgements, we can distinguish about 40% of the systems, 10% less.</S>\n', '    <S sid=""68"" ssid=""7"">We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.</S>\n', '    <S sid=""172"" ssid=""3"">Due to many similarly performing systems, we are not able to draw strong conclusions on the question of correlation of manual and automatic evaluation metrics.</S>\n']","['Hypothesis', 'Result', 'Result', 'Result', 'Implication']"
14,W06-3114,D07-1091,0,"Koehn and Monz, 2006",0,"The English? German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","The English German systems were trained on the full 751,088 sentence Europarl corpus and evaluated on the WMT 2006 test set (Koehn and Monz, 2006)","['50', '38', '126', '106', '68']","['    <S sid=""50"" ssid=""16"">Following this method, we repeatedly &#8212; say, 1000 times &#8212; sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.</S>\n', '    <S sid=""38"" ssid=""4"">The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).</S>\n', '    <S sid=""126"" ssid=""19"">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>\n', '    <S sid=""106"" ssid=""22"">Automatic scores are computed on a larger tested than manual scores (3064 sentences vs. 300&#8211;400 sentences). collected manual judgements, we do not necessarily have the same sentence judged for both systems (judges evaluate 5 systems out of the 8&#8211;10 participating systems).</S>\n', '    <S sid=""68"" ssid=""7"">We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.</S>\n']","['Result', 'Hypothesis', 'Result', 'Result', 'Result']"
15,W06-3114,D07-1091,0,"Koehn and Monz, 2006",0,"We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","We report results on the development test set, which is also the out-of-domain test set of the WMT06 workshop shared task (Koehn and Monz, 2006)","['58', '126', '17', '49', '8']","['    <S sid=""58"" ssid=""24"">We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set, 53 blocks for the out-of-domain test set), check for each block, if one system has a higher BLEU score than the other, and then use the sign test.</S>\n', '    <S sid=""126"" ssid=""19"">The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.</S>\n', '    <S sid=""17"" ssid=""10"">Participants were also provided with two sets of 2,000 sentences of parallel text to be used for system development and tuning.</S>\n', '    <S sid=""49"" ssid=""15"">Hence, we use the bootstrap resampling method described by Koehn (2004).</S>\n', '    <S sid=""8"" ssid=""1"">The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.</S>\n']","['Result', 'Result', 'Result', 'Method', 'Result']"
16,W06-3114,P07-1108,0,"Koehn and Monz, 2006",0,"A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Ma chine Translation (Koehn and Monz, 2006)","A shared task to evaluate machine translation performance was organized as part of the NAACL/HLT 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006)","['38', '39', '46', '166', '56']","['    <S sid=""38"" ssid=""4"">The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).</S>\n', '    <S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S>\n', '    <S sid=""46"" ssid=""12"">By taking the ratio of matching n-grams to the total number of n-grams in the system output, we obtain the precision pn for each n-gram order n. These values for n-gram precision are combined into a BLEU score: The formula for the BLEU metric also includes a brevity penalty for too short output, which is based on the total number of words in the system output c and in the reference r. BLEU is sensitive to tokenization.</S>\n', '    <S sid=""166"" ssid=""59"">Lack of correct reference translations was pointed out as a short-coming of our evaluation.</S>\n', '    <S sid=""56"" ssid=""22"">The bootstrap method has been critized by Riezler and Maxwell (2005) and Collins et al. (2005), as being too optimistic in deciding for statistical significant difference between systems.</S>\n']","['Hypothesis', 'Hypothesis', 'Hypothesis', 'Implication', 'Implication']"
18,W06-3114,E12-3010,0,"Koehn and Monz, 2006",0,"For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","For the same reason, human evaluation metrics based on adequacy and fluency were not suitable either (Koehn and Monz, 2006)","['68', '140', '43', '50', '125']","['    <S sid=""68"" ssid=""7"">We asked participants to each judge 200&#8211;300 sentences in terms of fluency and adequacy, the most commonly used manual evaluation metrics.</S>\n', '    <S sid=""140"" ssid=""33"">We confirm the finding by Callison-Burch et al. (2006) that the rule-based system of Systran is not adequately appreciated by BLEU.</S>\n', '    <S sid=""43"" ssid=""9"">At the very least, we are creating a data resource (the manual annotations) that may the basis of future research in evaluation metrics.</S>\n', '    <S sid=""50"" ssid=""16"">Following this method, we repeatedly &#8212; say, 1000 times &#8212; sample sets of sentences from the output of each system, measure their BLEU score, and use these 1000 BLEU scores as basis for estimating a confidence interval.</S>\n', '    <S sid=""125"" ssid=""18"">We can check, what the consequences of less manual annotation of results would have been: With half the number of manual judgements, we can distinguish about 40% of the systems, 10% less.</S>\n']","['Result', 'Result', 'Result', 'Result', 'Result']"
19,W06-3114,W09-0402,0,"Koehn and Monz, 2006",0,"The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","The correlations on the document level were computed on the English, French, Spanish and German texts generated by various translation systems in the framework of the first (Koehn and Monz, 2006), second (Callison-Burch et al, 2007) and third shared translation task (Callison-Burchet al, 2008)","['39', '38', '174', '139', '46']","['    <S sid=""39"" ssid=""5"">However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.</S>\n', '    <S sid=""38"" ssid=""4"">The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).</S>\n', '    <S sid=""174"" ssid=""5"">The manual evaluation of scoring translation on a graded scale from 1&#8211;5 seems to be very hard to perform.</S>\n', '    <S sid=""139"" ssid=""32"">Given the closeness of most systems and the wide over-lapping confidence intervals it is hard to make strong statements about the correlation between human judgements and automatic scoring methods such as BLEU.</S>\n', '    <S sid=""46"" ssid=""12"">By taking the ratio of matching n-grams to the total number of n-grams in the system output, we obtain the precision pn for each n-gram order n. These values for n-gram precision are combined into a BLEU score: The formula for the BLEU metric also includes a brevity penalty for too short output, which is based on the total number of words in the system output c and in the reference r. BLEU is sensitive to tokenization.</S>\n']","['Hypothesis', 'Hypothesis', 'Result', 'Hypothesis', 'Hypothesis']"
