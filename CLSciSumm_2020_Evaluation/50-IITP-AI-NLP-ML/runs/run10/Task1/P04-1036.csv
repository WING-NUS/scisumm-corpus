Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P04-1036,W04-0837,0,"McCarthy et al, 2004",0,"The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding con text into account (McCarthy et al, 2004)","The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account (McCarthy et al, 2004)","['8', '15', '60', '182', '70']","['    <S sid=""8"" ssid=""1"">The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account.</S>\n', '    <S sid=""15"" ssid=""8"">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>\n', '    <S sid=""60"" ssid=""16"">If is the set of co-occurrence types such that is positive then the similarity between two nouns, and , can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S>\n', '    <S sid=""182"" ssid=""5"">In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.</S>\n', '    <S sid=""70"" ssid=""26"">Jiang and Conrath specify a distance measure: , where the third class ( ) is the most informative, or most specific, superordinate synset of the two senses and .</S>\n']","['Result', 'Result', 'Hypothesis', 'Hypothesis', 'Hypothesis']"
2,P04-1036,W04-0837,0,"McCarthy et al, 2004",0,"Association for Computational Linguistics for the Semantic Analysis of Text, Barcelona, Spain, July 2004 SENSEVAL-3: Third International Workshop on the Evaluation of Systems PoS precision recall baseline Noun 95 73 45 Verb 79 43 22 Adjective 88 59 44 Adverb 91 72 59 All PoS 90 63 41Table 2: The SENSEVAL-2 first sense on the SEN SEVAL-2 English all-words data system can be tuned to a given genre or domain (McCarthy et al, 2004) and also because there will be words that occur with insufficient frequency inthe hand-tagged resources available","Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to a given genre or domain (McCarthy et al., 2004) and also because there will be words that occur with insufficient frequency in the hand-tagged resources available","['15', '154', '189', '182', '41']","['    <S sid=""15"" ssid=""8"">Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful, there is a strong case for obtaining a first, or predominant, sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand.</S>\n', '    <S sid=""154"" ssid=""2"">In contrast, our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one, and because handtagged data is not always available.</S>\n', '    <S sid=""189"" ssid=""12"">Additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses.</S>\n', '    <S sid=""182"" ssid=""5"">In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.</S>\n', '    <S sid=""41"" ssid=""34"">In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses.</S>\n']","['Result', 'Result', 'Result', 'Hypothesis', 'Result']"
3,P04-1036,W04-0837,0,"McCarthy et al, 2004",0,"The method is described in (McCarthy et al, 2004), which we summarise here","The method is described in (McCarthy et al, 2004), which we summarise here","['64', '45', '75', '189', '175']","['    <S sid=""64"" ssid=""20"">We briefly summarise the two measures here; for a more detailed summary see (Patwardhan et al., 2003).</S>\n', '    <S sid=""45"" ssid=""1"">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>\n', '    <S sid=""75"" ssid=""4"">We generated a thesaurus entry for all polysemous nouns which occurred in SemCor with a frequency 2, and in the BNC with a frequency 10 in the grammatical relations listed in section 2.1 above.</S>\n', '    <S sid=""189"" ssid=""12"">Additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses.</S>\n', '    <S sid=""175"" ssid=""23"">We are currently investigating the performance of the first sense heuristic, and this method, for other PoS on SENSEVAL-3 data (McCarthy et al., 2004), although not yet with rankings from domain specific corpora.</S>\n']","['Result', 'Result', 'Result', 'Result', 'Hypothesis']"
5,P04-1036,I08-2105,0,"McCarthy et al, 2004",0,"McCarthy et al (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD.We build upon this previous research, and pro pose an unsupervised WSD method in which senses for two grammatically related words in the sentence will be connected through directed edges",McCarthy et al (2004) use a corpus and word similarities to induce a ranking of word senses from an untagged corpus to be used in WSD,"['166', '182', '180', '49', '2']","['    <S sid=""166"" ssid=""14"">There has been some related work on using automatic thesauruses for discovering word senses from corpora Pantel and Lin (2002).</S>\n', '    <S sid=""182"" ssid=""5"">In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus.</S>\n', '    <S sid=""180"" ssid=""3"">The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64% on an all-nouns task.</S>\n', '    <S sid=""49"" ssid=""5"">Let be the ordered set of the top scoring neighbours of from the thesaurus with associated distributional similarity scores The thesaurus was acquired using the method described by Lin (1998).</S>\n', '    <S sid=""2"" ssid=""2"">The problem with using the predominant, or first sense heuristic, aside from the fact that it does not take surrounding context into account, is that it assumes some quantity of handtagged data.</S>\n']","['Result', 'Hypothesis', 'Result', 'Hypothesis', 'Hypothesis']"
6,P04-1036,I08-2105,0,"McCarthy et al, 2004",0,"Previous re search in inducing sense rankings from an untagged corpus (McCarthy et al, 2004), and inducing selectional preferences at the word level (for other applications) (Erk, 2007) will provide the starting point for research in this direction","Previous research in inducing sense rankings from an untagged corpus (McCarthy et al, 2004), and inducing selectional preferences at the word level (for other applications) (Erk, 2007) will provide the starting point for research in this direction","['175', '166', '95', '193', '156']","['    <S sid=""175"" ssid=""23"">We are currently investigating the performance of the first sense heuristic, and this method, for other PoS on SENSEVAL-3 data (McCarthy et al., 2004), although not yet with rankings from domain specific corpora.</S>\n', '    <S sid=""166"" ssid=""14"">There has been some related work on using automatic thesauruses for discovering word senses from corpora Pantel and Lin (2002).</S>\n', '    <S sid=""95"" ssid=""24"">Since SemCor is derived from the Brown corpus, which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6, the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S>\n', '    <S sid=""193"" ssid=""2"">This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies, UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.</S>\n', '    <S sid=""156"" ssid=""4"">Buitelaar and Sacaleanu (2001) have previously explored ranking and selection of synsets in GermaNet for specific domains using the words in a given synset, and those related by hyponymy, and a term relevance measure taken from information retrieval.</S>\n']","['Hypothesis', 'Result', 'Hypothesis', 'Hypothesis', 'Result']"
7,P04-1036,I08-2105,0,2004,0,"McCarthy et al (2004) report a disambiguation precision of 53.0% and recall of 49.0% on the Senseval-2 test data, using an approach that derives sense ranking based on word similarity and distributional analysis in a corpus","McCarthy et al (2004) report a disambiguation precision of 53.0% and recall of 49.0% on the Senseval-2 test data, using an approach that derives sense ranking based on word similarity and distributional analysis in a corpus","['166', '175', '165', '55', '164']","['    <S sid=""166"" ssid=""14"">There has been some related work on using automatic thesauruses for discovering word senses from corpora Pantel and Lin (2002).</S>\n', '    <S sid=""175"" ssid=""23"">We are currently investigating the performance of the first sense heuristic, and this method, for other PoS on SENSEVAL-3 data (McCarthy et al., 2004), although not yet with rankings from domain specific corpora.</S>\n', '    <S sid=""165"" ssid=""13"">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus, whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S>\n', '    <S sid=""55"" ssid=""11"">For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC.</S>\n', '    <S sid=""164"" ssid=""12"">They used syntactic evidence to find a prior distribution for verb classes, based on (Levin, 1993), and incorporate this in a WSD system.</S>\n']","['Result', 'Hypothesis', 'Result', 'Hypothesis', 'Hypothesis']"
8,P04-1036,P06-1012,0,"McCarthy et al, 2004",0,"Research by (McCarthy et al, 2004) highlighted that the sense priors of a word in a corpus depend on the domain from which the corpus is drawn","Research by (McCarthy et al, 2004) highlighted that the sense priors of a word in a corpus depend on the domain from which the corpus is drawn","['95', '163', '175', '186', '150']","['    <S sid=""95"" ssid=""24"">Since SemCor is derived from the Brown corpus, which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6, the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S>\n', '    <S sid=""163"" ssid=""11"">Lapata and Brew (2004) have recently also highlighted the importance of a good prior in WSD.</S>\n', '    <S sid=""175"" ssid=""23"">We are currently investigating the performance of the first sense heuristic, and this method, for other PoS on SENSEVAL-3 data (McCarthy et al., 2004), although not yet with rankings from domain specific corpora.</S>\n', '    <S sid=""186"" ssid=""9"">In particular, we will use balanced and domain specific corpora to isolate words having very different neighbours, and therefore rankings, in the different corpora and to detect and target words for which there is a highly skewed sense distribution in these corpora.</S>\n', '    <S sid=""150"" ssid=""27"">Figure 2 displays the results of the second experiment with the domain specific corpora.</S>\n']","['Hypothesis', 'Hypothesis', 'Hypothesis', 'Hypothesis', 'Hypothesis']"
9,P04-1036,P06-1012,0,"McCarthy et al, 2004",0,"In addition, we implemented the unsupervised method of (McCarthy et al, 2004), which cal cu lates a prevalence score for each sense of a word to predict the predominant sense","In addition, we implemented the unsupervised method of (McCarthy et al, 2004), which calculates a prevalence score for each sense of a word to predict the predominant sense","['129', '44', '168', '30', '33']","['    <S sid=""129"" ssid=""6"">Many of the articles are economy related, but several other topics are included too.</S>\n', '    <S sid=""44"" ssid=""37"">We experiment with several WordNet Similarity measures (Patwardhan and Pedersen, 2003) which aim to capture semantic relatedness within</S>\n', '    <S sid=""168"" ssid=""16"">They evaluate using the lin measure described above in section 2.2 to determine the precision and recall of these discovered classes with respect to WordNet synsets.</S>\n', '    <S sid=""30"" ssid=""23"">Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively.</S>\n', '    <S sid=""33"" ssid=""26"">Many researchers are developing thesauruses from automatically parsed data.</S>\n']","['Implication', 'Result', 'Result', 'Result', 'Result']"
11,P04-1036,P10-1155,0,2004,0,"McCarthy et al (2004) reported that the best results were obtained using k= 50 neighbors and the Wordnet Similarityjcn measure (Jiang and Conrath, 1997)","McCarthy et al (2004) reported that the best results were obtained using k= 50 neighbors and the Wordnet Similarity jcn measure (Jiang and Conrath, 1997)","['175', '21', '166', '61', '193']","['    <S sid=""175"" ssid=""23"">We are currently investigating the performance of the first sense heuristic, and this method, for other PoS on SENSEVAL-3 data (McCarthy et al., 2004), although not yet with rankings from domain specific corpora.</S>\n', '    <S sid=""21"" ssid=""14"">We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off (Wilks and Stevenson, 1998; Hoste et al., 2001) and for systems that use it in lexical acquisition (McCarthy, 1997; Merlo and Leybold, 2001; Korhonen, 2002) because of the limited size of hand-tagged resources.</S>\n', '    <S sid=""166"" ssid=""14"">There has been some related work on using automatic thesauruses for discovering word senses from corpora Pantel and Lin (2002).</S>\n', '    <S sid=""61"" ssid=""17"">We use the WordNet Similarity Package 0.05 and WordNet version 1.6.</S>\n', '    <S sid=""193"" ssid=""2"">This work was funded by EU-2001-34460 project MEANING: Developing Multilingual Web-scale Language Technologies, UK EPSRC project Robust Accurate Statistical Parsing (RASP) and a UK EPSRC studentship.</S>\n']","['Hypothesis', 'Hypothesis', 'Result', 'Hypothesis', 'Hypothesis']"
12,P04-1036,W12-3401,0,2004,0,"In doing so, we provide first results on the application to French parsing of WordNet automatic sense ranking (ASR), using the method of McCarthy et al (2004)","In doing so, we provide first results on the application to French parsing of WordNet automatic sense ranking (ASR), using the method of McCarthy et al (2004)","['175', '166', '95', '62', '4']","['    <S sid=""175"" ssid=""23"">We are currently investigating the performance of the first sense heuristic, and this method, for other PoS on SENSEVAL-3 data (McCarthy et al., 2004), although not yet with rankings from domain specific corpora.</S>\n', '    <S sid=""166"" ssid=""14"">There has been some related work on using automatic thesauruses for discovering word senses from corpora Pantel and Lin (2002).</S>\n', '    <S sid=""95"" ssid=""24"">Since SemCor is derived from the Brown corpus, which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6, the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S>\n', '    <S sid=""62"" ssid=""18"">2 The WordNet Similarity package supports a range of WordNet similarity scores.</S>\n', '    <S sid=""4"" ssid=""4"">We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically.</S>\n']","['Hypothesis', 'Result', 'Hypothesis', 'Hypothesis', 'Hypothesis']"
13,P04-1036,W12-3401,0,2004,0,"To define an appropriate categorical distribution over synsets for each 2 lemma x in our source vocabulary, we first use the WordNet resource to identify the set Sx of different senses of x. We then use a distributional thesaurus to perform ASR, which determines the prevalence with respect to x of each sense s? Sx, following the approach of McCarthy et al (2004)","We then use a distributional thesaurus to perform ASR, which determines the prevalence with respect to x of each senses' Sx, following the approach of McCarthy et al (2004)","['177', '38', '25', '33', '23']","['    <S sid=""177"" ssid=""25"">Another major advantage that lesk has is that it is applicable to lexical resources which do not have the hierarchical structure that WordNet does, but do have definitions associated with word senses.</S>\n', '    <S sid=""38"" ssid=""31"">For example, the neighbours of star in a dependency-based thesaurus provided by Lin 1 has the ordered list of neighbours: superstar, player, teammate, actor early in the list, but one can also see words that are related to another sense of star e.g. galaxy, sun, world and planet further down the list.</S>\n', '    <S sid=""25"" ssid=""18"">However, the most accurate WSD systems are those which require manually sense tagged data in the first place, and their accuracy depends on the quantity of training examples (Yarowsky and Florian, 2002) available.</S>\n', '    <S sid=""33"" ssid=""26"">Many researchers are developing thesauruses from automatically parsed data.</S>\n', '    <S sid=""23"" ssid=""16"">The first sense of star in SemCor is celestial body, however, if one were disambiguating popular news celebrity would be preferred.</S>\n']","['Result', 'Result', 'Result', 'Result', 'Result']"
14,P04-1036,W12-3401,0,2004,0,"As explained in Section 2.2, ASR is performed using the method of McCarthy et al (2004)","As explained in Section 2.2, ASR is performed using the method of McCarthy et al (2004)","['175', '20', '45', '21', '189']","['    <S sid=""175"" ssid=""23"">We are currently investigating the performance of the first sense heuristic, and this method, for other PoS on SENSEVAL-3 data (McCarthy et al., 2004), although not yet with rankings from domain specific corpora.</S>\n', '    <S sid=""20"" ssid=""13"">Another example is embryo, which does not occur at all in SemCor and the first sense is listed as rudimentary plant rather than the anticipated fertilised egg meaning.</S>\n', '    <S sid=""45"" ssid=""1"">In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of Lin (1998).</S>\n', '    <S sid=""21"" ssid=""14"">We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off (Wilks and Stevenson, 1998; Hoste et al., 2001) and for systems that use it in lexical acquisition (McCarthy, 1997; Merlo and Leybold, 2001; Korhonen, 2002) because of the limited size of hand-tagged resources.</S>\n', '    <S sid=""189"" ssid=""12"">Additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses.</S>\n']","['Hypothesis', 'Result', 'Result', 'Hypothesis', 'Result']"
16,P04-1036,S12-1097,0,"McCarthy et al, 2004",0,"This approach is commonly used as a baseline for word sense disambiguation (McCarthy et al, 2004)","This approach is commonly used as a baseline for word sense disambiguation (McCarthy et al, 2004)","['175', '152', '68', '166', '95']","['    <S sid=""175"" ssid=""23"">We are currently investigating the performance of the first sense heuristic, and this method, for other PoS on SENSEVAL-3 data (McCarthy et al., 2004), although not yet with rankings from domain specific corpora.</S>\n', '    <S sid=""152"" ssid=""29"">We see that both domains have a similarly high percentage of factotum (domain independent) labels, but as we would expect, the other peaks correspond to the economy label for the FINANCE corpus, and the sports label for the SPORTS corpus. inant senses for 38 polysemous words ranked using the SPORTS and FINANCE corpus.</S>\n', '    <S sid=""68"" ssid=""24"">We are of course able to apply the method to other versions of WordNet. synset, is incremented with the frequency counts from the corpus of all words belonging to that synset, directly or via the hyponymy relation.</S>\n', '    <S sid=""166"" ssid=""14"">There has been some related work on using automatic thesauruses for discovering word senses from corpora Pantel and Lin (2002).</S>\n', '    <S sid=""95"" ssid=""24"">Since SemCor is derived from the Brown corpus, which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6, the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S>\n']","['Hypothesis', 'Hypothesis', 'Hypothesis', 'Result', 'Hypothesis']"
17,P04-1036,W10-2803,0,"McCarthy et al, 2004",0,"More radical solutions than sense grouping that have been proposed are to restrict the task to determining predominant sense in a given domain (McCarthy et al, 2004), or to work directly with para phrases (McCarthy and Navigli, 2009)","More radical solutions than sense grouping that have been proposed are to restrict the task to determining predominant sense in a given domain (McCarthy et al, 2004), or to work directly with para phrases (McCarthy and Navigli, 2009)","['175', '166', '62', '95', '71']","['    <S sid=""175"" ssid=""23"">We are currently investigating the performance of the first sense heuristic, and this method, for other PoS on SENSEVAL-3 data (McCarthy et al., 2004), although not yet with rankings from domain specific corpora.</S>\n', '    <S sid=""166"" ssid=""14"">There has been some related work on using automatic thesauruses for discovering word senses from corpora Pantel and Lin (2002).</S>\n', '    <S sid=""62"" ssid=""18"">2 The WordNet Similarity package supports a range of WordNet similarity scores.</S>\n', '    <S sid=""95"" ssid=""24"">Since SemCor is derived from the Brown corpus, which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6, the high ranking for the tobacco pipe sense according to SemCor seems plausible.</S>\n', '    <S sid=""71"" ssid=""27"">This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal:</S>\n']","['Hypothesis', 'Result', 'Hypothesis', 'Hypothesis', 'Implication']"
18,P04-1036,W08-2107,0,2004,0,"In addition, we can also infer a positive contribution of the frequency of a sense with the choice of the first synset returned by Word net resulting in a reasonable WSD heuristic (which is compatible with the results by McCarthy et al (2004))","In addition, we can also infer a positive contribution of the frequency of a sense with the choice of the first synset returned by Word net resulting in a reasonable WSD heuristic (which is compatible with the results by McCarthy et al (2004))","['189', '60', '17', '165', '175']","['    <S sid=""189"" ssid=""12"">Additionally, we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity, and what can be done to combat this problem using relation specific thesauruses.</S>\n', '    <S sid=""60"" ssid=""16"">If is the set of co-occurrence types such that is positive then the similarity between two nouns, and , can be computed as: where: A thesaurus entry of size for a target noun is then defined as the most similar nouns to .</S>\n', '    <S sid=""17"" ssid=""10"">There are words where the first sense in WordNet is counter-intuitive, because of the size of the corpus, and because where the frequency data does not indicate a first sense, the ordering is arbitrary.</S>\n', '    <S sid=""165"" ssid=""13"">Lapata and Brew obtain their priors for verb classes directly from subcategorisation evidence in a parsed corpus, whereas we use parsed data to find distributionally similar words (nearest neighbours) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence.</S>\n', '    <S sid=""175"" ssid=""23"">We are currently investigating the performance of the first sense heuristic, and this method, for other PoS on SENSEVAL-3 data (McCarthy et al., 2004), although not yet with rankings from domain specific corpora.</S>\n']","['Result', 'Hypothesis', 'Result', 'Result', 'Hypothesis']"
19,P04-1036,D07-1026,0,"McCarthy et al, 2004",0,"It is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al, 2004)","It is worthwhile to remark here that, being the IBLE algorithm fully unsupervised, improving the most frequent baseline is an excellent result, rarely achieved in the literature on unsupervised methods for WSD (McCarthy et al, 2004)","['156', '115', '47', '25', '33']","['    <S sid=""156"" ssid=""4"">Buitelaar and Sacaleanu (2001) have previously explored ranking and selection of synsets in GermaNet for specific domains using the words in a given synset, and those related by hyponymy, and a term relevance measure taken from information retrieval.</S>\n', '    <S sid=""115"" ssid=""13"">Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor, which is very encouraging given that our method only uses raw text, with no manual labelling.</S>\n', '    <S sid=""47"" ssid=""3"">We then use the WordNet similarity package (Patwardhan and Pedersen, 2003) to give us a semantic similarity measure (hereafter referred to as the WordNet similarity measure) to weight the contribution that each neighbour makes to the various senses of the target word.</S>\n', '    <S sid=""25"" ssid=""18"">However, the most accurate WSD systems are those which require manually sense tagged data in the first place, and their accuracy depends on the quantity of training examples (Yarowsky and Florian, 2002) available.</S>\n', '    <S sid=""33"" ssid=""26"">Many researchers are developing thesauruses from automatically parsed data.</S>\n']","['Result', 'Result', 'Result', 'Result', 'Result']"
20,P04-1036,W12-2429,0,"McCarthy et al, 2004",0,"The first, most frequent sense (MFS) (McCarthy et al, 2004), is widely used baseline for supervised WSD systems","The first, most frequent sense (MFS) (McCarthy et al, 2004), is widely used baseline for supervised WSD systems","['64', '188', '90', '25', '65']","['    <S sid=""64"" ssid=""20"">We briefly summarise the two measures here; for a more detailed summary see (Patwardhan et al., 2003).</S>\n', '    <S sid=""188"" ssid=""11"">We want to investigate the effect of frequency and choice of distributional similarity measure (Weeds et al., 2004).</S>\n', '    <S sid=""90"" ssid=""19"">From manual analysis, there are cases where the acquired first sense disagrees with SemCor, yet is intuitively plausible.</S>\n', '    <S sid=""25"" ssid=""18"">However, the most accurate WSD systems are those which require manually sense tagged data in the first place, and their accuracy depends on the quantity of training examples (Yarowsky and Florian, 2002) available.</S>\n', '    <S sid=""65"" ssid=""21"">The measures provide a similarity score between two WordNet senses ( and ), these being synsets within WordNet. lesk (Banerjee and Pedersen, 2002) This score maximises the number of overlapping words in the gloss, or definition, of the senses.</S>\n']","['Result', 'Result', 'Result', 'Result', 'Result']"
