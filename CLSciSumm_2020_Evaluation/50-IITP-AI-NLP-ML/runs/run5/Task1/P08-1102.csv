Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P08-1102,C08-1049,0,2008,0,"Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","Following Jiang et al (2008), we describe segmentation and Joint S& amp; T as below: For a given Chinese sentence appearing as a character sequence: C 1: n= C 1 C 2.","['22', '35', '4', '92', '3']","['    <S sid=""22"" ssid=""18"">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here, Ci (i = L.n) denotes Chinese character, ti (i = L.m) denotes POS tag, and Cl:r (l &lt; r) denotes character sequence ranges from Cl to Cr.</S>\n', '    <S sid=""35"" ssid=""7"">To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation.</S>\n', '    <S sid=""4"" ssid=""4"">On the Penn Chinese Treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S>\n', '    <S sid=""92"" ssid=""3"">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>\n', '    <S sid=""3"" ssid=""3"">Experiments show that the cascaded model achieves improved accuracies on both segmentation only and joint segmentation and part-of-speech tagging.</S>\n']","['Result', 'Implication', 'Hypothesis', 'Method', 'Hypothesis']"
2,P08-1102,C08-1049,0,2008,0,"As described in Ng and Low (2004 )andJiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","As described in Ng and Low (2004) and Jiang et al (2008), we use s indicating a single character word, while b, m and e indicating the be gin, middle and end of a word respectively","['29', '10', '15', '62', '39']","['    <S sid=""29"" ssid=""1"">The perceptron algorithm introduced into NLP by Collins (2002), is a simple but effective discriminative training method.</S>\n', '    <S sid=""10"" ssid=""6"">Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&amp;T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low, 2004).</S>\n', '    <S sid=""15"" ssid=""11"">To cope with this problem, we propose a cascaded linear model inspired by the log-linear model (Och and Ney, 2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.</S>\n', '    <S sid=""62"" ssid=""13"">Suppose we have n features gj (j = 1..n) coupled with n corresponding weights wj (j = 1..n), each feature gj gives a score gj(r) to a candidate r, then the total score of r is given by: The decoding procedure aims to find the candidate r* with the highest score: While the mission of the training procedure is to tune the weights wj(j = 1..n) to guarantee that the candidate r with the highest score happens to be the best result with a high probability.</S>\n', '    <S sid=""39"" ssid=""11"">We called them non-lexical-target because predications derived from them can predicate without considering the current character C0.</S>\n']","['Implication', 'Implication', 'Hypothesis', 'Hypothesis', 'Implication']"
3,P08-1102,C08-1049,0,2008,0,plates called lexical-target in the column below areintroduced by Jiang et al (2008),plates called lexical-target in the column below are introduced by Jiang et al (2008),"['43', '38', '35', '98', '39']","['    <S sid=""43"" ssid=""15"">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S>\n', '    <S sid=""38"" ssid=""10"">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S>\n', '    <S sid=""35"" ssid=""7"">To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation.</S>\n', '    <S sid=""98"" ssid=""9"">We found that LEX outperforms NON-LEX with a margin of about 0.002 at each iteration, and its learning curve reaches a tableland at iteration 7.</S>\n', '    <S sid=""39"" ssid=""11"">We called them non-lexical-target because predications derived from them can predicate without considering the current character C0.</S>\n']","['Result', 'Implication', 'Implication', 'Result', 'Implication']"
4,P08-1102,P12-1110,0,2008,0,"For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","For CTB-5, we refer to the split by Duan et al (2007) as CTB-5d, and to the split by Jiang et al (2008) as CTB-5j","['139', '104', '110', '31', '10']","['    <S sid=""139"" ssid=""2"">The authors were supported by National Natural Science Foundation of China, Contracts 60736014 and 60573188, and 863 State Key Project No.</S>\n', '    <S sid=""104"" ssid=""15"">According to the usual practice in syntactic analysis, we choose chapters 1 &#8722; 260 (18074 sentences) as training set, chapter 271 &#8722; 300 (348 sentences) as test set and chapter 301 &#8722; 325 (350 sentences) as development set.</S>\n', '    <S sid=""110"" ssid=""21"">Similar trend appeared in experiments of Ng and Low (2004), where they conducted experiments on CTB 3.0 and achieved Fmeasure 0.919 on Joint S&amp;T, a ratio of 96% to the F-measure 0.952 on segmentation.</S>\n', '    <S sid=""31"" ssid=""3"">The perceptron has been used in many NLP tasks, such as POS tagging (Collins, 2002), Chinese word segmentation (Ng and Low, 2004; Zhang and Clark, 2007) and so on.</S>\n', '    <S sid=""10"" ssid=""6"">Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&amp;T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low, 2004).</S>\n']","['Hypothesis', 'Hypothesis', 'Result', 'Hypothesis', 'Implication']"
5,P08-1102,D12-1126,0,2008,0,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,Jiang et al (2008) proposes a cascaded linear model for joint Chinese word segmentation and POS tagging,"['1', '118', '11', '15', '62']","['    <S sid=""1"" ssid=""1"">We propose a cascaded linear model for joint Chinese word segmentation and partof-speech tagging.</S>\n', '    <S sid=""118"" ssid=""29"">We find that the cascaded model achieves a F-measure increment of about 0.5 points on segmentation and about 0.9 points on Joint S&amp;T, over the perceptron-only model POS+.</S>\n', '    <S sid=""11"" ssid=""7"">Compared to performing segmentation and POS tagging one at a time, Joint S&amp;T can achieve higher accuracy not only on segmentation but also on POS tagging (Ng and Low, 2004).</S>\n', '    <S sid=""15"" ssid=""11"">To cope with this problem, we propose a cascaded linear model inspired by the log-linear model (Och and Ney, 2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.</S>\n', '    <S sid=""62"" ssid=""13"">Suppose we have n features gj (j = 1..n) coupled with n corresponding weights wj (j = 1..n), each feature gj gives a score gj(r) to a candidate r, then the total score of r is given by: The decoding procedure aims to find the candidate r* with the highest score: While the mission of the training procedure is to tune the weights wj(j = 1..n) to guarantee that the candidate r with the highest score happens to be the best result with a high probability.</S>\n']","['Hypothesis', 'Hypothesis', 'Hypothesis', 'Hypothesis', 'Hypothesis']"
6,P08-1102,C10-1135,0,2008,0,"We use the feature templates the same as Jiang et al, (2008) to extract features form E model","We use the feature templates the same as Jiang et al, (2008) to extract features form E model","['34', '123', '103', '96', '42']","['    <S sid=""34"" ssid=""6"">The feature templates we adopted are selected from those of Ng and Low (2004).</S>\n', '    <S sid=""123"" ssid=""34"">Without it, the F-measure on segmentation and Joint S&amp;T both suffer a decrement of 0.2 points.</S>\n', '    <S sid=""103"" ssid=""14"">We turned to experiments on CTB 5.0 to test the performance of the cascaded model.</S>\n', '    <S sid=""96"" ssid=""7"">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>\n', '    <S sid=""42"" ssid=""14"">As predications generated from such templates depend on the current character, we name these templates lexical-target.</S>\n']","['Result', 'Result', 'Method', 'Result', 'Result']"
8,P08-1102,P12-1025,0,"Jiangetal., 2008a",0,"approach, where basic processing units are characters which compose words (Jiangetal., 2008a)","basic processing units are characters which compose words (Jiangetal., 2008a)","['86', '10', '91', '8', '62']","['    <S sid=""86"" ssid=""11"">Line 4 scans words of all possible lengths l (l = 1.. min(i, K), where i points to the current considering character).</S>\n', '    <S sid=""10"" ssid=""6"">Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&amp;T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low, 2004).</S>\n', '    <S sid=""91"" ssid=""2"">The first was conducted to test the performance of the perceptron on segmentation on the corpus from SIGHAN Bakeoff 2, including the Academia Sinica Corpus (AS), the Hong Kong City University Corpus (CityU), the Peking University Corpus (PKU) and the Microsoft Research Corpus (MSR).</S>\n', '    <S sid=""8"" ssid=""4"">Another widely used discriminative method is the perceptron algorithm (Collins, 2002), which achieves comparable performance to CRFs with much faster training, so we base this work on the perceptron.</S>\n', '    <S sid=""62"" ssid=""13"">Suppose we have n features gj (j = 1..n) coupled with n corresponding weights wj (j = 1..n), each feature gj gives a score gj(r) to a candidate r, then the total score of r is given by: The decoding procedure aims to find the candidate r* with the highest score: While the mission of the training procedure is to tune the weights wj(j = 1..n) to guarantee that the candidate r with the highest score happens to be the best result with a high probability.</S>\n']","['Hypothesis', 'Implication', 'Hypothesis', 'Hypothesis', 'Hypothesis']"
9,P08-1102,C10-2096,0,2008b,0,"The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","The solid lines show the 1-best result, which is wrong. Jiang et al (2008b) stress the problems in re ranking phase","['86', '114', '25', '100', '31']","['    <S sid=""86"" ssid=""11"">Line 4 scans words of all possible lengths l (l = 1.. min(i, K), where i points to the current considering character).</S>\n', '    <S sid=""114"" ssid=""25"">We used SRI Language Modelling Toolkit (Stolcke and Andreas, 2002) to train a 3gram word LM with modified Kneser-Ney smoothing (Chen and Goodman, 1998), and a 4-gram POS LM with Witten-Bell smoothing, and we trained a word-POS co-occurrence model simply by MLE without smoothing.</S>\n', '    <S sid=""25"" ssid=""21"">According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>\n', '    <S sid=""100"" ssid=""11"">Test results listed in Table 2 shows that this model obtains higher accuracy than the best of SIGHAN Bakeoff 2 in three corpora (AS, CityU and MSR).</S>\n', '    <S sid=""31"" ssid=""3"">The perceptron has been used in many NLP tasks, such as POS tagging (Collins, 2002), Chinese word segmentation (Ng and Low, 2004; Zhang and Clark, 2007) and so on.</S>\n']","['Hypothesis', 'Implication', 'Result', 'Hypothesis', 'Hypothesis']"
10,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.1 Baseline Forest-based System We first segment the Chinese sentences into the1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","We first segment the Chinese sentences into the 1-best segmentations using a state-of-the-art system (Jiang et al, 2008a), since it is not necessary for a conventional parser to take as input the POS tagging results","['4', '96', '85', '97', '22']","['    <S sid=""4"" ssid=""4"">On the Penn Chinese Treebank 5.0, we obtain an error reduction of segmentation and joint segmentation and part-of-speech tagging over the perceptron-only baseline.</S>\n', '    <S sid=""96"" ssid=""7"">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>\n', '    <S sid=""85"" ssid=""10"">Lines 3 &#8212; 11 generate a N-best list for each character position i.</S>\n', '    <S sid=""97"" ssid=""8"">Figure 3 shows their learning curves depicting the F-measure on the development set after 1 to 10 training iterations.</S>\n', '    <S sid=""22"" ssid=""18"">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here, Ci (i = L.n) denotes Chinese character, ti (i = L.m) denotes POS tag, and Cl:r (l &lt; r) denotes character sequence ranges from Cl to Cr.</S>\n']","['Hypothesis', 'Result', 'Result', 'Result', 'Result']"
11,P08-1102,C10-2096,0,"Jiang et al, 2008a",0,"6.1.2 Lattice-forest SystemWe first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","We first segment and POS tag the Chinese sentences into word lattices using the same system (Jiang et al, 2008a), and prune each lattice into a reasonable size using the marginal probability-based pruning algorithm","['33', '25', '97', '45', '80']","['    <S sid=""33"" ssid=""5"">In following subsections, we describe the feature templates and the perceptron training algorithm.</S>\n', '    <S sid=""25"" ssid=""21"">According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>\n', '    <S sid=""97"" ssid=""8"">Figure 3 shows their learning curves depicting the F-measure on the development set after 1 to 10 training iterations.</S>\n', '    <S sid=""45"" ssid=""17"">We adopt the perceptron training algorithm of Collins (2002) to learn a discriminative model mapping from inputs x &#8712; X to outputs y &#8712; Y , where X is the set of sentences in the training corpus and Y is the set of corresponding labelled results.</S>\n', '    <S sid=""80"" ssid=""5"">At each position i, we enumerate all possible word-POS pairs by assigning each POS to each possible word formed from the character subsequence spanning length l = L. min(i, K) (K is assigned 20 in all our experiments) and ending at position i, then we derive all candidate results by attaching each word-POS pair p (of length l) to the tail of each candidate result at the prior position of p (position i&#8722;l), and select for position i a N-best list of candidate results from all these candidates.</S>\n']","['Hypothesis', 'Result', 'Result', 'Hypothesis', 'Hypothesis']"
12,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","However, when we repeat the work of (Jiang et al, 2008), which reports to achieve the state-of-art performance in the data-sets that we adopt, it has been found that some features (e.g., C0) are unnoticeably trained several times in their model (which are implicitly generated from different feature templates used in the paper)","['116', '96', '12', '111', '59']","['    <S sid=""116"" ssid=""27"">In order to inspect how much improvement each feature brings into the cascaded model, every time we removed a feature while retaining others, then retrained the model and tested its performance on the test set.</S>\n', '    <S sid=""96"" ssid=""7"">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>\n', '    <S sid=""12"" ssid=""8"">Besides the usual character-based features, additional features dependent on POS&#8217;s or words can also be employed to improve the performance.</S>\n', '    <S sid=""111"" ssid=""22"">As the next step, a group of experiments were conducted to investigate how well the cascaded linear model performs.</S>\n', '    <S sid=""59"" ssid=""10"">Since the perceptron is fixed during the second training step, the whole training procedure need relative small time and memory cost.</S>\n']","['Method', 'Result', 'Result', 'Result', 'Implication']"
13,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice, which is generated from two different templates Cn (with n=0, generates C0) and [C0Cn] (used in (Jiang et al, 2008), with n=0, generates [C0C0])","['22', '96', '92', '53', '104']","['    <S sid=""22"" ssid=""18"">2 Segmentation and POS Tagging Given a Chinese character sequence: while the segmentation and POS tagging result can be depicted as: Here, Ci (i = L.n) denotes Chinese character, ti (i = L.m) denotes POS tag, and Cl:r (l &lt; r) denotes character sequence ranges from Cl to Cr.</S>\n', '    <S sid=""96"" ssid=""7"">In order to test the performance of the lexical-target templates and meanwhile determine the best iterations over the training corpus, we randomly chosen 2, 000 shorter sentences (less than 50 words) as the development set and the rest as the training set (84, 294 sentences), then trained a perceptron model named NON-LEX using only nonlexical-target features and another named LEX using both the two kinds of features.</S>\n', '    <S sid=""92"" ssid=""3"">The second was conducted on the Penn Chinese Treebank 5.0 (CTB5.0) to test the performance of the cascaded model on segmentation and Joint S&amp;T.</S>\n', '    <S sid=""53"" ssid=""4"">Figure 2 shows the growing tendency of feature space with the introduction of these features as well as the character-based ones.</S>\n', '    <S sid=""104"" ssid=""15"">According to the usual practice in syntactic analysis, we choose chapters 1 &#8722; 260 (18074 sentences) as training set, chapter 271 &#8722; 300 (348 sentences) as test set and chapter 301 &#8722; 325 (350 sentences) as development set.</S>\n']","['Result', 'Result', 'Method', 'Result', 'Hypothesis']"
14,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be? n?, at least in principle","As all the features adopted in (Jiang et al, 2008) possess binary values, if a binary feature is repeated n times, then it should behave like a real-valued feature with its value to be n, at least in principle","['7', '63', '113', '65', '76']","['    <S sid=""7"" ssid=""3"">CRFs have the advantage of flexibility in representing features compared to generative ones such as HMM, and usually behaves the best in the two tasks.</S>\n', '    <S sid=""63"" ssid=""14"">As all the sub-models, including the perceptron, are regarded as separate features of the outside-layer linear model, we can train them respectively with special algorithms.</S>\n', '    <S sid=""113"" ssid=""24"">Besides this perceptron, other sub-models are trained and used as additional features of the outside-layer linear model.</S>\n', '    <S sid=""65"" ssid=""16"">As shown in Figure 1, the character-based perceptron is used as the inside-layer linear model and sends its output to the outside-layer.</S>\n', '    <S sid=""76"" ssid=""1"">Sequence segmentation and labelling problem can be solved through a viterbi style decoding procedure.</S>\n']","['Result', 'Result', 'Result', 'Result', 'Result']"
15,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","Inspired by (Jiang et al, 2008), we set the real d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C-1C0anC0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based discriminative-plus model","['129', '110', '134', '35', '8']","['    <S sid=""129"" ssid=""40"">Experimental results show that, it achieves obvious improvement over the perceptron-only model, about from 0.973 to 0.978 on segmentation, and from 0.925 to 0.934 on Joint S&amp;T, with error reductions of 18.5% and 12% respectively.</S>\n', '    <S sid=""110"" ssid=""21"">Similar trend appeared in experiments of Ng and Low (2004), where they conducted experiments on CTB 3.0 and achieved Fmeasure 0.919 on Joint S&amp;T, a ratio of 96% to the F-measure 0.952 on segmentation.</S>\n', '    <S sid=""134"" ssid=""5"">If this cascaded linear model were chosen, could more accurate generative models (LMs, word-POS co-occurrence model) be obtained by training on large scale corpus even if the corpus is not correctly labelled entirely, or by self-training on raw corpus in a similar approach to that of McClosky (2006)?</S>\n', '    <S sid=""35"" ssid=""7"">To compare with others conveniently, we excluded the ones forbidden by the close test regulation of SIGHAN, for example, Pu(C0), indicating whether character C0 is a punctuation.</S>\n', '    <S sid=""8"" ssid=""4"">Another widely used discriminative method is the perceptron algorithm (Collins, 2002), which achieves comparable performance to CRFs with much faster training, so we base this work on the perceptron.</S>\n']","['Hypothesis', 'Result', 'Implication', 'Implication', 'Hypothesis']"
17,P08-1102,C10-1132,0,"Jiang et al, 2008",0,"Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","Last, (Jiang et al, 2008) 5 adds repeated features implicitly based on (Ng and Low, 2004)","['38', '110', '34', '43', '15']","['    <S sid=""38"" ssid=""10"">Templates immediately borrowed from Ng and Low (2004) are listed in the upper column named non-lexical-target.</S>\n', '    <S sid=""110"" ssid=""21"">Similar trend appeared in experiments of Ng and Low (2004), where they conducted experiments on CTB 3.0 and achieved Fmeasure 0.919 on Joint S&amp;T, a ratio of 96% to the F-measure 0.952 on segmentation.</S>\n', '    <S sid=""34"" ssid=""6"">The feature templates we adopted are selected from those of Ng and Low (2004).</S>\n', '    <S sid=""43"" ssid=""15"">Note that the templates of Ng and Low (2004) have already contained some lexical-target ones.</S>\n', '    <S sid=""15"" ssid=""11"">To cope with this problem, we propose a cascaded linear model inspired by the log-linear model (Och and Ney, 2004) widely used in statistical machine translation to incorporate different kinds of knowledge sources.</S>\n']","['Implication', 'Result', 'Result', 'Result', 'Hypothesis']"
20,P08-1102,D12-1046,0,Jiang et al2008a,0,"Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a) ,perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), re ranking (Jiang et al2008b)","['114', '10', '31', '127', '25']","['    <S sid=""114"" ssid=""25"">We used SRI Language Modelling Toolkit (Stolcke and Andreas, 2002) to train a 3gram word LM with modified Kneser-Ney smoothing (Chen and Goodman, 1998), and a 4-gram POS LM with Witten-Bell smoothing, and we trained a word-POS co-occurrence model simply by MLE without smoothing.</S>\n', '    <S sid=""10"" ssid=""6"">Since the typical approach of discriminative models treats segmentation as a labelling problem by assigning each character a boundary tag (Xue and Shen, 2003), Joint S&amp;T can be conducted in a labelling fashion by expanding boundary tags to include POS information (Ng and Low, 2004).</S>\n', '    <S sid=""31"" ssid=""3"">The perceptron has been used in many NLP tasks, such as POS tagging (Collins, 2002), Chinese word segmentation (Ng and Low, 2004; Zhang and Clark, 2007) and so on.</S>\n', '    <S sid=""127"" ssid=""38"">Finally, the word count penalty gives improvement to the cascaded model, 0.13 points on segmentation and 0.16 points on Joint S&amp;T.</S>\n', '    <S sid=""25"" ssid=""21"">According to Ng and Low (2004), the segmentation task can be transformed to a tagging problem by assigning each character a boundary tag of the following four types: We can extract segmentation result by splitting the labelled result into subsequences of pattern s or bm*e which denote single-character word and multicharacter word respectively.</S>\n']","['Implication', 'Implication', 'Hypothesis', 'Hypothesis', 'Result']"
