Citance Number,Reference Article,Citing Article,Citation Marker Offset,Citation Marker,Citation Offset,Citation Text,Citation Text Clean,Reference Offset,Reference Text,Discourse Facet
1,P11-1060,D11-1039,0,2011,0,"Clarkeet al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning","Clarke et al (2010) and Liang et al (2011) describe approaches for learning semantic parsers from questions paired with database answers, while Goldwasser et al (2011) presents work on unsupervised learning","['163', '153', '9', '132', '166']","['    <S sid=""163"" ssid=""48"">Feedback from the context; for example, the lexical entry for borders world has been used to guide both syntactic parsing is S\\NP/NP : Ay.Ax.border(x, y), which means (Schuler, 2003) and semantic parsing (Popescu et borders looks right for the first argument and left al., 2003; Clarke et al., 2010).</S>\n', '    <S sid=""153"" ssid=""38"">Trace inspired by Discourse Representation Theory (DRT) predicates can be inserted anywhere, but the fea- (Kamp and Reyle, 1993; Kamp et al., 2005), where tures favor some insertions depending on the words variables are discourse referents.</S>\n', '    <S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>\n', '    <S sid=""132"" ssid=""17"">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>\n', '    <S sid=""166"" ssid=""51"">Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language&#8212;think grounded al., 2010). compositional semantics.</S>\n']","['Result', 'Result', 'Implication', 'Result', 'Hypothesis']"
2,P11-1060,P13-1092,0,2011,0,"In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance","In particular, Clarke et al (2010) and Liang et al (2011) proposed methods to learn from question answer pairs alone, which represents a significant advance","['9', '165', '163', '153', '99']","['    <S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>\n', '    <S sid=""165"" ssid=""50"">These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others.</S>\n', '    <S sid=""163"" ssid=""48"">Feedback from the context; for example, the lexical entry for borders world has been used to guide both syntactic parsing is S\\NP/NP : Ay.Ax.border(x, y), which means (Schuler, 2003) and semantic parsing (Popescu et borders looks right for the first argument and left al., 2003; Clarke et al., 2010).</S>\n', '    <S sid=""153"" ssid=""38"">Trace inspired by Discourse Representation Theory (DRT) predicates can be inserted anywhere, but the fea- (Kamp and Reyle, 1993; Kamp et al., 2005), where tures favor some insertions depending on the words variables are discourse referents.</S>\n', '    <S sid=""99"" ssid=""75"">To further reduce the search space, F imposes a few additional constraints, e.g., limiting the number of marked nodes to 2 and only allowing trace predicates between arity 1 predicates.</S>\n']","['Implication', 'Result', 'Result', 'Result', 'Hypothesis']"
3,P11-1060,P13-1092,0,2011,0,"To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation 1Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments","To handle syntax-semantics mismatch, GUSP introduces a novel dependency-based meaning representation. Clarke et al (2010) and Liang et al (2011) used the annotated logical forms to compute answers for their experiments","['9', '165', '51', '163', '47']","['    <S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>\n', '    <S sid=""165"" ssid=""50"">These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others.</S>\n', '    <S sid=""51"" ssid=""27"">It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent&#8217;s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S>\n', '    <S sid=""163"" ssid=""48"">Feedback from the context; for example, the lexical entry for borders world has been used to guide both syntactic parsing is S\\NP/NP : Ay.Ax.border(x, y), which means (Schuler, 2003) and semantic parsing (Popescu et borders looks right for the first argument and left al., 2003; Clarke et al., 2010).</S>\n', '    <S sid=""47"" ssid=""23"">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.</S>\n']","['Implication', 'Result', 'Hypothesis', 'Result', 'Result']"
4,P11-1060,P13-1092,0,2011,0,"More recently, Liang et al (2011 )proposedDCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins","More recently, Liang et al (2011) proposed DCS for dependency-based compositional semantics, which represents a semantic parse as a tree with nodes representing database elements and operations, and edges representing relational joins","['25', '68', '47', '15', '51']","['    <S sid=""25"" ssid=""1"">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>\n', '    <S sid=""68"" ssid=""44"">The denotation of a DCS tree can now be defined recursively: The base case is defined in (3): if z is a single node with predicate p, then the denotation of z has one column with the tuples w(p) and an empty store.</S>\n', '    <S sid=""47"" ssid=""23"">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.</S>\n', '    <S sid=""15"" ssid=""11"">Unlike standard semantic parsing, our end goal is only to generate the correct y, so we are free to choose the representation for z.</S>\n', '    <S sid=""51"" ssid=""27"">It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent&#8217;s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S>\n']","['Hypothesis', 'Hypothesis', 'Result', 'Result', 'Hypothesis']"
5,P11-1060,P13-1092,0,"Liang et al, 2011",0,"GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011)","GUSP represents meaning by a semantic tree, which is similar to DCS (Liang et al, 2011)","['25', '47', '99', '141', '112']","['    <S sid=""25"" ssid=""1"">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>\n', '    <S sid=""47"" ssid=""23"">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.</S>\n', '    <S sid=""99"" ssid=""75"">To further reduce the search space, F imposes a few additional constraints, e.g., limiting the number of marked nodes to 2 and only allowing trace predicates between arity 1 predicates.</S>\n', '    <S sid=""141"" ssid=""26"">Rather than using lexical triggers, several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S>\n', '    <S sid=""112"" ssid=""88"">Our learning algorithm alternates between (i) using the current parameters &#952; to generate the K-best set &#732;ZL,&#952;(x) for each training example x, and (ii) optimizing the parameters to put probability mass on the correct trees in these sets; sets containing no correct answers are skipped.</S>\n']","['Hypothesis', 'Result', 'Hypothesis', 'Hypothesis', 'Result']"
6,P11-1060,W12-2802,0,2011,0,"Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world","Matuszek et al [2010], Liang et al [2011] and Chen and Mooney [2011] describe models that learn compositional semantics, but word meanings are symbolic structures rather than patterns of features in the external world","['165', '163', '154', '18', '47']","['    <S sid=""165"" ssid=""50"">These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others.</S>\n', '    <S sid=""163"" ssid=""48"">Feedback from the context; for example, the lexical entry for borders world has been used to guide both syntactic parsing is S\\NP/NP : Ay.Ax.border(x, y), which means (Schuler, 2003) and semantic parsing (Popescu et borders looks right for the first argument and left al., 2003; Clarke et al., 2010).</S>\n', '    <S sid=""154"" ssid=""39"">The restriction to present (for example, [in, loc] has high weight). trees is similar to economical DRT (Bos, 2009).</S>\n', '    <S sid=""18"" ssid=""14"">CCG is one instantiation (Steedman, 2000), which is used by many semantic parsers, e.g., Zettlemoyer and Collins (2005).</S>\n', '    <S sid=""47"" ssid=""23"">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.</S>\n']","['Result', 'Result', 'Hypothesis', 'Hypothesis', 'Result']"
7,P11-1060,P13-2009,0,"Liang et al, 2011",0,"It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it ,e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011)","It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it ,e.g. rule-based (Popescu et al, 2003), super vised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al, 2011)","['140', '135', '153', '20', '166']","['    <S sid=""140"" ssid=""25"">Table 3 shows that even DCS, which does not use prototypes, is comparable to the best previous system (Kwiatkowski et al., 2010), and by adding a few prototypes, DCS+ offers a decisive edge (91.1% over 88.9% on GEO).</S>\n', '    <S sid=""135"" ssid=""20"">SEMRESP requires a lexicon of 1.42 words per non-value predicate, WordNet features, and syntactic parse trees; DCS requires only words for the domain-independent predicates (overall, around 0.5 words per non-value predicate), POS tags, and very simple indicator features.</S>\n', '    <S sid=""153"" ssid=""38"">Trace inspired by Discourse Representation Theory (DRT) predicates can be inserted anywhere, but the fea- (Kamp and Reyle, 1993; Kamp et al., 2005), where tures favor some insertions depending on the words variables are discourse referents.</S>\n', '    <S sid=""20"" ssid=""16"">At the same time, representations such as FunQL (Kate et al., 2005), which was used in Clarke et al. (2010), are simpler but lack the full expressive power of lambda calculus.</S>\n', '    <S sid=""166"" ssid=""51"">Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language&#8212;think grounded al., 2010). compositional semantics.</S>\n']","['Hypothesis', 'Result', 'Result', 'Result', 'Hypothesis']"
8,P11-1060,D12-1069,0,Liangetal2011,0,"One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Liangetal2011) or even a binary correct/incorrect signal (Clarke et al2010)","One line of work eliminates the need for an annotated logical form, instead using only the correct answer for a database query (Lianget al 2011) or even a binary correct/incorrect signal (Clarke et al2010)","['165', '9', '22', '146', '51']","['    <S sid=""165"" ssid=""50"">These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others.</S>\n', '    <S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>\n', '    <S sid=""22"" ssid=""18"">The logical forms in this framework are trees, which is desirable for two reasons: (i) they parallel syntactic dependency trees, which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S>\n', '    <S sid=""146"" ssid=""31"">We find that only for a small fraction of training examples do the K-best sets contain any trees yielding the correct answer (29% for DCS on GEO).</S>\n', '    <S sid=""51"" ssid=""27"">It is impossible to represent the semantics of this phrase with just a CSP, so we introduce a new aggregate relation, notated E. Consider a tree hE:ci, whose root is connected to a child c via E. If the denotation of c is a set of values s, the parent&#8217;s denotation is then a singleton set containing s. Formally: Figure 3(a) shows the DCS tree for our running example.</S>\n']","['Result', 'Implication', 'Hypothesis', 'Hypothesis', 'Hypothesis']"
9,P11-1060,N12-1049,0,2011,0,"For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form","For example, Liang et al (2011) constructs a latent parse similar in structure to a dependency grammar, but representing a logical form","['150', '49', '165', '47', '82']","['    <S sid=""150"" ssid=""35"">For example, area (by virtue which results in programs (DCS trees) which are of being a noun) triggers many predicates: city, much simpler than the logically-equivalent lambda state, area, etc.</S>\n', '    <S sid=""49"" ssid=""25"">Aggregate relation DCS trees that only use join relations can represent arbitrarily complex compositional structures, but they cannot capture higherorder phenomena in language.</S>\n', '    <S sid=""165"" ssid=""50"">These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others.</S>\n', '    <S sid=""47"" ssid=""23"">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.</S>\n', '    <S sid=""82"" ssid=""58"">Marking is simple: When a node (e.g., size in Figure 5) is marked (e.g., with relation C), we simply put the relation r, current denotation d and child c&#8217;s denotation into the store of column 1: The execute operation Xi(d) processes columns i in reverse order.</S>\n']","['Result', 'Result', 'Result', 'Result', 'Result']"
10,P11-1060,P12-1045,0,2011,0,Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers,Clarke et al (2010) and Liang et al (2011) trained systems on question and answer pairs by automatically finding semantic interpretations of the questions that would generate the correct answers,"['157', '146', '22', '165', '80']","['    <S sid=""157"" ssid=""42"">Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem.</S>\n', '    <S sid=""146"" ssid=""31"">We find that only for a small fraction of training examples do the K-best sets contain any trees yielding the correct answer (29% for DCS on GEO).</S>\n', '    <S sid=""22"" ssid=""18"">The logical forms in this framework are trees, which is desirable for two reasons: (i) they parallel syntactic dependency trees, which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S>\n', '    <S sid=""165"" ssid=""50"">These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others.</S>\n', '    <S sid=""80"" ssid=""56"">The full definition of join is as follows: Aggregate The aggregate operation takes a denotation and forms a set out of the tuples in the first column for each setting of the rest of the columns: Now we turn to the mark (M) and execute (Xi) operations, which handles the divergence between syntactic and semantic scope.</S>\n']","['Result', 'Hypothesis', 'Hypothesis', 'Result', 'Result']"
11,P11-1060,P14-1008,0,"Liang et al,2011",0,"Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011)","Dependency-based Compositional Semantics (DCS) provides an intuitive way to model semantics of questions, by using simple dependency-like trees (Liang et al, 2011)","['99', '21', '36', '172', '25']","['    <S sid=""99"" ssid=""75"">To further reduce the search space, F imposes a few additional constraints, e.g., limiting the number of marked nodes to 2 and only allowing trace predicates between arity 1 predicates.</S>\n', '    <S sid=""21"" ssid=""17"">The main technical contribution of this work is a new semantic representation, dependency-based compositional semantics (DCS), which is both simple and expressive (Section 2).</S>\n', '    <S sid=""36"" ssid=""12"">It is this transparency between syntax and semantics provided by DCS which leads to a simple and streamlined compositional semantics suitable for program induction.</S>\n', '    <S sid=""172"" ssid=""57"">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms, we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus, and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S>\n', '    <S sid=""25"" ssid=""1"">We first present a basic version (Section 2.1) of dependency-based compositional semantics (DCS), which captures the core idea of using trees to represent formal semantics.</S>\n']","['Hypothesis', 'Implication', 'Hypothesis', 'Result', 'Hypothesis']"
12,P11-1060,P14-1008,0,"Liang et al, 2011",0,"DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1)","DCS trees has been proposed to represent natural language semantics with a structure similar to dependency trees (Liang et al, 2011) (Figure 1)","['47', '49', '135', '168', '153']","['    <S sid=""47"" ssid=""23"">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.</S>\n', '    <S sid=""49"" ssid=""25"">Aggregate relation DCS trees that only use join relations can represent arbitrarily complex compositional structures, but they cannot capture higherorder phenomena in language.</S>\n', '    <S sid=""135"" ssid=""20"">SEMRESP requires a lexicon of 1.42 words per non-value predicate, WordNet features, and syntactic parse trees; DCS requires only words for the domain-independent predicates (overall, around 0.5 words per non-value predicate), POS tags, and very simple indicator features.</S>\n', '    <S sid=""168"" ssid=""53"">A trigger for We built a system that interprets natural language borders specifies only that border can be used, but utterances much more accurately than existing sysnot how.</S>\n', '    <S sid=""153"" ssid=""38"">Trace inspired by Discourse Representation Theory (DRT) predicates can be inserted anywhere, but the fea- (Kamp and Reyle, 1993; Kamp et al., 2005), where tures favor some insertions depending on the words variables are discourse referents.</S>\n']","['Result', 'Result', 'Result', 'Result', 'Result']"
13,P11-1060,P14-1008,0,"Liang et al, 2011",0,"are explained in? 2.5. 5http: //nlp.stanford.edu/software/corenlp.shtml 6 In (Liang et al, 2011) DCS trees are learned from QApairs and database entries","In (Liang et al, 2011) DCS trees are learned from QA pairs and database entries","['160', '47', '153', '171', '69']","['    <S sid=""160"" ssid=""45"">To contrast, consider et al. (2010), which we discussed earlier.</S>\n', '    <S sid=""47"" ssid=""23"">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.</S>\n', '    <S sid=""153"" ssid=""38"">Trace inspired by Discourse Representation Theory (DRT) predicates can be inserted anywhere, but the fea- (Kamp and Reyle, 1993; Kamp et al., 2005), where tures favor some insertions depending on the words variables are discourse referents.</S>\n', '    <S sid=""171"" ssid=""56"">This yields a more system is based on a new semantic representation, factorized and flexible representation that is easier DCS, which offers a simple and expressive alterto search through and parametrize using features. native to lambda calculus.</S>\n', '    <S sid=""69"" ssid=""45"">The other six cases handle different edge relations.</S>\n']","['Result', 'Result', 'Result', 'Result', 'Result']"
14,P11-1060,P14-1008,0,"Liang et al, 2011",0,"as in the sentence? Tropi cal storm Debby is blamed for death?, which is a tropical storm, is Debby, etc. Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable","Technically, each germ in a DCS tree indicates a variable when the DCS tree is translated to a FOL formula, and the abstract denotation of the germ corresponds to the set of consistent values (Liang et al, 2011) of that variable","['47', '142', '165', '148', '99']","['    <S sid=""47"" ssid=""23"">This algorithm is linear in the number of nodes times the size of the denotations.1 Now the dual importance of trees in DCS is clear: We have seen that trees parallel syntactic dependency structure, which will facilitate parsing.</S>\n', '    <S sid=""142"" ssid=""27"">This option is not available to us since we do not have annotated logical forms, so we must instead rely on lexical triggers to define the search space.</S>\n', '    <S sid=""165"" ssid=""50"">These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others.</S>\n', '    <S sid=""148"" ssid=""33"">This bootstrapping behavior occurs naturally: The &#8220;easy&#8221; examples are processed first, where easy is defined by the ability of the current model to generate the correct answer using any tree. with scope variation.</S>\n', '    <S sid=""99"" ssid=""75"">To further reduce the search space, F imposes a few additional constraints, e.g., limiting the number of marked nodes to 2 and only allowing trace predicates between arity 1 predicates.</S>\n']","['Result', 'Hypothesis', 'Result', 'Result', 'Hypothesis']"
15,P11-1060,D11-1140,0,2011,0,Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available,Clarke et al (2010) and Liang et al (2011) replace semantic annotations in the training set with target answers which are more easily available,"['165', '146', '157', '22', '9']","['    <S sid=""165"" ssid=""50"">These rules are often too stringent, cused on aligning text to a world (Liang et al., 2009), and for complex utterances, especially in free word- using text in reinforcement learning (Branavan et al., order languages, either disharmonic combinators are 2009; Branavan et al., 2010), and many others.</S>\n', '    <S sid=""146"" ssid=""31"">We find that only for a small fraction of training examples do the K-best sets contain any trees yielding the correct answer (29% for DCS on GEO).</S>\n', '    <S sid=""157"" ssid=""42"">Eisenciations due to data sparsity, and having an insuffi- stein et al. (2009) induces conjunctive formulae and ciently large K. uses them as features in another learning problem.</S>\n', '    <S sid=""22"" ssid=""18"">The logical forms in this framework are trees, which is desirable for two reasons: (i) they parallel syntactic dependency trees, which facilitates parsing and learning; and (ii) evaluating them to obtain the answer is computationally efficient.</S>\n', '    <S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>\n']","['Result', 'Hypothesis', 'Result', 'Hypothesis', 'Implication']"
16,P11-1060,D11-1140,0,"Liang et al, 2011",0,"and Collins, 2005, 2007),? -WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011)","WASP (Wong and Mooney, 2007), UBL (Kwiatkowski et al, 2010) systems and DCS (Liang et al, 2011)","['125', '7', '18', '166', '163']","['    <S sid=""125"" ssid=""10"">We used the same training-test splits as Zettlemoyer and Collins (2005) (600+280 for GEO and 500+140 for JOBS).</S>\n', '    <S sid=""7"" ssid=""3"">Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive.</S>\n', '    <S sid=""18"" ssid=""14"">CCG is one instantiation (Steedman, 2000), which is used by many semantic parsers, e.g., Zettlemoyer and Collins (2005).</S>\n', '    <S sid=""166"" ssid=""51"">Our employed (Zettlemoyer and Collins, 2007) or words work pushes the grounded language agenda towards are given multiple lexical entries (Kwiatkowski et deeper representations of language&#8212;think grounded al., 2010). compositional semantics.</S>\n', '    <S sid=""163"" ssid=""48"">Feedback from the context; for example, the lexical entry for borders world has been used to guide both syntactic parsing is S\\NP/NP : Ay.Ax.border(x, y), which means (Schuler, 2003) and semantic parsing (Popescu et borders looks right for the first argument and left al., 2003; Clarke et al., 2010).</S>\n']","['Hypothesis', 'Hypothesis', 'Hypothesis', 'Hypothesis', 'Result']"
17,P11-1060,P13-1007,0,2011,0,"In general, every plural NPpotentially introduces an implicit universal, ranging 1For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model","For example, Liang et al (2011) in their state-of-the-art statistical semantic parser within the domain of natural language queries to databases, explicitly devise quantifier scoping in the semantic model","['142', '80', '141', '100', '99']","['    <S sid=""142"" ssid=""27"">This option is not available to us since we do not have annotated logical forms, so we must instead rely on lexical triggers to define the search space.</S>\n', '    <S sid=""80"" ssid=""56"">The full definition of join is as follows: Aggregate The aggregate operation takes a denotation and forms a set out of the tuples in the first column for each setting of the rest of the columns: Now we turn to the mark (M) and execute (Xi) operations, which handles the divergence between syntactic and semantic scope.</S>\n', '    <S sid=""141"" ssid=""26"">Rather than using lexical triggers, several of the other systems use IBM word alignment models to produce an initial word-predicate mapping.</S>\n', '    <S sid=""100"" ssid=""76"">Model We now present our discriminative semantic parsing model, which places a log-linear distribution over z &#8712; ZL(x) given an utterance x.</S>\n', '    <S sid=""99"" ssid=""75"">To further reduce the search space, F imposes a few additional constraints, e.g., limiting the number of marked nodes to 2 and only allowing trace predicates between arity 1 predicates.</S>\n']","['Hypothesis', 'Result', 'Hypothesis', 'Implication', 'Hypothesis']"
18,P11-1060,D11-1022,0,2011,0,"DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011)","DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al (2011)","['9', '158', '6', '172', '149']","['    <S sid=""9"" ssid=""5"">As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers.</S>\n', '    <S sid=""158"" ssid=""43"">5 Discussion Piantadosi et al. (2008) induces first-order formuA major focus of this work is on our semantic rep- lae using CCG in a small domain assuming observed resentation, DCS, which offers a new perspective lexical semantics.</S>\n', '    <S sid=""6"" ssid=""2"">Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing).</S>\n', '    <S sid=""172"" ssid=""57"">Free from the burden It also allows us to easily add new lexical triggers of annotating logical forms, we hope to use our without becoming mired in the semantic formalism. techniques in developing even more accurate and Quantifiers and superlatives significantly compli- broader-coverage language understanding systems. cate scoping in lambda calculus, and often type rais- Acknowledgments We thank Luke Zettlemoyer ing needs to be employed.</S>\n', '    <S sid=""149"" ssid=""34"">Think of DCS as a higher-level Our system learns lexical associations between programming language tailored to natural language, words and predicates.</S>\n']","['Implication', 'Hypothesis', 'Implication', 'Result', 'Result']"
19,P11-1060,P12-1051,0,2011,0,"In fact, for any CFG G, it 1See Liang et al (2011) for work in representing lambda calculus expressions with trees","In fact, for any CFG G, it 1See Liang et al (2011) for work in representing lambda calculus expressions with trees","['163', '146', '142', '20', '132']","['    <S sid=""163"" ssid=""48"">Feedback from the context; for example, the lexical entry for borders world has been used to guide both syntactic parsing is S\\NP/NP : Ay.Ax.border(x, y), which means (Schuler, 2003) and semantic parsing (Popescu et borders looks right for the first argument and left al., 2003; Clarke et al., 2010).</S>\n', '    <S sid=""146"" ssid=""31"">We find that only for a small fraction of training examples do the K-best sets contain any trees yielding the correct answer (29% for DCS on GEO).</S>\n', '    <S sid=""142"" ssid=""27"">This option is not available to us since we do not have annotated logical forms, so we must instead rely on lexical triggers to define the search space.</S>\n', '    <S sid=""20"" ssid=""16"">At the same time, representations such as FunQL (Kate et al., 2005), which was used in Clarke et al. (2010), are simpler but lack the full expressive power of lambda calculus.</S>\n', '    <S sid=""132"" ssid=""17"">Results We first compare our system with Clarke et al. (2010) (henceforth, SEMRESP), which also learns a semantic parser from question-answer pairs.</S>\n']","['Result', 'Hypothesis', 'Hypothesis', 'Result', 'Result']"
