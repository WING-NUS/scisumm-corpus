README_corpusconstruction

About this file:   A readme detailing the rules and steps followed to create the document
  corpus by randomly sampling 10 documents from the ACL Anthology corpus
  and selecting their citing papers.

Details about how the CL Shared Task corpus was constructed.

This corpus contains 10 documents randomly sampled from the 
ACL Anthology corpus <URL to be inserted here>.

STEPS followed to create this corpus

1) From the current, live ACL Anthology, there are approx 26K (exactly
25961) individual papers that are in the Anthology (as of 18 Sep;
including ones staged for publication but not actually published yet).
These only include files that we have PDFs hosted (e.g., LREC is not
represented as we don't hold these PDFs in the Anthology, just their
metadata).  This number is approximate as there are some files that
are not publications (frontmatter, author indices) that are included.

2) Culling all files before and including 2006, we get 13.8K (13838)
publications, which include conference and journal articles.

3) We randomized this list to remove any ordering affects.

4) Starting from the top of the list, We used Google search on (18
September 2013) to search for the publication - first by using its
Anthology ID as a query (e.g.,"H89-2014.pdf") and not productive,
re-queried by the title of the paper as a string (E.g., "Some
Experiments with a Naive Bayes WSD System")

We look for a Scholar search result that shows # of citations.  This is
an approximation.  We kept any paper with over 10 citations. 
Some papers had some similar versions that presented
different citation rates (marked with a *); however, all of these were
dropped anyways due to low citation rate.

The file for this step is tab delimited, and is basically just the top
of Step 3.  This step culled about 40% of the candidates (We went
through 33 candidates to find 20 papers with 10+ citations for the
initial round; later due to attrition in Step 5, We had to examine an
additional 12 papers, resulting in a total of 42 candidates to find 20
papers = 47% drop rate)

5) We vet the citations from Google Scholar for the citation spread
being over 3 years as per citing papers' year of publication (as in
Google Scholar). We do not attempt to check for publication years that
Google Scholar doesn't report for some publications.  We check only the
earliest range manually to ensure that the citation is the correct
one, as there are usually many examples of later citations.

We also vetted that there were at least 10 of these citing sources
available for download (these are not necessarily ACL Anthology files,
and not necessary PDF; so if you need them to be available in PDF and
vetted as scientific publications or additionally in the ACL
Anthology, that is an upgraded requirement; please let me know).  Some
candidates from Step 5 were dropped due to the few amount of available
files that were freely downloadable from the Web.

6) The rows that are marked as "Keep" in Step 5 are your resulting
corpus.  Step 6 just lists these; no processing was done for this
final "step". 

7) We used the ACL Anthology Network (AAN, Feb 2013 version) from their
website (http://clair.eecs.umich.edu/aan/index.php) to do the
following step.  AAN is manually curated by Prof. Dragomir R. Radev's 
team at University of Michigan and lists
incoming citations for each paper for the cases where the incoming
citation is also within the AAN (i.e., citations to papers outside of
the CL/NLP community are not recorded).

E.g., for the first paper (H89-2014) Google / Google Scholar reported
34 incoming citations; AAN records 9.

For each of the accepted papers in Step 6, we ran a title search to
find the paper in AAN.  We inspect and list the citing papers
(incoming citations) Anthology ID, title and year in Step 7, where the
citing papers are given in reverse chronological order.

Note the citation count from Google / Google Scholar (done in Step 4;
also earlier in 18 September) and AAN (Feb 2013 release) will differ
substantially.  By counting the "sub" rows for each paper, you can see
there is a marked difference in the citation numbers.

For papers that are multiply listed in the Anthology (sometimes papers
are jointly listed in the case of joint conferences), we use the AAN
entry where incoming citations are recorded.  This happened for
paper where their original Anthology ID as listed in Step 6 is
different from the ones where I drew the citations from):

(original) X96-1048 => (citations) M95-1002

8) To report the final list of citing papers, we agreed to provide at
least 3 citing paper for each paper.  The criteria were (in order or
priority): 1) non-list citation (i.e., at least one citation for the
target paper not of the form [X,a,b,c]); 2) the oldest and newest
citations within AAN; and, 3) citations from different years.  

We thus provide the oldest and newest citation regardless of criteria
1) and 3) and include a randomized sample of up to 8 additional citing
paper IDs that meet either criteria 1) and 3).  To do this, we start
by first randomizing the list of citing papers, detailed in the text
file for Step 8, and enumerating up to 8 additional citing papers.  At
this point, the citing papers are listed as either "old(est)",
"new(est)", 1-8 (additional citing papers), or "sub" (substitute
backup citing paper in case of disqualification of one of the 1-8
additional papers due to criterion 1).

9) We comb through the lists of the additional 1-8 citing papers per
target paper, from the top to the bottom of the randomized list in
Step 8.  We unilaterally collect the top most oldest and top most
newest paper (this is needed in case of ties).  For the remaining (up
to) 8 papers, we examine the citing paper's PDF file to ensure that at
least one citation made is of a single citation format (e.g., [X] and
not [X,a,b,c]).  Any invalidated files are marked with "list"
(citation) mark.

10) The resulting final list is in Step 10.  The constructed zip file
corresponds to the files in Step 10.

 Replicated below for your easy reference:
--------------------------------------
ACL-anthology-id	Tile of the paper
--------------------------------------
H89-2014 Augmenting a Hidden Markov Model for Phrase-Dependent Word Tagging
X96-1048 OVERVIEW OF RESULTS OF THE MUC-6 EVALUATION 
C94-2154 THE CORRECT AND EFFICIENT IMPLEMENTATION OF APPROPRIATENESS SPECIFICIATIONS FOR TYPED FEATURE STRUCTURES
E03-1020 Discovering Corpus-Specific Word Senses
C90-2039 Strategic Lazy Incremental Copy Graph Unification
J00-3003 Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech
P98-1081 Improving Data Driven Wordclass Tagging by System Combination
N01-1011 A Decision Tree of Bigrams is an Accurate Predictor of Word Sense
H05-1115 Using Random Walks for Question-focused Sentence Retrieval
J98-2005 Estimation of Probabilistic Context-Free Grammars

-------------------------------------------------------------------------
Contact Information

For further information about this data release, contact the following
members of the BiomedSumm Organizing Committee:

  Min-Yen Kan (National University of Singapore)	<kanmy@gmail.com>
  Kokil Jaidka (Nanyang Technological University)	<koki0001@e.ntu.edu.sg>
  Muthu Kumar Chandrasekaran (National University of Singapore)	<muthu.chandra@comp.nus.edu.sg>
  Ankur Khanna (National University of Singapore)
  
  
--------------------------------------------------------------------------
README created by Min-Yen Kan on Aug 25, 2014

